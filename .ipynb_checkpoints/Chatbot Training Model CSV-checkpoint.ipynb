{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "# things we need for NLP\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "from nltk.corpus import wordnet\n",
    "syns = wordnet.synsets(\"program\")\n",
    "\n",
    "# things we need for Tensorflow\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.response\n",
    "import sys\n",
    "import os, glob\n",
    "import http.client, urllib\n",
    "import json\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
    "\n",
    "# Replace the accessKey string value with your valid access key.\n",
    "accessKey = '82e4612615634c398bfd26e7d6327833'\n",
    "url = 'westcentralus.api.cognitive.microsoft.com'\n",
    "path = '/text/analytics/v2.0/keyPhrases'\n",
    "\n",
    "def extract_keywords(body):\n",
    "    headers = {'Ocp-Apim-Subscription-Key': accessKey}\n",
    "    conn = http.client.HTTPSConnection(url)\n",
    "    body_req = {'documents':[{'language': 'en', 'id': 1, 'text': body}]}\n",
    "    body_json = json.dumps(body_req)\n",
    "    conn.request (\"POST\", path, body_json, headers)\n",
    "    response = conn.getresponse ()\n",
    "    string = response.read().decode('utf-8')\n",
    "    json_obj = json.loads(string)\n",
    "    print(\"json_obj: \", json_obj)\n",
    "    keyphrases_list = ((json_obj['documents'])[0])['keyPhrases']\n",
    "    print(\"keyphrases_list: \", keyphrases_list)\n",
    "    return keyphrases_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 total classes in training data\n",
      "Common Cold | Upper Respiratory Tract;Nose and Throat Infection | Hi, I have a runny nose all the time and I don't know what to do about it. I was sniffing all the time in the past few days and it was getting very annoying;One of my nausal was blocked and I couldn't breath properly;My throat hurts after a cold shower last night, I am not sure what to do about it;It feels like there was something in my throat and I've been dry coughing for several days already;I recently developed a condition that requires frequent washroom visits througout the day;I am so tired this morning, every single one of my muscle hurts so much;All my joints are sour since last morning;I feel so cold all the time, I sneezed all the time | Stay hydrated;rest;sooth a sore throat using saltwater gargle;take over the counter cold and cough medications\n",
      " | 1\n",
      "Allergies | Allergic Reaction | After the soccer game yesterday, I found small bumps on my skin and extreme itchiness;When I was playing with my friends today, it felt like I had something in my eyes and i was crying uncontrollably;My nose were going off in the park the other day, and I kept sniffing the entire time;Sometimes I could not breath through one of my nose and it feels like it was blocked;When I was walking in the park this morning, I kept sneezing around the flowers even though I am not cold;In the zoo today, it felt like I have a feather or something in my throat the entire day, it was really itchy and I coughed the entire day;I picked a flower today and the part of my hand that touched the flower became swollen, I can barely bend it and it feels numb | Remove the cause of allergy; \n",
      "otherwise take Antihistamines to relieve sneezing;Decongestants to relieve congestions in nasal membranes;Anti inflamary agents to reduce;Allergy shots | 1\n",
      "Conjunctivitis | Pink Eye | I stayed up for a full night grinding Hack the North project and the next morning my eye was bright red. All the blood vessels in my blood were visible;My eyes were really itchy this morning when I woke up and it was pink when I saw it through the mirror;After swimming in the community swimming pool this week, I felt a burning sensation in my eye and i cried uncontrollably. | See a doctor;Bacterial cases can be treated with antibiotic eye drops;Allergic reactions can be treated with other eye dro | 1\n",
      "E Coli Infection |  | I went to a dirty restaurant a few days ago, and I began to feel dizzy this morning initially, later I puked everything out; After eating some street food, I started to feel dizzy, weak and tired all the time, furthermore I began to catch a fever halfway through the week too;I went to the washroom alot more frequently recently and I saw blood in my toilet, I think I might be pooping blood; My stomach hurts after a meal of funny tasting fish in the school cafeteria, it hurts so much that I can barely standup straight. I also vomitted after the meal and constantly tired | Visit a hospital immediately | 1\n",
      "Chlamydia | Sexually Transmitted Infection (STI) | I saw some yellow-green discharge, light bleeding between my periods and an occasional burning sensation while urinating. Are those symptoms of chlamydia or a sexually transmitted infection;Hi, I recently experience  a burning sensation while peeing, frequent urination or a yellow-green discharge, sometimes, even difficulty urinating;I recently have trouble going to the washroom because my genital areas hurt during urination and they sometimes swell up randomly. | See a professional doctor;Use Azithromycin or Doxycycline | 4\n",
      "Hepatitis B | Herpes | Hello, I had this burning sensationâ€”I thought it was cystitis. I went to the doctor and he gave me some antibiotics. But it didn't go away. Then, a few days later, I got some blisters on my vagina, all along my lips, what is that blister;I think I got some skin irritation, razor burn or just some random itch, but they repeat once every a few month,what are those? | See a doctor immediately;Take antiviral drugs | 6\n",
      "West Nile Virus |  | I visited the jungle a few days ago and I got a strange bite because it didn't go away. A few days later, I began to develop a fever and muscle weakness, what should I do?;I discovered a weird mosquito bite on my hand that does not go away, but instead it swell and my hand is numb; My roommate has a weird bite on his forehead, He caught a fever and was confused about his surroundings. He experiences vision loss and fall into occassional comas. What should I do about him?;I have a minor headache, vomiting and I saw red rashes all over my back, chest and stomach | Please see a healthcare professional;There are no specific treatments for West Nile Virus in humans | 8\n",
      "Chickenpox | Varicella Zoster Virus | hey there. i have been feeling extremely bad lately, with a fever developing the past few days;i have been getting lot of blisters and rashes on my body recently. my mouth has also been very sore in a lot of places and there has been itchiness all over my body;i went to my friend's house last night and ended up developing a fever and red spots al over my body. i couldn't even eat anything because i had no appetite, and started getting itchy all over my body;my mouth has been feeling bad lately, and i never feel like eating. my muscles have also been more sore lately even though i have not been to the gym, and my skin has become more itchy recently;blisters and rashes have appeared all over my body recently, and they have all started to itch. my arms and hamstrings are also starting to ache and i believe i may be developing a fever because my temperature is high from the last measurement;red spots are appealing all over my arm, and they are feeling quite uncomfortable. just this morning, i ended up developing a fever and my mouth has felt very sore and uncomfortable;my friend gabrielle and i went to the the movies and i ended up developing rashes all over my body which turned into itchy spots. later that night, i also measured my temperature and had developed a fever. this morning, i cannot even move well because my muscles are aching greatly. | Drink plenty of fluid;Take Tylenol;Consult with a doctor | 2\n",
      "Alzheimer |  | I...can't remember;People say I've gotten angrier, and sometimes I do feel angry, although I don't know why;I couldn't remember her face...she was important to me but I could not, for the life of me, remember her face;Where did I leave my bank notes? It was just yesterday when I retrieved them;What did you say? Can you speak louder?;my muscles are also hurting and ache quite a lot. it's also weird because my mother was home last weekend and we prepared a big dinner, but i had no appetite;completing tasks and doing things has been very difficult lately, as i seem to not remember things i did in the past. doing things i was faimiliar with has been confusing and felt out of place, and it appears my spatial and visual senses are also diminishing | Cholinesterase inhibitors;Memantine(Namenda);Please consult with a professional doctor | 4\n",
      "Pregnancy |  | My back hurts so much just from walking around and my ankles are swollen too;Don't talk to me, just go away and leave me alone;My period is late by three days...it's almost never late;My side was cramping so hard, it felt like someone was shredding me inside out | You are pregnant! Remember to take your prenatal vitamin;quit smoking;stop drinking alcohol;cut down on caffeine; avoid hazardous foods;eat well and sleep well! | 2\n",
      "Eating Disorders including Anorexia & Bulimia Nervosa |  | No matter how little I eat, I get fatter;food is the enemy and I refuse to surrender;How many calories are in that sandwiche? I'll just take an apple instead;I feel so cold, even when I wear an extra coat;I purged myself clean today once more, just like I always have, except there was a burning sensation down my throat. I guess if that's the price I have to pay | Seek help from a therapist, physician, and nutritionist | 3\n",
      "Coronary Heart Disease |  | Recently my chest is feeling pretty tight and even numb at times. I have become light headed and dizzy for most of the day rendering me sad and useless. I also frequently vomit and expierience heartburn which is hella painful. Im really worried that I might have a serious disease and my indigestion is not helping. | quit smoking;have a healthy diet;exercise regularly;consult with a doctor | 7\n",
      "Breast Cancer |  | There are a bunch of red spots on my chest and I don't think I've contacted any allergens...why won't they go away;there are small bits of milky white liquid coming out of my nipples...should I be concerned?;Parts of my breast felt a bit hard...almost lumpy. Would this go away after puberty?;It just occurred to me today that my nipples look...different. Are they supposed to be sunken in? | You may have breast cancer, please consult with a doctor and conduct further diagnostics at your local hospital | 10\n",
      "Prostate Cancer |  | I've been feeling like I needed to go to the washroom more and more often these days;I'm beginning to dread my washroom trips these days because it hurts to urinate;When I went to piss this morning, there was blood and my piss was redish;What's wrong with me? Having sex with my girlfriend yesterday had hurt so much | You may have prostate cancer, please consult with a doctor and conduct further diagnostics at your local hospital | 10\n"
     ]
    }
   ],
   "source": [
    "# medical conditions training data\n",
    "training_data = []\n",
    "\n",
    "import csv\n",
    "\n",
    "# adds every medical condition along with associated info from csv file\n",
    "with open('Disease Database CSV NEW.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count != 0:\n",
    "            training_data.append({\"class\":row[1], \"synonyms\":row[2], \n",
    "                                  \"sentence\":row[3], \"treatments\":row[4], \"danger level\":row[5]})\n",
    "        line_count += 1\n",
    "\n",
    "print (\"%s total classes in training data\" % len(training_data))\n",
    "\n",
    "for term in training_data:\n",
    "    print(term[\"class\"] + \" | \" + term[\"synonyms\"] + \" | \" + term[\"sentence\"] + \n",
    "          \" | \" + term[\"treatments\"] + \" | \" + term[\"danger level\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a table structure to hold the different punctuation used\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                    if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "# method to remove punctuations from sentences.\n",
    "def remove_punctuation(text):\n",
    "     return text.translate(tbl)\n",
    "\n",
    "# method to slice strings\n",
    "def slicer(my_str,sub):\n",
    "    index=my_str.find(sub)\n",
    "    if index !=-1 :\n",
    "         return my_str[index:] \n",
    "    else :\n",
    "         raise Exception('Sub string not found!')\n",
    "\n",
    "# # find synonyms of all the words in a sentence\n",
    "# def find_syns(sentence):\n",
    "#     words = []\n",
    "#     for word in sentence.split():\n",
    "#         if word != '':\n",
    "#             synonyms = []\n",
    "#             for syn in wordnet.synsets(word):\n",
    "#                 for l in syn.lemmas():\n",
    "#                     if l.name() not in synonyms and l.name() != word:\n",
    "#                         synonyms.append(l.name())\n",
    "#             word = (word + ' ' + ' '.join(synonyms)).strip()\n",
    "#             words.append(word)\n",
    "#     print(\"SYNONYMS:\",words)\n",
    "#     return words\n",
    "\n",
    "# # clean all synonyms and words\n",
    "# def clean_syns(arg):\n",
    "#     clean_synonyms = []\n",
    "#     for syn_set in synonyms:\n",
    "#         syn_set = ' '.join(s for s in syn_set.split() if not any(c.isdigit() for c in s))\n",
    "#         syn_set = re.sub(r'\\d+', '', syn_set)\n",
    "#         syn_set = syn_set.replace(\"_\",\" \")\n",
    "#         syn_set = remove_punctuation(syn_set)\n",
    "#         clean_synonyms.append(syn_set)\n",
    "#     print(\"CLEAN SYNONYMS: \",clean_synonyms)\n",
    "#     return clean_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD_LIST: [\"Hi, I have a runny nose all the time and I don't know what to do about it. I was sniffing all the time in the past few days and it was getting very annoying\", \"One of my nausal was blocked and I couldn't breath properly\", 'My throat hurts after a cold shower last night, I am not sure what to do about it', \"It feels like there was something in my throat and I've been dry coughing for several days already\", 'I recently developed a condition that requires frequent washroom visits througout the day', 'I am so tired this morning, every single one of my muscle hurts so much', 'All my joints are sour since last morning', 'I feel so cold all the time, I sneezed all the time']\n",
      "WORD_LIST_NEW: ['hi i have runny nose all time i dont know what do about it i sniffing all time the past few days it getting very annoying', 'one my nausal blocked i couldnt breath properly', 'my throat hurts after cold shower last night i not sure what do about it', 'it feels like there something my throat ive been dry coughing for several days already', 'i recently developed condition that requires frequent washroom visits througout day', 'i so tired this morning every single one my muscle hurts so much', 'all my joints sour since last morning', 'i feel so cold all time i sneezed all time']\n",
      "joined_symps: hi i have runny nose all time i dont know what do about it i sniffing all time the past few days it getting very annoying one my nausal blocked i couldnt breath properly my throat hurts after cold shower last night i not sure what do about it it feels like there something my throat ive been dry coughing for several days already i recently developed condition that requires frequent washroom visits througout day i so tired this morning every single one my muscle hurts so much all my joints sour since last morning i feel so cold all time i sneezed all time\n",
      "SYNONYMS: [\"hi hello hullo howdy how-do-you-do Hawaii Hawai'i Aloha_State HI\", 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'have rich_person wealthy_person have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'runny fluid', 'nose olfactory_organ nozzle intrude horn_in pry poke scent wind nuzzle', 'all wholly entirely completely totally altogether whole', 'time clip clock_time fourth_dimension meter metre prison_term sentence clock', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'dont', 'know cognize cognise experience live acknowledge recognize recognise sleep_together roll_in_the_hay love make_out make_love sleep_with get_laid have_sex do_it be_intimate have_intercourse have_it_away have_it_off screw fuck jazz eff hump lie_with bed have_a_go_at_it bang get_it_on bonk', 'what', 'do bash brawl doh ut Doctor_of_Osteopathy DO make perform execute fare make_out come get_along cause practice practise exercise suffice answer serve act behave manage dress arrange set coif coiffe coiffure', 'about astir approximately close_to just_about some roughly more_or_less around or_so almost most nearly near nigh virtually well-nigh', 'it information_technology IT', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'sniffing sniff whiff sniffle', 'all wholly entirely completely totally altogether whole', 'time clip clock_time fourth_dimension meter metre prison_term sentence clock', 'the', 'past past_times yesteryear past_tense preceding retiring by', 'few', 'days years day twenty-four_hours twenty-four_hour_period 24-hour_interval solar_day mean_solar_day daytime daylight sidereal_day Day Clarence_Day Clarence_Shepard_Day_Jr.', 'it information_technology IT', \"getting acquiring get acquire become go let have receive find obtain incur arrive come bring convey fetch experience pay_back pay_off fix make induce stimulate cause catch capture grow develop produce contract take drive aim arrest scram buzz_off fuck_off bugger_off get_under_one's_skin draw perplex vex stick puzzle mystify baffle beat pose bewilder flummox stupefy nonplus gravel amaze dumbfound get_down begin start_out start set_about set_out commence suffer sustain beget engender father mother sire generate bring_forth\", 'very identical selfsame really real rattling', 'annoying annoyance irritation vexation annoy rag get_to bother get_at irritate rile nark nettle gravel vex chafe devil bothersome galling irritating nettlesome pesky pestering pestiferous plaguy plaguey teasing vexatious vexing', 'one 1 I ace single unity i ane unitary matchless nonpareil one_and_only peerless unmatched unmatchable unrivaled unrivalled', 'my', 'nausal', 'blocked barricade block blockade stop block_off block_up bar obstruct hinder stymie stymy embarrass halt kibosh jam stuff lug choke_up obturate impede occlude close_up parry deflect forget blank_out draw_a_blank freeze immobilize immobilise out_of_use plugged', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'couldnt', 'breath breather breathing_place breathing_space breathing_spell breathing_time hint intimation', 'properly decently decent in_good_order right the_right_way by_rights', 'my', 'throat pharynx', 'hurts injury hurt harm trauma distress suffering detriment damage scathe ache smart pain anguish injure wound bruise offend spite suffer', 'after subsequently later afterwards afterward later_on', 'cold common_cold coldness low_temperature frigidity frigidness stale dusty moth-eaten frigid cold-blooded inhuman insensate', 'shower shower_bath rain_shower cascade exhibitor exhibitioner lavish shower_down', \"last stopping_point finale finis finish conclusion close death end final_stage shoemaker's_last cobbler's_last endure survive live live_on go hold_up hold_out concluding final terminal net utmost last-place lowest lastly in_conclusion finally\", 'night nighttime dark Nox Night', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'not non', 'sure certain trusted indisputable surely certainly for_sure for_certain sure_enough sure_as_shooting', 'what', 'do bash brawl doh ut Doctor_of_Osteopathy DO make perform execute fare make_out come get_along cause practice practise exercise suffice answer serve act behave manage dress arrange set coif coiffe coiffure', 'about astir approximately close_to just_about some roughly more_or_less around or_so almost most nearly near nigh virtually well-nigh', 'it information_technology IT', 'it information_technology IT', 'feels feel spirit tone feeling flavor flavour look smell tactile_property experience find sense finger palpate', 'like the_like the_likes_of ilk wish care similar same alike comparable corresponding', 'there at_that_place in_that_location in_that_respect on_that_point thither', 'something', 'my', 'throat pharynx', 'ive', 'been be exist equal constitute represent make_up comprise follow embody personify live cost', 'dry prohibitionist dry_out ironic ironical wry juiceless teetotal', 'coughing cough', 'for', 'several respective various', 'days years day twenty-four_hours twenty-four_hour_period 24-hour_interval solar_day mean_solar_day daytime daylight sidereal_day Day Clarence_Day Clarence_Shepard_Day_Jr.', 'already', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'recently late lately of_late latterly', 'developed develop evolve germinate acquire grow produce get originate arise rise uprise spring_up build_up explicate formulate train prepare educate modernize modernise make_grow break recrudesce highly-developed', 'condition status precondition stipulation circumstance consideration shape term experimental_condition discipline train check stipulate qualify specify', 'that', 'requires necessitate ask postulate need require take involve call_for demand expect command want', 'frequent patronize patronise shop shop_at buy_at sponsor haunt', 'washroom', 'visits visit sojourn see travel_to call_in call inspect inflict bring_down impose chew_the_fat shoot_the_breeze chat confabulate confab chitchat chit-chat chatter chaffer natter gossip jaw claver', 'througout', 'day twenty-four_hours twenty-four_hour_period 24-hour_interval solar_day mean_solar_day daytime daylight sidereal_day Day Clarence_Day Clarence_Shepard_Day_Jr.', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'so sol soh thus thusly then and_so and_then therefore hence thence indeed', 'tired tire pall weary fatigue jade wear_upon tire_out wear wear_out outwear wear_down fag_out fag run_down exhaust play_out sap bore banal commonplace hackneyed old-hat shopworn stock threadbare timeworn trite well-worn', 'this', 'morning morn morning_time forenoon good_morning dawn dawning aurora first_light daybreak break_of_day break_of_the_day dayspring sunrise sunup cockcrow', 'every', 'single bingle one 1 I ace unity individual unmarried undivided exclusive', 'one 1 I ace single unity i ane unitary matchless nonpareil one_and_only peerless unmatched unmatchable unrivaled unrivalled', 'my', 'muscle musculus muscular_tissue muscleman brawn brawniness muscularity sinew heftiness', 'hurts injury hurt harm trauma distress suffering detriment damage scathe ache smart pain anguish injure wound bruise offend spite suffer', 'so sol soh thus thusly then and_so and_then therefore hence thence indeed', 'much a_lot lots a_good_deal a_great_deal very_much practically often', 'all wholly entirely completely totally altogether whole', 'my', 'joints joint articulation articulatio join juncture junction roast marijuana_cigarette reefer stick spliff articulate', 'sour sourness tartness acidity turn ferment work acidify acidulate acetify rancid off turned false off-key dark dour glowering glum moody morose saturnine sullen', 'since', \"last stopping_point finale finis finish conclusion close death end final_stage shoemaker's_last cobbler's_last endure survive live live_on go hold_up hold_out concluding final terminal net utmost last-place lowest lastly in_conclusion finally\", 'morning morn morning_time forenoon good_morning dawn dawning aurora first_light daybreak break_of_day break_of_the_day dayspring sunrise sunup cockcrow', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'feel spirit tone feeling flavor flavour look smell tactile_property experience find sense finger palpate', 'so sol soh thus thusly then and_so and_then therefore hence thence indeed', 'cold common_cold coldness low_temperature frigidity frigidness stale dusty moth-eaten frigid cold-blooded inhuman insensate', 'all wholly entirely completely totally altogether whole', 'time clip clock_time fourth_dimension meter metre prison_term sentence clock', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'sneezed sneeze', 'all wholly entirely completely totally altogether whole', 'time clip clock_time fourth_dimension meter metre prison_term sentence clock']\n",
      "CLEAN SYNONYMS:  ['hi hello hullo howdy howdoyoudo Hawaii Hawaii Aloha State HI', 'i iodine iodin I one ace single unity ane', 'have rich person wealthy person have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'runny fluid', 'nose olfactory organ nozzle intrude horn in pry poke scent wind nuzzle', 'all wholly entirely completely totally altogether whole', 'time clip clock time fourth dimension meter metre prison term sentence clock', 'i iodine iodin I one ace single unity ane', 'dont', 'know cognize cognise experience live acknowledge recognize recognise sleep together roll in the hay love make out make love sleep with get laid have sex do it be intimate have intercourse have it away have it off screw fuck jazz eff hump lie with bed have a go at it bang get it on bonk', 'what', 'do bash brawl doh ut Doctor of Osteopathy DO make perform execute fare make out come get along cause practice practise exercise suffice answer serve act behave manage dress arrange set coif coiffe coiffure', 'about astir approximately close to just about some roughly more or less around or so almost most nearly near nigh virtually wellnigh', 'it information technology IT', 'i iodine iodin I one ace single unity ane', 'sniffing sniff whiff sniffle', 'all wholly entirely completely totally altogether whole', 'time clip clock time fourth dimension meter metre prison term sentence clock', 'the', 'past past times yesteryear past tense preceding retiring by', 'few', 'days years day twentyfour hours twentyfour hour period solar day mean solar day daytime daylight sidereal day Day Clarence Day Clarence Shepard Day Jr', 'it information technology IT', 'getting acquiring get acquire become go let have receive find obtain incur arrive come bring convey fetch experience pay back pay off fix make induce stimulate cause catch capture grow develop produce contract take drive aim arrest scram buzz off fuck off bugger off get under ones skin draw perplex vex stick puzzle mystify baffle beat pose bewilder flummox stupefy nonplus gravel amaze dumbfound get down begin start out start set about set out commence suffer sustain beget engender father mother sire generate bring forth', 'very identical selfsame really real rattling', 'annoying annoyance irritation vexation annoy rag get to bother get at irritate rile nark nettle gravel vex chafe devil bothersome galling irritating nettlesome pesky pestering pestiferous plaguy plaguey teasing vexatious vexing', 'one I ace single unity i ane unitary matchless nonpareil one and only peerless unmatched unmatchable unrivaled unrivalled', 'my', 'nausal', 'blocked barricade block blockade stop block off block up bar obstruct hinder stymie stymy embarrass halt kibosh jam stuff lug choke up obturate impede occlude close up parry deflect forget blank out draw a blank freeze immobilize immobilise out of use plugged', 'i iodine iodin I one ace single unity ane', 'couldnt', 'breath breather breathing place breathing space breathing spell breathing time hint intimation', 'properly decently decent in good order right the right way by rights', 'my', 'throat pharynx', 'hurts injury hurt harm trauma distress suffering detriment damage scathe ache smart pain anguish injure wound bruise offend spite suffer', 'after subsequently later afterwards afterward later on', 'cold common cold coldness low temperature frigidity frigidness stale dusty motheaten frigid coldblooded inhuman insensate', 'shower shower bath rain shower cascade exhibitor exhibitioner lavish shower down', 'last stopping point finale finis finish conclusion close death end final stage shoemakers last cobblers last endure survive live live on go hold up hold out concluding final terminal net utmost lastplace lowest lastly in conclusion finally', 'night nighttime dark Nox Night', 'i iodine iodin I one ace single unity ane', 'not non', 'sure certain trusted indisputable surely certainly for sure for certain sure enough sure as shooting', 'what', 'do bash brawl doh ut Doctor of Osteopathy DO make perform execute fare make out come get along cause practice practise exercise suffice answer serve act behave manage dress arrange set coif coiffe coiffure', 'about astir approximately close to just about some roughly more or less around or so almost most nearly near nigh virtually wellnigh', 'it information technology IT', 'it information technology IT', 'feels feel spirit tone feeling flavor flavour look smell tactile property experience find sense finger palpate', 'like the like the likes of ilk wish care similar same alike comparable corresponding', 'there at that place in that location in that respect on that point thither', 'something', 'my', 'throat pharynx', 'ive', 'been be exist equal constitute represent make up comprise follow embody personify live cost', 'dry prohibitionist dry out ironic ironical wry juiceless teetotal', 'coughing cough', 'for', 'several respective various', 'days years day twentyfour hours twentyfour hour period solar day mean solar day daytime daylight sidereal day Day Clarence Day Clarence Shepard Day Jr', 'already', 'i iodine iodin I one ace single unity ane', 'recently late lately of late latterly', 'developed develop evolve germinate acquire grow produce get originate arise rise uprise spring up build up explicate formulate train prepare educate modernize modernise make grow break recrudesce highlydeveloped', 'condition status precondition stipulation circumstance consideration shape term experimental condition discipline train check stipulate qualify specify', 'that', 'requires necessitate ask postulate need require take involve call for demand expect command want', 'frequent patronize patronise shop shop at buy at sponsor haunt', 'washroom', 'visits visit sojourn see travel to call in call inspect inflict bring down impose chew the fat shoot the breeze chat confabulate confab chitchat chitchat chatter chaffer natter gossip jaw claver', 'througout', 'day twentyfour hours twentyfour hour period solar day mean solar day daytime daylight sidereal day Day Clarence Day Clarence Shepard Day Jr', 'i iodine iodin I one ace single unity ane', 'so sol soh thus thusly then and so and then therefore hence thence indeed', 'tired tire pall weary fatigue jade wear upon tire out wear wear out outwear wear down fag out fag run down exhaust play out sap bore banal commonplace hackneyed oldhat shopworn stock threadbare timeworn trite wellworn', 'this', 'morning morn morning time forenoon good morning dawn dawning aurora first light daybreak break of day break of the day dayspring sunrise sunup cockcrow', 'every', 'single bingle one I ace unity individual unmarried undivided exclusive', 'one I ace single unity i ane unitary matchless nonpareil one and only peerless unmatched unmatchable unrivaled unrivalled', 'my', 'muscle musculus muscular tissue muscleman brawn brawniness muscularity sinew heftiness', 'hurts injury hurt harm trauma distress suffering detriment damage scathe ache smart pain anguish injure wound bruise offend spite suffer', 'so sol soh thus thusly then and so and then therefore hence thence indeed', 'much a lot lots a good deal a great deal very much practically often', 'all wholly entirely completely totally altogether whole', 'my', 'joints joint articulation articulatio join juncture junction roast marijuana cigarette reefer stick spliff articulate', 'sour sourness tartness acidity turn ferment work acidify acidulate acetify rancid off turned false offkey dark dour glowering glum moody morose saturnine sullen', 'since', 'last stopping point finale finis finish conclusion close death end final stage shoemakers last cobblers last endure survive live live on go hold up hold out concluding final terminal net utmost lastplace lowest lastly in conclusion finally', 'morning morn morning time forenoon good morning dawn dawning aurora first light daybreak break of day break of the day dayspring sunrise sunup cockcrow', 'i iodine iodin I one ace single unity ane', 'feel spirit tone feeling flavor flavour look smell tactile property experience find sense finger palpate', 'so sol soh thus thusly then and so and then therefore hence thence indeed', 'cold common cold coldness low temperature frigidity frigidness stale dusty motheaten frigid coldblooded inhuman insensate', 'all wholly entirely completely totally altogether whole', 'time clip clock time fourth dimension meter metre prison term sentence clock', 'i iodine iodin I one ace single unity ane', 'sneezed sneeze', 'all wholly entirely completely totally altogether whole', 'time clip clock time fourth dimension meter metre prison term sentence clock']\n",
      "WORD_LIST: ['After the soccer game yesterday, I found small bumps on my skin and extreme itchiness', 'When I was playing with my friends today, it felt like I had something in my eyes and i was crying uncontrollably', 'My nose were going off in the park the other day, and I kept sniffing the entire time', 'Sometimes I could not breath through one of my nose and it feels like it was blocked', 'When I was walking in the park this morning, I kept sneezing around the flowers even though I am not cold', 'In the zoo today, it felt like I have a feather or something in my throat the entire day, it was really itchy and I coughed the entire day', 'I picked a flower today and the part of my hand that touched the flower became swollen, I can barely bend it and it feels numb']\n",
      "WORD_LIST_NEW: ['after soccer game yesterday i found small bumps my skin extreme itchiness', 'when i playing with my friends today it felt like i had something my eyes i crying uncontrollably', 'my nose going in park other day i kept sniffing entire time', 'sometimes i could not breath through one my nose it feels like it blocked', 'when i walking park this morning i kept sneezing around the flowers even though i not cold', 'zoo today it felt like i have feather something my throat entire day it really itchy i coughed the entire day', 'i picked flower today part my hand that touched the flower became swollen i barely bend it it feels numb']\n",
      "joined_symps: after soccer game yesterday i found small bumps my skin extreme itchiness when i playing with my friends today it felt like i had something my eyes i crying uncontrollably my nose going in park other day i kept sniffing entire time sometimes i could not breath through one my nose it feels like it blocked when i walking park this morning i kept sneezing around the flowers even though i not cold zoo today it felt like i have feather something my throat entire day it really itchy i coughed the entire day i picked flower today part my hand that touched the flower became swollen i barely bend it it feels numb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYNONYMS: ['after subsequently later afterwards afterward later_on', 'soccer association_football', 'game plot secret_plan biz bet_on back gage stake punt crippled halt halting lame gimpy gamy gamey gritty mettlesome spirited spunky', 'yesterday', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'found establish set_up launch plant constitute institute base ground find happen chance bump encounter detect observe discover notice regain determine find_out ascertain feel witness see line_up get_hold come_up rule receive get obtain incur recover retrieve find_oneself', 'small little minor modest small-scale pocket-size pocket-sized humble low lowly minuscule belittled diminished', 'bumps bump bulge hump swelling gibbosity gibbousness jut prominence protuberance protrusion extrusion excrescence blow knock find happen chance encounter demote relegate break kick_downstairs dislodge', 'my', 'skin tegument cutis hide pelt peel clamber scramble shin shinny struggle sputter scrape bark pare', 'extreme extreme_point extremum utmost uttermost', 'itchiness itch itching', 'when', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'playing acting playacting performing play act represent spiel act_as recreate toy run fiddle diddle dally trifle flirt roleplay playact bring work wreak make_for bet wager meet encounter take_on', 'with', 'my', 'friends friend ally acquaintance supporter protagonist champion admirer booster Friend Quaker', 'today nowadays now', 'it information_technology IT', 'felt felt_up mat_up matt-up matte_up matte mat feel experience find sense finger palpate', 'like the_like the_likes_of ilk wish care similar same alike comparable corresponding', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'had have have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'something', 'my', 'eyes eye oculus optic center centre middle heart eyeball', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'crying weeping tears cry outcry call yell shout vociferation war_cry rallying_cry battle_cry watchword shout_out scream holler hollo squall weep exclaim cry_out call_out blazon_out clamant exigent insistent instant egregious flagrant glaring gross rank', 'uncontrollably', 'my', 'nose olfactory_organ nozzle intrude horn_in pry poke scent wind nuzzle', \"going departure going_away leaving passing loss exit expiration release sledding travel go move locomote proceed go_away depart become get run pass lead extend sound function work operate run_low run_short survive last live live_on endure hold_up hold_out die decease perish pass_away expire kick_the_bucket cash_in_one's_chips buy_the_farm conk give-up_the_ghost drop_dead pop_off choke croak snuff_it belong start get_going blend blend_in fit rifle plump fail go_bad give_way give_out conk_out break break_down\", 'in inch indium In atomic_number_49 Indiana Hoosier_State IN inwards inward', 'park parkland commons common green ballpark Park Mungo_Park parking_lot car_park parking_area', 'other early former', 'day twenty-four_hours twenty-four_hour_period 24-hour_interval solar_day mean_solar_day daytime daylight sidereal_day Day Clarence_Day Clarence_Shepard_Day_Jr.', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'kept keep maintain hold continue go_on proceed go_along hold_on prevent observe retain keep_on sustain stay_fresh celebrate restrain keep_back hold_back preserve keep_open hold_open save unbroken', 'sniffing sniff whiff sniffle', 'entire stallion full total integral intact', 'time clip clock_time fourth_dimension meter metre prison_term sentence clock', 'sometimes', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'could', 'not non', 'breath breather breathing_place breathing_space breathing_spell breathing_time hint intimation', 'through done through_with through_and_through', 'one 1 I ace single unity i ane unitary matchless nonpareil one_and_only peerless unmatched unmatchable unrivaled unrivalled', 'my', 'nose olfactory_organ nozzle intrude horn_in pry poke scent wind nuzzle', 'it information_technology IT', 'feels feel spirit tone feeling flavor flavour look smell tactile_property experience find sense finger palpate', 'like the_like the_likes_of ilk wish care similar same alike comparable corresponding', 'it information_technology IT', 'blocked barricade block blockade stop block_off block_up bar obstruct hinder stymie stymy embarrass halt kibosh jam stuff lug choke_up obturate impede occlude close_up parry deflect forget blank_out draw_a_blank freeze immobilize immobilise out_of_use plugged', 'when', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'walking walk take_the_air walk-to', 'park parkland commons common green ballpark Park Mungo_Park parking_lot car_park parking_area', 'this', 'morning morn morning_time forenoon good_morning dawn dawning aurora first_light daybreak break_of_day break_of_the_day dayspring sunrise sunup cockcrow', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'kept keep maintain hold continue go_on proceed go_along hold_on prevent observe retain keep_on sustain stay_fresh celebrate restrain keep_back hold_back preserve keep_open hold_open save unbroken', 'sneezing sneeze sternutation', 'around about approximately close_to just_about some roughly more_or_less or_so round', 'the', 'flowers flower bloom blossom prime peak heyday efflorescence flush', 'even evening eve eventide flush level even_out fifty-fifty regular tied yet still', 'though', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'not non', 'cold common_cold coldness low_temperature frigidity frigidness stale dusty moth-eaten frigid cold-blooded inhuman insensate', 'zoo menagerie zoological_garden', 'today nowadays now', 'it information_technology IT', 'felt felt_up mat_up matt-up matte_up matte mat feel experience find sense finger palpate', 'like the_like the_likes_of ilk wish care similar same alike comparable corresponding', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'have rich_person wealthy_person have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'feather plume plumage feathering square fledge', 'something', 'my', 'throat pharynx', 'entire stallion full total integral intact', 'day twenty-four_hours twenty-four_hour_period 24-hour_interval solar_day mean_solar_day daytime daylight sidereal_day Day Clarence_Day Clarence_Shepard_Day_Jr.', 'it information_technology IT', 'really truly genuinely actually in_truth very real rattling', 'itchy antsy fidgety fretful', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'coughed cough', 'the', 'entire stallion full total integral intact', 'day twenty-four_hours twenty-four_hour_period 24-hour_interval solar_day mean_solar_day daytime daylight sidereal_day Day Clarence_Day Clarence_Shepard_Day_Jr.', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'picked pick pluck cull blame find_fault clean foot plunk break_up peck beak nibble piece', 'flower bloom blossom prime peak heyday efflorescence flush', 'today nowadays now', 'part portion component_part component constituent piece region function office role character theatrical_role persona share percentage section division parting voice contribution separate split split_up break break_up depart start start_out set_forth set_off set_out take_off divide disunite partially partly', 'my', 'hand manus mitt paw hired_hand hired_man handwriting script deal bridge_player helping_hand pass reach pass_on turn_over give', 'that', 'touched touch stir refer pertain relate concern come_to bear_on touch_on have-to_doe_with adjoin meet contact affect impact bear_upon reach extend_to equal rival match disturb allude advert partake tint tinct tinge moved affected stirred fey', 'the', 'flower bloom blossom prime peak heyday efflorescence flush', 'became become go get turn suit', 'swollen swell puff_up swell_up intumesce tumefy tumesce well_up well conceited egotistic egotistical self-conceited swollen-headed vain', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'barely hardly just scarcely scarce scantily', 'bend crook twist turn bending curve fold crease plication flexure crimp Bend bend_dexter flex deform crouch stoop bow deflect turn_away', 'it information_technology IT', 'it information_technology IT', 'feels feel spirit tone feeling flavor flavour look smell tactile_property experience find sense finger palpate', 'numb benumb blunt dull asleep benumbed dead']\n",
      "CLEAN SYNONYMS:  ['after subsequently later afterwards afterward later on', 'soccer association football', 'game plot secret plan biz bet on back gage stake punt crippled halt halting lame gimpy gamy gamey gritty mettlesome spirited spunky', 'yesterday', 'i iodine iodin I one ace single unity ane', 'found establish set up launch plant constitute institute base ground find happen chance bump encounter detect observe discover notice regain determine find out ascertain feel witness see line up get hold come up rule receive get obtain incur recover retrieve find oneself', 'small little minor modest smallscale pocketsize pocketsized humble low lowly minuscule belittled diminished', 'bumps bump bulge hump swelling gibbosity gibbousness jut prominence protuberance protrusion extrusion excrescence blow knock find happen chance encounter demote relegate break kick downstairs dislodge', 'my', 'skin tegument cutis hide pelt peel clamber scramble shin shinny struggle sputter scrape bark pare', 'extreme extreme point extremum utmost uttermost', 'itchiness itch itching', 'when', 'i iodine iodin I one ace single unity ane', 'playing acting playacting performing play act represent spiel act as recreate toy run fiddle diddle dally trifle flirt roleplay playact bring work wreak make for bet wager meet encounter take on', 'with', 'my', 'friends friend ally acquaintance supporter protagonist champion admirer booster Friend Quaker', 'today nowadays now', 'it information technology IT', 'felt felt up mat up mattup matte up matte mat feel experience find sense finger palpate', 'like the like the likes of ilk wish care similar same alike comparable corresponding', 'i iodine iodin I one ace single unity ane', 'had have have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'something', 'my', 'eyes eye oculus optic center centre middle heart eyeball', 'i iodine iodin I one ace single unity ane', 'crying weeping tears cry outcry call yell shout vociferation war cry rallying cry battle cry watchword shout out scream holler hollo squall weep exclaim cry out call out blazon out clamant exigent insistent instant egregious flagrant glaring gross rank', 'uncontrollably', 'my', 'nose olfactory organ nozzle intrude horn in pry poke scent wind nuzzle', 'going departure going away leaving passing loss exit expiration release sledding travel go move locomote proceed go away depart become get run pass lead extend sound function work operate run low run short survive last live live on endure hold up hold out die decease perish pass away expire kick the bucket cash in ones chips buy the farm conk giveup the ghost drop dead pop off choke croak snuff it belong start get going blend blend in fit rifle plump fail go bad give way give out conk out break break down', 'in inch indium In Indiana Hoosier State IN inwards inward', 'park parkland commons common green ballpark Park Mungo Park parking lot car park parking area', 'other early former', 'day twentyfour hours twentyfour hour period solar day mean solar day daytime daylight sidereal day Day Clarence Day Clarence Shepard Day Jr', 'i iodine iodin I one ace single unity ane', 'kept keep maintain hold continue go on proceed go along hold on prevent observe retain keep on sustain stay fresh celebrate restrain keep back hold back preserve keep open hold open save unbroken', 'sniffing sniff whiff sniffle', 'entire stallion full total integral intact', 'time clip clock time fourth dimension meter metre prison term sentence clock', 'sometimes', 'i iodine iodin I one ace single unity ane', 'could', 'not non', 'breath breather breathing place breathing space breathing spell breathing time hint intimation', 'through done through with through and through', 'one I ace single unity i ane unitary matchless nonpareil one and only peerless unmatched unmatchable unrivaled unrivalled', 'my', 'nose olfactory organ nozzle intrude horn in pry poke scent wind nuzzle', 'it information technology IT', 'feels feel spirit tone feeling flavor flavour look smell tactile property experience find sense finger palpate', 'like the like the likes of ilk wish care similar same alike comparable corresponding', 'it information technology IT', 'blocked barricade block blockade stop block off block up bar obstruct hinder stymie stymy embarrass halt kibosh jam stuff lug choke up obturate impede occlude close up parry deflect forget blank out draw a blank freeze immobilize immobilise out of use plugged', 'when', 'i iodine iodin I one ace single unity ane', 'walking walk take the air walkto', 'park parkland commons common green ballpark Park Mungo Park parking lot car park parking area', 'this', 'morning morn morning time forenoon good morning dawn dawning aurora first light daybreak break of day break of the day dayspring sunrise sunup cockcrow', 'i iodine iodin I one ace single unity ane', 'kept keep maintain hold continue go on proceed go along hold on prevent observe retain keep on sustain stay fresh celebrate restrain keep back hold back preserve keep open hold open save unbroken', 'sneezing sneeze sternutation', 'around about approximately close to just about some roughly more or less or so round', 'the', 'flowers flower bloom blossom prime peak heyday efflorescence flush', 'even evening eve eventide flush level even out fiftyfifty regular tied yet still', 'though', 'i iodine iodin I one ace single unity ane', 'not non', 'cold common cold coldness low temperature frigidity frigidness stale dusty motheaten frigid coldblooded inhuman insensate', 'zoo menagerie zoological garden', 'today nowadays now', 'it information technology IT', 'felt felt up mat up mattup matte up matte mat feel experience find sense finger palpate', 'like the like the likes of ilk wish care similar same alike comparable corresponding', 'i iodine iodin I one ace single unity ane', 'have rich person wealthy person have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'feather plume plumage feathering square fledge', 'something', 'my', 'throat pharynx', 'entire stallion full total integral intact', 'day twentyfour hours twentyfour hour period solar day mean solar day daytime daylight sidereal day Day Clarence Day Clarence Shepard Day Jr', 'it information technology IT', 'really truly genuinely actually in truth very real rattling', 'itchy antsy fidgety fretful', 'i iodine iodin I one ace single unity ane', 'coughed cough', 'the', 'entire stallion full total integral intact', 'day twentyfour hours twentyfour hour period solar day mean solar day daytime daylight sidereal day Day Clarence Day Clarence Shepard Day Jr', 'i iodine iodin I one ace single unity ane', 'picked pick pluck cull blame find fault clean foot plunk break up peck beak nibble piece', 'flower bloom blossom prime peak heyday efflorescence flush', 'today nowadays now', 'part portion component part component constituent piece region function office role character theatrical role persona share percentage section division parting voice contribution separate split split up break break up depart start start out set forth set off set out take off divide disunite partially partly', 'my', 'hand manus mitt paw hired hand hired man handwriting script deal bridge player helping hand pass reach pass on turn over give', 'that', 'touched touch stir refer pertain relate concern come to bear on touch on haveto doe with adjoin meet contact affect impact bear upon reach extend to equal rival match disturb allude advert partake tint tinct tinge moved affected stirred fey', 'the', 'flower bloom blossom prime peak heyday efflorescence flush', 'became become go get turn suit', 'swollen swell puff up swell up intumesce tumefy tumesce well up well conceited egotistic egotistical selfconceited swollenheaded vain', 'i iodine iodin I one ace single unity ane', 'barely hardly just scarcely scarce scantily', 'bend crook twist turn bending curve fold crease plication flexure crimp Bend bend dexter flex deform crouch stoop bow deflect turn away', 'it information technology IT', 'it information technology IT', 'feels feel spirit tone feeling flavor flavour look smell tactile property experience find sense finger palpate', 'numb benumb blunt dull asleep benumbed dead']\n",
      "WORD_LIST: ['I stayed up for a full night grinding Hack the North project and the next morning my eye was bright red. All the blood vessels in my blood were visible', 'My eyes were really itchy this morning when I woke up and it was pink when I saw it through the mirror', 'After swimming in the community swimming pool this week, I felt a burning sensation in my eye and i cried uncontrollably.']\n",
      "WORD_LIST_NEW: ['i stayed up for full night grinding hack north project next morning my eye bright red all the blood vessels my blood visible', 'my eyes really itchy this morning when i woke up it pink when i saw it through mirror', 'after swimming the community swimming pool this week i felt burning sensation my eye i cried uncontrollably']\n",
      "joined_symps: i stayed up for full night grinding hack north project next morning my eye bright red all the blood vessels my blood visible my eyes really itchy this morning when i woke up it pink when i saw it through mirror after swimming the community swimming pool this week i felt burning sensation my eye i cried uncontrollably\n",
      "SYNONYMS: ['i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'stayed stay remain rest stick stick_around stay_put bide abide stay_on continue detain delay persist last_out ride_out outride quell appease', 'up astir improving upward upwards upwardly', 'for', 'full full_moon full-of-the-moon full_phase_of_the_moon wax entire total replete good broad wide wide-cut fully to_the_full', 'night nighttime dark Nox Night', 'grinding abrasion attrition detrition crunch cranch craunch grind grate labor labour toil fag travail drudge dig moil mash bray comminute', 'hack drudge hacker machine_politician ward-heeler political_hack hack_writer literary_hack cab taxi taxicab jade nag plug chop cut hack_on cut_up whoop', 'north North Union due_north northward N magnetic_north compass_north Frederick_North Second_Earl_of_Guilford northerly northwards', 'project undertaking task labor projection stick_out protrude jut_out jut plan contrive design propose visualize visualise envision fancy see figure picture image cast throw send_off externalize externalise', 'next following adjacent side_by_side future succeeding', 'morning morn morning_time forenoon good_morning dawn dawning aurora first_light daybreak break_of_day break_of_the_day dayspring sunrise sunup cockcrow', 'my', 'eye oculus optic center centre middle heart eyeball', 'bright brilliant vivid smart burnished lustrous shining shiny undimmed hopeful promising brilliantly brightly', 'red redness Red Red_River Bolshevik Marxist bolshie bolshy loss red_ink reddish ruddy blood-red carmine cerise cherry cherry-red crimson ruby ruby-red scarlet violent reddened red-faced flushed', 'all wholly entirely completely totally altogether whole', 'the', 'blood rake rakehell profligate rip roue lineage line line_of_descent descent bloodline blood_line pedigree ancestry origin parentage stemma stock', 'vessels vessel vas watercraft', 'my', 'blood rake rakehell profligate rip roue lineage line line_of_descent descent bloodline blood_line pedigree ancestry origin parentage stemma stock', 'visible seeable', 'my', 'eyes eye oculus optic center centre middle heart eyeball', 'really truly genuinely actually in_truth very real rattling', 'itchy antsy fidgety fretful', 'this', 'morning morn morning_time forenoon good_morning dawn dawning aurora first_light daybreak break_of_day break_of_the_day dayspring sunrise sunup cockcrow', 'when', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'woke wake wake_up awake arouse awaken come_alive waken inflame stir_up ignite heat fire_up rouse', 'up astir improving upward upwards upwardly', 'it information_technology IT', 'pink garden_pink pinko tap rap knock ping pinkish', 'when', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'saw proverb adage byword power_saw sawing_machine see understand realize realise witness find visualize visualise envision project fancy figure picture image consider reckon view regard learn hear get_word get_wind pick_up find_out get_a_line discover watch catch take_in meet run_into encounter run_across come_across determine check ascertain insure see_to_it ensure control assure visit attend take_care look go_steady go_out date examine experience go_through escort interpret construe', 'it information_technology IT', 'through done through_with through_and_through', 'mirror', 'after subsequently later afterwards afterward later_on', 'swimming swim float drown liquid naiant', 'the', 'community community_of_interests residential_district residential_area biotic_community', 'swimming swim float drown liquid naiant', 'pool pond consortium syndicate puddle kitty pocket_billiards', 'this', 'week hebdomad workweek calendar_week', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'felt felt_up mat_up matt-up matte_up matte mat feel experience find sense finger palpate', 'burning combustion burn electrocution burning_at_the_stake fire burn_down glow combust bite sting incinerate cauterize cauterise sunburn cut burn_off burn_up', 'sensation esthesis aesthesis sense_experience sense_impression sense_datum ace adept champion maven mavin virtuoso genius hotshot star superstar whiz whizz wizard wiz sense sentience sentiency sensory_faculty', 'my', 'eye oculus optic center centre middle heart eyeball', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'cried shout shout_out cry call yell scream holler hollo squall weep exclaim cry_out outcry call_out blazon_out', 'uncontrollably']\n",
      "CLEAN SYNONYMS:  ['i iodine iodin I one ace single unity ane', 'stayed stay remain rest stick stick around stay put bide abide stay on continue detain delay persist last out ride out outride quell appease', 'up astir improving upward upwards upwardly', 'for', 'full full moon fullofthemoon full phase of the moon wax entire total replete good broad wide widecut fully to the full', 'night nighttime dark Nox Night', 'grinding abrasion attrition detrition crunch cranch craunch grind grate labor labour toil fag travail drudge dig moil mash bray comminute', 'hack drudge hacker machine politician wardheeler political hack hack writer literary hack cab taxi taxicab jade nag plug chop cut hack on cut up whoop', 'north North Union due north northward N magnetic north compass north Frederick North Second Earl of Guilford northerly northwards', 'project undertaking task labor projection stick out protrude jut out jut plan contrive design propose visualize visualise envision fancy see figure picture image cast throw send off externalize externalise', 'next following adjacent side by side future succeeding', 'morning morn morning time forenoon good morning dawn dawning aurora first light daybreak break of day break of the day dayspring sunrise sunup cockcrow', 'my', 'eye oculus optic center centre middle heart eyeball', 'bright brilliant vivid smart burnished lustrous shining shiny undimmed hopeful promising brilliantly brightly', 'red redness Red Red River Bolshevik Marxist bolshie bolshy loss red ink reddish ruddy bloodred carmine cerise cherry cherryred crimson ruby rubyred scarlet violent reddened redfaced flushed', 'all wholly entirely completely totally altogether whole', 'the', 'blood rake rakehell profligate rip roue lineage line line of descent descent bloodline blood line pedigree ancestry origin parentage stemma stock', 'vessels vessel vas watercraft', 'my', 'blood rake rakehell profligate rip roue lineage line line of descent descent bloodline blood line pedigree ancestry origin parentage stemma stock', 'visible seeable', 'my', 'eyes eye oculus optic center centre middle heart eyeball', 'really truly genuinely actually in truth very real rattling', 'itchy antsy fidgety fretful', 'this', 'morning morn morning time forenoon good morning dawn dawning aurora first light daybreak break of day break of the day dayspring sunrise sunup cockcrow', 'when', 'i iodine iodin I one ace single unity ane', 'woke wake wake up awake arouse awaken come alive waken inflame stir up ignite heat fire up rouse', 'up astir improving upward upwards upwardly', 'it information technology IT', 'pink garden pink pinko tap rap knock ping pinkish', 'when', 'i iodine iodin I one ace single unity ane', 'saw proverb adage byword power saw sawing machine see understand realize realise witness find visualize visualise envision project fancy figure picture image consider reckon view regard learn hear get word get wind pick up find out get a line discover watch catch take in meet run into encounter run across come across determine check ascertain insure see to it ensure control assure visit attend take care look go steady go out date examine experience go through escort interpret construe', 'it information technology IT', 'through done through with through and through', 'mirror', 'after subsequently later afterwards afterward later on', 'swimming swim float drown liquid naiant', 'the', 'community community of interests residential district residential area biotic community', 'swimming swim float drown liquid naiant', 'pool pond consortium syndicate puddle kitty pocket billiards', 'this', 'week hebdomad workweek calendar week', 'i iodine iodin I one ace single unity ane', 'felt felt up mat up mattup matte up matte mat feel experience find sense finger palpate', 'burning combustion burn electrocution burning at the stake fire burn down glow combust bite sting incinerate cauterize cauterise sunburn cut burn off burn up', 'sensation esthesis aesthesis sense experience sense impression sense datum ace adept champion maven mavin virtuoso genius hotshot star superstar whiz whizz wizard wiz sense sentience sentiency sensory faculty', 'my', 'eye oculus optic center centre middle heart eyeball', 'i iodine iodin I one ace single unity ane', 'cried shout shout out cry call yell scream holler hollo squall weep exclaim cry out outcry call out blazon out', 'uncontrollably']\n",
      "WORD_LIST: ['I went to a dirty restaurant a few days ago, and I began to feel dizzy this morning initially, later I puked everything out', ' After eating some street food, I started to feel dizzy, weak and tired all the time, furthermore I began to catch a fever halfway through the week too', 'I went to the washroom alot more frequently recently and I saw blood in my toilet, I think I might be pooping blood', ' My stomach hurts after a meal of funny tasting fish in the school cafeteria, it hurts so much that I can barely standup straight. I also vomitted after the meal and constantly tired']\n",
      "WORD_LIST_NEW: ['i went dirty restaurant a few days ago i began feel dizzy this morning initially later i puked everything out', 'after eating some street food i started feel dizzy weak tired all time furthermore i began catch fever halfway through week too', 'i went the washroom alot more frequently recently i saw blood my toilet i think i might be pooping blood', 'my stomach hurts after meal funny tasting fish school cafeteria it hurts so much that i barely standup straight i also vomitted after the meal constantly tired']\n",
      "joined_symps: i went dirty restaurant a few days ago i began feel dizzy this morning initially later i puked everything out after eating some street food i started feel dizzy weak tired all time furthermore i began catch fever halfway through week too i went the washroom alot more frequently recently i saw blood my toilet i think i might be pooping blood my stomach hurts after meal funny tasting fish school cafeteria it hurts so much that i barely standup straight i also vomitted after the meal constantly tired\n",
      "SYNONYMS: ['i iodine iodin I atomic_number_53 one 1 ace single unity ane', \"went travel go move locomote proceed go_away depart become get run pass lead extend sound function work operate run_low run_short survive last live live_on endure hold_up hold_out die decease perish exit pass_away expire kick_the_bucket cash_in_one's_chips buy_the_farm conk give-up_the_ghost drop_dead pop_off choke croak snuff_it belong start get_going blend blend_in fit rifle plump fail go_bad give_way give_out conk_out break break_down\", 'dirty soil begrime grime colly bemire soiled unclean filthy lousy contaminating pestiferous dingy muddied muddy foul marked-up ill-gotten cheating unsporting unsportsmanlike sordid', 'restaurant eating_house eating_place eatery', 'a angstrom angstrom_unit A vitamin_A antiophthalmic_factor axerophthol deoxyadenosine_monophosphate adenine ampere amp type_A group_A', 'few', 'days years day twenty-four_hours twenty-four_hour_period 24-hour_interval solar_day mean_solar_day daytime daylight sidereal_day Day Clarence_Day Clarence_Shepard_Day_Jr.', 'ago agone', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'began get_down begin get start_out start set_about set_out commence lead_off', 'feel spirit tone feeling flavor flavour look smell tactile_property experience find sense finger palpate', 'dizzy giddy woozy vertiginous airheaded empty-headed featherbrained light-headed lightheaded silly', 'this', 'morning morn morning_time forenoon good_morning dawn dawning aurora first_light daybreak break_of_day break_of_the_day dayspring sunrise sunup cockcrow', 'initially ab_initio', 'later ulterior posterior late belated tardy recent former previous subsequently afterwards afterward after later_on by_and_by', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'puked vomit vomit_up purge cast sick cat be_sick disgorge regorge retch puke barf spew spue chuck upchuck honk regurgitate throw_up', 'everything', \"out come_out_of_the_closet come_out extinct forbidden prohibited proscribed taboo tabu verboten knocked_out kayoed KO'd stunned away\", 'after subsequently later afterwards afterward later_on', 'eating feeding eat feed eat_on consume eat_up use_up deplete exhaust run_through wipe_out corrode rust', 'some approximately about close_to just_about roughly more_or_less around or_so', 'street', 'food nutrient solid_food food_for_thought intellectual_nourishment', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'started get_down begin get start_out start set_about set_out commence lead_off depart part set_forth set_off take_off originate initiate start_up embark_on startle jump go get_going take_up protrude pop pop_out bulge bulge_out bug_out come_out', 'feel spirit tone feeling flavor flavour look smell tactile_property experience find sense finger palpate', 'dizzy giddy woozy vertiginous airheaded empty-headed featherbrained light-headed lightheaded silly', 'weak watery washy unaccented light fallible frail imperfect decrepit debile feeble infirm rickety sapless weakly faint', 'tired tire pall weary fatigue jade wear_upon tire_out wear wear_out outwear wear_down fag_out fag run_down exhaust play_out sap bore banal commonplace hackneyed old-hat shopworn stock threadbare timeworn trite well-worn', 'all wholly entirely completely totally altogether whole', 'time clip clock_time fourth_dimension meter metre prison_term sentence clock', 'furthermore moreover what_is_more', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'began get_down begin get start_out start set_about set_out commence lead_off', 'catch gimmick haul match stop grab snatch snap apprehension arrest collar pinch taking_into_custody pick_up get take_hold_of capture hitch overtake catch_up_with take_in overhear watch view see trip_up enamour trance becharm enamor captivate beguile charm fascinate bewitch entrance enchant', 'fever febrility febricity pyrexia feverishness', 'halfway center middle midway', 'through done through_with through_and_through', 'week hebdomad workweek calendar_week', 'too excessively overly to_a_fault besides also likewise as_well', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', \"went travel go move locomote proceed go_away depart become get run pass lead extend sound function work operate run_low run_short survive last live live_on endure hold_up hold_out die decease perish exit pass_away expire kick_the_bucket cash_in_one's_chips buy_the_farm conk give-up_the_ghost drop_dead pop_off choke croak snuff_it belong start get_going blend blend_in fit rifle plump fail go_bad give_way give_out conk_out break break_down\", 'the', 'washroom', 'alot', 'more More Thomas_More Sir_Thomas_More more_than to_a_greater_extent', 'frequently often oftentimes oft ofttimes', 'recently late lately of_late latterly', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'saw proverb adage byword power_saw sawing_machine see understand realize realise witness find visualize visualise envision project fancy figure picture image consider reckon view regard learn hear get_word get_wind pick_up find_out get_a_line discover watch catch take_in meet run_into encounter run_across come_across determine check ascertain insure see_to_it ensure control assure visit attend take_care look go_steady go_out date examine experience go_through escort interpret construe', 'blood rake rakehell profligate rip roue lineage line line_of_descent descent bloodline blood_line pedigree ancestry origin parentage stemma stock', 'my', 'toilet lavatory lav can john privy bathroom commode crapper pot potty stool throne gutter sewer toilette', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'think believe consider conceive opine suppose imagine reckon guess cogitate cerebrate remember retrieve recall call_back call_up recollect intend mean', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'might mightiness power', 'be beryllium Be glucinium atomic_number_4 exist equal constitute represent make_up comprise follow embody personify live cost', 'pooping', 'blood rake rakehell profligate rip roue lineage line line_of_descent descent bloodline blood_line pedigree ancestry origin parentage stemma stock', 'my', 'stomach tummy tum breadbasket abdomen venter belly digest endure stick_out bear stand tolerate support brook abide suffer put_up', 'hurts injury hurt harm trauma distress suffering detriment damage scathe ache smart pain anguish injure wound bruise offend spite suffer', 'after subsequently later afterwards afterward later_on', 'meal repast', 'funny funny_story good_story funny_remark amusing comic comical laughable mirthful risible curious odd peculiar queer rum rummy singular fishy shady suspect suspicious', 'tasting taste savoring savouring relishing degustation savor savour sample try try_out smack', 'fish Pisces Fish Pisces_the_Fishes angle', 'school schoolhouse schooling schooltime school_day shoal educate train cultivate civilize civilise', 'cafeteria', 'it information_technology IT', 'hurts injury hurt harm trauma distress suffering detriment damage scathe ache smart pain anguish injure wound bruise offend spite suffer', 'so sol soh thus thusly then and_so and_then therefore hence thence indeed', 'much a_lot lots a_good_deal a_great_deal very_much practically often', 'that', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'barely hardly just scarcely scarce scantily', 'standup', 'straight heterosexual heterosexual_person straight_person straightaway consecutive unbent unbowed square uncoiled true straightforward neat full-strength directly direct flat', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'also besides too likewise as_well', 'vomitted', 'after subsequently later afterwards afterward later_on', 'the', 'meal repast', 'constantly invariably always forever perpetually incessantly', 'tired tire pall weary fatigue jade wear_upon tire_out wear wear_out outwear wear_down fag_out fag run_down exhaust play_out sap bore banal commonplace hackneyed old-hat shopworn stock threadbare timeworn trite well-worn']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEAN SYNONYMS:  ['i iodine iodin I one ace single unity ane', 'went travel go move locomote proceed go away depart become get run pass lead extend sound function work operate run low run short survive last live live on endure hold up hold out die decease perish exit pass away expire kick the bucket cash in ones chips buy the farm conk giveup the ghost drop dead pop off choke croak snuff it belong start get going blend blend in fit rifle plump fail go bad give way give out conk out break break down', 'dirty soil begrime grime colly bemire soiled unclean filthy lousy contaminating pestiferous dingy muddied muddy foul markedup illgotten cheating unsporting unsportsmanlike sordid', 'restaurant eating house eating place eatery', 'a angstrom angstrom unit A vitamin A antiophthalmic factor axerophthol deoxyadenosine monophosphate adenine ampere amp type A group A', 'few', 'days years day twentyfour hours twentyfour hour period solar day mean solar day daytime daylight sidereal day Day Clarence Day Clarence Shepard Day Jr', 'ago agone', 'i iodine iodin I one ace single unity ane', 'began get down begin get start out start set about set out commence lead off', 'feel spirit tone feeling flavor flavour look smell tactile property experience find sense finger palpate', 'dizzy giddy woozy vertiginous airheaded emptyheaded featherbrained lightheaded lightheaded silly', 'this', 'morning morn morning time forenoon good morning dawn dawning aurora first light daybreak break of day break of the day dayspring sunrise sunup cockcrow', 'initially ab initio', 'later ulterior posterior late belated tardy recent former previous subsequently afterwards afterward after later on by and by', 'i iodine iodin I one ace single unity ane', 'puked vomit vomit up purge cast sick cat be sick disgorge regorge retch puke barf spew spue chuck upchuck honk regurgitate throw up', 'everything', 'out come out of the closet come out extinct forbidden prohibited proscribed taboo tabu verboten knocked out kayoed KOd stunned away', 'after subsequently later afterwards afterward later on', 'eating feeding eat feed eat on consume eat up use up deplete exhaust run through wipe out corrode rust', 'some approximately about close to just about roughly more or less around or so', 'street', 'food nutrient solid food food for thought intellectual nourishment', 'i iodine iodin I one ace single unity ane', 'started get down begin get start out start set about set out commence lead off depart part set forth set off take off originate initiate start up embark on startle jump go get going take up protrude pop pop out bulge bulge out bug out come out', 'feel spirit tone feeling flavor flavour look smell tactile property experience find sense finger palpate', 'dizzy giddy woozy vertiginous airheaded emptyheaded featherbrained lightheaded lightheaded silly', 'weak watery washy unaccented light fallible frail imperfect decrepit debile feeble infirm rickety sapless weakly faint', 'tired tire pall weary fatigue jade wear upon tire out wear wear out outwear wear down fag out fag run down exhaust play out sap bore banal commonplace hackneyed oldhat shopworn stock threadbare timeworn trite wellworn', 'all wholly entirely completely totally altogether whole', 'time clip clock time fourth dimension meter metre prison term sentence clock', 'furthermore moreover what is more', 'i iodine iodin I one ace single unity ane', 'began get down begin get start out start set about set out commence lead off', 'catch gimmick haul match stop grab snatch snap apprehension arrest collar pinch taking into custody pick up get take hold of capture hitch overtake catch up with take in overhear watch view see trip up enamour trance becharm enamor captivate beguile charm fascinate bewitch entrance enchant', 'fever febrility febricity pyrexia feverishness', 'halfway center middle midway', 'through done through with through and through', 'week hebdomad workweek calendar week', 'too excessively overly to a fault besides also likewise as well', 'i iodine iodin I one ace single unity ane', 'went travel go move locomote proceed go away depart become get run pass lead extend sound function work operate run low run short survive last live live on endure hold up hold out die decease perish exit pass away expire kick the bucket cash in ones chips buy the farm conk giveup the ghost drop dead pop off choke croak snuff it belong start get going blend blend in fit rifle plump fail go bad give way give out conk out break break down', 'the', 'washroom', 'alot', 'more More Thomas More Sir Thomas More more than to a greater extent', 'frequently often oftentimes oft ofttimes', 'recently late lately of late latterly', 'i iodine iodin I one ace single unity ane', 'saw proverb adage byword power saw sawing machine see understand realize realise witness find visualize visualise envision project fancy figure picture image consider reckon view regard learn hear get word get wind pick up find out get a line discover watch catch take in meet run into encounter run across come across determine check ascertain insure see to it ensure control assure visit attend take care look go steady go out date examine experience go through escort interpret construe', 'blood rake rakehell profligate rip roue lineage line line of descent descent bloodline blood line pedigree ancestry origin parentage stemma stock', 'my', 'toilet lavatory lav can john privy bathroom commode crapper pot potty stool throne gutter sewer toilette', 'i iodine iodin I one ace single unity ane', 'think believe consider conceive opine suppose imagine reckon guess cogitate cerebrate remember retrieve recall call back call up recollect intend mean', 'i iodine iodin I one ace single unity ane', 'might mightiness power', 'be beryllium Be glucinium exist equal constitute represent make up comprise follow embody personify live cost', 'pooping', 'blood rake rakehell profligate rip roue lineage line line of descent descent bloodline blood line pedigree ancestry origin parentage stemma stock', 'my', 'stomach tummy tum breadbasket abdomen venter belly digest endure stick out bear stand tolerate support brook abide suffer put up', 'hurts injury hurt harm trauma distress suffering detriment damage scathe ache smart pain anguish injure wound bruise offend spite suffer', 'after subsequently later afterwards afterward later on', 'meal repast', 'funny funny story good story funny remark amusing comic comical laughable mirthful risible curious odd peculiar queer rum rummy singular fishy shady suspect suspicious', 'tasting taste savoring savouring relishing degustation savor savour sample try try out smack', 'fish Pisces Fish Pisces the Fishes angle', 'school schoolhouse schooling schooltime school day shoal educate train cultivate civilize civilise', 'cafeteria', 'it information technology IT', 'hurts injury hurt harm trauma distress suffering detriment damage scathe ache smart pain anguish injure wound bruise offend spite suffer', 'so sol soh thus thusly then and so and then therefore hence thence indeed', 'much a lot lots a good deal a great deal very much practically often', 'that', 'i iodine iodin I one ace single unity ane', 'barely hardly just scarcely scarce scantily', 'standup', 'straight heterosexual heterosexual person straight person straightaway consecutive unbent unbowed square uncoiled true straightforward neat fullstrength directly direct flat', 'i iodine iodin I one ace single unity ane', 'also besides too likewise as well', 'vomitted', 'after subsequently later afterwards afterward later on', 'the', 'meal repast', 'constantly invariably always forever perpetually incessantly', 'tired tire pall weary fatigue jade wear upon tire out wear wear out outwear wear down fag out fag run down exhaust play out sap bore banal commonplace hackneyed oldhat shopworn stock threadbare timeworn trite wellworn']\n",
      "WORD_LIST: ['I saw some yellow-green discharge, light bleeding between my periods and an occasional burning sensation while urinating. Are those symptoms of chlamydia or a sexually transmitted infection', 'Hi, I recently experience  a burning sensation while peeing, frequent urination or a yellow-green discharge, sometimes, even difficulty urinating', 'I recently have trouble going to the washroom because my genital areas hurt during urination and they sometimes swell up randomly.']\n",
      "WORD_LIST_NEW: ['i saw some yellowgreen discharge light bleeding between my periods an occasional burning sensation while urinating those symptoms chlamydia a sexually transmitted infection', 'hi i recently experience burning sensation while peeing frequent urination a yellowgreen discharge sometimes even difficulty urinating', 'i recently have trouble going the washroom because my genital areas hurt during urination they sometimes swell up randomly']\n",
      "joined_symps: i saw some yellowgreen discharge light bleeding between my periods an occasional burning sensation while urinating those symptoms chlamydia a sexually transmitted infection hi i recently experience burning sensation while peeing frequent urination a yellowgreen discharge sometimes even difficulty urinating i recently have trouble going the washroom because my genital areas hurt during urination they sometimes swell up randomly\n",
      "SYNONYMS: ['i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'saw proverb adage byword power_saw sawing_machine see understand realize realise witness find visualize visualise envision project fancy figure picture image consider reckon view regard learn hear get_word get_wind pick_up find_out get_a_line discover watch catch take_in meet run_into encounter run_across come_across determine check ascertain insure see_to_it ensure control assure visit attend take_care look go_steady go_out date examine experience go_through escort interpret construe', 'some approximately about close_to just_about roughly more_or_less around or_so', 'yellowgreen', 'discharge venting emission expelling spark arc electric_arc electric_discharge outpouring run dismissal dismission firing liberation release sack sacking waiver firing_off dispatch complete free fire go_off acquit assoil clear exonerate exculpate exhaust expel eject drop drop_off set_down put_down unload muster_out empty', 'light visible_light visible_radiation light_source luminosity brightness brightness_level luminance luminousness illumination lightness lighting sparkle twinkle spark Inner_Light Light Light_Within Christ_Within lighter igniter ignitor illume illumine light_up illuminate fire_up alight perch ignite fall unhorse dismount get_off get_down light-colored unaccented weak clean clear unclouded lightsome tripping faint swooning light-headed lightheaded abstemious scant short idle lite low-cal calorie-free wakeful easy loose promiscuous sluttish wanton lightly', 'bleeding hemorrhage haemorrhage shed_blood bleed leech phlebotomize phlebotomise run', \"between betwixt 'tween\", 'my', 'periods time_period period_of_time period geological_period menstruation menses menstruum catamenia flow point full_stop stop full_point', 'an Associate_in_Nursing AN', 'occasional episodic casual periodic', 'burning combustion burn electrocution burning_at_the_stake fire burn_down glow combust bite sting incinerate cauterize cauterise sunburn cut burn_off burn_up', 'sensation esthesis aesthesis sense_experience sense_impression sense_datum ace adept champion maven mavin virtuoso genius hotshot star superstar whiz whizz wizard wiz sense sentience sentiency sensory_faculty', 'while piece spell patch', 'urinating urinate make piddle puddle micturate piss pee pee-pee make_water relieve_oneself take_a_leak spend_a_penny wee wee-wee pass_water', 'those', 'symptoms symptom', 'chlamydia', 'a angstrom angstrom_unit A vitamin_A antiophthalmic_factor axerophthol deoxyadenosine_monophosphate adenine ampere amp type_A group_A', 'sexually', 'transmitted convey transmit communicate impart conduct carry channel air send broadcast beam transfer transport channelize channelise familial genetic hereditary inherited transmissible', 'infection contagion transmission', \"hi hello hullo howdy how-do-you-do Hawaii Hawai'i Aloha_State HI\", 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'recently late lately of_late latterly', 'experience see go_through know live receive have get feel', 'burning combustion burn electrocution burning_at_the_stake fire burn_down glow combust bite sting incinerate cauterize cauterise sunburn cut burn_off burn_up', 'sensation esthesis aesthesis sense_experience sense_impression sense_datum ace adept champion maven mavin virtuoso genius hotshot star superstar whiz whizz wizard wiz sense sentience sentiency sensory_faculty', 'while piece spell patch', 'peeing pee pissing piss make urinate piddle puddle micturate pee-pee make_water relieve_oneself take_a_leak spend_a_penny wee wee-wee pass_water', 'frequent patronize patronise shop shop_at buy_at sponsor haunt', 'urination micturition', 'a angstrom angstrom_unit A vitamin_A antiophthalmic_factor axerophthol deoxyadenosine_monophosphate adenine ampere amp type_A group_A', 'yellowgreen', 'discharge venting emission expelling spark arc electric_arc electric_discharge outpouring run dismissal dismission firing liberation release sack sacking waiver firing_off dispatch complete free fire go_off acquit assoil clear exonerate exculpate exhaust expel eject drop drop_off set_down put_down unload muster_out empty', 'sometimes', 'even evening eve eventide flush level even_out fifty-fifty regular tied yet still', 'difficulty trouble difficultness', 'urinating urinate make piddle puddle micturate piss pee pee-pee make_water relieve_oneself take_a_leak spend_a_penny wee wee-wee pass_water', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'recently late lately of_late latterly', 'have rich_person wealthy_person have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'trouble problem fuss bother hassle difficulty worry disturb upset put_out inconvenience disoblige discommode incommode perturb unhinge disquiet cark distract disorder trouble_oneself inconvenience_oneself ail pain', \"going departure going_away leaving passing loss exit expiration release sledding travel go move locomote proceed go_away depart become get run pass lead extend sound function work operate run_low run_short survive last live live_on endure hold_up hold_out die decease perish pass_away expire kick_the_bucket cash_in_one's_chips buy_the_farm conk give-up_the_ghost drop_dead pop_off choke croak snuff_it belong start get_going blend blend_in fit rifle plump fail go_bad give_way give_out conk_out break break_down\", 'the', 'washroom', 'because', 'my', 'genital venereal', 'areas area country region sphere domain orbit field arena expanse surface_area', 'hurt injury harm trauma distress suffering detriment damage scathe ache smart pain anguish injure wound bruise offend spite suffer wounded weakened', 'during', 'urination micturition', 'they', 'sometimes', 'swell crestless_wave dandy dude fop gallant sheik beau fashion_plate clotheshorse puff_up swell_up intumesce tumefy tumesce well_up well bang-up bully corking cracking great groovy keen neat nifty not_bad peachy slap-up smashing', 'up astir improving upward upwards upwardly', 'randomly indiscriminately haphazardly willy-nilly arbitrarily at_random every_which_way']\n",
      "CLEAN SYNONYMS:  ['i iodine iodin I one ace single unity ane', 'saw proverb adage byword power saw sawing machine see understand realize realise witness find visualize visualise envision project fancy figure picture image consider reckon view regard learn hear get word get wind pick up find out get a line discover watch catch take in meet run into encounter run across come across determine check ascertain insure see to it ensure control assure visit attend take care look go steady go out date examine experience go through escort interpret construe', 'some approximately about close to just about roughly more or less around or so', 'yellowgreen', 'discharge venting emission expelling spark arc electric arc electric discharge outpouring run dismissal dismission firing liberation release sack sacking waiver firing off dispatch complete free fire go off acquit assoil clear exonerate exculpate exhaust expel eject drop drop off set down put down unload muster out empty', 'light visible light visible radiation light source luminosity brightness brightness level luminance luminousness illumination lightness lighting sparkle twinkle spark Inner Light Light Light Within Christ Within lighter igniter ignitor illume illumine light up illuminate fire up alight perch ignite fall unhorse dismount get off get down lightcolored unaccented weak clean clear unclouded lightsome tripping faint swooning lightheaded lightheaded abstemious scant short idle lite lowcal caloriefree wakeful easy loose promiscuous sluttish wanton lightly', 'bleeding hemorrhage haemorrhage shed blood bleed leech phlebotomize phlebotomise run', 'between betwixt tween', 'my', 'periods time period period of time period geological period menstruation menses menstruum catamenia flow point full stop stop full point', 'an Associate in Nursing AN', 'occasional episodic casual periodic', 'burning combustion burn electrocution burning at the stake fire burn down glow combust bite sting incinerate cauterize cauterise sunburn cut burn off burn up', 'sensation esthesis aesthesis sense experience sense impression sense datum ace adept champion maven mavin virtuoso genius hotshot star superstar whiz whizz wizard wiz sense sentience sentiency sensory faculty', 'while piece spell patch', 'urinating urinate make piddle puddle micturate piss pee peepee make water relieve oneself take a leak spend a penny wee weewee pass water', 'those', 'symptoms symptom', 'chlamydia', 'a angstrom angstrom unit A vitamin A antiophthalmic factor axerophthol deoxyadenosine monophosphate adenine ampere amp type A group A', 'sexually', 'transmitted convey transmit communicate impart conduct carry channel air send broadcast beam transfer transport channelize channelise familial genetic hereditary inherited transmissible', 'infection contagion transmission', 'hi hello hullo howdy howdoyoudo Hawaii Hawaii Aloha State HI', 'i iodine iodin I one ace single unity ane', 'recently late lately of late latterly', 'experience see go through know live receive have get feel', 'burning combustion burn electrocution burning at the stake fire burn down glow combust bite sting incinerate cauterize cauterise sunburn cut burn off burn up', 'sensation esthesis aesthesis sense experience sense impression sense datum ace adept champion maven mavin virtuoso genius hotshot star superstar whiz whizz wizard wiz sense sentience sentiency sensory faculty', 'while piece spell patch', 'peeing pee pissing piss make urinate piddle puddle micturate peepee make water relieve oneself take a leak spend a penny wee weewee pass water', 'frequent patronize patronise shop shop at buy at sponsor haunt', 'urination micturition', 'a angstrom angstrom unit A vitamin A antiophthalmic factor axerophthol deoxyadenosine monophosphate adenine ampere amp type A group A', 'yellowgreen', 'discharge venting emission expelling spark arc electric arc electric discharge outpouring run dismissal dismission firing liberation release sack sacking waiver firing off dispatch complete free fire go off acquit assoil clear exonerate exculpate exhaust expel eject drop drop off set down put down unload muster out empty', 'sometimes', 'even evening eve eventide flush level even out fiftyfifty regular tied yet still', 'difficulty trouble difficultness', 'urinating urinate make piddle puddle micturate piss pee peepee make water relieve oneself take a leak spend a penny wee weewee pass water', 'i iodine iodin I one ace single unity ane', 'recently late lately of late latterly', 'have rich person wealthy person have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'trouble problem fuss bother hassle difficulty worry disturb upset put out inconvenience disoblige discommode incommode perturb unhinge disquiet cark distract disorder trouble oneself inconvenience oneself ail pain', 'going departure going away leaving passing loss exit expiration release sledding travel go move locomote proceed go away depart become get run pass lead extend sound function work operate run low run short survive last live live on endure hold up hold out die decease perish pass away expire kick the bucket cash in ones chips buy the farm conk giveup the ghost drop dead pop off choke croak snuff it belong start get going blend blend in fit rifle plump fail go bad give way give out conk out break break down', 'the', 'washroom', 'because', 'my', 'genital venereal', 'areas area country region sphere domain orbit field arena expanse surface area', 'hurt injury harm trauma distress suffering detriment damage scathe ache smart pain anguish injure wound bruise offend spite suffer wounded weakened', 'during', 'urination micturition', 'they', 'sometimes', 'swell crestless wave dandy dude fop gallant sheik beau fashion plate clotheshorse puff up swell up intumesce tumefy tumesce well up well bangup bully corking cracking great groovy keen neat nifty not bad peachy slapup smashing', 'up astir improving upward upwards upwardly', 'randomly indiscriminately haphazardly willynilly arbitrarily at random every which way']\n",
      "WORD_LIST: [\"Hello, I had this burning sensationâ€”I thought it was cystitis. I went to the doctor and he gave me some antibiotics. But it didn't go away. Then, a few days later, I got some blisters on my vagina, all along my lips, what is that blister\", 'I think I got some skin irritation, razor burn or just some random itch, but they repeat once every a few month,what are those?']\n",
      "WORD_LIST_NEW: ['hello i had this burning sensationâ€i thought it cystitis i went the doctor he gave me some antibiotics it didnt go away a few days later i got some blisters my vagina all along my lips what that blister', 'i think i got some skin irritation razor burn just some random itch they repeat once every few monthwhat those']\n",
      "joined_symps: hello i had this burning sensationâ€i thought it cystitis i went the doctor he gave me some antibiotics it didnt go away a few days later i got some blisters my vagina all along my lips what that blister i think i got some skin irritation razor burn just some random itch they repeat once every few monthwhat those\n",
      "SYNONYMS: ['hello hullo hi howdy how-do-you-do', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'had have have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'this', 'burning combustion burn electrocution burning_at_the_stake fire burn_down glow combust bite sting incinerate cauterize cauterise sunburn cut burn_off burn_up', 'sensationâ€i', 'thought idea thinking thought_process cerebration intellection mentation opinion sentiment persuasion view think believe consider conceive opine suppose imagine reckon guess cogitate cerebrate remember retrieve recall call_back call_up recollect intend mean', 'it information_technology IT', 'cystitis', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', \"went travel go move locomote proceed go_away depart become get run pass lead extend sound function work operate run_low run_short survive last live live_on endure hold_up hold_out die decease perish exit pass_away expire kick_the_bucket cash_in_one's_chips buy_the_farm conk give-up_the_ghost drop_dead pop_off choke croak snuff_it belong start get_going blend blend_in fit rifle plump fail go_bad give_way give_out conk_out break break_down\", 'the', 'doctor doc physician MD Dr. medico Doctor_of_the_Church Doctor sophisticate doctor_up repair mend fix bushel furbish_up restore touch_on', 'he helium He atomic_number_2', 'gave give yield afford pay hold throw have make gift present devote render return generate impart leave pass_on establish sacrifice pass hand reach turn_over dedicate consecrate commit apply grant move_over give_way ease_up feed contribute chip_in kick_in collapse fall_in cave_in break founder open', 'me Maine Pine_Tree_State ME', 'some approximately about close_to just_about roughly more_or_less around or_so', 'antibiotics antibiotic antibiotic_drug', 'it information_technology IT', 'didnt', \"go spell tour turn Adam ecstasy XTC disco_biscuit cristal X hug_drug crack fling pass whirl offer go_game travel move locomote proceed go_away depart become get run lead extend sound function work operate run_low run_short survive last live live_on endure hold_up hold_out die decease perish exit pass_away expire kick_the_bucket cash_in_one's_chips buy_the_farm conk give-up_the_ghost drop_dead pop_off choke croak snuff_it belong start get_going blend blend_in fit rifle plump fail go_bad give_way give_out conk_out break break_down\", 'away outside off forth out aside by', 'a angstrom angstrom_unit A vitamin_A antiophthalmic_factor axerophthol deoxyadenosine_monophosphate adenine ampere amp type_A group_A', 'few', 'days years day twenty-four_hours twenty-four_hour_period 24-hour_interval solar_day mean_solar_day daytime daylight sidereal_day Day Clarence_Day Clarence_Shepard_Day_Jr.', 'later ulterior posterior late belated tardy recent former previous subsequently afterwards afterward after later_on by_and_by', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', \"got get acquire become go let have receive find obtain incur arrive come bring convey fetch experience pay_back pay_off fix make induce stimulate cause catch capture grow develop produce contract take drive aim arrest scram buzz_off fuck_off bugger_off get_under_one's_skin draw perplex vex stick puzzle mystify baffle beat pose bewilder flummox stupefy nonplus gravel amaze dumbfound get_down begin start_out start set_about set_out commence suffer sustain beget engender father mother sire generate bring_forth\", 'some approximately about close_to just_about roughly more_or_less around or_so', 'blisters blister bulla bleb vesicate scald whip', 'my', 'vagina', 'all wholly entirely completely totally altogether whole', 'along on', 'my', 'lips lip sass sassing backtalk back_talk mouth brim rim', 'what', 'that', 'blister bulla bleb vesicate scald whip', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'think believe consider conceive opine suppose imagine reckon guess cogitate cerebrate remember retrieve recall call_back call_up recollect intend mean', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', \"got get acquire become go let have receive find obtain incur arrive come bring convey fetch experience pay_back pay_off fix make induce stimulate cause catch capture grow develop produce contract take drive aim arrest scram buzz_off fuck_off bugger_off get_under_one's_skin draw perplex vex stick puzzle mystify baffle beat pose bewilder flummox stupefy nonplus gravel amaze dumbfound get_down begin start_out start set_about set_out commence suffer sustain beget engender father mother sire generate bring_forth\", 'some approximately about close_to just_about roughly more_or_less around or_so', 'skin tegument cutis hide pelt peel clamber scramble shin shinny struggle sputter scrape bark pare', 'irritation annoyance vexation botheration pique temper excitation innervation discomfort soreness aggravation provocation annoying', 'razor', 'burn burning tan suntan sunburn burn_mark fire burn_down glow combust bite sting incinerate cauterize cauterise cut burn_off burn_up', 'just equitable fair good upright merely simply only but precisely exactly just_now barely hardly scarcely scarce', 'some approximately about close_to just_about roughly more_or_less around or_so', 'random', 'itch scabies urge itchiness itching rub scratch spoil', 'they', 'repeat repetition reiterate ingeminate iterate restate retell duplicate reduplicate double replicate recur echo take_over reprise reprize recapitulate', 'once one_time in_one_case formerly at_one_time erstwhile erst', 'every', 'few', 'monthwhat', 'those']\n",
      "CLEAN SYNONYMS:  ['hello hullo hi howdy howdoyoudo', 'i iodine iodin I one ace single unity ane', 'had have have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'this', 'burning combustion burn electrocution burning at the stake fire burn down glow combust bite sting incinerate cauterize cauterise sunburn cut burn off burn up', 'sensationâ€i', 'thought idea thinking thought process cerebration intellection mentation opinion sentiment persuasion view think believe consider conceive opine suppose imagine reckon guess cogitate cerebrate remember retrieve recall call back call up recollect intend mean', 'it information technology IT', 'cystitis', 'i iodine iodin I one ace single unity ane', 'went travel go move locomote proceed go away depart become get run pass lead extend sound function work operate run low run short survive last live live on endure hold up hold out die decease perish exit pass away expire kick the bucket cash in ones chips buy the farm conk giveup the ghost drop dead pop off choke croak snuff it belong start get going blend blend in fit rifle plump fail go bad give way give out conk out break break down', 'the', 'doctor doc physician MD Dr medico Doctor of the Church Doctor sophisticate doctor up repair mend fix bushel furbish up restore touch on', 'he helium He', 'gave give yield afford pay hold throw have make gift present devote render return generate impart leave pass on establish sacrifice pass hand reach turn over dedicate consecrate commit apply grant move over give way ease up feed contribute chip in kick in collapse fall in cave in break founder open', 'me Maine Pine Tree State ME', 'some approximately about close to just about roughly more or less around or so', 'antibiotics antibiotic antibiotic drug', 'it information technology IT', 'didnt', 'go spell tour turn Adam ecstasy XTC disco biscuit cristal X hug drug crack fling pass whirl offer go game travel move locomote proceed go away depart become get run lead extend sound function work operate run low run short survive last live live on endure hold up hold out die decease perish exit pass away expire kick the bucket cash in ones chips buy the farm conk giveup the ghost drop dead pop off choke croak snuff it belong start get going blend blend in fit rifle plump fail go bad give way give out conk out break break down', 'away outside off forth out aside by', 'a angstrom angstrom unit A vitamin A antiophthalmic factor axerophthol deoxyadenosine monophosphate adenine ampere amp type A group A', 'few', 'days years day twentyfour hours twentyfour hour period solar day mean solar day daytime daylight sidereal day Day Clarence Day Clarence Shepard Day Jr', 'later ulterior posterior late belated tardy recent former previous subsequently afterwards afterward after later on by and by', 'i iodine iodin I one ace single unity ane', 'got get acquire become go let have receive find obtain incur arrive come bring convey fetch experience pay back pay off fix make induce stimulate cause catch capture grow develop produce contract take drive aim arrest scram buzz off fuck off bugger off get under ones skin draw perplex vex stick puzzle mystify baffle beat pose bewilder flummox stupefy nonplus gravel amaze dumbfound get down begin start out start set about set out commence suffer sustain beget engender father mother sire generate bring forth', 'some approximately about close to just about roughly more or less around or so', 'blisters blister bulla bleb vesicate scald whip', 'my', 'vagina', 'all wholly entirely completely totally altogether whole', 'along on', 'my', 'lips lip sass sassing backtalk back talk mouth brim rim', 'what', 'that', 'blister bulla bleb vesicate scald whip', 'i iodine iodin I one ace single unity ane', 'think believe consider conceive opine suppose imagine reckon guess cogitate cerebrate remember retrieve recall call back call up recollect intend mean', 'i iodine iodin I one ace single unity ane', 'got get acquire become go let have receive find obtain incur arrive come bring convey fetch experience pay back pay off fix make induce stimulate cause catch capture grow develop produce contract take drive aim arrest scram buzz off fuck off bugger off get under ones skin draw perplex vex stick puzzle mystify baffle beat pose bewilder flummox stupefy nonplus gravel amaze dumbfound get down begin start out start set about set out commence suffer sustain beget engender father mother sire generate bring forth', 'some approximately about close to just about roughly more or less around or so', 'skin tegument cutis hide pelt peel clamber scramble shin shinny struggle sputter scrape bark pare', 'irritation annoyance vexation botheration pique temper excitation innervation discomfort soreness aggravation provocation annoying', 'razor', 'burn burning tan suntan sunburn burn mark fire burn down glow combust bite sting incinerate cauterize cauterise cut burn off burn up', 'just equitable fair good upright merely simply only but precisely exactly just now barely hardly scarcely scarce', 'some approximately about close to just about roughly more or less around or so', 'random', 'itch scabies urge itchiness itching rub scratch spoil', 'they', 'repeat repetition reiterate ingeminate iterate restate retell duplicate reduplicate double replicate recur echo take over reprise reprize recapitulate', 'once one time in one case formerly at one time erstwhile erst', 'every', 'few', 'monthwhat', 'those']\n",
      "WORD_LIST: [\"I visited the jungle a few days ago and I got a strange bite because it didn't go away. A few days later, I began to develop a fever and muscle weakness, what should I do?\", 'I discovered a weird mosquito bite on my hand that does not go away, but instead it swell and my hand is numb', ' My roommate has a weird bite on his forehead, He caught a fever and was confused about his surroundings. He experiences vision loss and fall into occassional comas. What should I do about him?', 'I have a minor headache, vomiting and I saw red rashes all over my back, chest and stomach']\n",
      "WORD_LIST_NEW: ['i visited jungle few days ago i got strange bite because it didnt go away few days later i began develop fever muscle weakness what should i do', 'i discovered weird mosquito bite my hand that does not go away instead it swell my hand numb', 'my roommate has weird bite his forehead he caught fever was confused about his surroundings he experiences vision loss fall into occassional comas what should i do about him', 'i have minor headache vomiting i saw red rashes all over my back chest stomach']\n",
      "joined_symps: i visited jungle few days ago i got strange bite because it didnt go away few days later i began develop fever muscle weakness what should i do i discovered weird mosquito bite my hand that does not go away instead it swell my hand numb my roommate has weird bite his forehead he caught fever was confused about his surroundings he experiences vision loss fall into occassional comas what should i do about him i have minor headache vomiting i saw red rashes all over my back chest stomach\n",
      "SYNONYMS: ['i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'visited visit see travel_to call_in call inspect inflict bring_down impose chew_the_fat shoot_the_breeze chat confabulate confab chitchat chit-chat chatter chaffer natter gossip jaw claver', 'jungle hobo_camp', 'few', 'days years day twenty-four_hours twenty-four_hour_period 24-hour_interval solar_day mean_solar_day daytime daylight sidereal_day Day Clarence_Day Clarence_Shepard_Day_Jr.', 'ago agone', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', \"got get acquire become go let have receive find obtain incur arrive come bring convey fetch experience pay_back pay_off fix make induce stimulate cause catch capture grow develop produce contract take drive aim arrest scram buzz_off fuck_off bugger_off get_under_one's_skin draw perplex vex stick puzzle mystify baffle beat pose bewilder flummox stupefy nonplus gravel amaze dumbfound get_down begin start_out start set_about set_out commence suffer sustain beget engender father mother sire generate bring_forth\", 'strange unusual unknown foreign', 'bite morsel bit sting insect_bite collation snack pungency sharpness raciness chomp seize_with_teeth burn prick', 'because', 'it information_technology IT', 'didnt', \"go spell tour turn Adam ecstasy XTC disco_biscuit cristal X hug_drug crack fling pass whirl offer go_game travel move locomote proceed go_away depart become get run lead extend sound function work operate run_low run_short survive last live live_on endure hold_up hold_out die decease perish exit pass_away expire kick_the_bucket cash_in_one's_chips buy_the_farm conk give-up_the_ghost drop_dead pop_off choke croak snuff_it belong start get_going blend blend_in fit rifle plump fail go_bad give_way give_out conk_out break break_down\", 'away outside off forth out aside by', 'few', 'days years day twenty-four_hours twenty-four_hour_period 24-hour_interval solar_day mean_solar_day daytime daylight sidereal_day Day Clarence_Day Clarence_Shepard_Day_Jr.', 'later ulterior posterior late belated tardy recent former previous subsequently afterwards afterward after later_on by_and_by', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'began get_down begin get start_out start set_about set_out commence lead_off', 'develop evolve germinate acquire grow produce get originate arise rise uprise spring_up build_up explicate formulate train prepare educate modernize modernise make_grow break recrudesce', 'fever febrility febricity pyrexia feverishness', 'muscle musculus muscular_tissue muscleman brawn brawniness muscularity sinew heftiness', 'weakness failing helplessness impuissance', 'what', 'should', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'do bash brawl doh ut Doctor_of_Osteopathy DO make perform execute fare make_out come get_along cause practice practise exercise suffice answer serve act behave manage dress arrange set coif coiffe coiffure', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'discovered detect observe find discover notice learn hear get_word get_wind pick_up find_out get_a_line see fall_upon strike come_upon light_upon chance_upon come_across chance_on happen_upon attain unwrap disclose let_on bring_out reveal expose divulge break give_away let_out identify key key_out distinguish describe name ascertained observed', 'weird Wyrd Weird eldritch uncanny unearthly', 'mosquito', 'bite morsel bit sting insect_bite collation snack pungency sharpness raciness chomp seize_with_teeth burn prick', 'my', 'hand manus mitt paw hired_hand hired_man handwriting script deal bridge_player helping_hand pass reach pass_on turn_over give', 'that', 'does Department_of_Energy Energy_Department Energy DOE doe make do perform execute fare make_out come get_along cause practice practise exercise suffice answer serve act behave manage dress arrange set coif coiffe coiffure', 'not non', \"go spell tour turn Adam ecstasy XTC disco_biscuit cristal X hug_drug crack fling pass whirl offer go_game travel move locomote proceed go_away depart become get run lead extend sound function work operate run_low run_short survive last live live_on endure hold_up hold_out die decease perish exit pass_away expire kick_the_bucket cash_in_one's_chips buy_the_farm conk give-up_the_ghost drop_dead pop_off choke croak snuff_it belong start get_going blend blend_in fit rifle plump fail go_bad give_way give_out conk_out break break_down\", 'away outside off forth out aside by', 'instead alternatively or_else rather', 'it information_technology IT', 'swell crestless_wave dandy dude fop gallant sheik beau fashion_plate clotheshorse puff_up swell_up intumesce tumefy tumesce well_up well bang-up bully corking cracking great groovy keen neat nifty not_bad peachy slap-up smashing', 'my', 'hand manus mitt paw hired_hand hired_man handwriting script deal bridge_player helping_hand pass reach pass_on turn_over give', 'numb benumb blunt dull asleep benumbed dead', 'my', 'roommate roomie roomy', 'has hour_angle HA have have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'weird Wyrd Weird eldritch uncanny unearthly', 'bite morsel bit sting insect_bite collation snack pungency sharpness raciness chomp seize_with_teeth burn prick', 'his', 'forehead brow frontal_bone os_frontale', 'he helium He atomic_number_2', 'caught catch pick_up get grab take_hold_of capture hitch arrest overtake catch_up_with take_in overhear watch view see trip_up enamour trance becharm enamor captivate beguile charm fascinate bewitch entrance enchant', 'fever febrility febricity pyrexia feverishness', 'was Washington Evergreen_State WA be exist equal constitute represent make_up comprise follow embody personify live cost', 'confused confuse confound throw fox befuddle fuddle bedevil discombobulate flurry disconcert put_off jumble mix_up blur obscure obnubilate baffled befuddled bemused bewildered confounded lost mazed mixed-up at_sea disconnected disjointed disordered garbled illogical scattered unconnected disoriented broken upset', 'about astir approximately close_to just_about some roughly more_or_less around or_so almost most nearly near nigh virtually well-nigh', 'his', 'surroundings milieu environment environs surround environ ring skirt border smother besiege beleaguer hem_in circumvent wall palisade fence fence_in', 'he helium He atomic_number_2', 'experiences experience see go_through know live receive have get feel', 'vision sight visual_sense visual_modality visual_sensation imagination imaginativeness', 'loss deprivation red_ink red personnel_casualty passing departure exit expiration going release', 'fall autumn spill tumble Fall descent declivity decline declination declension downslope downfall capitulation surrender twilight dusk gloaming gloam nightfall evenfall crepuscule crepuscle pin drop dip free_fall descend go_down come_down come precipitate shine strike decrease diminish lessen accrue light return pass devolve fall_down hang flow settle', 'into', 'occassional', 'comas coma comatoseness', 'what', 'should', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'do bash brawl doh ut Doctor_of_Osteopathy DO make perform execute fare make_out come get_along cause practice practise exercise suffice answer serve act behave manage dress arrange set coif coiffe coiffure', 'about astir approximately close_to just_about some roughly more_or_less around or_so almost most nearly near nigh virtually well-nigh', 'him', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'have rich_person wealthy_person have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'minor child kid youngster shaver nipper small_fry tiddler tike tyke fry nestling nonaged underage venial modest small small-scale pocket-size pocket-sized', 'headache concern worry vexation head_ache cephalalgia', 'vomiting vomit emesis regurgitation disgorgement puking vomit_up purge cast sick cat be_sick disgorge regorge retch puke barf spew spue chuck upchuck honk regurgitate throw_up', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'saw proverb adage byword power_saw sawing_machine see understand realize realise witness find visualize visualise envision project fancy figure picture image consider reckon view regard learn hear get_word get_wind pick_up find_out get_a_line discover watch catch take_in meet run_into encounter run_across come_across determine check ascertain insure see_to_it ensure control assure visit attend take_care look go_steady go_out date examine experience go_through escort interpret construe', 'red redness Red Red_River Bolshevik Marxist bolshie bolshy loss red_ink reddish ruddy blood-red carmine cerise cherry cherry-red crimson ruby ruby-red scarlet violent reddened red-faced flushed', 'rashes rash roseola efflorescence skin_rash blizzard', 'all wholly entirely completely totally altogether whole', \"over complete concluded ended all_over terminated o'er\", 'my', 'back dorsum rear spinal_column vertebral_column spine backbone rachis binding book_binding cover backrest endorse indorse plump_for plunk_for support second bet_on gage stake game punt back_up hind hinder backward backwards rearward rearwards', 'chest thorax pectus breast chest_of_drawers bureau dresser', 'stomach tummy tum breadbasket abdomen venter belly digest endure stick_out bear stand tolerate support brook abide suffer put_up']\n",
      "CLEAN SYNONYMS:  ['i iodine iodin I one ace single unity ane', 'visited visit see travel to call in call inspect inflict bring down impose chew the fat shoot the breeze chat confabulate confab chitchat chitchat chatter chaffer natter gossip jaw claver', 'jungle hobo camp', 'few', 'days years day twentyfour hours twentyfour hour period solar day mean solar day daytime daylight sidereal day Day Clarence Day Clarence Shepard Day Jr', 'ago agone', 'i iodine iodin I one ace single unity ane', 'got get acquire become go let have receive find obtain incur arrive come bring convey fetch experience pay back pay off fix make induce stimulate cause catch capture grow develop produce contract take drive aim arrest scram buzz off fuck off bugger off get under ones skin draw perplex vex stick puzzle mystify baffle beat pose bewilder flummox stupefy nonplus gravel amaze dumbfound get down begin start out start set about set out commence suffer sustain beget engender father mother sire generate bring forth', 'strange unusual unknown foreign', 'bite morsel bit sting insect bite collation snack pungency sharpness raciness chomp seize with teeth burn prick', 'because', 'it information technology IT', 'didnt', 'go spell tour turn Adam ecstasy XTC disco biscuit cristal X hug drug crack fling pass whirl offer go game travel move locomote proceed go away depart become get run lead extend sound function work operate run low run short survive last live live on endure hold up hold out die decease perish exit pass away expire kick the bucket cash in ones chips buy the farm conk giveup the ghost drop dead pop off choke croak snuff it belong start get going blend blend in fit rifle plump fail go bad give way give out conk out break break down', 'away outside off forth out aside by', 'few', 'days years day twentyfour hours twentyfour hour period solar day mean solar day daytime daylight sidereal day Day Clarence Day Clarence Shepard Day Jr', 'later ulterior posterior late belated tardy recent former previous subsequently afterwards afterward after later on by and by', 'i iodine iodin I one ace single unity ane', 'began get down begin get start out start set about set out commence lead off', 'develop evolve germinate acquire grow produce get originate arise rise uprise spring up build up explicate formulate train prepare educate modernize modernise make grow break recrudesce', 'fever febrility febricity pyrexia feverishness', 'muscle musculus muscular tissue muscleman brawn brawniness muscularity sinew heftiness', 'weakness failing helplessness impuissance', 'what', 'should', 'i iodine iodin I one ace single unity ane', 'do bash brawl doh ut Doctor of Osteopathy DO make perform execute fare make out come get along cause practice practise exercise suffice answer serve act behave manage dress arrange set coif coiffe coiffure', 'i iodine iodin I one ace single unity ane', 'discovered detect observe find discover notice learn hear get word get wind pick up find out get a line see fall upon strike come upon light upon chance upon come across chance on happen upon attain unwrap disclose let on bring out reveal expose divulge break give away let out identify key key out distinguish describe name ascertained observed', 'weird Wyrd Weird eldritch uncanny unearthly', 'mosquito', 'bite morsel bit sting insect bite collation snack pungency sharpness raciness chomp seize with teeth burn prick', 'my', 'hand manus mitt paw hired hand hired man handwriting script deal bridge player helping hand pass reach pass on turn over give', 'that', 'does Department of Energy Energy Department Energy DOE doe make do perform execute fare make out come get along cause practice practise exercise suffice answer serve act behave manage dress arrange set coif coiffe coiffure', 'not non', 'go spell tour turn Adam ecstasy XTC disco biscuit cristal X hug drug crack fling pass whirl offer go game travel move locomote proceed go away depart become get run lead extend sound function work operate run low run short survive last live live on endure hold up hold out die decease perish exit pass away expire kick the bucket cash in ones chips buy the farm conk giveup the ghost drop dead pop off choke croak snuff it belong start get going blend blend in fit rifle plump fail go bad give way give out conk out break break down', 'away outside off forth out aside by', 'instead alternatively or else rather', 'it information technology IT', 'swell crestless wave dandy dude fop gallant sheik beau fashion plate clotheshorse puff up swell up intumesce tumefy tumesce well up well bangup bully corking cracking great groovy keen neat nifty not bad peachy slapup smashing', 'my', 'hand manus mitt paw hired hand hired man handwriting script deal bridge player helping hand pass reach pass on turn over give', 'numb benumb blunt dull asleep benumbed dead', 'my', 'roommate roomie roomy', 'has hour angle HA have have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'weird Wyrd Weird eldritch uncanny unearthly', 'bite morsel bit sting insect bite collation snack pungency sharpness raciness chomp seize with teeth burn prick', 'his', 'forehead brow frontal bone os frontale', 'he helium He', 'caught catch pick up get grab take hold of capture hitch arrest overtake catch up with take in overhear watch view see trip up enamour trance becharm enamor captivate beguile charm fascinate bewitch entrance enchant', 'fever febrility febricity pyrexia feverishness', 'was Washington Evergreen State WA be exist equal constitute represent make up comprise follow embody personify live cost', 'confused confuse confound throw fox befuddle fuddle bedevil discombobulate flurry disconcert put off jumble mix up blur obscure obnubilate baffled befuddled bemused bewildered confounded lost mazed mixedup at sea disconnected disjointed disordered garbled illogical scattered unconnected disoriented broken upset', 'about astir approximately close to just about some roughly more or less around or so almost most nearly near nigh virtually wellnigh', 'his', 'surroundings milieu environment environs surround environ ring skirt border smother besiege beleaguer hem in circumvent wall palisade fence fence in', 'he helium He', 'experiences experience see go through know live receive have get feel', 'vision sight visual sense visual modality visual sensation imagination imaginativeness', 'loss deprivation red ink red personnel casualty passing departure exit expiration going release', 'fall autumn spill tumble Fall descent declivity decline declination declension downslope downfall capitulation surrender twilight dusk gloaming gloam nightfall evenfall crepuscule crepuscle pin drop dip free fall descend go down come down come precipitate shine strike decrease diminish lessen accrue light return pass devolve fall down hang flow settle', 'into', 'occassional', 'comas coma comatoseness', 'what', 'should', 'i iodine iodin I one ace single unity ane', 'do bash brawl doh ut Doctor of Osteopathy DO make perform execute fare make out come get along cause practice practise exercise suffice answer serve act behave manage dress arrange set coif coiffe coiffure', 'about astir approximately close to just about some roughly more or less around or so almost most nearly near nigh virtually wellnigh', 'him', 'i iodine iodin I one ace single unity ane', 'have rich person wealthy person have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'minor child kid youngster shaver nipper small fry tiddler tike tyke fry nestling nonaged underage venial modest small smallscale pocketsize pocketsized', 'headache concern worry vexation head ache cephalalgia', 'vomiting vomit emesis regurgitation disgorgement puking vomit up purge cast sick cat be sick disgorge regorge retch puke barf spew spue chuck upchuck honk regurgitate throw up', 'i iodine iodin I one ace single unity ane', 'saw proverb adage byword power saw sawing machine see understand realize realise witness find visualize visualise envision project fancy figure picture image consider reckon view regard learn hear get word get wind pick up find out get a line discover watch catch take in meet run into encounter run across come across determine check ascertain insure see to it ensure control assure visit attend take care look go steady go out date examine experience go through escort interpret construe', 'red redness Red Red River Bolshevik Marxist bolshie bolshy loss red ink reddish ruddy bloodred carmine cerise cherry cherryred crimson ruby rubyred scarlet violent reddened redfaced flushed', 'rashes rash roseola efflorescence skin rash blizzard', 'all wholly entirely completely totally altogether whole', 'over complete concluded ended all over terminated oer', 'my', 'back dorsum rear spinal column vertebral column spine backbone rachis binding book binding cover backrest endorse indorse plump for plunk for support second bet on gage stake game punt back up hind hinder backward backwards rearward rearwards', 'chest thorax pectus breast chest of drawers bureau dresser', 'stomach tummy tum breadbasket abdomen venter belly digest endure stick out bear stand tolerate support brook abide suffer put up']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD_LIST: ['hey there. i have been feeling extremely bad lately, with a fever developing the past few days', 'i have been getting lot of blisters and rashes on my body recently. my mouth has also been very sore in a lot of places and there has been itchiness all over my body', \"i went to my friend's house last night and ended up developing a fever and red spots al over my body. i couldn't even eat anything because i had no appetite, and started getting itchy all over my body\", 'my mouth has been feeling bad lately, and i never feel like eating. my muscles have also been more sore lately even though i have not been to the gym, and my skin has become more itchy recently', 'blisters and rashes have appeared all over my body recently, and they have all started to itch. my arms and hamstrings are also starting to ache and i believe i may be developing a fever because my temperature is high from the last measurement', 'red spots are appealing all over my arm, and they are feeling quite uncomfortable. just this morning, i ended up developing a fever and my mouth has felt very sore and uncomfortable', 'my friend gabrielle and i went to the the movies and i ended up developing rashes all over my body which turned into itchy spots. later that night, i also measured my temperature and had developed a fever. this morning, i cannot even move well because my muscles are aching greatly.']\n",
      "WORD_LIST_NEW: ['hey there i have been feeling extremely bad lately with fever developing past few days', 'i have been getting of blisters rashes my body recently my mouth has also been very sore a of places there has been itchiness all over my body', 'i went my friends house last night ended up developing fever red spots al over my body i couldnt even eat anything because i had no appetite started getting itchy all over my body', 'my mouth has been feeling bad lately i never feel like eating my muscles have also been more sore lately even though i have not been the gym my skin has become more itchy recently', 'blisters rashes have appeared all over my body recently they have all started itch my arms hamstrings also starting ache i believe i may be developing fever because my temperature high the last measurement', 'red spots appealing all over my arm they feeling quite uncomfortable just this morning i ended up developing fever my mouth has felt very sore uncomfortable', 'my friend gabrielle i went the movies i ended up developing rashes all over my body which turned into itchy spots later that night i also measured my temperature had developed fever this morning i cannot even move well because my muscles aching greatly']\n",
      "joined_symps: hey there i have been feeling extremely bad lately with fever developing past few days i have been getting of blisters rashes my body recently my mouth has also been very sore a of places there has been itchiness all over my body i went my friends house last night ended up developing fever red spots al over my body i couldnt even eat anything because i had no appetite started getting itchy all over my body my mouth has been feeling bad lately i never feel like eating my muscles have also been more sore lately even though i have not been the gym my skin has become more itchy recently blisters rashes have appeared all over my body recently they have all started itch my arms hamstrings also starting ache i believe i may be developing fever because my temperature high the last measurement red spots appealing all over my arm they feeling quite uncomfortable just this morning i ended up developing fever my mouth has felt very sore uncomfortable my friend gabrielle i went the movies i ended up developing rashes all over my body which turned into itchy spots later that night i also measured my temperature had developed fever this morning i cannot even move well because my muscles aching greatly\n",
      "SYNONYMS: ['hey', 'there at_that_place in_that_location in_that_respect on_that_point thither', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'have rich_person wealthy_person have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'been be exist equal constitute represent make_up comprise follow embody personify live cost', 'feeling impression belief notion opinion spirit tone feel flavor flavour look smell touch touch_sensation tactual_sensation tactile_sensation intuitive_feeling experience find sense finger palpate', 'extremely highly exceedingly super passing', 'bad badness big tough spoiled spoilt regretful sorry uncollectible risky high-risk speculative unfit unsound forged defective badly', 'lately recently late of_late latterly', 'with', 'fever febrility febricity pyrexia feverishness', 'developing development develop evolve germinate acquire grow produce get originate arise rise uprise spring_up build_up explicate formulate train prepare educate modernize modernise make_grow break recrudesce underdeveloped', 'past past_times yesteryear past_tense preceding retiring by', 'few', 'days years day twenty-four_hours twenty-four_hour_period 24-hour_interval solar_day mean_solar_day daytime daylight sidereal_day Day Clarence_Day Clarence_Shepard_Day_Jr.', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'have rich_person wealthy_person have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'been be exist equal constitute represent make_up comprise follow embody personify live cost', \"getting acquiring get acquire become go let have receive find obtain incur arrive come bring convey fetch experience pay_back pay_off fix make induce stimulate cause catch capture grow develop produce contract take drive aim arrest scram buzz_off fuck_off bugger_off get_under_one's_skin draw perplex vex stick puzzle mystify baffle beat pose bewilder flummox stupefy nonplus gravel amaze dumbfound get_down begin start_out start set_about set_out commence suffer sustain beget engender father mother sire generate bring_forth\", 'of', 'blisters blister bulla bleb vesicate scald whip', 'rashes rash roseola efflorescence skin_rash blizzard', 'my', 'body organic_structure physical_structure dead_body torso trunk consistency consistence eubstance soundbox personify', 'recently late lately of_late latterly', 'my', 'mouth oral_cavity oral_fissure rima_oris mouthpiece sass sassing backtalk back_talk lip talk speak utter verbalize verbalise', 'has hour_angle HA have have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'also besides too likewise as_well', 'been be exist equal constitute represent make_up comprise follow embody personify live cost', 'very identical selfsame really real rattling', 'sore sensitive raw tender afflictive painful huffy mad', 'a angstrom angstrom_unit A vitamin_A antiophthalmic_factor axerophthol deoxyadenosine_monophosphate adenine ampere amp type_A group_A', 'of', 'places topographic_point place spot property stead position lieu shoes home post berth office billet situation station seat plaza piazza space blank_space put set pose lay rate rank range order grade locate site come_in come_out target aim direct point identify localize localise invest commit send', 'there at_that_place in_that_location in_that_respect on_that_point thither', 'has hour_angle HA have have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'been be exist equal constitute represent make_up comprise follow embody personify live cost', 'itchiness itch itching', 'all wholly entirely completely totally altogether whole', \"over complete concluded ended all_over terminated o'er\", 'my', 'body organic_structure physical_structure dead_body torso trunk consistency consistence eubstance soundbox personify', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', \"went travel go move locomote proceed go_away depart become get run pass lead extend sound function work operate run_low run_short survive last live live_on endure hold_up hold_out die decease perish exit pass_away expire kick_the_bucket cash_in_one's_chips buy_the_farm conk give-up_the_ghost drop_dead pop_off choke croak snuff_it belong start get_going blend blend_in fit rifle plump fail go_bad give_way give_out conk_out break break_down\", 'my', 'friends friend ally acquaintance supporter protagonist champion admirer booster Friend Quaker', 'house firm business_firm sign_of_the_zodiac star_sign sign mansion planetary_house family household home menage theater theatre put_up domiciliate', \"last stopping_point finale finis finish conclusion close death end final_stage shoemaker's_last cobbler's_last endure survive live live_on go hold_up hold_out concluding final terminal net utmost last-place lowest lastly in_conclusion finally\", 'night nighttime dark Nox Night', 'ended end stop finish terminate cease complete concluded over all_over terminated', 'up astir improving upward upwards upwardly', 'developing development develop evolve germinate acquire grow produce get originate arise rise uprise spring_up build_up explicate formulate train prepare educate modernize modernise make_grow break recrudesce underdeveloped', 'fever febrility febricity pyrexia feverishness', 'red redness Red Red_River Bolshevik Marxist bolshie bolshy loss red_ink reddish ruddy blood-red carmine cerise cherry cherry-red crimson ruby ruby-red scarlet violent reddened red-faced flushed', 'spots musca_volitans muscae_volitantes floater topographic_point place spot point smudge blot daub smear smirch slur speckle dapple patch fleck maculation position post berth office billet situation touch bit pip spotlight stain descry espy spy recognize recognise distinguish discern pick_out make_out tell_apart blemish blob', 'al aluminum aluminium Al atomic_number_13 Alabama Heart_of_Dixie Camellia_State AL', \"over complete concluded ended all_over terminated o'er\", 'my', 'body organic_structure physical_structure dead_body torso trunk consistency consistence eubstance soundbox personify', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'couldnt', 'even evening eve eventide flush level even_out fifty-fifty regular tied yet still', 'eat feed eat_on consume eat_up use_up deplete exhaust run_through wipe_out corrode rust', 'anything', 'because', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'had have have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'no nobelium No atomic_number_102 no_more', 'appetite appetency appetence', 'started get_down begin get start_out start set_about set_out commence lead_off depart part set_forth set_off take_off originate initiate start_up embark_on startle jump go get_going take_up protrude pop pop_out bulge bulge_out bug_out come_out', \"getting acquiring get acquire become go let have receive find obtain incur arrive come bring convey fetch experience pay_back pay_off fix make induce stimulate cause catch capture grow develop produce contract take drive aim arrest scram buzz_off fuck_off bugger_off get_under_one's_skin draw perplex vex stick puzzle mystify baffle beat pose bewilder flummox stupefy nonplus gravel amaze dumbfound get_down begin start_out start set_about set_out commence suffer sustain beget engender father mother sire generate bring_forth\", 'itchy antsy fidgety fretful', 'all wholly entirely completely totally altogether whole', \"over complete concluded ended all_over terminated o'er\", 'my', 'body organic_structure physical_structure dead_body torso trunk consistency consistence eubstance soundbox personify', 'my', 'mouth oral_cavity oral_fissure rima_oris mouthpiece sass sassing backtalk back_talk lip talk speak utter verbalize verbalise', 'has hour_angle HA have have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'been be exist equal constitute represent make_up comprise follow embody personify live cost', 'feeling impression belief notion opinion spirit tone feel flavor flavour look smell touch touch_sensation tactual_sensation tactile_sensation intuitive_feeling experience find sense finger palpate', 'bad badness big tough spoiled spoilt regretful sorry uncollectible risky high-risk speculative unfit unsound forged defective badly', 'lately recently late of_late latterly', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', \"never ne'er\", 'feel spirit tone feeling flavor flavour look smell tactile_property experience find sense finger palpate', 'like the_like the_likes_of ilk wish care similar same alike comparable corresponding', 'eating feeding eat feed eat_on consume eat_up use_up deplete exhaust run_through wipe_out corrode rust', 'my', 'muscles muscle musculus muscular_tissue muscleman brawn brawniness muscularity sinew heftiness', 'have rich_person wealthy_person have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'also besides too likewise as_well', 'been be exist equal constitute represent make_up comprise follow embody personify live cost', 'more More Thomas_More Sir_Thomas_More more_than to_a_greater_extent', 'sore sensitive raw tender afflictive painful huffy mad', 'lately recently late of_late latterly', 'even evening eve eventide flush level even_out fifty-fifty regular tied yet still', 'though', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'have rich_person wealthy_person have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'not non', 'been be exist equal constitute represent make_up comprise follow embody personify live cost', 'the', 'gym gymnasium', 'my', 'skin tegument cutis hide pelt peel clamber scramble shin shinny struggle sputter scrape bark pare', 'has hour_angle HA have have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'become go get turn suit', 'more More Thomas_More Sir_Thomas_More more_than to_a_greater_extent', 'itchy antsy fidgety fretful', 'recently late lately of_late latterly', 'blisters blister bulla bleb vesicate scald whip', 'rashes rash roseola efflorescence skin_rash blizzard', 'have rich_person wealthy_person have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'appeared look appear seem come_out come_along', 'all wholly entirely completely totally altogether whole', \"over complete concluded ended all_over terminated o'er\", 'my', 'body organic_structure physical_structure dead_body torso trunk consistency consistence eubstance soundbox personify', 'recently late lately of_late latterly', 'they', 'have rich_person wealthy_person have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'all wholly entirely completely totally altogether whole', 'started get_down begin get start_out start set_about set_out commence lead_off depart part set_forth set_off take_off originate initiate start_up embark_on startle jump go get_going take_up protrude pop pop_out bulge bulge_out bug_out come_out', 'itch scabies urge itchiness itching rub scratch spoil', 'my', 'arms weaponry implements_of_war weapons_system munition coat_of_arms blazon blazonry arm branch limb weapon weapon_system subdivision sleeve build_up fortify gird', 'hamstrings hamstring hamstring_tendon', 'also besides too likewise as_well', 'starting start get_down begin get start_out set_about set_out commence lead_off depart part set_forth set_off take_off originate initiate start_up embark_on startle jump go get_going take_up protrude pop pop_out bulge bulge_out bug_out come_out', 'ache aching hurt suffer yearn yen pine languish smart', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'believe think consider conceive trust', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'may May whitethorn English_hawthorn Crataegus_laevigata Crataegus_oxycantha', 'be beryllium Be glucinium atomic_number_4 exist equal constitute represent make_up comprise follow embody personify live cost', 'developing development develop evolve germinate acquire grow produce get originate arise rise uprise spring_up build_up explicate formulate train prepare educate modernize modernise make_grow break recrudesce underdeveloped', 'fever febrility febricity pyrexia feverishness', 'because', 'my', 'temperature', 'high heights senior_high_school senior_high highschool high_school high_gear eminent high-pitched in_high_spirits gamey gamy mellow high_up richly luxuriously', 'the', \"last stopping_point finale finis finish conclusion close death end final_stage shoemaker's_last cobbler's_last endure survive live live_on go hold_up hold_out concluding final terminal net utmost last-place lowest lastly in_conclusion finally\", 'measurement measuring measure mensuration', 'red redness Red Red_River Bolshevik Marxist bolshie bolshy loss red_ink reddish ruddy blood-red carmine cerise cherry cherry-red crimson ruby ruby-red scarlet violent reddened red-faced flushed', 'spots musca_volitans muscae_volitantes floater topographic_point place spot point smudge blot daub smear smirch slur speckle dapple patch fleck maculation position post berth office billet situation touch bit pip spotlight stain descry espy spy recognize recognise distinguish discern pick_out make_out tell_apart blemish blob', 'appealing appeal invoke attract sympathetic likeable likable', 'all wholly entirely completely totally altogether whole', \"over complete concluded ended all_over terminated o'er\", 'my', 'arm branch limb weapon weapon_system subdivision sleeve build_up fortify gird', 'they', 'feeling impression belief notion opinion spirit tone feel flavor flavour look smell touch touch_sensation tactual_sensation tactile_sensation intuitive_feeling experience find sense finger palpate', 'quite rather quite_a quite_an', 'uncomfortable', 'just equitable fair good upright merely simply only but precisely exactly just_now barely hardly scarcely scarce', 'this', 'morning morn morning_time forenoon good_morning dawn dawning aurora first_light daybreak break_of_day break_of_the_day dayspring sunrise sunup cockcrow', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'ended end stop finish terminate cease complete concluded over all_over terminated', 'up astir improving upward upwards upwardly', 'developing development develop evolve germinate acquire grow produce get originate arise rise uprise spring_up build_up explicate formulate train prepare educate modernize modernise make_grow break recrudesce underdeveloped', 'fever febrility febricity pyrexia feverishness', 'my', 'mouth oral_cavity oral_fissure rima_oris mouthpiece sass sassing backtalk back_talk lip talk speak utter verbalize verbalise', 'has hour_angle HA have have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'felt felt_up mat_up matt-up matte_up matte mat feel experience find sense finger palpate', 'very identical selfsame really real rattling', 'sore sensitive raw tender afflictive painful huffy mad', 'uncomfortable', 'my', 'friend ally acquaintance supporter protagonist champion admirer booster Friend Quaker', 'gabrielle', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', \"went travel go move locomote proceed go_away depart become get run pass lead extend sound function work operate run_low run_short survive last live live_on endure hold_up hold_out die decease perish exit pass_away expire kick_the_bucket cash_in_one's_chips buy_the_farm conk give-up_the_ghost drop_dead pop_off choke croak snuff_it belong start get_going blend blend_in fit rifle plump fail go_bad give_way give_out conk_out break break_down\", 'the', 'movies movie film picture moving_picture moving-picture_show motion_picture motion-picture_show picture_show pic flick', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'ended end stop finish terminate cease complete concluded over all_over terminated', 'up astir improving upward upwards upwardly', 'developing development develop evolve germinate acquire grow produce get originate arise rise uprise spring_up build_up explicate formulate train prepare educate modernize modernise make_grow break recrudesce underdeveloped', 'rashes rash roseola efflorescence skin_rash blizzard', 'all wholly entirely completely totally altogether whole', \"over complete concluded ended all_over terminated o'er\", 'my', 'body organic_structure physical_structure dead_body torso trunk consistency consistence eubstance soundbox personify', 'which', 'turned turn change_state become change_by_reversal reverse move_around grow release turn_over plow plough twist sprain wrench wrick rick flex bend deform call_on sour ferment work off', 'into', 'itchy antsy fidgety fretful', 'spots musca_volitans muscae_volitantes floater topographic_point place spot point smudge blot daub smear smirch slur speckle dapple patch fleck maculation position post berth office billet situation touch bit pip spotlight stain descry espy spy recognize recognise distinguish discern pick_out make_out tell_apart blemish blob', 'later ulterior posterior late belated tardy recent former previous subsequently afterwards afterward after later_on by_and_by', 'that', 'night nighttime dark Nox Night', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'also besides too likewise as_well', 'measured measure mensurate measure_out quantify evaluate valuate assess appraise value mensural mensurable metrical metric deliberate calculated careful', 'my', 'temperature', 'had have have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'developed develop evolve germinate acquire grow produce get originate arise rise uprise spring_up build_up explicate formulate train prepare educate modernize modernise make_grow break recrudesce highly-developed', 'fever febrility febricity pyrexia feverishness', 'this', 'morning morn morning_time forenoon good_morning dawn dawning aurora first_light daybreak break_of_day break_of_the_day dayspring sunrise sunup cockcrow', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'cannot', 'even evening eve eventide flush level even_out fifty-fifty regular tied yet still', 'move relocation motion movement motility travel go locomote displace proceed be_active act affect impress strike motivate actuate propel prompt incite run make_a_motion', 'well wellspring fountainhead swell good easily considerably substantially intimately advantageously comfortably', 'because', 'my', 'muscles muscle musculus muscular_tissue muscleman brawn brawniness muscularity sinew heftiness', 'aching ache hurt suffer yearn yen pine languish smart achy', 'greatly']\n",
      "CLEAN SYNONYMS:  ['hey', 'there at that place in that location in that respect on that point thither', 'i iodine iodin I one ace single unity ane', 'have rich person wealthy person have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'been be exist equal constitute represent make up comprise follow embody personify live cost', 'feeling impression belief notion opinion spirit tone feel flavor flavour look smell touch touch sensation tactual sensation tactile sensation intuitive feeling experience find sense finger palpate', 'extremely highly exceedingly super passing', 'bad badness big tough spoiled spoilt regretful sorry uncollectible risky highrisk speculative unfit unsound forged defective badly', 'lately recently late of late latterly', 'with', 'fever febrility febricity pyrexia feverishness', 'developing development develop evolve germinate acquire grow produce get originate arise rise uprise spring up build up explicate formulate train prepare educate modernize modernise make grow break recrudesce underdeveloped', 'past past times yesteryear past tense preceding retiring by', 'few', 'days years day twentyfour hours twentyfour hour period solar day mean solar day daytime daylight sidereal day Day Clarence Day Clarence Shepard Day Jr', 'i iodine iodin I one ace single unity ane', 'have rich person wealthy person have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'been be exist equal constitute represent make up comprise follow embody personify live cost', 'getting acquiring get acquire become go let have receive find obtain incur arrive come bring convey fetch experience pay back pay off fix make induce stimulate cause catch capture grow develop produce contract take drive aim arrest scram buzz off fuck off bugger off get under ones skin draw perplex vex stick puzzle mystify baffle beat pose bewilder flummox stupefy nonplus gravel amaze dumbfound get down begin start out start set about set out commence suffer sustain beget engender father mother sire generate bring forth', 'of', 'blisters blister bulla bleb vesicate scald whip', 'rashes rash roseola efflorescence skin rash blizzard', 'my', 'body organic structure physical structure dead body torso trunk consistency consistence eubstance soundbox personify', 'recently late lately of late latterly', 'my', 'mouth oral cavity oral fissure rima oris mouthpiece sass sassing backtalk back talk lip talk speak utter verbalize verbalise', 'has hour angle HA have have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'also besides too likewise as well', 'been be exist equal constitute represent make up comprise follow embody personify live cost', 'very identical selfsame really real rattling', 'sore sensitive raw tender afflictive painful huffy mad', 'a angstrom angstrom unit A vitamin A antiophthalmic factor axerophthol deoxyadenosine monophosphate adenine ampere amp type A group A', 'of', 'places topographic point place spot property stead position lieu shoes home post berth office billet situation station seat plaza piazza space blank space put set pose lay rate rank range order grade locate site come in come out target aim direct point identify localize localise invest commit send', 'there at that place in that location in that respect on that point thither', 'has hour angle HA have have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'been be exist equal constitute represent make up comprise follow embody personify live cost', 'itchiness itch itching', 'all wholly entirely completely totally altogether whole', 'over complete concluded ended all over terminated oer', 'my', 'body organic structure physical structure dead body torso trunk consistency consistence eubstance soundbox personify', 'i iodine iodin I one ace single unity ane', 'went travel go move locomote proceed go away depart become get run pass lead extend sound function work operate run low run short survive last live live on endure hold up hold out die decease perish exit pass away expire kick the bucket cash in ones chips buy the farm conk giveup the ghost drop dead pop off choke croak snuff it belong start get going blend blend in fit rifle plump fail go bad give way give out conk out break break down', 'my', 'friends friend ally acquaintance supporter protagonist champion admirer booster Friend Quaker', 'house firm business firm sign of the zodiac star sign sign mansion planetary house family household home menage theater theatre put up domiciliate', 'last stopping point finale finis finish conclusion close death end final stage shoemakers last cobblers last endure survive live live on go hold up hold out concluding final terminal net utmost lastplace lowest lastly in conclusion finally', 'night nighttime dark Nox Night', 'ended end stop finish terminate cease complete concluded over all over terminated', 'up astir improving upward upwards upwardly', 'developing development develop evolve germinate acquire grow produce get originate arise rise uprise spring up build up explicate formulate train prepare educate modernize modernise make grow break recrudesce underdeveloped', 'fever febrility febricity pyrexia feverishness', 'red redness Red Red River Bolshevik Marxist bolshie bolshy loss red ink reddish ruddy bloodred carmine cerise cherry cherryred crimson ruby rubyred scarlet violent reddened redfaced flushed', 'spots musca volitans muscae volitantes floater topographic point place spot point smudge blot daub smear smirch slur speckle dapple patch fleck maculation position post berth office billet situation touch bit pip spotlight stain descry espy spy recognize recognise distinguish discern pick out make out tell apart blemish blob', 'al aluminum aluminium Al Alabama Heart of Dixie Camellia State AL', 'over complete concluded ended all over terminated oer', 'my', 'body organic structure physical structure dead body torso trunk consistency consistence eubstance soundbox personify', 'i iodine iodin I one ace single unity ane', 'couldnt', 'even evening eve eventide flush level even out fiftyfifty regular tied yet still', 'eat feed eat on consume eat up use up deplete exhaust run through wipe out corrode rust', 'anything', 'because', 'i iodine iodin I one ace single unity ane', 'had have have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'no nobelium No no more', 'appetite appetency appetence', 'started get down begin get start out start set about set out commence lead off depart part set forth set off take off originate initiate start up embark on startle jump go get going take up protrude pop pop out bulge bulge out bug out come out', 'getting acquiring get acquire become go let have receive find obtain incur arrive come bring convey fetch experience pay back pay off fix make induce stimulate cause catch capture grow develop produce contract take drive aim arrest scram buzz off fuck off bugger off get under ones skin draw perplex vex stick puzzle mystify baffle beat pose bewilder flummox stupefy nonplus gravel amaze dumbfound get down begin start out start set about set out commence suffer sustain beget engender father mother sire generate bring forth', 'itchy antsy fidgety fretful', 'all wholly entirely completely totally altogether whole', 'over complete concluded ended all over terminated oer', 'my', 'body organic structure physical structure dead body torso trunk consistency consistence eubstance soundbox personify', 'my', 'mouth oral cavity oral fissure rima oris mouthpiece sass sassing backtalk back talk lip talk speak utter verbalize verbalise', 'has hour angle HA have have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'been be exist equal constitute represent make up comprise follow embody personify live cost', 'feeling impression belief notion opinion spirit tone feel flavor flavour look smell touch touch sensation tactual sensation tactile sensation intuitive feeling experience find sense finger palpate', 'bad badness big tough spoiled spoilt regretful sorry uncollectible risky highrisk speculative unfit unsound forged defective badly', 'lately recently late of late latterly', 'i iodine iodin I one ace single unity ane', 'never neer', 'feel spirit tone feeling flavor flavour look smell tactile property experience find sense finger palpate', 'like the like the likes of ilk wish care similar same alike comparable corresponding', 'eating feeding eat feed eat on consume eat up use up deplete exhaust run through wipe out corrode rust', 'my', 'muscles muscle musculus muscular tissue muscleman brawn brawniness muscularity sinew heftiness', 'have rich person wealthy person have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'also besides too likewise as well', 'been be exist equal constitute represent make up comprise follow embody personify live cost', 'more More Thomas More Sir Thomas More more than to a greater extent', 'sore sensitive raw tender afflictive painful huffy mad', 'lately recently late of late latterly', 'even evening eve eventide flush level even out fiftyfifty regular tied yet still', 'though', 'i iodine iodin I one ace single unity ane', 'have rich person wealthy person have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'not non', 'been be exist equal constitute represent make up comprise follow embody personify live cost', 'the', 'gym gymnasium', 'my', 'skin tegument cutis hide pelt peel clamber scramble shin shinny struggle sputter scrape bark pare', 'has hour angle HA have have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'become go get turn suit', 'more More Thomas More Sir Thomas More more than to a greater extent', 'itchy antsy fidgety fretful', 'recently late lately of late latterly', 'blisters blister bulla bleb vesicate scald whip', 'rashes rash roseola efflorescence skin rash blizzard', 'have rich person wealthy person have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'appeared look appear seem come out come along', 'all wholly entirely completely totally altogether whole', 'over complete concluded ended all over terminated oer', 'my', 'body organic structure physical structure dead body torso trunk consistency consistence eubstance soundbox personify', 'recently late lately of late latterly', 'they', 'have rich person wealthy person have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'all wholly entirely completely totally altogether whole', 'started get down begin get start out start set about set out commence lead off depart part set forth set off take off originate initiate start up embark on startle jump go get going take up protrude pop pop out bulge bulge out bug out come out', 'itch scabies urge itchiness itching rub scratch spoil', 'my', 'arms weaponry implements of war weapons system munition coat of arms blazon blazonry arm branch limb weapon weapon system subdivision sleeve build up fortify gird', 'hamstrings hamstring hamstring tendon', 'also besides too likewise as well', 'starting start get down begin get start out set about set out commence lead off depart part set forth set off take off originate initiate start up embark on startle jump go get going take up protrude pop pop out bulge bulge out bug out come out', 'ache aching hurt suffer yearn yen pine languish smart', 'i iodine iodin I one ace single unity ane', 'believe think consider conceive trust', 'i iodine iodin I one ace single unity ane', 'may May whitethorn English hawthorn Crataegus laevigata Crataegus oxycantha', 'be beryllium Be glucinium exist equal constitute represent make up comprise follow embody personify live cost', 'developing development develop evolve germinate acquire grow produce get originate arise rise uprise spring up build up explicate formulate train prepare educate modernize modernise make grow break recrudesce underdeveloped', 'fever febrility febricity pyrexia feverishness', 'because', 'my', 'temperature', 'high heights senior high school senior high highschool high school high gear eminent highpitched in high spirits gamey gamy mellow high up richly luxuriously', 'the', 'last stopping point finale finis finish conclusion close death end final stage shoemakers last cobblers last endure survive live live on go hold up hold out concluding final terminal net utmost lastplace lowest lastly in conclusion finally', 'measurement measuring measure mensuration', 'red redness Red Red River Bolshevik Marxist bolshie bolshy loss red ink reddish ruddy bloodred carmine cerise cherry cherryred crimson ruby rubyred scarlet violent reddened redfaced flushed', 'spots musca volitans muscae volitantes floater topographic point place spot point smudge blot daub smear smirch slur speckle dapple patch fleck maculation position post berth office billet situation touch bit pip spotlight stain descry espy spy recognize recognise distinguish discern pick out make out tell apart blemish blob', 'appealing appeal invoke attract sympathetic likeable likable', 'all wholly entirely completely totally altogether whole', 'over complete concluded ended all over terminated oer', 'my', 'arm branch limb weapon weapon system subdivision sleeve build up fortify gird', 'they', 'feeling impression belief notion opinion spirit tone feel flavor flavour look smell touch touch sensation tactual sensation tactile sensation intuitive feeling experience find sense finger palpate', 'quite rather quite a quite an', 'uncomfortable', 'just equitable fair good upright merely simply only but precisely exactly just now barely hardly scarcely scarce', 'this', 'morning morn morning time forenoon good morning dawn dawning aurora first light daybreak break of day break of the day dayspring sunrise sunup cockcrow', 'i iodine iodin I one ace single unity ane', 'ended end stop finish terminate cease complete concluded over all over terminated', 'up astir improving upward upwards upwardly', 'developing development develop evolve germinate acquire grow produce get originate arise rise uprise spring up build up explicate formulate train prepare educate modernize modernise make grow break recrudesce underdeveloped', 'fever febrility febricity pyrexia feverishness', 'my', 'mouth oral cavity oral fissure rima oris mouthpiece sass sassing backtalk back talk lip talk speak utter verbalize verbalise', 'has hour angle HA have have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'felt felt up mat up mattup matte up matte mat feel experience find sense finger palpate', 'very identical selfsame really real rattling', 'sore sensitive raw tender afflictive painful huffy mad', 'uncomfortable', 'my', 'friend ally acquaintance supporter protagonist champion admirer booster Friend Quaker', 'gabrielle', 'i iodine iodin I one ace single unity ane', 'went travel go move locomote proceed go away depart become get run pass lead extend sound function work operate run low run short survive last live live on endure hold up hold out die decease perish exit pass away expire kick the bucket cash in ones chips buy the farm conk giveup the ghost drop dead pop off choke croak snuff it belong start get going blend blend in fit rifle plump fail go bad give way give out conk out break break down', 'the', 'movies movie film picture moving picture movingpicture show motion picture motionpicture show picture show pic flick', 'i iodine iodin I one ace single unity ane', 'ended end stop finish terminate cease complete concluded over all over terminated', 'up astir improving upward upwards upwardly', 'developing development develop evolve germinate acquire grow produce get originate arise rise uprise spring up build up explicate formulate train prepare educate modernize modernise make grow break recrudesce underdeveloped', 'rashes rash roseola efflorescence skin rash blizzard', 'all wholly entirely completely totally altogether whole', 'over complete concluded ended all over terminated oer', 'my', 'body organic structure physical structure dead body torso trunk consistency consistence eubstance soundbox personify', 'which', 'turned turn change state become change by reversal reverse move around grow release turn over plow plough twist sprain wrench wrick rick flex bend deform call on sour ferment work off', 'into', 'itchy antsy fidgety fretful', 'spots musca volitans muscae volitantes floater topographic point place spot point smudge blot daub smear smirch slur speckle dapple patch fleck maculation position post berth office billet situation touch bit pip spotlight stain descry espy spy recognize recognise distinguish discern pick out make out tell apart blemish blob', 'later ulterior posterior late belated tardy recent former previous subsequently afterwards afterward after later on by and by', 'that', 'night nighttime dark Nox Night', 'i iodine iodin I one ace single unity ane', 'also besides too likewise as well', 'measured measure mensurate measure out quantify evaluate valuate assess appraise value mensural mensurable metrical metric deliberate calculated careful', 'my', 'temperature', 'had have have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'developed develop evolve germinate acquire grow produce get originate arise rise uprise spring up build up explicate formulate train prepare educate modernize modernise make grow break recrudesce highlydeveloped', 'fever febrility febricity pyrexia feverishness', 'this', 'morning morn morning time forenoon good morning dawn dawning aurora first light daybreak break of day break of the day dayspring sunrise sunup cockcrow', 'i iodine iodin I one ace single unity ane', 'cannot', 'even evening eve eventide flush level even out fiftyfifty regular tied yet still', 'move relocation motion movement motility travel go locomote displace proceed be active act affect impress strike motivate actuate propel prompt incite run make a motion', 'well wellspring fountainhead swell good easily considerably substantially intimately advantageously comfortably', 'because', 'my', 'muscles muscle musculus muscular tissue muscleman brawn brawniness muscularity sinew heftiness', 'aching ache hurt suffer yearn yen pine languish smart achy', 'greatly']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD_LIST: [\"I...can't remember\", \"People say I've gotten angrier, and sometimes I do feel angry, although I don't know why\", \"I couldn't remember her face...she was important to me but I could not, for the life of me, remember her face\", 'Where did I leave my bank notes? It was just yesterday when I retrieved them', 'What did you say? Can you speak louder?', \"my muscles are also hurting and ache quite a lot. it's also weird because my mother was home last weekend and we prepared a big dinner, but i had no appetite\", 'completing tasks and doing things has been very difficult lately, as i seem to not remember things i did in the past. doing things i was faimiliar with has been confusing and felt out of place, and it appears my spatial and visual senses are also diminishing']\n",
      "WORD_LIST_NEW: ['icant remember', 'people say ive gotten angrier sometimes i do feel angry although i dont know why', 'i couldnt remember her faceshe important me i could not for life me remember her face', 'where did i leave my bank notes it just yesterday when i retrieved them', 'what did you say you speak louder', 'my muscles also hurting ache quite lot its also weird because my mother home last weekend we prepared big dinner i had no appetite', 'completing tasks doing things has been very difficult lately i seem not remember things i did the past doing things i faimiliar with has been confusing felt out place it appears my spatial visual senses also diminishing']\n",
      "joined_symps: icant remember people say ive gotten angrier sometimes i do feel angry although i dont know why i couldnt remember her faceshe important me i could not for life me remember her face where did i leave my bank notes it just yesterday when i retrieved them what did you say you speak louder my muscles also hurting ache quite lot its also weird because my mother home last weekend we prepared big dinner i had no appetite completing tasks doing things has been very difficult lately i seem not remember things i did the past doing things i faimiliar with has been confusing felt out place it appears my spatial visual senses also diminishing\n",
      "SYNONYMS: ['icant', 'remember retrieve recall call_back call_up recollect think think_of think_back commend commemorate', 'people citizenry multitude masses mass hoi_polloi the_great_unwashed', 'say state tell allege aver suppose read order enjoin pronounce articulate enounce sound_out enunciate', 'ive', \"gotten get acquire become go let have receive find obtain incur arrive come bring convey fetch experience pay_back pay_off fix make induce stimulate cause catch capture grow develop produce contract take drive aim arrest scram buzz_off fuck_off bugger_off get_under_one's_skin draw perplex vex stick puzzle mystify baffle beat pose bewilder flummox stupefy nonplus gravel amaze dumbfound get_down begin start_out start set_about set_out commence suffer sustain beget engender father mother sire generate bring_forth\", 'angrier angry furious raging tempestuous wild', 'sometimes', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'do bash brawl doh ut Doctor_of_Osteopathy DO make perform execute fare make_out come get_along cause practice practise exercise suffice answer serve act behave manage dress arrange set coif coiffe coiffure', 'feel spirit tone feeling flavor flavour look smell tactile_property experience find sense finger palpate', 'angry furious raging tempestuous wild', 'although', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'dont', 'know cognize cognise experience live acknowledge recognize recognise sleep_together roll_in_the_hay love make_out make_love sleep_with get_laid have_sex do_it be_intimate have_intercourse have_it_away have_it_off screw fuck jazz eff hump lie_with bed have_a_go_at_it bang get_it_on bonk', 'why wherefore', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'couldnt', 'remember retrieve recall call_back call_up recollect think think_of think_back commend commemorate', 'her', 'faceshe', 'important of_import significant crucial authoritative', 'me Maine Pine_Tree_State ME', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'could', 'not non', 'for', 'life living animation aliveness lifetime life-time lifespan liveliness spirit sprightliness biography life_story life_history life_sentence', 'me Maine Pine_Tree_State ME', 'remember retrieve recall call_back call_up recollect think think_of think_back commend commemorate', 'her', 'face human_face expression look aspect facial_expression side grimace font fount typeface case boldness nerve brass cheek confront face_up front present', 'where', 'did make do perform execute fare make_out come get_along cause practice practise exercise suffice answer serve act behave manage dress arrange set coif coiffe coiffure', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'leave leave_of_absence farewell leave-taking parting go_forth go_away leave_alone leave_behind exit go_out get_out allow_for allow provide result lead depart pull_up_stakes entrust bequeath will impart give pass_on forget', 'my', 'bank depository_financial_institution banking_concern banking_company cant camber savings_bank coin_bank money_box bank_building deposit trust swear rely', \"notes note short_letter line billet musical_note tone bill government_note bank_bill banker's_bill bank_note banknote Federal_Reserve_note greenback annotation notation eminence distinction preeminence promissory_note note_of_hand observe mention remark notice mark take_note take_down\", 'it information_technology IT', 'just equitable fair good upright merely simply only but precisely exactly just_now barely hardly scarcely scarce', 'yesterday', 'when', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'retrieved recover retrieve find regain remember recall call_back call_up recollect think', 'them', 'what', 'did make do perform execute fare make_out come get_along cause practice practise exercise suffice answer serve act behave manage dress arrange set coif coiffe coiffure', 'you', 'say state tell allege aver suppose read order enjoin pronounce articulate enounce sound_out enunciate', 'you', 'speak talk utter mouth verbalize verbalise address', 'louder loud brassy cheap flash flashy garish gaudy gimcrack meretricious tacky tatty tawdry trashy forte', 'my', 'muscles muscle musculus muscular_tissue muscleman brawn brawniness muscularity sinew heftiness', 'also besides too likewise as_well', 'hurting pain ache smart hurt anguish injure wound bruise offend spite suffer', 'ache aching hurt suffer yearn yen pine languish smart', 'quite rather quite_a quite_an', 'lot batch deal flock good_deal great_deal hatful heap mass mess mickle mint mountain muckle passel peck pile plenty pot quite_a_little raft sight slew spate stack tidy_sum wad set circle band fortune destiny fate luck circumstances portion draw bunch caboodle Lot distribute administer mete_out parcel_out dispense shell_out deal_out dish_out allot dole_out', 'its information_technology IT', 'also besides too likewise as_well', 'weird Wyrd Weird eldritch uncanny unearthly', 'because', 'my', 'mother female_parent fuss overprotect beget get engender father sire generate bring_forth', 'home place dwelling domicile abode habitation dwelling_house home_plate home_base plate base family household house menage nursing_home rest_home interior internal national', \"last stopping_point finale finis finish conclusion close death end final_stage shoemaker's_last cobbler's_last endure survive live live_on go hold_up hold_out concluding final terminal net utmost last-place lowest lastly in_conclusion finally\", 'weekend', 'we', 'prepared fix prepare set_up ready gear_up set cook make organize organise devise get_up machinate groom train develop educate disposed fain inclined', 'big large bad prominent heavy boastful braggart bragging braggy cock-a-hoop crowing self-aggrandizing self-aggrandising swelled vainglorious adult full-grown fully_grown grown grownup magnanimous bighearted bounteous bountiful freehanded handsome giving liberal openhanded enceinte expectant gravid great with_child boastfully vauntingly', 'dinner dinner_party', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'had have have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'no nobelium No atomic_number_102 no_more', 'appetite appetency appetence', 'completing complete finish dispatch discharge nail fill_out fill_in make_out complemental complementary', 'tasks undertaking project task labor job chore tax', 'doing make do perform execute fare make_out come get_along cause practice practise exercise suffice answer serve act behave manage dress arrange set coif coiffe coiffure', 'things thing matter affair', 'has hour_angle HA have have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'been be exist equal constitute represent make_up comprise follow embody personify live cost', 'very identical selfsame really real rattling', 'difficult hard unmanageable', 'lately recently late of_late latterly', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'seem look appear', 'not non', 'remember retrieve recall call_back call_up recollect think think_of think_back commend commemorate', 'things thing matter affair', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'did make do perform execute fare make_out come get_along cause practice practise exercise suffice answer serve act behave manage dress arrange set coif coiffe coiffure', 'the', 'past past_times yesteryear past_tense preceding retiring by', 'doing make do perform execute fare make_out come get_along cause practice practise exercise suffice answer serve act behave manage dress arrange set coif coiffe coiffure', 'things thing matter affair', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'faimiliar', 'with', 'has hour_angle HA have have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'been be exist equal constitute represent make_up comprise follow embody personify live cost', 'confusing confuse confound throw fox befuddle fuddle bedevil discombobulate flurry disconcert put_off jumble mix_up blur obscure obnubilate perplexing puzzling', 'felt felt_up mat_up matt-up matte_up matte mat feel experience find sense finger palpate', \"out come_out_of_the_closet come_out extinct forbidden prohibited proscribed taboo tabu verboten knocked_out kayoed KO'd stunned away\", 'place topographic_point spot property stead position lieu shoes home post berth office billet situation station seat plaza piazza space blank_space put set pose lay rate rank range order grade locate site come_in come_out target aim direct point identify localize localise invest commit send', 'it information_technology IT', 'appears look appear seem come_out come_along', 'my', 'spatial spacial', 'visual ocular optic optical', 'senses sense signified sensation sentience sentiency sensory_faculty common_sense good_sense gumption horse_sense mother_wit pot grass green_goddess dope weed gage sess sens smoke skunk locoweed Mary_Jane feel smell smell_out', 'also besides too likewise as_well', 'diminishing decrease diminish lessen fall belittle']\n",
      "CLEAN SYNONYMS:  ['icant', 'remember retrieve recall call back call up recollect think think of think back commend commemorate', 'people citizenry multitude masses mass hoi polloi the great unwashed', 'say state tell allege aver suppose read order enjoin pronounce articulate enounce sound out enunciate', 'ive', 'gotten get acquire become go let have receive find obtain incur arrive come bring convey fetch experience pay back pay off fix make induce stimulate cause catch capture grow develop produce contract take drive aim arrest scram buzz off fuck off bugger off get under ones skin draw perplex vex stick puzzle mystify baffle beat pose bewilder flummox stupefy nonplus gravel amaze dumbfound get down begin start out start set about set out commence suffer sustain beget engender father mother sire generate bring forth', 'angrier angry furious raging tempestuous wild', 'sometimes', 'i iodine iodin I one ace single unity ane', 'do bash brawl doh ut Doctor of Osteopathy DO make perform execute fare make out come get along cause practice practise exercise suffice answer serve act behave manage dress arrange set coif coiffe coiffure', 'feel spirit tone feeling flavor flavour look smell tactile property experience find sense finger palpate', 'angry furious raging tempestuous wild', 'although', 'i iodine iodin I one ace single unity ane', 'dont', 'know cognize cognise experience live acknowledge recognize recognise sleep together roll in the hay love make out make love sleep with get laid have sex do it be intimate have intercourse have it away have it off screw fuck jazz eff hump lie with bed have a go at it bang get it on bonk', 'why wherefore', 'i iodine iodin I one ace single unity ane', 'couldnt', 'remember retrieve recall call back call up recollect think think of think back commend commemorate', 'her', 'faceshe', 'important of import significant crucial authoritative', 'me Maine Pine Tree State ME', 'i iodine iodin I one ace single unity ane', 'could', 'not non', 'for', 'life living animation aliveness lifetime lifetime lifespan liveliness spirit sprightliness biography life story life history life sentence', 'me Maine Pine Tree State ME', 'remember retrieve recall call back call up recollect think think of think back commend commemorate', 'her', 'face human face expression look aspect facial expression side grimace font fount typeface case boldness nerve brass cheek confront face up front present', 'where', 'did make do perform execute fare make out come get along cause practice practise exercise suffice answer serve act behave manage dress arrange set coif coiffe coiffure', 'i iodine iodin I one ace single unity ane', 'leave leave of absence farewell leavetaking parting go forth go away leave alone leave behind exit go out get out allow for allow provide result lead depart pull up stakes entrust bequeath will impart give pass on forget', 'my', 'bank depository financial institution banking concern banking company cant camber savings bank coin bank money box bank building deposit trust swear rely', 'notes note short letter line billet musical note tone bill government note bank bill bankers bill bank note banknote Federal Reserve note greenback annotation notation eminence distinction preeminence promissory note note of hand observe mention remark notice mark take note take down', 'it information technology IT', 'just equitable fair good upright merely simply only but precisely exactly just now barely hardly scarcely scarce', 'yesterday', 'when', 'i iodine iodin I one ace single unity ane', 'retrieved recover retrieve find regain remember recall call back call up recollect think', 'them', 'what', 'did make do perform execute fare make out come get along cause practice practise exercise suffice answer serve act behave manage dress arrange set coif coiffe coiffure', 'you', 'say state tell allege aver suppose read order enjoin pronounce articulate enounce sound out enunciate', 'you', 'speak talk utter mouth verbalize verbalise address', 'louder loud brassy cheap flash flashy garish gaudy gimcrack meretricious tacky tatty tawdry trashy forte', 'my', 'muscles muscle musculus muscular tissue muscleman brawn brawniness muscularity sinew heftiness', 'also besides too likewise as well', 'hurting pain ache smart hurt anguish injure wound bruise offend spite suffer', 'ache aching hurt suffer yearn yen pine languish smart', 'quite rather quite a quite an', 'lot batch deal flock good deal great deal hatful heap mass mess mickle mint mountain muckle passel peck pile plenty pot quite a little raft sight slew spate stack tidy sum wad set circle band fortune destiny fate luck circumstances portion draw bunch caboodle Lot distribute administer mete out parcel out dispense shell out deal out dish out allot dole out', 'its information technology IT', 'also besides too likewise as well', 'weird Wyrd Weird eldritch uncanny unearthly', 'because', 'my', 'mother female parent fuss overprotect beget get engender father sire generate bring forth', 'home place dwelling domicile abode habitation dwelling house home plate home base plate base family household house menage nursing home rest home interior internal national', 'last stopping point finale finis finish conclusion close death end final stage shoemakers last cobblers last endure survive live live on go hold up hold out concluding final terminal net utmost lastplace lowest lastly in conclusion finally', 'weekend', 'we', 'prepared fix prepare set up ready gear up set cook make organize organise devise get up machinate groom train develop educate disposed fain inclined', 'big large bad prominent heavy boastful braggart bragging braggy cockahoop crowing selfaggrandizing selfaggrandising swelled vainglorious adult fullgrown fully grown grown grownup magnanimous bighearted bounteous bountiful freehanded handsome giving liberal openhanded enceinte expectant gravid great with child boastfully vauntingly', 'dinner dinner party', 'i iodine iodin I one ace single unity ane', 'had have have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'no nobelium No no more', 'appetite appetency appetence', 'completing complete finish dispatch discharge nail fill out fill in make out complemental complementary', 'tasks undertaking project task labor job chore tax', 'doing make do perform execute fare make out come get along cause practice practise exercise suffice answer serve act behave manage dress arrange set coif coiffe coiffure', 'things thing matter affair', 'has hour angle HA have have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'been be exist equal constitute represent make up comprise follow embody personify live cost', 'very identical selfsame really real rattling', 'difficult hard unmanageable', 'lately recently late of late latterly', 'i iodine iodin I one ace single unity ane', 'seem look appear', 'not non', 'remember retrieve recall call back call up recollect think think of think back commend commemorate', 'things thing matter affair', 'i iodine iodin I one ace single unity ane', 'did make do perform execute fare make out come get along cause practice practise exercise suffice answer serve act behave manage dress arrange set coif coiffe coiffure', 'the', 'past past times yesteryear past tense preceding retiring by', 'doing make do perform execute fare make out come get along cause practice practise exercise suffice answer serve act behave manage dress arrange set coif coiffe coiffure', 'things thing matter affair', 'i iodine iodin I one ace single unity ane', 'faimiliar', 'with', 'has hour angle HA have have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'been be exist equal constitute represent make up comprise follow embody personify live cost', 'confusing confuse confound throw fox befuddle fuddle bedevil discombobulate flurry disconcert put off jumble mix up blur obscure obnubilate perplexing puzzling', 'felt felt up mat up mattup matte up matte mat feel experience find sense finger palpate', 'out come out of the closet come out extinct forbidden prohibited proscribed taboo tabu verboten knocked out kayoed KOd stunned away', 'place topographic point spot property stead position lieu shoes home post berth office billet situation station seat plaza piazza space blank space put set pose lay rate rank range order grade locate site come in come out target aim direct point identify localize localise invest commit send', 'it information technology IT', 'appears look appear seem come out come along', 'my', 'spatial spacial', 'visual ocular optic optical', 'senses sense signified sensation sentience sentiency sensory faculty common sense good sense gumption horse sense mother wit pot grass green goddess dope weed gage sess sens smoke skunk locoweed Mary Jane feel smell smell out', 'also besides too likewise as well', 'diminishing decrease diminish lessen fall belittle']\n",
      "WORD_LIST: ['My back hurts so much just from walking around and my ankles are swollen too', \"Don't talk to me, just go away and leave me alone\", \"My period is late by three days...it's almost never late\", 'My side was cramping so hard, it felt like someone was shredding me inside out']\n",
      "WORD_LIST_NEW: ['my back hurts so much just walking around my ankles swollen too', 'dont talk me just go away leave me alone', 'my period late by three daysits almost never late', 'my side cramping so hard it felt like someone shredding me inside out']\n",
      "joined_symps: my back hurts so much just walking around my ankles swollen too dont talk me just go away leave me alone my period late by three daysits almost never late my side cramping so hard it felt like someone shredding me inside out\n",
      "SYNONYMS: ['my', 'back dorsum rear spinal_column vertebral_column spine backbone rachis binding book_binding cover backrest endorse indorse plump_for plunk_for support second bet_on gage stake game punt back_up hind hinder backward backwards rearward rearwards', 'hurts injury hurt harm trauma distress suffering detriment damage scathe ache smart pain anguish injure wound bruise offend spite suffer', 'so sol soh thus thusly then and_so and_then therefore hence thence indeed', 'much a_lot lots a_good_deal a_great_deal very_much practically often', 'just equitable fair good upright merely simply only but precisely exactly just_now barely hardly scarcely scarce', 'walking walk take_the_air walk-to', 'around about approximately close_to just_about some roughly more_or_less or_so round', 'my', 'ankles ankle ankle_joint mortise_joint articulatio_talocruralis', 'swollen swell puff_up swell_up intumesce tumefy tumesce well_up well conceited egotistic egotistical self-conceited swollen-headed vain', 'too excessively overly to_a_fault besides also likewise as_well', 'dont', 'talk talking lecture public_lecture talk_of_the_town speak utter mouth verbalize verbalise spill spill_the_beans let_the_cat_out_of_the_bag tattle blab peach babble sing babble_out blab_out', 'me Maine Pine_Tree_State ME', 'just equitable fair good upright merely simply only but precisely exactly just_now barely hardly scarcely scarce', \"go spell tour turn Adam ecstasy XTC disco_biscuit cristal X hug_drug crack fling pass whirl offer go_game travel move locomote proceed go_away depart become get run lead extend sound function work operate run_low run_short survive last live live_on endure hold_up hold_out die decease perish exit pass_away expire kick_the_bucket cash_in_one's_chips buy_the_farm conk give-up_the_ghost drop_dead pop_off choke croak snuff_it belong start get_going blend blend_in fit rifle plump fail go_bad give_way give_out conk_out break break_down\", 'away outside off forth out aside by', 'leave leave_of_absence farewell leave-taking parting go_forth go_away leave_alone leave_behind exit go_out get_out allow_for allow provide result lead depart pull_up_stakes entrust bequeath will impart give pass_on forget', 'me Maine Pine_Tree_State ME', 'alone lone lonely solitary only unique unequaled unequalled unparalleled entirely exclusively solely solo unaccompanied', 'my', 'period time_period period_of_time geological_period menstruation menses menstruum catamenia flow point full_stop stop full_point', 'late belated tardy recent later former previous belatedly tardily deep recently lately of_late latterly', 'by past aside away', 'three 3 III trio threesome tierce leash troika triad trine trinity ternary ternion triplet tercet terzetto trey deuce-ace iii', 'daysits', 'almost about most nearly near nigh virtually well-nigh', \"never ne'er\", 'late belated tardy recent later former previous belatedly tardily deep recently lately of_late latterly', 'my', 'side face side_of_meat position slope incline English', 'cramping cramp hamper halter strangle', 'so sol soh thus thusly then and_so and_then therefore hence thence indeed', 'hard difficult knockout severe arduous backbreaking grueling gruelling heavy laborious operose punishing toilsome unvoiced voiceless surd concentrated intemperate strong tough firmly severely heavily intemperately', 'it information_technology IT', 'felt felt_up mat_up matt-up matte_up matte mat feel experience find sense finger palpate', 'like the_like the_likes_of ilk wish care similar same alike comparable corresponding', 'someone person individual somebody mortal soul', 'shredding shred tear_up rip_up', 'me Maine Pine_Tree_State ME', 'inside interior inner privileged indoors within inwardly at_heart at_bottom deep_down in_spite_of_appearance', \"out come_out_of_the_closet come_out extinct forbidden prohibited proscribed taboo tabu verboten knocked_out kayoed KO'd stunned away\"]\n",
      "CLEAN SYNONYMS:  ['my', 'back dorsum rear spinal column vertebral column spine backbone rachis binding book binding cover backrest endorse indorse plump for plunk for support second bet on gage stake game punt back up hind hinder backward backwards rearward rearwards', 'hurts injury hurt harm trauma distress suffering detriment damage scathe ache smart pain anguish injure wound bruise offend spite suffer', 'so sol soh thus thusly then and so and then therefore hence thence indeed', 'much a lot lots a good deal a great deal very much practically often', 'just equitable fair good upright merely simply only but precisely exactly just now barely hardly scarcely scarce', 'walking walk take the air walkto', 'around about approximately close to just about some roughly more or less or so round', 'my', 'ankles ankle ankle joint mortise joint articulatio talocruralis', 'swollen swell puff up swell up intumesce tumefy tumesce well up well conceited egotistic egotistical selfconceited swollenheaded vain', 'too excessively overly to a fault besides also likewise as well', 'dont', 'talk talking lecture public lecture talk of the town speak utter mouth verbalize verbalise spill spill the beans let the cat out of the bag tattle blab peach babble sing babble out blab out', 'me Maine Pine Tree State ME', 'just equitable fair good upright merely simply only but precisely exactly just now barely hardly scarcely scarce', 'go spell tour turn Adam ecstasy XTC disco biscuit cristal X hug drug crack fling pass whirl offer go game travel move locomote proceed go away depart become get run lead extend sound function work operate run low run short survive last live live on endure hold up hold out die decease perish exit pass away expire kick the bucket cash in ones chips buy the farm conk giveup the ghost drop dead pop off choke croak snuff it belong start get going blend blend in fit rifle plump fail go bad give way give out conk out break break down', 'away outside off forth out aside by', 'leave leave of absence farewell leavetaking parting go forth go away leave alone leave behind exit go out get out allow for allow provide result lead depart pull up stakes entrust bequeath will impart give pass on forget', 'me Maine Pine Tree State ME', 'alone lone lonely solitary only unique unequaled unequalled unparalleled entirely exclusively solely solo unaccompanied', 'my', 'period time period period of time geological period menstruation menses menstruum catamenia flow point full stop stop full point', 'late belated tardy recent later former previous belatedly tardily deep recently lately of late latterly', 'by past aside away', 'three III trio threesome tierce leash troika triad trine trinity ternary ternion triplet tercet terzetto trey deuceace iii', 'daysits', 'almost about most nearly near nigh virtually wellnigh', 'never neer', 'late belated tardy recent later former previous belatedly tardily deep recently lately of late latterly', 'my', 'side face side of meat position slope incline English', 'cramping cramp hamper halter strangle', 'so sol soh thus thusly then and so and then therefore hence thence indeed', 'hard difficult knockout severe arduous backbreaking grueling gruelling heavy laborious operose punishing toilsome unvoiced voiceless surd concentrated intemperate strong tough firmly severely heavily intemperately', 'it information technology IT', 'felt felt up mat up mattup matte up matte mat feel experience find sense finger palpate', 'like the like the likes of ilk wish care similar same alike comparable corresponding', 'someone person individual somebody mortal soul', 'shredding shred tear up rip up', 'me Maine Pine Tree State ME', 'inside interior inner privileged indoors within inwardly at heart at bottom deep down in spite of appearance', 'out come out of the closet come out extinct forbidden prohibited proscribed taboo tabu verboten knocked out kayoed KOd stunned away']\n",
      "WORD_LIST: ['No matter how little I eat, I get fatter', 'food is the enemy and I refuse to surrender', \"How many calories are in that sandwiche? I'll just take an apple instead\", 'I feel so cold, even when I wear an extra coat', \"I purged myself clean today once more, just like I always have, except there was a burning sensation down my throat. I guess if that's the price I have to pay\"]\n",
      "WORD_LIST_NEW: ['no matter how little i eat i get fatter', 'food the enemy i refuse surrender', 'how many calories in that sandwiche ill just take apple instead', 'i feel so cold even when i wear extra coat', 'i purged myself clean today once more just like i always have except there a burning sensation down my throat i guess if thats price i have pay']\n",
      "joined_symps: no matter how little i eat i get fatter food the enemy i refuse surrender how many calories in that sandwiche ill just take apple instead i feel so cold even when i wear extra coat i purged myself clean today once more just like i always have except there a burning sensation down my throat i guess if thats price i have pay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYNONYMS: ['no nobelium No atomic_number_102 no_more', 'matter affair thing topic subject issue count weigh', 'how', 'little small slight fiddling footling lilliputian niggling piddling piffling petty picayune trivial short minuscule', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'eat feed eat_on consume eat_up use_up deplete exhaust run_through wipe_out corrode rust', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', \"get acquire become go let have receive find obtain incur arrive come bring convey fetch experience pay_back pay_off fix make induce stimulate cause catch capture grow develop produce contract take drive aim arrest scram buzz_off fuck_off bugger_off get_under_one's_skin draw perplex vex stick puzzle mystify baffle beat pose bewilder flummox stupefy nonplus gravel amaze dumbfound get_down begin start_out start set_about set_out commence suffer sustain beget engender father mother sire generate bring_forth\", 'fatter fat fatty juicy fertile productive rich', 'food nutrient solid_food food_for_thought intellectual_nourishment', 'the', 'enemy foe foeman opposition', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'refuse garbage food_waste scraps decline reject pass_up turn_down defy resist deny turn_away', 'surrender resignation giving_up yielding capitulation fall give_up cede deliver', 'how', 'many', \"calories Calorie kilogram_calorie kilocalorie large_calorie nutritionist's_calorie calorie gram_calorie small_calorie\", 'in inch indium In atomic_number_49 Indiana Hoosier_State IN inwards inward', 'that', 'sandwiche', 'ill ailment complaint sick inauspicious ominous badly poorly', 'just equitable fair good upright merely simply only but precisely exactly just_now barely hardly scarcely scarce', 'take return issue takings proceeds yield payoff occupy use_up lead direct conduct guide get_hold_of assume acquire adopt take_on read bring convey choose select pick_out accept have fill consider deal look_at necessitate ask postulate need require involve call_for demand film shoot remove take_away withdraw consume ingest take_in submit strike take_up admit learn study claim exact make aim train take_aim carry pack lease rent hire charter engage subscribe subscribe_to contain hold drive contract get', 'apple orchard_apple_tree Malus_pumila', 'instead alternatively or_else rather', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'feel spirit tone feeling flavor flavour look smell tactile_property experience find sense finger palpate', 'so sol soh thus thusly then and_so and_then therefore hence thence indeed', 'cold common_cold coldness low_temperature frigidity frigidness stale dusty moth-eaten frigid cold-blooded inhuman insensate', 'even evening eve eventide flush level even_out fifty-fifty regular tied yet still', 'when', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'wear clothing article_of_clothing vesture wearable habiliment wearing have_on bear wear_off wear_out wear_down wear_thin hold_out endure break bust fall_apart tire wear_upon tire_out weary jade outwear fag_out fag fatigue put_on get_into don assume', 'extra supernumerary spear_carrier duplicate excess redundant spare supererogatory superfluous surplus special additional', 'coat coating pelage surface cake', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'purged purge purify sanctify flush scour vomit vomit_up cast sick cat be_sick disgorge regorge retch puke barf spew spue chuck upchuck honk regurgitate throw_up', 'myself', 'clean clean_and_jerk make_clean pick houseclean clean_house cleanse strip scavenge clear light unclouded fresh uncontaminating unobjectionable uninfected clean-living fair blank white sporting sporty sportsmanlike neat plumb plum fairly', 'today nowadays now', 'once one_time in_one_case formerly at_one_time erstwhile erst', 'more More Thomas_More Sir_Thomas_More more_than to_a_greater_extent', 'just equitable fair good upright merely simply only but precisely exactly just_now barely hardly scarcely scarce', 'like the_like the_likes_of ilk wish care similar same alike comparable corresponding', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', \"always ever e'er constantly invariably forever perpetually incessantly\", 'have rich_person wealthy_person have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'except demur exclude leave_out leave_off omit take_out', 'there at_that_place in_that_location in_that_respect on_that_point thither', 'a angstrom angstrom_unit A vitamin_A antiophthalmic_factor axerophthol deoxyadenosine_monophosphate adenine ampere amp type_A group_A', 'burning combustion burn electrocution burning_at_the_stake fire burn_down glow combust bite sting incinerate cauterize cauterise sunburn cut burn_off burn_up', 'sensation esthesis aesthesis sense_experience sense_impression sense_datum ace adept champion maven mavin virtuoso genius hotshot star superstar whiz whizz wizard wiz sense sentience sentiency sensory_faculty', 'down down_feather Down John_L._H._Down pile toss_off pop bolt_down belt_down pour_down drink_down kill devour consume go_through shoot_down land knock_down cut_down push_down pull_down polish refine fine-tune downward down_pat mastered depressed gloomy grim blue dispirited downcast downhearted down_in_the_mouth low low-spirited downwards downwardly', 'my', 'throat pharynx', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'guess conjecture supposition surmise surmisal speculation hypothesis guesswork guessing shot dead_reckoning think opine suppose imagine reckon venture pretend hazard estimate gauge approximate judge infer', 'if', 'thats', 'price monetary_value cost terms damage toll Price Leontyne_Price Mary_Leontyne_Price', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'have rich_person wealthy_person have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'pay wage earnings remuneration salary give pay_up ante_up yield bear pay_off make_up compensate devote']\n",
      "CLEAN SYNONYMS:  ['no nobelium No no more', 'matter affair thing topic subject issue count weigh', 'how', 'little small slight fiddling footling lilliputian niggling piddling piffling petty picayune trivial short minuscule', 'i iodine iodin I one ace single unity ane', 'eat feed eat on consume eat up use up deplete exhaust run through wipe out corrode rust', 'i iodine iodin I one ace single unity ane', 'get acquire become go let have receive find obtain incur arrive come bring convey fetch experience pay back pay off fix make induce stimulate cause catch capture grow develop produce contract take drive aim arrest scram buzz off fuck off bugger off get under ones skin draw perplex vex stick puzzle mystify baffle beat pose bewilder flummox stupefy nonplus gravel amaze dumbfound get down begin start out start set about set out commence suffer sustain beget engender father mother sire generate bring forth', 'fatter fat fatty juicy fertile productive rich', 'food nutrient solid food food for thought intellectual nourishment', 'the', 'enemy foe foeman opposition', 'i iodine iodin I one ace single unity ane', 'refuse garbage food waste scraps decline reject pass up turn down defy resist deny turn away', 'surrender resignation giving up yielding capitulation fall give up cede deliver', 'how', 'many', 'calories Calorie kilogram calorie kilocalorie large calorie nutritionists calorie calorie gram calorie small calorie', 'in inch indium In Indiana Hoosier State IN inwards inward', 'that', 'sandwiche', 'ill ailment complaint sick inauspicious ominous badly poorly', 'just equitable fair good upright merely simply only but precisely exactly just now barely hardly scarcely scarce', 'take return issue takings proceeds yield payoff occupy use up lead direct conduct guide get hold of assume acquire adopt take on read bring convey choose select pick out accept have fill consider deal look at necessitate ask postulate need require involve call for demand film shoot remove take away withdraw consume ingest take in submit strike take up admit learn study claim exact make aim train take aim carry pack lease rent hire charter engage subscribe subscribe to contain hold drive contract get', 'apple orchard apple tree Malus pumila', 'instead alternatively or else rather', 'i iodine iodin I one ace single unity ane', 'feel spirit tone feeling flavor flavour look smell tactile property experience find sense finger palpate', 'so sol soh thus thusly then and so and then therefore hence thence indeed', 'cold common cold coldness low temperature frigidity frigidness stale dusty motheaten frigid coldblooded inhuman insensate', 'even evening eve eventide flush level even out fiftyfifty regular tied yet still', 'when', 'i iodine iodin I one ace single unity ane', 'wear clothing article of clothing vesture wearable habiliment wearing have on bear wear off wear out wear down wear thin hold out endure break bust fall apart tire wear upon tire out weary jade outwear fag out fag fatigue put on get into don assume', 'extra supernumerary spear carrier duplicate excess redundant spare supererogatory superfluous surplus special additional', 'coat coating pelage surface cake', 'i iodine iodin I one ace single unity ane', 'purged purge purify sanctify flush scour vomit vomit up cast sick cat be sick disgorge regorge retch puke barf spew spue chuck upchuck honk regurgitate throw up', 'myself', 'clean clean and jerk make clean pick houseclean clean house cleanse strip scavenge clear light unclouded fresh uncontaminating unobjectionable uninfected cleanliving fair blank white sporting sporty sportsmanlike neat plumb plum fairly', 'today nowadays now', 'once one time in one case formerly at one time erstwhile erst', 'more More Thomas More Sir Thomas More more than to a greater extent', 'just equitable fair good upright merely simply only but precisely exactly just now barely hardly scarcely scarce', 'like the like the likes of ilk wish care similar same alike comparable corresponding', 'i iodine iodin I one ace single unity ane', 'always ever eer constantly invariably forever perpetually incessantly', 'have rich person wealthy person have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'except demur exclude leave out leave off omit take out', 'there at that place in that location in that respect on that point thither', 'a angstrom angstrom unit A vitamin A antiophthalmic factor axerophthol deoxyadenosine monophosphate adenine ampere amp type A group A', 'burning combustion burn electrocution burning at the stake fire burn down glow combust bite sting incinerate cauterize cauterise sunburn cut burn off burn up', 'sensation esthesis aesthesis sense experience sense impression sense datum ace adept champion maven mavin virtuoso genius hotshot star superstar whiz whizz wizard wiz sense sentience sentiency sensory faculty', 'down down feather Down John L H Down pile toss off pop bolt down belt down pour down drink down kill devour consume go through shoot down land knock down cut down push down pull down polish refine finetune downward down pat mastered depressed gloomy grim blue dispirited downcast downhearted down in the mouth low lowspirited downwards downwardly', 'my', 'throat pharynx', 'i iodine iodin I one ace single unity ane', 'guess conjecture supposition surmise surmisal speculation hypothesis guesswork guessing shot dead reckoning think opine suppose imagine reckon venture pretend hazard estimate gauge approximate judge infer', 'if', 'thats', 'price monetary value cost terms damage toll Price Leontyne Price Mary Leontyne Price', 'i iodine iodin I one ace single unity ane', 'have rich person wealthy person have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'pay wage earnings remuneration salary give pay up ante up yield bear pay off make up compensate devote']\n",
      "WORD_LIST: ['Recently my chest is feeling pretty tight and even numb at times. I have become light headed and dizzy for most of the day rendering me sad and useless. I also frequently vomit and expierience heartburn which is hella painful. Im really worried that I might have a serious disease and my indigestion is not helping.']\n",
      "WORD_LIST_NEW: ['recently my chest feeling pretty tight even numb at times i have become light headed dizzy for most the day rendering me sad useless i also frequently vomit expierience heartburn which hella painful im really worried that i might have serious disease my indigestion not helping']\n",
      "joined_symps: recently my chest feeling pretty tight even numb at times i have become light headed dizzy for most the day rendering me sad useless i also frequently vomit expierience heartburn which hella painful im really worried that i might have serious disease my indigestion not helping\n",
      "SYNONYMS: ['recently late lately of_late latterly', 'my', 'chest thorax pectus breast chest_of_drawers bureau dresser', 'feeling impression belief notion opinion spirit tone feel flavor flavour look smell touch touch_sensation tactual_sensation tactile_sensation intuitive_feeling experience find sense finger palpate', 'pretty reasonably moderately jolly somewhat fairly middling passably', 'tight taut compressed mean mingy miserly close besotted blind_drunk blotto crocked cockeyed fuddled loaded pie-eyed pissed pixilated plastered slopped sloshed smashed soaked soused sozzled squiffy stiff wet nasty rigorous stringent fast closely', 'even evening eve eventide flush level even_out fifty-fifty regular tied yet still', 'numb benumb blunt dull asleep benumbed dead', 'at astatine At atomic_number_85', 'times multiplication time clip clock_time fourth_dimension meter metre prison_term sentence clock', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'have rich_person wealthy_person have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'become go get turn suit', 'light visible_light visible_radiation light_source luminosity brightness brightness_level luminance luminousness illumination lightness lighting sparkle twinkle spark Inner_Light Light Light_Within Christ_Within lighter igniter ignitor illume illumine light_up illuminate fire_up alight perch ignite fall unhorse dismount get_off get_down light-colored unaccented weak clean clear unclouded lightsome tripping faint swooning light-headed lightheaded abstemious scant short idle lite low-cal calorie-free wakeful easy loose promiscuous sluttish wanton lightly', 'headed head lead head_up steer maneuver manoeuver manoeuvre direct point guide channelize channelise', 'dizzy giddy woozy vertiginous airheaded empty-headed featherbrained light-headed lightheaded silly', 'for', 'most to_the_highest_degree about almost nearly near nigh virtually well-nigh', 'the', 'day twenty-four_hours twenty-four_hour_period 24-hour_interval solar_day mean_solar_day daytime daylight sidereal_day Day Clarence_Day Clarence_Shepard_Day_Jr.', 'rendering rendition interpretation interpreting translation interlingual_rendition version render supply provide furnish interpret yield return give generate deliver submit hand_over fork_over fork_out fork_up turn_in picture depict show translate try', 'me Maine Pine_Tree_State ME', 'sad deplorable distressing lamentable pitiful sorry', 'useless', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'also besides too likewise as_well', 'frequently often oftentimes oft ofttimes', 'vomit vomitus puke barf emetic vomitive nauseant vomiting emesis regurgitation disgorgement puking vomit_up purge cast sick cat be_sick disgorge regorge retch spew spue chuck upchuck honk regurgitate throw_up', 'expierience', 'heartburn pyrosis', 'which', 'hella', 'painful afflictive sore atrocious abominable awful dreadful terrible unspeakable irritating', 'im', 'really truly genuinely actually in_truth very real rattling', 'worried worry care vex concern interest occupy disquieted distressed disturbed upset apprehensive', 'that', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'might mightiness power', 'have rich_person wealthy_person have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'serious dangerous grave grievous severe life-threatening good unplayful sober', 'disease', 'my', 'indigestion dyspepsia stomach_upset upset_stomach', 'not non', 'helping portion serving help assist aid facilitate help_oneself serve avail']\n",
      "CLEAN SYNONYMS:  ['recently late lately of late latterly', 'my', 'chest thorax pectus breast chest of drawers bureau dresser', 'feeling impression belief notion opinion spirit tone feel flavor flavour look smell touch touch sensation tactual sensation tactile sensation intuitive feeling experience find sense finger palpate', 'pretty reasonably moderately jolly somewhat fairly middling passably', 'tight taut compressed mean mingy miserly close besotted blind drunk blotto crocked cockeyed fuddled loaded pieeyed pissed pixilated plastered slopped sloshed smashed soaked soused sozzled squiffy stiff wet nasty rigorous stringent fast closely', 'even evening eve eventide flush level even out fiftyfifty regular tied yet still', 'numb benumb blunt dull asleep benumbed dead', 'at astatine At', 'times multiplication time clip clock time fourth dimension meter metre prison term sentence clock', 'i iodine iodin I one ace single unity ane', 'have rich person wealthy person have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'become go get turn suit', 'light visible light visible radiation light source luminosity brightness brightness level luminance luminousness illumination lightness lighting sparkle twinkle spark Inner Light Light Light Within Christ Within lighter igniter ignitor illume illumine light up illuminate fire up alight perch ignite fall unhorse dismount get off get down lightcolored unaccented weak clean clear unclouded lightsome tripping faint swooning lightheaded lightheaded abstemious scant short idle lite lowcal caloriefree wakeful easy loose promiscuous sluttish wanton lightly', 'headed head lead head up steer maneuver manoeuver manoeuvre direct point guide channelize channelise', 'dizzy giddy woozy vertiginous airheaded emptyheaded featherbrained lightheaded lightheaded silly', 'for', 'most to the highest degree about almost nearly near nigh virtually wellnigh', 'the', 'day twentyfour hours twentyfour hour period solar day mean solar day daytime daylight sidereal day Day Clarence Day Clarence Shepard Day Jr', 'rendering rendition interpretation interpreting translation interlingual rendition version render supply provide furnish interpret yield return give generate deliver submit hand over fork over fork out fork up turn in picture depict show translate try', 'me Maine Pine Tree State ME', 'sad deplorable distressing lamentable pitiful sorry', 'useless', 'i iodine iodin I one ace single unity ane', 'also besides too likewise as well', 'frequently often oftentimes oft ofttimes', 'vomit vomitus puke barf emetic vomitive nauseant vomiting emesis regurgitation disgorgement puking vomit up purge cast sick cat be sick disgorge regorge retch spew spue chuck upchuck honk regurgitate throw up', 'expierience', 'heartburn pyrosis', 'which', 'hella', 'painful afflictive sore atrocious abominable awful dreadful terrible unspeakable irritating', 'im', 'really truly genuinely actually in truth very real rattling', 'worried worry care vex concern interest occupy disquieted distressed disturbed upset apprehensive', 'that', 'i iodine iodin I one ace single unity ane', 'might mightiness power', 'have rich person wealthy person have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'serious dangerous grave grievous severe lifethreatening good unplayful sober', 'disease', 'my', 'indigestion dyspepsia stomach upset upset stomach', 'not non', 'helping portion serving help assist aid facilitate help oneself serve avail']\n",
      "WORD_LIST: [\"There are a bunch of red spots on my chest and I don't think I've contacted any allergens...why won't they go away\", 'there are small bits of milky white liquid coming out of my nipples...should I be concerned?', 'Parts of my breast felt a bit hard...almost lumpy. Would this go away after puberty?', 'It just occurred to me today that my nipples look...different. Are they supposed to be sunken in?']\n",
      "WORD_LIST_NEW: ['there a bunch red spots my chest i dont think ive contacted any allergenswhy wont they go away', 'there small bits milky white liquid coming out my nipplesshould i be concerned', 'parts my breast felt bit hardalmost lumpy would this go away after puberty', 'it just occurred me today that my nipples lookdifferent they supposed be sunken']\n",
      "joined_symps: there a bunch red spots my chest i dont think ive contacted any allergenswhy wont they go away there small bits milky white liquid coming out my nipplesshould i be concerned parts my breast felt bit hardalmost lumpy would this go away after puberty it just occurred me today that my nipples lookdifferent they supposed be sunken\n",
      "SYNONYMS: ['there at_that_place in_that_location in_that_respect on_that_point thither', 'a angstrom angstrom_unit A vitamin_A antiophthalmic_factor axerophthol deoxyadenosine_monophosphate adenine ampere amp type_A group_A', 'bunch clump cluster clustering crowd crew gang lot caboodle bunch_together bunch_up bundle', 'red redness Red Red_River Bolshevik Marxist bolshie bolshy loss red_ink reddish ruddy blood-red carmine cerise cherry cherry-red crimson ruby ruby-red scarlet violent reddened red-faced flushed', 'spots musca_volitans muscae_volitantes floater topographic_point place spot point smudge blot daub smear smirch slur speckle dapple patch fleck maculation position post berth office billet situation touch bit pip spotlight stain descry espy spy recognize recognise distinguish discern pick_out make_out tell_apart blemish blob', 'my', 'chest thorax pectus breast chest_of_drawers bureau dresser', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'dont', 'think believe consider conceive opine suppose imagine reckon guess cogitate cerebrate remember retrieve recall call_back call_up recollect intend mean', 'ive', 'contacted reach get_through get_hold_of contact touch adjoin meet', 'any whatever whatsoever', 'allergenswhy', 'wont habit', 'they', \"go spell tour turn Adam ecstasy XTC disco_biscuit cristal X hug_drug crack fling pass whirl offer go_game travel move locomote proceed go_away depart become get run lead extend sound function work operate run_low run_short survive last live live_on endure hold_up hold_out die decease perish exit pass_away expire kick_the_bucket cash_in_one's_chips buy_the_farm conk give-up_the_ghost drop_dead pop_off choke croak snuff_it belong start get_going blend blend_in fit rifle plump fail go_bad give_way give_out conk_out break break_down\", 'away outside off forth out aside by', 'there at_that_place in_that_location in_that_respect on_that_point thither', 'small little minor modest small-scale pocket-size pocket-sized humble low lowly minuscule belittled diminished', 'bits spot bit chip flake fleck scrap moment mo minute second piece morsel bite snatch act routine number turn', 'milky milklike whitish', 'white White White_person Caucasian whiteness Edward_White Edward_D._White Edward_Douglas_White_Jr. Patrick_White Patrick_Victor_Martindale_White T._H._White Theodore_Harold_White Stanford_White E._B._White Elwyn_Brooks_White Andrew_D._White Andrew_Dickson_White White_River egg_white albumen ovalbumin flannel gabardine tweed whiten snowy lily-white white-hot blank clean whitened ashen blanched bloodless livid', 'liquid liquidness liquidity liquid_state swimming limpid melted liquified fluent fluid smooth', 'coming approach approaching advent orgasm climax sexual_climax come come_up arrive get follow issue_forth hail fall total number add_up amount come_in occur derive descend do fare make_out get_along forthcoming upcoming', \"out come_out_of_the_closet come_out extinct forbidden prohibited proscribed taboo tabu verboten knocked_out kayoed KO'd stunned away\", 'my', 'nipplesshould', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'be beryllium Be glucinium atomic_number_4 exist equal constitute represent make_up comprise follow embody personify live cost', 'concerned refer pertain relate concern come_to bear_on touch touch_on have-to_doe_with interest occupy worry interested implicated', 'parts part portion component_part component constituent piece region function office role character theatrical_role persona share percentage section division parting voice contribution separate split split_up break break_up depart start start_out set_forth set_off set_out take_off divide disunite', 'my', 'breast chest bosom knocker boob tit titty white_meat summit front', 'felt felt_up mat_up matt-up matte_up matte mat feel experience find sense finger palpate', 'bit spot chip flake fleck scrap moment mo minute second piece morsel bite snatch act routine number turn seize_with_teeth sting burn prick', 'hardalmost', 'lumpy chunky', 'would', 'this', \"go spell tour turn Adam ecstasy XTC disco_biscuit cristal X hug_drug crack fling pass whirl offer go_game travel move locomote proceed go_away depart become get run lead extend sound function work operate run_low run_short survive last live live_on endure hold_up hold_out die decease perish exit pass_away expire kick_the_bucket cash_in_one's_chips buy_the_farm conk give-up_the_ghost drop_dead pop_off choke croak snuff_it belong start get_going blend blend_in fit rifle plump fail go_bad give_way give_out conk_out break break_down\", 'away outside off forth out aside by', 'after subsequently later afterwards afterward later_on', 'puberty pubescence', 'it information_technology IT', 'just equitable fair good upright merely simply only but precisely exactly just_now barely hardly scarcely scarce', 'occurred happen hap go_on pass_off occur pass fall_out come_about take_place come', 'me Maine Pine_Tree_State ME', 'today nowadays now', 'that', 'my', 'nipples nipple mammilla mamilla pap teat tit', 'lookdifferent', 'they', 'supposed suppose say think opine imagine reckon guess speculate theorize theorise conjecture hypothesize hypothesise hypothecate presuppose alleged so-called conjectural divinatory hypothetical hypothetic suppositional suppositious supposititious', 'be beryllium Be glucinium atomic_number_4 exist equal constitute represent make_up comprise follow embody personify live cost', 'sunken sink drop drop_down pass lapse settle go_down go_under subside dip slump fall_off slide_down bury deep-set recessed']\n",
      "CLEAN SYNONYMS:  ['there at that place in that location in that respect on that point thither', 'a angstrom angstrom unit A vitamin A antiophthalmic factor axerophthol deoxyadenosine monophosphate adenine ampere amp type A group A', 'bunch clump cluster clustering crowd crew gang lot caboodle bunch together bunch up bundle', 'red redness Red Red River Bolshevik Marxist bolshie bolshy loss red ink reddish ruddy bloodred carmine cerise cherry cherryred crimson ruby rubyred scarlet violent reddened redfaced flushed', 'spots musca volitans muscae volitantes floater topographic point place spot point smudge blot daub smear smirch slur speckle dapple patch fleck maculation position post berth office billet situation touch bit pip spotlight stain descry espy spy recognize recognise distinguish discern pick out make out tell apart blemish blob', 'my', 'chest thorax pectus breast chest of drawers bureau dresser', 'i iodine iodin I one ace single unity ane', 'dont', 'think believe consider conceive opine suppose imagine reckon guess cogitate cerebrate remember retrieve recall call back call up recollect intend mean', 'ive', 'contacted reach get through get hold of contact touch adjoin meet', 'any whatever whatsoever', 'allergenswhy', 'wont habit', 'they', 'go spell tour turn Adam ecstasy XTC disco biscuit cristal X hug drug crack fling pass whirl offer go game travel move locomote proceed go away depart become get run lead extend sound function work operate run low run short survive last live live on endure hold up hold out die decease perish exit pass away expire kick the bucket cash in ones chips buy the farm conk giveup the ghost drop dead pop off choke croak snuff it belong start get going blend blend in fit rifle plump fail go bad give way give out conk out break break down', 'away outside off forth out aside by', 'there at that place in that location in that respect on that point thither', 'small little minor modest smallscale pocketsize pocketsized humble low lowly minuscule belittled diminished', 'bits spot bit chip flake fleck scrap moment mo minute second piece morsel bite snatch act routine number turn', 'milky milklike whitish', 'white White White person Caucasian whiteness Edward White Edward D White Edward Douglas White Jr Patrick White Patrick Victor Martindale White T H White Theodore Harold White Stanford White E B White Elwyn Brooks White Andrew D White Andrew Dickson White White River egg white albumen ovalbumin flannel gabardine tweed whiten snowy lilywhite whitehot blank clean whitened ashen blanched bloodless livid', 'liquid liquidness liquidity liquid state swimming limpid melted liquified fluent fluid smooth', 'coming approach approaching advent orgasm climax sexual climax come come up arrive get follow issue forth hail fall total number add up amount come in occur derive descend do fare make out get along forthcoming upcoming', 'out come out of the closet come out extinct forbidden prohibited proscribed taboo tabu verboten knocked out kayoed KOd stunned away', 'my', 'nipplesshould', 'i iodine iodin I one ace single unity ane', 'be beryllium Be glucinium exist equal constitute represent make up comprise follow embody personify live cost', 'concerned refer pertain relate concern come to bear on touch touch on haveto doe with interest occupy worry interested implicated', 'parts part portion component part component constituent piece region function office role character theatrical role persona share percentage section division parting voice contribution separate split split up break break up depart start start out set forth set off set out take off divide disunite', 'my', 'breast chest bosom knocker boob tit titty white meat summit front', 'felt felt up mat up mattup matte up matte mat feel experience find sense finger palpate', 'bit spot chip flake fleck scrap moment mo minute second piece morsel bite snatch act routine number turn seize with teeth sting burn prick', 'hardalmost', 'lumpy chunky', 'would', 'this', 'go spell tour turn Adam ecstasy XTC disco biscuit cristal X hug drug crack fling pass whirl offer go game travel move locomote proceed go away depart become get run lead extend sound function work operate run low run short survive last live live on endure hold up hold out die decease perish exit pass away expire kick the bucket cash in ones chips buy the farm conk giveup the ghost drop dead pop off choke croak snuff it belong start get going blend blend in fit rifle plump fail go bad give way give out conk out break break down', 'away outside off forth out aside by', 'after subsequently later afterwards afterward later on', 'puberty pubescence', 'it information technology IT', 'just equitable fair good upright merely simply only but precisely exactly just now barely hardly scarcely scarce', 'occurred happen hap go on pass off occur pass fall out come about take place come', 'me Maine Pine Tree State ME', 'today nowadays now', 'that', 'my', 'nipples nipple mammilla mamilla pap teat tit', 'lookdifferent', 'they', 'supposed suppose say think opine imagine reckon guess speculate theorize theorise conjecture hypothesize hypothesise hypothecate presuppose alleged socalled conjectural divinatory hypothetical hypothetic suppositional suppositious supposititious', 'be beryllium Be glucinium exist equal constitute represent make up comprise follow embody personify live cost', 'sunken sink drop drop down pass lapse settle go down go under subside dip slump fall off slide down bury deepset recessed']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD_LIST: [\"I've been feeling like I needed to go to the washroom more and more often these days\", \"I'm beginning to dread my washroom trips these days because it hurts to urinate\", 'When I went to piss this morning, there was blood and my piss was redish', \"What's wrong with me? Having sex with my girlfriend yesterday had hurt so much\"]\n",
      "WORD_LIST_NEW: ['ive been feeling like i needed go the washroom more more often these days', 'im beginning dread my washroom trips these days because it hurts urinate', 'when i went piss this morning there blood my piss redish', 'whats wrong with me having sex with my girlfriend yesterday had hurt so much']\n",
      "joined_symps: ive been feeling like i needed go the washroom more more often these days im beginning dread my washroom trips these days because it hurts urinate when i went piss this morning there blood my piss redish whats wrong with me having sex with my girlfriend yesterday had hurt so much\n",
      "SYNONYMS: ['ive', 'been be exist equal constitute represent make_up comprise follow embody personify live cost', 'feeling impression belief notion opinion spirit tone feel flavor flavour look smell touch touch_sensation tactual_sensation tactile_sensation intuitive_feeling experience find sense finger palpate', 'like the_like the_likes_of ilk wish care similar same alike comparable corresponding', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', 'needed necessitate ask postulate need require take involve call_for demand want needful required requisite', \"go spell tour turn Adam ecstasy XTC disco_biscuit cristal X hug_drug crack fling pass whirl offer go_game travel move locomote proceed go_away depart become get run lead extend sound function work operate run_low run_short survive last live live_on endure hold_up hold_out die decease perish exit pass_away expire kick_the_bucket cash_in_one's_chips buy_the_farm conk give-up_the_ghost drop_dead pop_off choke croak snuff_it belong start get_going blend blend_in fit rifle plump fail go_bad give_way give_out conk_out break break_down\", 'the', 'washroom', 'more More Thomas_More Sir_Thomas_More more_than to_a_greater_extent', 'more More Thomas_More Sir_Thomas_More more_than to_a_greater_extent', 'often frequently oftentimes oft ofttimes much a_great_deal', 'these', 'days years day twenty-four_hours twenty-four_hour_period 24-hour_interval solar_day mean_solar_day daytime daylight sidereal_day Day Clarence_Day Clarence_Shepard_Day_Jr.', 'im', 'beginning commencement first outset get-go start kickoff starting_time showtime offset origin root rootage source get_down begin get start_out set_about set_out commence lead_off', 'dread apprehension apprehensiveness fear awful dire direful dreaded dreadful fearful fearsome frightening horrendous horrific terrible', 'my', 'washroom', 'trips trip slip head_trip tripper trip-up stumble misstep trip_up travel jaunt actuate trigger activate set_off spark_off spark trigger_off touch_off trip_out turn_on get_off', 'these', 'days years day twenty-four_hours twenty-four_hour_period 24-hour_interval solar_day mean_solar_day daytime daylight sidereal_day Day Clarence_Day Clarence_Shepard_Day_Jr.', 'because', 'it information_technology IT', 'hurts injury hurt harm trauma distress suffering detriment damage scathe ache smart pain anguish injure wound bruise offend spite suffer', 'urinate make piddle puddle micturate piss pee pee-pee make_water relieve_oneself take_a_leak spend_a_penny wee wee-wee pass_water', 'when', 'i iodine iodin I atomic_number_53 one 1 ace single unity ane', \"went travel go move locomote proceed go_away depart become get run pass lead extend sound function work operate run_low run_short survive last live live_on endure hold_up hold_out die decease perish exit pass_away expire kick_the_bucket cash_in_one's_chips buy_the_farm conk give-up_the_ghost drop_dead pop_off choke croak snuff_it belong start get_going blend blend_in fit rifle plump fail go_bad give_way give_out conk_out break break_down\", 'piss urine pee piddle weewee water peeing pissing make urinate puddle micturate pee-pee make_water relieve_oneself take_a_leak spend_a_penny wee wee-wee pass_water', 'this', 'morning morn morning_time forenoon good_morning dawn dawning aurora first_light daybreak break_of_day break_of_the_day dayspring sunrise sunup cockcrow', 'there at_that_place in_that_location in_that_respect on_that_point thither', 'blood rake rakehell profligate rip roue lineage line line_of_descent descent bloodline blood_line pedigree ancestry origin parentage stemma stock', 'my', 'piss urine pee piddle weewee water peeing pissing make urinate puddle micturate pee-pee make_water relieve_oneself take_a_leak spend_a_penny wee wee-wee pass_water', 'redish', 'whats', 'wrong wrongfulness legal_injury damage incorrect improper amiss awry haywire ill-timed unseasonable untimely faulty incorrectly wrongly', 'with', 'me Maine Pine_Tree_State ME', 'having have have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'sex sexual_activity sexual_practice sex_activity sexual_urge gender sexuality arouse excite turn_on wind_up', 'with', 'my', 'girlfriend girl lady_friend', 'yesterday', 'had have have_got hold feature experience receive get own possess let consume ingest take_in take throw make give induce stimulate cause accept suffer sustain give_birth deliver bear birth', 'hurt injury harm trauma distress suffering detriment damage scathe ache smart pain anguish injure wound bruise offend spite suffer wounded weakened', 'so sol soh thus thusly then and_so and_then therefore hence thence indeed', 'much a_lot lots a_good_deal a_great_deal very_much practically often']\n",
      "CLEAN SYNONYMS:  ['ive', 'been be exist equal constitute represent make up comprise follow embody personify live cost', 'feeling impression belief notion opinion spirit tone feel flavor flavour look smell touch touch sensation tactual sensation tactile sensation intuitive feeling experience find sense finger palpate', 'like the like the likes of ilk wish care similar same alike comparable corresponding', 'i iodine iodin I one ace single unity ane', 'needed necessitate ask postulate need require take involve call for demand want needful required requisite', 'go spell tour turn Adam ecstasy XTC disco biscuit cristal X hug drug crack fling pass whirl offer go game travel move locomote proceed go away depart become get run lead extend sound function work operate run low run short survive last live live on endure hold up hold out die decease perish exit pass away expire kick the bucket cash in ones chips buy the farm conk giveup the ghost drop dead pop off choke croak snuff it belong start get going blend blend in fit rifle plump fail go bad give way give out conk out break break down', 'the', 'washroom', 'more More Thomas More Sir Thomas More more than to a greater extent', 'more More Thomas More Sir Thomas More more than to a greater extent', 'often frequently oftentimes oft ofttimes much a great deal', 'these', 'days years day twentyfour hours twentyfour hour period solar day mean solar day daytime daylight sidereal day Day Clarence Day Clarence Shepard Day Jr', 'im', 'beginning commencement first outset getgo start kickoff starting time showtime offset origin root rootage source get down begin get start out set about set out commence lead off', 'dread apprehension apprehensiveness fear awful dire direful dreaded dreadful fearful fearsome frightening horrendous horrific terrible', 'my', 'washroom', 'trips trip slip head trip tripper tripup stumble misstep trip up travel jaunt actuate trigger activate set off spark off spark trigger off touch off trip out turn on get off', 'these', 'days years day twentyfour hours twentyfour hour period solar day mean solar day daytime daylight sidereal day Day Clarence Day Clarence Shepard Day Jr', 'because', 'it information technology IT', 'hurts injury hurt harm trauma distress suffering detriment damage scathe ache smart pain anguish injure wound bruise offend spite suffer', 'urinate make piddle puddle micturate piss pee peepee make water relieve oneself take a leak spend a penny wee weewee pass water', 'when', 'i iodine iodin I one ace single unity ane', 'went travel go move locomote proceed go away depart become get run pass lead extend sound function work operate run low run short survive last live live on endure hold up hold out die decease perish exit pass away expire kick the bucket cash in ones chips buy the farm conk giveup the ghost drop dead pop off choke croak snuff it belong start get going blend blend in fit rifle plump fail go bad give way give out conk out break break down', 'piss urine pee piddle weewee water peeing pissing make urinate puddle micturate peepee make water relieve oneself take a leak spend a penny wee weewee pass water', 'this', 'morning morn morning time forenoon good morning dawn dawning aurora first light daybreak break of day break of the day dayspring sunrise sunup cockcrow', 'there at that place in that location in that respect on that point thither', 'blood rake rakehell profligate rip roue lineage line line of descent descent bloodline blood line pedigree ancestry origin parentage stemma stock', 'my', 'piss urine pee piddle weewee water peeing pissing make urinate puddle micturate peepee make water relieve oneself take a leak spend a penny wee weewee pass water', 'redish', 'whats', 'wrong wrongfulness legal injury damage incorrect improper amiss awry haywire illtimed unseasonable untimely faulty incorrectly wrongly', 'with', 'me Maine Pine Tree State ME', 'having have have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'sex sexual activity sexual practice sex activity sexual urge gender sexuality arouse excite turn on wind up', 'with', 'my', 'girlfriend girl lady friend', 'yesterday', 'had have have got hold feature experience receive get own possess let consume ingest take in take throw make give induce stimulate cause accept suffer sustain give birth deliver bear birth', 'hurt injury harm trauma distress suffering detriment damage scathe ache smart pain anguish injure wound bruise offend spite suffer wounded weakened', 'so sol soh thus thusly then and so and then therefore hence thence indeed', 'much a lot lots a good deal a great deal very much practically often']\n",
      "14 documents [(['aloh hawai hello hi howdoyoudo howdy hullo stat', 'ac an i iodin on singl un', 'acceiv bear bir caus consum del expery feat get giv got hav hold in induc ingest let mak own person possess receiv rich stim suff sustain tak throw wealthy', 'fluid runny', 'horn intrud nos nozzl nuzzl olfact org pok pry scent wind', 'al altogeth complet entir tot whol', 'clip clock dimend four met prison sent term tim', 'an iodin singl', 'dont', 'a acknowledg at away bang be bed bonk cogn do eff fuck go hay hump intercours intim it jazz know laid lie liv lov off out recogn rol screw sex sleep the togeth with', 'what', 'act along answ arrang bash behav brawl coif coiff com doct doh dress execut exerc far mak man of osteopathy out perform pract serv set ut', 'about almost approxim around astir clos just less mor most near nigh or rough so som to virt wellnigh', 'inform technolog', 'an iodin singl', 'sniff sniffl whiff', 'altogeth entir whol', 'clock four prison term', '', 'by past prec retir tens yesteryear', 'few', 'clar day daylight daytim hour jr mean period shepard sid sol twentyfo year', 'it technolog', 'acquir aim amaz ar arrest back baffl beat becom beget begin bewild bring bug buzz capt catch com contract convey develop down draw driv dumbfound engend fath fetch find fix flummox for fuck gen go gravel grow induc int mak moth myst nonpl obtain off out pay perplex pos produc puzzl scram sir skin start stick stupefy sustain und vex', 'id rattl real selfsam very', 'annoy both bothersom chaf devil gal gravel irrit nark nettl nettlesom pesky pest plaguey plaguy rag ril teas vex vexaty', 'an and matchless nonpareil peerless un unit unmatch unr unrival', 'my', 'naus', 'bar barricad blank block blockad chok deflect embarrass forget freez halt hind immobl imp jam kibosh lug obstruct obt occlud off parry plug stop stuff stymy up us', 'an iodin singl', 'couldnt', 'brea breath hint plac spac spel', 'dec good ord prop right way', '', 'pharynx throat', 'ach anct bru dam detry distress harm hurt ind injury offend pain scath smart spit traum wound', 'aft afterward lat subsequ', 'cold coldblood common dusty frigid inhum insens low stal temp', 'bath cascad exhibit lav rain show', 'cobbl conclud dea end fin go in last lastplac lowest net out point shoemak stag surv termin utmost', 'dark night nighttim nox', 'an iodin singl', 'non not', 'as certain enough indisput shoot sur trust', '', 'along arrang behav caus coif com doct dress exerc get man osteopathy perform serv suff ut', 'almost around clos less most nigh rough som virt', 'it', 'it', 'feel fing flav flavo look palp property sens smel spirit tactil ton', 'alik car comp correspond ilk lik sam simil wish', 'in loc plac respect that ther thith', 'someth', '', 'throat', 'iv', 'been compr constitut cost embody eq ex follow mak repres', 'dry iron juiceless prohibit teetot wry', 'cough', '', 'respect sev vary', 'day daytim jr period sid twentyfo', 'already', 'an iodin singl', 'of rec', 'acquir break build educ evolv exply form germin grow highlydevelop modern origin prep recrudesc ris spring train upr', 'check circumst condit consid disciplin precondit qual shap spec stip train', '', 'ask cal command demand expect involv necessit nee post requir want', 'buy frequ haunt patron shop spons', 'washroom', 'breez cal chaff chat chew chitch clav confab fat gossip impos inflict inspect jaw nat see sojourn to travel visit', 'througout', 'day daytim jr period sid twentyfo', 'an iodin singl', 'hent indee soh then thent theref thu thus', 'ban bor commonplac exhaust fag fatigu hackney jad oldh outwear pal play run sap shopworn stock threadbare timeworn tir trit upon wear weary wellworn', 'thi', 'auror break cockcrow dawn daybreak dayspr first forenoon light morn sunr sunup tim', 'every', 'bingl exclud individ singl undivid unmarry', 'an i matchless on peerless un unmatch unrival', '', 'brawn brawny hefty musc musclem muscul sinew tissu', 'anct dam distress hurt injury pain smart suff traum', 'hent so soh then theref thus', 'deal gre lot much oft very', 'altogeth entir whol', '', 'artic articulatio cigaret join joint junct marijuan reef roast spliff', 'acet acid dour fals fer glow glum moody moros offkey rancid saturnin sour sul tart turn work', 'sint', 'cobbl dea fin hold last liv lowest on point stag surv up utmost', 'break dawn daybreak first good light of sunr the', 'an iodin singl', 'feel fing flavo palp sens spirit ton', 'hent so sol then theref thus', 'coldblood dusty inhum low stal', 'altogeth entir whol', 'clock four prison term', 'an iodin singl', 'sneez', 'altogeth entir whol', 'clock four prison term'], 'Common Cold'), (['afterward on subsequ', 'assocy footbal socc', 'bet biz crippl gag gam gamey gamy gimpy gritty lam mettlesom plan plot punt secret spunky stak', 'yesterday', 'an iodin singl', 'ascertain bas bump chant constitut detect determin discov encount est feel found ground hap institut launch lin observ oneself plant recov regain retriev rul set wit', 'belittl dimin humbl littl min minusc modest pockets smal smallsc', 'blow bulg chant demot dislodg downstair excresc extrud gib gibbos hump jut kick knock promin protrud protub releg swel', '', 'bark clamb cut hid par peel pelt scramble scrape shin shinny sput struggle tegu', 'extrem utmost uttermost', 'itch itchy', 'when', 'an iodin singl', 'as bet dal diddl fiddl flirt mak meet perform playact recr repres roleplay spiel toy trifl wag wreak', '', '', 'acquaint admir boost champ friend protagon quak support', 'now nowaday today', 'it', 'feel felt fing mat mattup sens', 'car correspond lik sam the wish', 'an iodin singl', 'bear caus del feat giv had hold induc let own receiv suff tak', '', '', 'cent ey eyebal heart middl ocul opt', 'an iodin singl', 'battl blazon clam cry egregy exclaim exig flagr glar gross hol hollo insist inst outcry ral rank scream shout squal tear voc war watchword weep yel', 'uncontrol', '', 'in intrud nozzl olfact pok scent', 'bad belong blend bucket buy cash chip conk croak dead deceas depart die drop exit expir extend fail farm fit funct ghost giveup going in kick lead leav locomot loss mov on op pass per plump pop process releas rifl short sled snuff sound surv travel way', 'hoosy inch indian inward', 'are ballpark common green mungo park parkland', 'ear oth', 'day daytim jr period sid twentyfo', 'an iodin singl', 'back celebr continu fresh hold keep kept maintain observ op preserv prev restrain retain sav stay unbrok', 'sniffl', 'ful intact integr stal', 'clock four prison term', 'sometim', 'an iodin singl', 'could', 'not', 'breath intim plac spel', 'don through', 'an i nonpareil peerless un unmatch unrival', '', 'in nos nuzzl org pry wind', 'it', 'feel fing flavo palp sens spirit ton', 'car correspond lik sam the', 'it', 'bar blank blockad clos deflect embarrass freez hind imp kibosh obstruct occlud off parry stop stymy us', '', 'an iodin singl', 'air the walk walkto', 'ballpark common lot mungo parkland', '', 'break dawn daybreak first good morn sunr the', 'an iodin singl', 'back continu go keep maintain on preserv process restrain sav sustain unbrok', 'sternut', 'approxim clos less or round som', '', 'bloom blossom effloresc flow flush heyday peak prim', 'ev eventid fiftyfifty level regul stil tied yet', 'though', 'an iodin singl', 'not', 'coldblood dusty inhum low stal', 'gard menagery zoo zoolog', 'nowaday', 'it', 'feel find fing mattup sens', 'car correspond lik sam the', 'an iodin singl', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'feath fledg plum squ', '', '', 'throat', 'ful integr tot', 'day daytim jr period sid twentyfo', 'it', 'genuin rattl tru', 'antsy fidg fret', 'an iodin singl', '', '', 'ful integr tot', 'day daytim jr period sid twentyfo', 'an iodin singl', 'beak blam cle cul fault foot nibbl peck pick piec pluck plunk', 'blossom flow heyday prim', 'nowaday', 'charact compon constitu contribut disunit divid funct out part perc piec port reg sect sep shar split tak up voic', '', 'bridg deal hand handwrit help hir mit ov pass paw reach script', '', 'adjoin advert affect allud com concern contact disturb doe extend fey haveto impact match mov partak pertain reach ref rel riv stir tinct ting tint touch with', '', 'blossom flow heyday prim', 'becam get suit', 'conceit egot intumesc puff selfconceit swol swollenhead tumefy tumesc vain wel', 'an iodin singl', 'bar hard scanty scarc', 'bend bow creas crimp crook crouch curv deform dext flex fold plic stoop twist', 'it', 'it', 'feel fing flavo palp sens spirit ton', 'asleep benumb blunt dul numb'], 'Allergies'), (['an iodin singl', 'abid appeas bid delay detain on outrid persist put quel remain rest rid stick', 'improv upward', '', 'broad ful fullofthemoon moon phas replet to wax wid widecut', 'night nox', 'abras attrit bray comminut cranch craunch crunch detrit dig drudg grat grind lab labo mash moil toil travail', 'cab chop drudg hack lit machin nag plug polit tax taxicab wardheel whoop writ', 'compass due earl frederick guilford magnet n nor north northward second', 'cast cont design envid extern fant fig im lab out pict plan project propos see send task undertak vis', 'adjac follow fut next success', 'break dawn daybreak first good morn sunr the', '', 'ey heart ocul', 'bright bril burn hop lust prom shiny undim vivid', 'bloodr bolshevik bolshy carmin cer cherry cherryr crimson ink marx red redfac ruby rubyr ruddy scarlet viol', 'altogeth entir whol', '', 'ancestry blood bloodlin desc of par pedigr proflig rak rakehel rip rou stemm', 'vas vessel watercraft', '', 'blood desc of par proflig rakehel rou stock', 'vis', '', 'ey heart ocul', 'genuin rattl tru', 'fidg itchy', '', 'break dawn daybreak first good morn sunr the', '', 'an iodin singl', 'ar awak fir heat ignit inflam rous up wak wok', 'improv upward', 'it', 'knock ping pink pinko rap tap', '', 'an iodin singl', 'across ad ass attend byword catch com constru control dat discov ens escort examin fant find go hear in ins interpret into learn look meet pick pow proverb reckon regard saw steady through understand view visit watch wit word', 'it', 'don with', 'mir', 'afterward on', 'drown flo liquid nai swim', '', 'biot commun district interest resid', 'flo nai', 'billiard consort kitty pocket pond pool puddl synd', '', 'calend hebdomad week workweek', 'an iodin singl', 'feel find mat palp up', 'bit caut combust down electrocut fir incin stak sting sunburn up', 'adept aesthes dat esthes facul geni hotshot impress mav mavin senty star superst virtuoso whiz whizz wiz wizard', '', 'ey heart ocul', 'an iodin singl', 'cal cri exclaim hollo outcry shout weep', ''], 'Conjunctivitis'), (['an iodin singl', 'bad belong break bucket cash chok conk dead depart down drop exit extend farm funct ghost giveup going in kick lead locomot mov on out per pop rifl short sound surv travel way went', 'begrim bemir che col contamin dingy dirty filthy foul grim illgot lousy markedup muddy soil sordid unc unsport unsportsmanlik', 'eat eatery hous resta', 'adenin amp angstrom antiophthalm axerophthol deoxyadenosin fact group monophosph typ vitamin', '', 'day daytim jr period sid twentyfo', 'ago agon', 'an iodin singl', 'beg com get lead out start', 'feel fing flavo palp sens spirit ton', 'airhead dizzy emptyhead featherbrain giddy lighthead sil vertigin woozy', '', 'break dawn daybreak first good morn sunr the', 'ab init initio', 'afterward bel form on postery prevy subsequ tardy ultery', 'an iodin singl', 'barf cast cat chuck disgorg honk puk purg regorg regurgit retch sick spew spu up upchuck vomit', 'everyth', 'closet extinct forbid kayo kod out prohibit proscrib stun taboo tabu verbot', 'afterward on', 'corrod deplet exhaust fee out rust up wip', 'approxim clos less or so to', 'street', 'food intellect nour nutry solid thought', 'an iodin singl', 'begin bulg depart embark get going jump off origin part protrud start startl up', 'feel fing flavo palp sens spirit ton', 'dizzy featherbrain lighthead vertigin', 'debl decrepit faint fall feebl frail imperfect infirm rick sapless unacc washy watery weak', 'bor down exhaust fatigu jad out outwear play sap stock timeworn trit wear wellworn', 'altogeth entir whol', 'clock four prison term', 'furtherm is moreov', 'an iodin singl', 'beg com get off set', 'apprehend becharm beguil bewitch catch charm coll custody enam enamo ench ent fascin gimmick grab haul hitch in match overhear overtak pinch snap snatch tak trant trip view with', 'febr febril fev pyrex', 'halfway midway', 'don with', 'hebdomad workweek', 'also besid excess likew ov too', 'an iodin singl', 'bad belong break buy chip conk dead depart down end exit extend farm funct ghost giveup going in kick lead locomot mov on out per pop rifl short sound surv travel way work', '', '', 'alot', 'ext gre sir than thoma', 'oft oftentim ofttim', 'of', 'an iodin singl', 'across ascertain ass byword catch com constru dat discov ens escort expery fig get hear in interpret it learn look meet pick pow proverb reckon run saw steady through understand view visit wind wit', 'blood desc of par proflig rakehel rou stock', '', 'bathroom can commod crap gut john pot potty privy sew stool throne toilet', 'an iodin singl', 'believ cerebr cogit conceiv guess imagin intend opin recal reckon recollect rememb suppos think', 'an iodin singl', 'might mighty pow', 'beryll constitut embody ex glucin mak repres', 'poop', 'blood desc of par proflig rakehel rou stock', '', 'abdom bear bel breadbasket brook digest out stand stomach support tol tum tummy vent', 'anct dam distress hurt injury pain smart suff wound', 'afterward on', 'meal repast', 'amus cury fishy funny laugh mirth od peculi que remark rum rummy shady singul story suspect suspicy', 'degust rel sampl savo smack tast try', 'angl fish pisc', 'civil cult educ school schoolhouse schooling schooltime sho', 'cafeter', 'it', 'anct dam distress hurt injury pain smart suff wound', 'hent so sol thent thu', 'deal gre much pract', '', 'an iodin singl', 'hard scanty', 'standup', 'consecut direct flat fullstreng heterosex neat squ straight straightaway straightforward tru unb unbow uncoil', 'an iodin singl', 'also besid likew too', '', 'afterward on', '', 'repast', 'alway const forev incess invary perpet', 'bor down fag hackney oldh outwear play sap stock timeworn trit wear wellworn'], 'E Coli Infection'), (['an iodin singl', 'across ascertain attend car check consid constru dat discov ens escort expery fig get hear in interpret it lin machin out pict project proverb reckon run see steady through understand view visit wind word', 'approxim clos less or so to', 'yellowgreen', 'acquit arc assoil clear discharg dismiss dismit dispatch drop eject elect emit empty exculp exon expel fre lib must out outpo releas sack spark unload vent waiv', 'abstemy alight caloriefr christ cle dismount easy fal get idl illum illumin level lightcol lightsom lit loos lowc lumin luminos perch promiscu rady scant short slut sourc spark sparkl swoon twinkl uncloud unhors vis wanton within', 'blee haemorrh hemorrh leech phlebotomise phlebotomize shed', 'between betwixt tween', '', 'catamen ful geolog mens menstru period stop', 'assocy nurs', 'cas episod occas', 'bit caut cut electrocut glow incin stak sunburn up', 'adept champ esthes facul hotshot mav sens senty superst whiz wiz', 'patch spel whil', 'leak mict oneself pee peep penny piddl piss reliev spend urin wat wee weew', 'thos', 'symptom', 'chlamydia', 'adenin angstrom axerophthol fact monophosph unit vitamin', '', 'beam broadcast carry channel conduc famil genet heredit impart inherit transf transmiss transmit transport', 'infect', 'hawai hi howdy stat', 'an iodin singl', 'of', 'feel go know receiv through', 'bit caut cut electrocut glow off sting the', 'adept champ esthes facul hotshot mav sens star virtuoso whizz wizard', 'piec whil', 'leak mict pass pee penny piss reliev tak urin wee', 'buy haunt shop', 'micturit', 'adenin angstrom axerophthol fact monophosph unit', '', 'arc clear discharg dismit down eject emit exculp exon fir fre lib off outpo releas sack spark vent', '', 'eventid flush level regul tied', 'difficul difficult troubl', 'leak mict pass peep piddl puddl reliev tak wat weew', 'an iodin singl', 'of', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'ail both cark discommod disoblig disord disquiet distract fuss hassl incommod inconveny out perturb problem troubl unh upset worry', 'bad belong break buy chip conk dead depart down end expir fail fit get ghost giveup going in kick lead liv locomot low mov on out per pop releas run sled sound surv travel way', '', '', 'becaus', '', 'genit ven', 'area aren country domain expans field orbit sphere surfac', 'anct dam distress hurt injury pain smart suff weak', 'dur', 'urin', 'they', '', 'bad bangup beau bul clotheshors cork crack crestless dandy dud fash fop gre groovy keen nifty peachy plat sheik slapup smash tumefy up wav', 'improv upward', 'arbit every haphazard indiscrimin random way which willynil'], 'Chlamydia'), (['hi howdy', 'an iodin singl', 'bear caus del feat giv had hold induc let own receiv suff tak', '', 'bit caut cut electrocut glow off sting the', 'sensationâ€i', 'believ cerebr conceiv guess ide intellect mean ment opin persuas recal recollect retriev suppos thought view', 'it', 'cystit', 'an iodin singl', 'bad belong break buy chip conk dead depart down end expir fail fit get giv giveup going in kick lead locomot mov on out per pop rifl short sound surv travel way work', '', 'bushel church doc doct dr furb md medico mend on phys repair soph touch', 'he hel', 'afford apply cav collaps commit consecr ded devot eas fal found gav gift grant hand hold in leav mov op pass pres rend return sacr turn way yield', 'main me pin tre', 'approxim clos less or so to', 'antibiot drug', 'it', 'didnt', 'adam bad belong biscuit break buy chip conk crist dead depart disco drop ecstasy exit extend farm fling gam ghost giveup going hug it last liv low off op pass plump process run snuff spel surv tour turn way whirl x xtc', 'asid by off outsid', 'adenin angstrom axerophthol fact monophosph unit', '', 'day daytim jr period sid twentyfo', 'afterward bel form on prevy subsequ ultery', 'an iodin singl', 'acquir amaz arrest baffl becom begin bring buzz catch com convey down driv engend fath find flummox fuck get got grow induc let moth nonpl off out perplex produc receiv scram sir start stim stupefy sustain und', 'approxim clos less or so to', 'bleb blist bull scald ves whip', '', 'vagin', 'altogeth entir whol', 'on', '', 'backtalk brim lip mou rim sass talk', '', '', 'blist scald whip', 'an iodin singl', 'believ cerebr conceiv guess intend opin reckon rememb suppos up', 'an iodin singl', 'acquir amaz arrest baffl becom begin bring buzz catch com convey down driv engend fath find flummox fuck get got grow induc let moth nonpl off out perplex produc receiv set sir start stim suff tak und', 'approxim clos less or so to', 'clamb hid peel scramble shin skin sput tegu', 'aggrav both discomfort excit innerv piqu provoc sor vex', 'raz', 'bit caut cut fir incin mark sting sunt tan', 'but equit exact fair hard mer on scarc simply upright', 'approxim clos less or so to', '', 'itchy rub scaby scratch spoil urg', '', 'doubl duply echo ingemin ov recapit reduply reit rep repetit reply repr retel', 'cas erst erstwhil in ont', '', '', 'monthwh', ''], 'Hepatitis B'), (['an iodin singl', 'bring chaff chew clav down fat impos inflict jaw see the travel', 'camp hobo jungl', '', 'day daytim jr period sid twentyfo', 'agon', 'an iodin singl', 'acquir amaz arrest baffl becom begin bring buzz catch com convey down driv engend fath find flummox fuck get got grow induc let moth nonpl off out perplex produc receiv set skin stick stupefy sustain und', 'foreign strange unknown unus', 'burn chomp insect morsel prick pung racy seiz sharp snack tee', '', 'it', '', 'away becom belong blend bucket cash chok crack crist dead depart disco drop ecstasy exit extend farm fling gam ghost giveup going hug it last liv low off op pass plump process run snuff spel surv tour turn way work x', 'away for out outsid', '', 'day daytim jr period sid twentyfo', 'afterward bel form on prevy subsequ ultery', 'an iodin singl', 'beg com get off set', 'ar build educ exply germin grow modern prep recrudesc spring up upr', 'febril pyrex', 'brawny musc muscul tissu', 'helpless impuiss', '', 'should', 'an iodin singl', 'along arrang behav caus coiff do doh execut far mak of out pract set ut', 'an iodin singl', 'across attain break chant describ disclos distinct divulg expos fal get hap ident key let lin nam observ out rev strike unwrap upon word', 'eldritch uncanny unearth weird wyrd', 'mosquito', 'burn col insect prick racy sharp sting tee', '', 'deal hand help man mit ov paw reach turn', '', 'along arrang caus coiff depart doe energy exerc get man out pract set', 'not', 'away becom biscuit break buy chip conk crist dead depart disco drop ecstasy exit extend farm fling gam ghost giveup going hug it last liv low off op pass plump process run snuff spel surv tour turn way work xtc', 'away for out', 'altern els instead rath', 'it', 'bangup bul cork crestless dud fop gre intumesc keen nifty peachy puff sheik smash tumefy up wel', '', 'deal hand help man on pass play script', 'benumb dead dul', '', 'room', 'angl bir consum expery get got ha has hold in ingest mak possess stim sustain throw', 'uncanny weird', 'burn col morsel pung seiz snack tee', 'his', 'bon brow forehead front os', 'hel', 'becharm bewitch catch caught enam ench fascin grab hold of overhear pick tak trip view with', 'febril pyrex', 'compr cost eq evergreen follow mak repres up wa was washington', 'baffl bedevil befuddl bemus blur brok confound confus dis discombob disconcert disconnect disjoint flurry fox fuddl garbl illog jumbl lost maz mix mixedup obnubl obsc put scat sea unconnect upset', 'almost around clos less most nigh rough som virt', '', 'beleagu besieg bord circumv environ fent hem milieu palisad ring skirt smoth surround wal', 'hel', 'feel go know receiv through', 'mod sight', 'casual depr expir going ink pass personnel releas', 'accru autumn capit crepusc dec declend declin decreas descend devolv dip downfal downslop dusk evenfal flow gloam hang light nightfal pin precipit settl spil strike surrend tumbl twilight', '', 'occass', 'coma comatos', '', '', 'an iodin singl', 'along arrang behav caus coiff do doh execut far mak of out pract set ut', 'almost around clos less most nigh rough som virt', 'him', 'an iodin singl', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'child fry kid min nestl nip pockets shav smallsc tiddl tik tyk ven youngst', 'cephalalg head headach worry', 'be cat disgorg emes puk regorg retch spew throw upchuck', 'an iodin singl', 'across ascertain attend car check consid control determin encount ens escort expery fig get hear in interpret it lin machin out pict project real regard saw steady through understand view visit wind word', 'bolshevik carmin cherry crimson ink marx redfac ruby ruddy viol', 'blizzard rash roseol', 'altogeth entir whol', 'complet end oer termin', '', 'backbon backrest backward bind book column cov dors endors gag hind indors plump punt rach rear rearward second spin support vertebr', 'breast bureau chest dress pect thorax', 'abid bel brook end put stand stomach support tum up'], 'West Nile Virus'), (['hey', 'in on point that thith', 'an iodin singl', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'been constitut embody ex liv person up', 'believ feel fing flavo intuit not palp smel tact ton touch', 'extrem high sup', 'big defect forg highrisk regret risky sorry spoil spoilt tough uncollect unfit unsound', 'of', '', 'febril pyrex', 'ar build educ exply germin grow modern prep recrudesc spring underdevelop upr', 'past retir tim yesteryear', '', 'day daytim jr period sid twentyfo', 'an iodin singl', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'been constitut embody ex liv person up', 'acquir amaz arrest baffl becom begin bring buzz catch com convey down driv engend fath find flummox fuck get gravel hav int mak myst obtain on pay pos puzzl scram sir start stim suff tak vex', '', 'blist scald whip', 'effloresc rash skin', '', 'body consist eubst person soundbox structure torso trunk', 'of', '', 'backtalk cav fiss mou mouthpiec rim speak ut verb', 'angl bir consum expery get got has hold in ingest mak possess stim sustain throw', 'as besid too', 'been constitut embody ex liv person up', 'rattl selfsam', 'afflict huffy mad raw sensit tend', 'adenin angstrom axerophthol fact monophosph unit', '', 'ber billet com direct grad hom ident invest lay lieu off ord piazz plaz pos posit property rang rat seat set sit situ spot stead target topograph', 'in on point that thith', 'angl bir consum expery get got has hold in ingest mak possess stim sustain throw', 'been constitut embody ex liv person up', 'itchy', 'altogeth entir whol', 'complet end ov', '', 'consist eubst person soundbox torso', 'an iodin singl', 'bad belong break buy chip conk dead depart down end expir fail fit get giv go hold it last liv low off op pass plump process run snuff start surv travel way work', '', 'admir boost friend quak', 'busy domicy famy firm hous household mand men planet sign the zodiac', 'cobbl dea fin hold last liv net out shoemak stop termin utmost', 'night nox', 'ceas conclud fin stop', 'improv upward', 'ar build educ exply germin grow modern prep recrudesc spring underdevelop upr', 'febril pyrex', 'bolshevik carmin cherry crimson ink marx redfac ruby ruddy viol', 'apart ber bit blem blob blot dappl daub descry discern distinct espy fleck mac musc musca out pick pip point post situ slur smear smirch smudg speckl spot spotlight spy stain tel touch volit', 'alabam alumin camell dixy of', 'complet end ov', '', 'consist eubst person soundbox torso', 'an iodin singl', '', 'eventid flush out regul tied', 'corrod eat fee out rust up wip', 'anyth', '', 'an iodin singl', 'bear caus del feat giv had hold induc let own receiv suff tak', 'no nobel', 'appet appetit', 'begin bulg depart embark get going jump off origin part protrud start tak', 'acquir amaz arrest baffl becom begin bring buzz catch com convey down driv engend fath find flummox fuck get gravel hav int mak myst obtain on pay pos puzzl scram sir start stim suff tak vex', 'fidg itchy', 'altogeth entir whol', 'complet end ov', '', 'consist eubst person soundbox torso', '', 'backtalk fiss mou or sass speak ut', 'angl bir consum expery get got has hold in ingest mak possess stim sustain throw', 'been constitut embody ex liv person up', 'expery find flav impress intuit not palp smel tact ton', 'big forg regret sorry spoil tough unfit', 'of', 'an iodin singl', 'neer nev', 'feel fing flavo palp sens spirit ton', 'car correspond lik sam the', 'corrod eat fee out rust up wip', '', 'brawny musc muscul tissu', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'as likew wel', 'been constitut embody ex liv person up', 'ext mor than to', 'huffy pain raw sor tend', 'of', 'eventid flush out stil yet', '', 'an iodin singl', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'not', 'been constitut embody ex liv person up', '', 'gym gymnas', '', 'clamb hid peel scramble shin skin struggle', 'angl bir consum expery get got has hold in ingest mak possess stim sustain throw', 'get suit', 'ext mor than to', 'fidg itchy', 'of', 'blist scald whip', 'effloresc roseol', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'appear look seem', 'altogeth entir whol', 'complet end ov', '', 'consist eubst person soundbox torso', 'of', '', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'altogeth entir whol', 'begin bulg depart embark get going jump off origin part protrud start tak', 'itchy scaby spoil', '', 'arm blazonry branch coat fort gird impl limb munit sleev subdivid system war weapon weaponry', 'hamst tendon', 'as likew wel', 'begin bulg depart embark get going jump off origin part protrud start tak', 'hurt lanct smart yearn yen', 'an iodin singl', 'conceiv think', 'an iodin singl', 'crataeg engl hawthorn laevigat may oxycanth whitethorn', 'beryll constitut embody ex glucin mak repres', 'ar build educ exply germin grow modern prep recrudesc spring underdevelop upr', 'febril pyrex', '', '', '', 'emin gamy gear height highpitch highschool luxury mellow school seny up', '', 'cobbl dea fin hold last liv net out shoemak stop termin utmost', 'meas', 'bolshevik carmin cherry crimson ink marx redfac ruby ruddy viol', 'ber bit blob dappl descry distinct fleck mac musc off patch pip point post situ smear smudg spot spy tel touch', 'ap attract invok sympathet', 'altogeth entir whol', 'complet end ov', '', 'branch fort limb sleev system weapon', '', 'expery find flav impress look opin sens spirit tact ton', 'an quit rath', 'uncomfort', 'but exact good just mer on scarc upright', '', 'break dawn daybreak first good morn sunr the', 'an iodin singl', 'ceas conclud fin stop', 'improv upward', 'ar build educ exply germin grow modern prep recrudesc spring underdevelop upr', 'febril pyrex', '', 'backtalk fiss mou or sass talk verb', 'angl bir consum expery get got has hold in ingest mak possess stim sustain throw', 'feel find mat palp up', 'rattl selfsam', 'huffy pain sensit tend', '', '', 'admir boost friend quak', 'gabriel', 'an iodin singl', 'bad belong break buy chip conk dead depart down end expir fail fit get giv go hold it last liv low off op pass plump process run snuff start the up went', '', 'film flick mot motionpict movingpict movy pic show', 'an iodin singl', 'ceas conclud fin stop', 'improv upward', 'ar build educ exply germin grow modern prep recrudesc spring underdevelop upr', 'effloresc roseol', 'altogeth entir whol', 'complet end ov', '', 'consist eubst person soundbox torso', '', 'becom bend cal chang deform flex mov on plough plow revers rick sprain turn twist wrench wrick', '', 'fidg itchy', 'ber bit blob dappl descry distinct fleck mac musc off patch pip point post situ smear smudg spot spy tel touch', 'afterward bel form on prevy subsequ ultery', '', 'night nox', 'an iodin singl', 'as likew wel', 'appra assess calc delib evalu mens out quant valu', '', '', 'bear caus del feat giv had hold induc let own receiv suff tak', 'ar build educ exply germin grow mak modern prep recrudesc spring up', 'febril pyrex', '', 'break dawn daybreak first good morn sunr the', 'an iodin singl', 'not', 'eventid flush out stil yet', 'act actu be displac impress incit mak motil process prompt propel reloc strike', 'adv comfort easy fountainhead intim subst wel wellspr', '', '', 'brawny musc muscul tissu', 'achy lanct smart yearn', ''], 'Chickenpox'), (['ic', 'cal commem commend recal rememb think', 'citizenry hoi mass multitud peopl pollo unwash', 'alleg av enjoin enount enunt out pronount read say stat suppos', '', 'acquir amaz arrest baffl becom begin bring buzz catch com convey down driv engend fath find flummox fuck get got grow induc let moth nonpl off out perplex produc receiv set skin stick stupefy sustain und', 'angry fury tempestu wild', '', 'an iodin singl', 'along arrang behav caus coiff do doh execut far mak of out pract set ut', 'feel fing flavo palp sens spirit ton', 'fury tempestu', 'although', 'an iodin singl', '', 'acknowledg away be bonk do expery get hav hay in intercours it know lie lov off out rol sex the with', 'wheref why', 'an iodin singl', '', 'cal commend recal rememb think', 'her', 'facesh', 'authorit cruc import sign', 'me stat tre', 'an iodin singl', '', 'not', '', 'anim biograph hist lif lifesp lifetim sent sprightliness', 'me stat', 'cal commend recal rememb think', '', 'aspect bold brass cheek confront express fac font fount grimac hum nerv pres typefac', 'wher', 'along arrang caus coiff did dress exerc get man perform serv suff', 'an iodin singl', 'abs allow alon behind bequea entrust farewel forget giv impart leav leavetak on part provid pul result up wil', '', 'bank box camb cant coin company deposit institut money rely swear', 'annot banknot bil distinct emin fed govern greenback let mark mus observ preemin promiss reserv tak', 'it', 'but exact good just now prec simply', '', '', 'an iodin singl', 'cal recal recov rememb think', 'them', '', 'along arrang caus coiff did dress exerc get man perform serv suff', 'you', 'artic av enount ord pronount say stat tel', '', 'address speak ut', 'brassy cheap flash flashy gar gaudy gimcrack loud meretricy tacky tatty tawdry trashy', '', 'brawny musc muscul tissu', 'as likew wel', 'anct hurt offend smart suff', 'hurt pin suff yearn', 'an rath', 'admin allot band batch bunch caboodl circ deal destiny dish dispens distribut dol fat flock fortun gre hat heap lot luck mess mickl mint mountain muckl parcel passel pil plenty pot raft shel sight slew spat stack sum tidy wad', 'it', 'as likew wel', 'uncanny weird', '', '', 'bring fath fem fuss get overprotect sir', 'abod domicil dwel habit hous intern intery nat nurs plat', 'cobbl dea fin hold last liv net out shoemak stop termin utmost', 'weekend', 'we', 'cook dev dispos educ fain gear groom inclin mak prep ready train', 'adult big bigheart boast bount bounty brag braggart braggy cockahoop crow enceint freehand ful fullgrown gravid grown grownup handsom heavy larg lib magnanim openhand selfaggrand vaingl vaunt', 'din party', 'an iodin singl', 'bear caus del feat giv had hold induc let own receiv suff tak', 'no', 'appetit', 'compl discharg fil in nail', 'chor job project task undertak', 'along arrang caus coiff do doing execut far mak out pract set', 'affair thing', 'angl bir consum expery get got has hold in ingest mak possess stim sustain throw', 'been constitut embody ex liv person up', 'rattl selfsam', 'hard unm', 'of', 'an iodin singl', 'appear seem', 'not', 'cal commend recal rememb think', 'mat thing', 'an iodin singl', 'along arrang caus coiff did dress exerc get man perform serv suff', '', 'past retir tim', 'along arrang caus coiff do dress exerc get man perform serv suff', 'mat', 'an iodin singl', 'faimili', '', 'angl bir consum expery get got has hold in ingest mak possess stim sustain throw', 'been constitut embody ex liv person up', 'befuddl blur confus disconcert flurry fuddl mix obsc perplex puzzl up', 'feel find mat palp up', 'closet extinct kayo kod out proscrib taboo the verbot', 'ber blank commit grad ident invest lieu off out piazz plaz pos post put rang rat send sho sit spac stat stead topograph', 'it', 'appear look seem', '', 'spat', 'opt', 'dop facul gag goddess grass gumpt hors jan locowee mary out pot sen senty sess sign skunk smok wit', 'as likew wel', 'decreas fal'], 'Alzheimer'), (['', 'backbon backward bind column dors for gam indors plump punt rear second stak up vertebr', 'anct dam distress hurt injury pain smart suff wound', 'hent so sol thent thu', 'deal gre much pract', 'but exact good just now prec simply', 'tak walk', 'approxim clos less or round som', '', 'ankl joint mort talocr', 'egot puff swel swol tumefy up vain', 'also besid fault ov too', '', 'babbl bag bean blab lect mou out peach publ sing spil tattl town verb', 'me stat', 'but exact good just now prec simply', 'away becom biscuit break buy chip conk crist dead depart disco drop ecstasy exit extend farm fling gam ghost giveup going hug it last liv low off op pass plump process run snuff spel surv tour turn way work xtc', 'away for out', 'allow away behind depart entrust farewel forget giv impart leav of out pass provid result up', 'me stat', 'entir lon sol solit solo unaccompany uneq unequal unparallel', '', 'flow geolog menstru period stop', 'deep lat prevy tardy', 'away past', 'deuceac ii leash tercet tern terzetto three threesome tierc trey triad trin trio triplet troik', 'daysit', 'almost near virt', 'nev', 'deep lat prevy tardy', '', 'fac meat posit slop', 'cramp hamp strangle', 'hent so sol thent thu', 'ardu backbreak difficult gruel heavy intemp knockout operos pun strong surd toilsom unvo voiceless', 'it', 'feel find mat palp up', 'car correspond lik sam the', 'individ person somebody someon soul', 'shred shredding up', 'me stat', 'at bottom deep heart indo insid inward privileg within', 'closet extinct kayo kod out proscrib taboo the'], 'Pregnancy'), (['no', 'count issu subject top weigh', 'how', 'fiddl footl lilliput minusc niggl petty picayun piffl slight triv', 'an iodin singl', 'corrod eat fee out rust up wip', 'an iodin singl', 'acquir amaz arrest baffl becom begin bring buzz catch com convey down driv engend fath find flummox fuck get gravel hav int mak myst obtain on pay pos puzzl scram sir start stim suff tak vex', 'fatty fertil juicy rich', 'for intellect nutry thought', '', 'enemy foe foem opposit', 'an iodin singl', 'declin defy deny food garb refus reject resist scraps up wast', 'ced fal resign up', '', 'many', 'gram kiloc kilogram nutrit', 'in inch indian stat', '', 'sandwich', 'bad complaint il inauspicy omin poor', 'but exact good just now prec simply', 'acquir admit adopt ask assum away cal chart choos claim conduc consum contain convey demand driv eng fil for guid hir in involv lead leas mak nee occupy on pack payoff post read remov rent return select strike study submit subscrib to up withdraw', 'appl mal orchard pumil', 'els or', 'an iodin singl', 'feel fing flavo palp sens spirit ton', 'hent so sol thent thu', 'coldblood dusty inhum low stal', 'eventid flush out stil yet', '', 'an iodin singl', 'artic bear bust cloth don end fal get haby hold jad off out put thin upon vest weary', 'addit duply extr redund spar spear supererog superflu supernum surpl', 'cak coat pel', 'an iodin singl', 'be cat disgorg honk pur regorg retch sanct sco spew throw upchuck', 'myself', 'blank cleanl cleans fair hous housec jerk mak pick plumb scaveng sport sportsmanlik sporty strip uncloud uncontamin uninfect unobject whit', 'nowaday', 'cas erstwhil in ont', 'ext mor than to', 'but exact good just now prec simply', 'car correspond lik sam the', 'an iodin singl', 'const eer forev invary', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'dem exceiv leav omit tak', 'in on point that thith', 'adenin angstrom axerophthol fact monophosph unit', 'bit caut cut electrocut glow off sting the', 'adept champ esthes facul hotshot mav sens star virtuoso whizz wizard', 'belt blu bolt cut depress devo dispirit downcast downheart downward drink finetun gloom grim h john kil l land lowspirit mast off pat pil pol pour push refin the toss', '', 'throat', 'an iodin singl', 'conject estim gaug guess guesswork hazard hypothes inf judg pretend shot suppos supposit surm vent', 'if', '', 'dam leontyn monet pric tol', 'an iodin singl', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'ant compens earn mak pay remun sal wag'], 'Eating Disorders including Anorexia & Bulimia Nervosa'), (['of', '', 'bureau draw of pect', 'expery find flav impress look opin sens spirit tactil touch', 'jol mod pretty reason somewh', 'besot blind blotto cockey compress crock drunk fast load mingy mis nasty pieey pixil plast rig slop slosh soak sous sozzl squiffy stiff stringent taut tight wet', 'eventid flush out stil yet', 'benumb dead numb', 'astatin', 'clock four multiply sent tim', 'an iodin singl', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'get suit', 'alight caloriefr cle dismount easy fal get ignit illum in light lightcol lightsom loos lumin off perch rady short sourc sparkl trip twinkl uncloud up wak wanton within', 'direct guid head maneuv manoeuv manoeuvr ste', 'dizzy featherbrain lighthead vertigin', '', 'almost degr highest near the virt', '', 'day daytim jr period sid twentyfo', 'depict fork furn giv in interl out pict rend rendit show supply transl turn vert', 'me stat', 'depl lam pity sad', 'useless', 'an iodin singl', 'as likew wel', 'oft ofttim', 'be cat disgorg emet naus purg regurgit sick spu up vomit', 'expiery', 'heartburn pyros', '', 'hell', 'abomin atrocy aw dread pain terr unspeak', '', 'genuin rattl tru', 'car disquiet disturb occupy vex', '', 'an iodin singl', 'mighty', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'dang grav griev lifethr sery sob unplay', 'diseas', '', 'dyspeps indigest stomach', 'not', 'aid assist avail facilit oneself serv'], 'Coronary Heart Disease'), (['in on point that thith', 'adenin angstrom axerophthol fact monophosph unit', 'bundl clump clust crew crowd gang togeth', 'bolshevik carmin cherry crimson ink marx redfac ruby ruddy viol', 'ber bit blob dappl descry distinct fleck mac musc off patch pip point post situ smear smudg spot spy tel touch', '', 'bureau draw of thorax', 'an iodin singl', '', 'believ cerebr conceiv guess intend opin reckon rememb suppos up', '', 'contact hold of through', 'any whatev whatsoev', 'allergenswhy', 'habit wont', '', 'away becom biscuit break buy chip conk crist dead depart disco drop ecstasy exit extend farm fling gam ghost giveup going hug it last liv low off op pass plump process run snuff spel surv tour turn way work xtc', 'away for out', 'in on point that thith', 'dimin littl min modest smal', 'bit flak minut mo mom numb routin scrap snatch turn', 'milklik milky', 'album andrew ash b blanch bloodless caucas d dickson dougla e edward eg elwyn flannel gabardin harold lilywhit livid martind ovalbumin patrick riv snowy stanford t theod twee vict whitehot', 'flu limpid liqu melt smoo swim', 'adv amount approach climax der descend fal follow forthcom hail issu numb occ orgasm sex up upcom', 'closet extinct kayo kod out proscrib taboo the', '', 'nipplesshould', 'an iodin singl', 'beryll constitut embody ex glucin mak repres', 'com doe imply occupy pertain rel touch worry', 'charact constitu depart disunit for off part person port rol sect set shar start the voic', '', 'boob bosom chest knock meat summit tit titty', 'feel find mat palp up', 'bit chip flak minut mom numb prick routin second snatch sting turn', 'hardalmost', 'chunky lumpy', 'would', '', 'away becom biscuit break buy chip conk crist dead depart disco drop ecstasy exit extend farm fling gam ghost giveup going hug it last liv low off op pass plump process run snuff spel surv tour turn way work xtc', 'away for out', 'afterward on', 'puberty pubesc', 'it', 'but exact good just now prec simply', 'com go occ occur on pass tak', 'me stat', 'nowaday', '', '', 'mamill mammill nippl pap teat', 'lookdiff', '', 'conject divin hypothec hypothet opin presuppos say socal suppos the', 'beryll constitut embody ex glucin mak repres', 'bury deepset dip drop go laps pass recess settl sink slid slump subsid sunk'], 'Breast Cancer'), (['', 'been constitut embody ex liv person up', 'expery find flav impress look opin sens spirit tactil touch', 'car correspond lik sam the', 'an iodin singl', 'cal for involv nee requir requisit want', 'away becom biscuit break buy chip conk crist dead depart disco drop ecstasy exit extend farm fling gam ghost giveup going hug it last liv low off op pass plump process run snuff spel surv tour turn way work xtc', '', '', 'ext mor than to', 'ext mor than to', 'deal gre oft ofttim', 'thes', 'day daytim jr period sid twentyfo', '', 'begin down get getgo kickoff off offset out outset root showtim start', 'aw dir fear fearsom fright hor horrend terr', '', '', 'actu head jaunt misstep on set slip stumbl travel trig tripup up', '', 'day daytim jr period sid twentyfo', '', 'it', 'anct dam distress hurt injury pain smart suff wound', 'leak mict pass peep piddl puddl spend urin wee', '', 'an iodin singl', 'bad belong break buy chip conk dead depart down end expir fail fit get giv go hold it last liv low off op pass plump process run snuff start the up went', 'leak mict pass peep piddl puddl spend urin wee', '', 'break dawn daybreak first good morn sunr the', 'in on point that thith', 'blood desc of par proflig rakehel rou stock', '', 'leak mict pass peep piddl puddl spend urin wee', '', '', 'amiss awry faul haywir illtim improp incorrect leg unseason untim wrong', '', 'me stat', 'bear caus del feat giv hav in ingest mak possess stim sustain throw', 'ar gend pract turn urg', '', '', 'girl girlfriend lady', '', 'bear caus del feat giv had hold induc let own receiv suff tak', 'anct dam distress hurt injury pain smart suff weak', 'hent so sol thent thu', 'deal gre much pract'], 'Prostate Cancer')]\n",
      "14 classes ['Allergies', 'Alzheimer', 'Breast Cancer', 'Chickenpox', 'Chlamydia', 'Common Cold', 'Conjunctivitis', 'Coronary Heart Disease', 'E Coli Infection', 'Eating Disorders including Anorexia & Bulimia Nervosa', 'Hepatitis B', 'Pregnancy', 'Prostate Cancer', 'West Nile Virus']\n",
      "607 unique stemmed words ['a acknowledg at away bang be bed bonk cogn do eff fuck go hay hump intercours intim it jazz know laid lie liv lov off out recogn rol screw sex sleep the togeth with', 'ab init initio', 'abdom bear bel breadbasket brook digest out stand stomach support tol tum tummy vent', 'abid appeas bid delay detain on outrid persist put quel remain rest rid stick', 'abid bel brook end put stand stomach support tum up', 'abod domicil dwel habit hous intern intery nat nurs plat', 'abomin atrocy aw dread pain terr unspeak', 'about almost approxim around astir clos just less mor most near nigh or rough so som to virt wellnigh', 'abras attrit bray comminut cranch craunch crunch detrit dig drudg grat grind lab labo mash moil toil travail', 'abs allow alon behind bequea entrust farewel forget giv impart leav leavetak on part provid pul result up wil', 'abstemy alight caloriefr christ cle dismount easy fal get idl illum illumin level lightcol lightsom lit loos lowc lumin luminos perch promiscu rady scant short slut sourc spark sparkl swoon twinkl uncloud unhors vis wanton within', 'ac an i iodin on singl un', 'acceiv bear bir caus consum del expery feat get giv got hav hold in induc ingest let mak own person possess receiv rich stim suff sustain tak throw wealthy', 'accru autumn capit crepusc dec declend declin decreas descend devolv dip downfal downslop dusk evenfal flow gloam hang light nightfal pin precipit settl spil strike surrend tumbl twilight', 'acet acid dour fals fer glow glum moody moros offkey rancid saturnin sour sul tart turn work', 'ach anct bru dam detry distress harm hurt ind injury offend pain scath smart spit traum wound', 'achy lanct smart yearn', 'acknowledg away be bonk do expery get hav hay in intercours it know lie lov off out rol sex the with', 'acquaint admir boost champ friend protagon quak support', 'acquir admit adopt ask assum away cal chart choos claim conduc consum contain convey demand driv eng fil for guid hir in involv lead leas mak nee occupy on pack payoff post read remov rent return select strike study submit subscrib to up withdraw', 'acquir aim amaz ar arrest back baffl beat becom beget begin bewild bring bug buzz capt catch com contract convey develop down draw driv dumbfound engend fath fetch find fix flummox for fuck gen go gravel grow induc int mak moth myst nonpl obtain off out pay perplex pos produc puzzl scram sir skin start stick stupefy sustain und vex', 'acquir amaz arrest baffl becom begin bring buzz catch com convey down driv engend fath find flummox fuck get got grow induc let moth nonpl off out perplex produc receiv scram sir start stim stupefy sustain und', 'acquir amaz arrest baffl becom begin bring buzz catch com convey down driv engend fath find flummox fuck get got grow induc let moth nonpl off out perplex produc receiv set sir start stim suff tak und', 'acquir amaz arrest baffl becom begin bring buzz catch com convey down driv engend fath find flummox fuck get got grow induc let moth nonpl off out perplex produc receiv set skin stick stupefy sustain und', 'acquir amaz arrest baffl becom begin bring buzz catch com convey down driv engend fath find flummox fuck get gravel hav int mak myst obtain on pay pos puzzl scram sir start stim suff tak vex', 'acquir break build educ evolv exply form germin grow highlydevelop modern origin prep recrudesc ris spring train upr', 'acquit arc assoil clear discharg dismiss dismit dispatch drop eject elect emit empty exculp exon expel fre lib must out outpo releas sack spark unload vent waiv', 'across ad ass attend byword catch com constru control dat discov ens escort examin fant find go hear in ins interpret into learn look meet pick pow proverb reckon regard saw steady through understand view visit watch wit word', 'across ascertain ass byword catch com constru dat discov ens escort expery fig get hear in interpret it learn look meet pick pow proverb reckon run saw steady through understand view visit wind wit', 'across ascertain attend car check consid constru dat discov ens escort expery fig get hear in interpret it lin machin out pict project proverb reckon run see steady through understand view visit wind word', 'across ascertain attend car check consid control determin encount ens escort expery fig get hear in interpret it lin machin out pict project real regard saw steady through understand view visit wind word', 'across attain break chant describ disclos distinct divulg expos fal get hap ident key let lin nam observ out rev strike unwrap upon word', 'act actu be displac impress incit mak motil process prompt propel reloc strike', 'act along answ arrang bash behav brawl coif coiff com doct doh dress execut exerc far mak man of osteopathy out perform pract serv set ut', 'actu head jaunt misstep on set slip stumbl travel trig tripup up', 'adam bad belong biscuit break buy chip conk crist dead depart disco drop ecstasy exit extend farm fling gam ghost giveup going hug it last liv low off op pass plump process run snuff spel surv tour turn way whirl x xtc', 'addit duply extr redund spar spear supererog superflu supernum surpl', 'address speak ut', 'adenin amp angstrom antiophthalm axerophthol deoxyadenosin fact group monophosph typ vitamin', 'adenin angstrom axerophthol fact monophosph unit', 'adenin angstrom axerophthol fact monophosph unit vitamin', 'adept aesthes dat esthes facul geni hotshot impress mav mavin senty star superst virtuoso whiz whizz wiz wizard', 'adept champ esthes facul hotshot mav sens senty superst whiz wiz', 'adept champ esthes facul hotshot mav sens star virtuoso whizz wizard', 'adjac follow fut next success', 'adjoin advert affect allud com concern contact disturb doe extend fey haveto impact match mov partak pertain reach ref rel riv stir tinct ting tint touch with', 'admin allot band batch bunch caboodl circ deal destiny dish dispens distribut dol fat flock fortun gre hat heap lot luck mess mickl mint mountain muckl parcel passel pil plenty pot raft shel sight slew spat stack sum tidy wad', 'admir boost friend quak', 'adult big bigheart boast bount bounty brag braggart braggy cockahoop crow enceint freehand ful fullgrown gravid grown grownup handsom heavy larg lib magnanim openhand selfaggrand vaingl vaunt', 'adv amount approach climax der descend fal follow forthcom hail issu numb occ orgasm sex up upcom', 'adv comfort easy fountainhead intim subst wel wellspr', 'affair thing', 'afflict huffy mad raw sensit tend', 'afford apply cav collaps commit consecr ded devot eas fal found gav gift grant hand hold in leav mov op pass pres rend return sacr turn way yield', 'aft afterward lat subsequ', 'afterward bel form on postery prevy subsequ tardy ultery', 'afterward bel form on prevy subsequ ultery', 'afterward on', 'afterward on subsequ', 'aggrav both discomfort excit innerv piqu provoc sor vex', 'ago agon', 'agon', 'aid assist avail facilit oneself serv', 'ail both cark discommod disoblig disord disquiet distract fuss hassl incommod inconveny out perturb problem troubl unh upset worry', 'air the walk walkto', 'airhead dizzy emptyhead featherbrain giddy lighthead sil vertigin woozy', 'al altogeth complet entir tot whol', 'alabam alumin camell dixy of', 'album andrew ash b blanch bloodless caucas d dickson dougla e edward eg elwyn flannel gabardin harold lilywhit livid martind ovalbumin patrick riv snowy stanford t theod twee vict whitehot', 'alight caloriefr cle dismount easy fal get ignit illum in light lightcol lightsom loos lumin off perch rady short sourc sparkl trip twinkl uncloud up wak wanton within', 'alik car comp correspond ilk lik sam simil wish', 'alleg av enjoin enount enunt out pronount read say stat suppos', 'allergenswhy', 'allow away behind depart entrust farewel forget giv impart leav of out pass provid result up', 'almost around clos less most nigh rough som virt', 'almost degr highest near the virt', 'almost near virt', 'aloh hawai hello hi howdoyoudo howdy hullo stat', 'along arrang behav caus coif com doct dress exerc get man osteopathy perform serv suff ut', 'along arrang behav caus coiff do doh execut far mak of out pract set ut', 'along arrang caus coiff depart doe energy exerc get man out pract set', 'along arrang caus coiff did dress exerc get man perform serv suff', 'along arrang caus coiff do doing execut far mak out pract set', 'along arrang caus coiff do dress exerc get man perform serv suff', 'alot', 'already', 'also besid excess likew ov too', 'also besid fault ov too', 'also besid likew too', 'altern els instead rath', 'although', 'altogeth entir whol', 'alway const forev incess invary perpet', 'amiss awry faul haywir illtim improp incorrect leg unseason untim wrong', 'amus cury fishy funny laugh mirth od peculi que remark rum rummy shady singul story suspect suspicy', 'an and matchless nonpareil peerless un unit unmatch unr unrival', 'an i matchless on peerless un unmatch unrival', 'an i nonpareil peerless un unmatch unrival', 'an iodin singl', 'an quit rath', 'an rath', 'ancestry blood bloodlin desc of par pedigr proflig rak rakehel rip rou stemm', 'anct dam distress hurt injury pain smart suff traum', 'anct dam distress hurt injury pain smart suff weak', 'anct dam distress hurt injury pain smart suff wound', 'anct hurt offend smart suff', 'angl bir consum expery get got ha has hold in ingest mak possess stim sustain throw', 'angl bir consum expery get got has hold in ingest mak possess stim sustain throw', 'angl fish pisc', 'angry fury tempestu wild', 'anim biograph hist lif lifesp lifetim sent sprightliness', 'ankl joint mort talocr', 'annot banknot bil distinct emin fed govern greenback let mark mus observ preemin promiss reserv tak', 'annoy both bothersom chaf devil gal gravel irrit nark nettl nettlesom pesky pest plaguey plaguy rag ril teas vex vexaty', 'ant compens earn mak pay remun sal wag', 'antibiot drug', 'antsy fidg fret', 'any whatev whatsoev', 'anyth', 'ap attract invok sympathet', 'apart ber bit blem blob blot dappl daub descry discern distinct espy fleck mac musc musca out pick pip point post situ slur smear smirch smudg speckl spot spotlight spy stain tel touch volit', 'appear look seem', 'appear seem', 'appet appetit', 'appetit', 'appl mal orchard pumil', 'appra assess calc delib evalu mens out quant valu', 'apprehend becharm beguil bewitch catch charm coll custody enam enamo ench ent fascin gimmick grab haul hitch in match overhear overtak pinch snap snatch tak trant trip view with', 'approxim clos less or round som', 'approxim clos less or so to', 'ar awak fir heat ignit inflam rous up wak wok', 'ar build educ exply germin grow mak modern prep recrudesc spring up', 'ar build educ exply germin grow modern prep recrudesc spring underdevelop upr', 'ar build educ exply germin grow modern prep recrudesc spring up upr', 'ar gend pract turn urg', 'arbit every haphazard indiscrimin random way which willynil', 'arc clear discharg dismit down eject emit exculp exon fir fre lib off outpo releas sack spark vent', 'ardu backbreak difficult gruel heavy intemp knockout operos pun strong surd toilsom unvo voiceless', 'are ballpark common green mungo park parkland', 'area aren country domain expans field orbit sphere surfac', 'arm blazonry branch coat fort gird impl limb munit sleev subdivid system war weapon weaponry', 'artic articulatio cigaret join joint junct marijuan reef roast spliff', 'artic av enount ord pronount say stat tel', 'artic bear bust cloth don end fal get haby hold jad off out put thin upon vest weary', 'as besid too', 'as bet dal diddl fiddl flirt mak meet perform playact recr repres roleplay spiel toy trifl wag wreak', 'as certain enough indisput shoot sur trust', 'as likew wel', 'ascertain bas bump chant constitut detect determin discov encount est feel found ground hap institut launch lin observ oneself plant recov regain retriev rul set wit', 'asid by off outsid', 'ask cal command demand expect involv necessit nee post requir want', 'asleep benumb blunt dul numb', 'aspect bold brass cheek confront express fac font fount grimac hum nerv pres typefac', 'assocy footbal socc', 'assocy nurs', 'astatin', 'at bottom deep heart indo insid inward privileg within', 'auror break cockcrow dawn daybreak dayspr first forenoon light morn sunr sunup tim', 'authorit cruc import sign', 'aw dir fear fearsom fright hor horrend terr', 'away becom belong blend bucket cash chok crack crist dead depart disco drop ecstasy exit extend farm fling gam ghost giveup going hug it last liv low off op pass plump process run snuff spel surv tour turn way work x', 'away becom biscuit break buy chip conk crist dead depart disco drop ecstasy exit extend farm fling gam ghost giveup going hug it last liv low off op pass plump process run snuff spel surv tour turn way work xtc', 'away for out', 'away for out outsid', 'away past', 'babbl bag bean blab lect mou out peach publ sing spil tattl town verb', 'back celebr continu fresh hold keep kept maintain observ op preserv prev restrain retain sav stay unbrok', 'back continu go keep maintain on preserv process restrain sav sustain unbrok', 'backbon backrest backward bind book column cov dors endors gag hind indors plump punt rach rear rearward second spin support vertebr', 'backbon backward bind column dors for gam indors plump punt rear second stak up vertebr', 'backtalk brim lip mou rim sass talk', 'backtalk cav fiss mou mouthpiec rim speak ut verb', 'backtalk fiss mou or sass speak ut', 'backtalk fiss mou or sass talk verb', 'bad bangup beau bul clotheshors cork crack crestless dandy dud fash fop gre groovy keen nifty peachy plat sheik slapup smash tumefy up wav', 'bad belong blend bucket buy cash chip conk croak dead deceas depart die drop exit expir extend fail farm fit funct ghost giveup going in kick lead leav locomot loss mov on op pass per plump pop process releas rifl short sled snuff sound surv travel way', 'bad belong break bucket cash chok conk dead depart down drop exit extend farm funct ghost giveup going in kick lead locomot mov on out per pop rifl short sound surv travel way went', 'bad belong break buy chip conk dead depart down end exit extend farm funct ghost giveup going in kick lead locomot mov on out per pop rifl short sound surv travel way work', 'bad belong break buy chip conk dead depart down end expir fail fit get ghost giveup going in kick lead liv locomot low mov on out per pop releas run sled sound surv travel way', 'bad belong break buy chip conk dead depart down end expir fail fit get giv giveup going in kick lead locomot mov on out per pop rifl short sound surv travel way work', 'bad belong break buy chip conk dead depart down end expir fail fit get giv go hold it last liv low off op pass plump process run snuff start surv travel way work', 'bad belong break buy chip conk dead depart down end expir fail fit get giv go hold it last liv low off op pass plump process run snuff start the up went', 'bad complaint il inauspicy omin poor', 'baffl bedevil befuddl bemus blur brok confound confus dis discombob disconcert disconnect disjoint flurry fox fuddl garbl illog jumbl lost maz mix mixedup obnubl obsc put scat sea unconnect upset', 'ballpark common lot mungo parkland', 'ban bor commonplac exhaust fag fatigu hackney jad oldh outwear pal play run sap shopworn stock threadbare timeworn tir trit upon wear weary wellworn', 'bangup bul cork crestless dud fop gre intumesc keen nifty peachy puff sheik smash tumefy up wel', 'bank box camb cant coin company deposit institut money rely swear', 'bar barricad blank block blockad chok deflect embarrass forget freez halt hind immobl imp jam kibosh lug obstruct obt occlud off parry plug stop stuff stymy up us', 'bar blank blockad clos deflect embarrass freez hind imp kibosh obstruct occlud off parry stop stymy us', 'bar hard scanty scarc', 'barf cast cat chuck disgorg honk puk purg regorg regurgit retch sick spew spu up upchuck vomit', 'bark clamb cut hid par peel pelt scramble scrape shin shinny sput struggle tegu', 'bath cascad exhibit lav rain show', 'bathroom can commod crap gut john pot potty privy sew stool throne toilet', 'battl blazon clam cry egregy exclaim exig flagr glar gross hol hollo insist inst outcry ral rank scream shout squal tear voc war watchword weep yel', 'be cat disgorg emes puk regorg retch spew throw upchuck', 'be cat disgorg emet naus purg regurgit sick spu up vomit', 'be cat disgorg honk pur regorg retch sanct sco spew throw upchuck', 'beak blam cle cul fault foot nibbl peck pick piec pluck plunk', 'beam broadcast carry channel conduc famil genet heredit impart inherit transf transmiss transmit transport', 'bear caus del feat giv had hold induc let own receiv suff tak', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'bear caus del feat giv hav in ingest mak possess stim sustain throw', 'becam get suit', 'becaus', 'becharm bewitch catch caught enam ench fascin grab hold of overhear pick tak trip view with', 'becom bend cal chang deform flex mov on plough plow revers rick sprain turn twist wrench wrick', 'been compr constitut cost embody eq ex follow mak repres', 'been constitut embody ex liv person up', 'befuddl blur confus disconcert flurry fuddl mix obsc perplex puzzl up', 'beg com get lead out start', 'beg com get off set', 'begin bulg depart embark get going jump off origin part protrud start startl up', 'begin bulg depart embark get going jump off origin part protrud start tak', 'begin down get getgo kickoff off offset out outset root showtim start', 'begrim bemir che col contamin dingy dirty filthy foul grim illgot lousy markedup muddy soil sordid unc unsport unsportsmanlik', 'beleagu besieg bord circumv environ fent hem milieu palisad ring skirt smoth surround wal', 'believ cerebr cogit conceiv guess imagin intend opin recal reckon recollect rememb suppos think', 'believ cerebr conceiv guess ide intellect mean ment opin persuas recal recollect retriev suppos thought view', 'believ cerebr conceiv guess intend opin reckon rememb suppos up', 'believ feel fing flavo intuit not palp smel tact ton touch', 'belittl dimin humbl littl min minusc modest pockets smal smallsc', 'belt blu bolt cut depress devo dispirit downcast downheart downward drink finetun gloom grim h john kil l land lowspirit mast off pat pil pol pour push refin the toss', 'bend bow creas crimp crook crouch curv deform dext flex fold plic stoop twist', 'benumb dead dul', 'benumb dead numb', 'ber billet com direct grad hom ident invest lay lieu off ord piazz plaz pos posit property rang rat seat set sit situ spot stead target topograph', 'ber bit blob dappl descry distinct fleck mac musc off patch pip point post situ smear smudg spot spy tel touch', 'ber blank commit grad ident invest lieu off out piazz plaz pos post put rang rat send sho sit spac stat stead topograph', 'beryll constitut embody ex glucin mak repres', 'besot blind blotto cockey compress crock drunk fast load mingy mis nasty pieey pixil plast rig slop slosh soak sous sozzl squiffy stiff stringent taut tight wet', 'bet biz crippl gag gam gamey gamy gimpy gritty lam mettlesom plan plot punt secret spunky stak', 'between betwixt tween', 'big defect forg highrisk regret risky sorry spoil spoilt tough uncollect unfit unsound', 'big forg regret sorry spoil tough unfit', 'billiard consort kitty pocket pond pool puddl synd', 'bingl exclud individ singl undivid unmarry', 'biot commun district interest resid', 'bit caut combust down electrocut fir incin stak sting sunburn up', 'bit caut cut electrocut glow incin stak sunburn up', 'bit caut cut electrocut glow off sting the', 'bit caut cut fir incin mark sting sunt tan', 'bit chip flak minut mom numb prick routin second snatch sting turn', 'bit flak minut mo mom numb routin scrap snatch turn', 'blank cleanl cleans fair hous housec jerk mak pick plumb scaveng sport sportsmanlik sporty strip uncloud uncontamin uninfect unobject whit', 'bleb blist bull scald ves whip', 'blee haemorrh hemorrh leech phlebotomise phlebotomize shed', 'blist scald whip', 'blizzard rash roseol', 'blood desc of par proflig rakehel rou stock', 'bloodr bolshevik bolshy carmin cer cherry cherryr crimson ink marx red redfac ruby rubyr ruddy scarlet viol', 'bloom blossom effloresc flow flush heyday peak prim', 'blossom flow heyday prim', 'blow bulg chant demot dislodg downstair excresc extrud gib gibbos hump jut kick knock promin protrud protub releg swel', 'body consist eubst person soundbox structure torso trunk', 'bolshevik carmin cherry crimson ink marx redfac ruby ruddy viol', 'bon brow forehead front os', 'boob bosom chest knock meat summit tit titty', 'bor down exhaust fatigu jad out outwear play sap stock timeworn trit wear wellworn', 'bor down fag hackney oldh outwear play sap stock timeworn trit wear wellworn', 'branch fort limb sleev system weapon', 'brassy cheap flash flashy gar gaudy gimcrack loud meretricy tacky tatty tawdry trashy', 'brawn brawny hefty musc musclem muscul sinew tissu', 'brawny musc muscul tissu', 'brea breath hint plac spac spel', 'break dawn daybreak first good light of sunr the', 'break dawn daybreak first good morn sunr the', 'breast bureau chest dress pect thorax', 'breath intim plac spel', 'breez cal chaff chat chew chitch clav confab fat gossip impos inflict inspect jaw nat see sojourn to travel visit', 'bridg deal hand handwrit help hir mit ov pass paw reach script', 'bright bril burn hop lust prom shiny undim vivid', 'bring chaff chew clav down fat impos inflict jaw see the travel', 'bring fath fem fuss get overprotect sir', 'broad ful fullofthemoon moon phas replet to wax wid widecut', 'bundl clump clust crew crowd gang togeth', 'bureau draw of pect', 'bureau draw of thorax', 'burn chomp insect morsel prick pung racy seiz sharp snack tee', 'burn col insect prick racy sharp sting tee', 'burn col morsel pung seiz snack tee', 'bury deepset dip drop go laps pass recess settl sink slid slump subsid sunk', 'bushel church doc doct dr furb md medico mend on phys repair soph touch', 'busy domicy famy firm hous household mand men planet sign the zodiac', 'but equit exact fair hard mer on scarc simply upright', 'but exact good just mer on scarc upright', 'but exact good just now prec simply', 'buy frequ haunt patron shop spons', 'buy haunt shop', 'by past prec retir tens yesteryear', 'cab chop drudg hack lit machin nag plug polit tax taxicab wardheel whoop writ', 'cafeter', 'cak coat pel', 'cal commem commend recal rememb think', 'cal commend recal rememb think', 'cal cri exclaim hollo outcry shout weep', 'cal for involv nee requir requisit want', 'cal recal recov rememb think', 'calend hebdomad week workweek', 'camp hobo jungl', 'car correspond lik sam the', 'car correspond lik sam the wish', 'car disquiet disturb occupy vex', 'cas episod occas', 'cas erst erstwhil in ont', 'cas erstwhil in ont', 'cast cont design envid extern fant fig im lab out pict plan project propos see send task undertak vis', 'casual depr expir going ink pass personnel releas', 'catamen ful geolog mens menstru period stop', 'ceas conclud fin stop', 'ced fal resign up', 'cent ey eyebal heart middl ocul opt', 'cephalalg head headach worry', 'charact compon constitu contribut disunit divid funct out part perc piec port reg sect sep shar split tak up voic', 'charact constitu depart disunit for off part person port rol sect set shar start the voic', 'check circumst condit consid disciplin precondit qual shap spec stip train', 'child fry kid min nestl nip pockets shav smallsc tiddl tik tyk ven youngst', 'chlamydia', 'chor job project task undertak', 'chunky lumpy', 'citizenry hoi mass multitud peopl pollo unwash', 'civil cult educ school schoolhouse schooling schooltime sho', 'clamb hid peel scramble shin skin sput tegu', 'clamb hid peel scramble shin skin struggle', 'clar day daylight daytim hour jr mean period shepard sid sol twentyfo year', 'clip clock dimend four met prison sent term tim', 'clock four multiply sent tim', 'clock four prison term', 'closet extinct forbid kayo kod out prohibit proscrib stun taboo tabu verbot', 'closet extinct kayo kod out proscrib taboo the', 'closet extinct kayo kod out proscrib taboo the verbot', 'cobbl conclud dea end fin go in last lastplac lowest net out point shoemak stag surv termin utmost', 'cobbl dea fin hold last liv lowest on point stag surv up utmost', 'cobbl dea fin hold last liv net out shoemak stop termin utmost', 'cold coldblood common dusty frigid inhum insens low stal temp', 'coldblood dusty inhum low stal', 'com doe imply occupy pertain rel touch worry', 'com go occ occur on pass tak', 'coma comatos', 'compass due earl frederick guilford magnet n nor north northward second', 'compl discharg fil in nail', 'complet end oer termin', 'complet end ov', 'compr cost eq evergreen follow mak repres up wa was washington', 'conceit egot intumesc puff selfconceit swol swollenhead tumefy tumesc vain wel', 'conceiv think', 'conject divin hypothec hypothet opin presuppos say socal suppos the', 'conject estim gaug guess guesswork hazard hypothes inf judg pretend shot suppos supposit surm vent', 'consecut direct flat fullstreng heterosex neat squ straight straightaway straightforward tru unb unbow uncoil', 'consist eubst person soundbox torso', 'const eer forev invary', 'contact hold of through', 'cook dev dispos educ fain gear groom inclin mak prep ready train', 'corrod deplet exhaust fee out rust up wip', 'corrod eat fee out rust up wip', 'cough', 'could', 'couldnt', 'count issu subject top weigh', 'cramp hamp strangle', 'crataeg engl hawthorn laevigat may oxycanth whitethorn', 'cystit', 'dam leontyn monet pric tol', 'dang grav griev lifethr sery sob unplay', 'dark night nighttim nox', 'day daytim jr period sid twentyfo', 'daysit', 'deal gre lot much oft very', 'deal gre much pract', 'deal gre oft ofttim', 'deal hand help man mit ov paw reach turn', 'deal hand help man on pass play script', 'debl decrepit faint fall feebl frail imperfect infirm rick sapless unacc washy watery weak', 'dec good ord prop right way', 'declin defy deny food garb refus reject resist scraps up wast', 'decreas fal', 'deep lat prevy tardy', 'degust rel sampl savo smack tast try', 'dem exceiv leav omit tak', 'depict fork furn giv in interl out pict rend rendit show supply transl turn vert', 'depl lam pity sad', 'deuceac ii leash tercet tern terzetto three threesome tierc trey triad trin trio triplet troik', 'didnt', 'difficul difficult troubl', 'dimin littl min modest smal', 'din party', 'direct guid head maneuv manoeuv manoeuvr ste', 'diseas', 'dizzy featherbrain lighthead vertigin', 'don through', 'don with', 'dont', 'dop facul gag goddess grass gumpt hors jan locowee mary out pot sen senty sess sign skunk smok wit', 'doubl duply echo ingemin ov recapit reduply reit rep repetit reply repr retel', 'drown flo liquid nai swim', 'dry iron juiceless prohibit teetot wry', 'dur', 'dyspeps indigest stomach', 'ear oth', 'eat eatery hous resta', 'effloresc rash skin', 'effloresc roseol', 'egot puff swel swol tumefy up vain', 'eldritch uncanny unearth weird wyrd', 'els or', 'emin gamy gear height highpitch highschool luxury mellow school seny up', 'enemy foe foem opposit', 'entir lon sol solit solo unaccompany uneq unequal unparallel', 'ev eventid fiftyfifty level regul stil tied yet', 'eventid flush level regul tied', 'eventid flush out regul tied', 'eventid flush out stil yet', 'every', 'everyth', 'expery find flav impress intuit not palp smel tact ton', 'expery find flav impress look opin sens spirit tact ton', 'expery find flav impress look opin sens spirit tactil touch', 'expiery', 'ext gre sir than thoma', 'ext mor than to', 'extrem high sup', 'extrem utmost uttermost', 'ey heart ocul', 'fac meat posit slop', 'facesh', 'faimili', 'fatty fertil juicy rich', 'feath fledg plum squ', 'febr febril fev pyrex', 'febril pyrex', 'feel felt fing mat mattup sens', 'feel find fing mattup sens', 'feel find mat palp up', 'feel fing flav flavo look palp property sens smel spirit tactil ton', 'feel fing flavo palp sens spirit ton', 'feel go know receiv through', 'few', 'fiddl footl lilliput minusc niggl petty picayun piffl slight triv', 'fidg itchy', 'film flick mot motionpict movingpict movy pic show', 'flo nai', 'flow geolog menstru period stop', 'flu limpid liqu melt smoo swim', 'fluid runny', 'food intellect nour nutry solid thought', 'for intellect nutry thought', 'foreign strange unknown unus', 'ful intact integr stal', 'ful integr tot', 'furtherm is moreov', 'fury tempestu', 'gabriel', 'gard menagery zoo zoolog', 'genit ven', 'genuin rattl tru', 'get suit', 'girl girlfriend lady', 'gram kiloc kilogram nutrit', 'gym gymnas', 'habit wont', 'halfway midway', 'hamst tendon', 'hard scanty', 'hard unm', 'hardalmost', 'hawai hi howdy stat', 'he hel', 'heartburn pyros', 'hebdomad workweek', 'hel', 'hell', 'helpless impuiss', 'hent indee soh then thent theref thu thus', 'hent so soh then theref thus', 'hent so sol then theref thus', 'hent so sol thent thu', 'her', 'hey', 'hi howdy', 'him', 'his', 'hoosy inch indian inward', 'horn intrud nos nozzl nuzzl olfact org pok pry scent wind', 'how', 'huffy pain raw sor tend', 'huffy pain sensit tend', 'hurt lanct smart yearn yen', 'hurt pin suff yearn', 'ic', 'id rattl real selfsam very', 'if', 'improv upward', 'in inch indian stat', 'in intrud nozzl olfact pok scent', 'in loc plac respect that ther thith', 'in nos nuzzl org pry wind', 'in on point that thith', 'individ person somebody someon soul', 'infect', 'inform technolog', 'it', 'it technolog', 'itch itchy', 'itchy', 'itchy rub scaby scratch spoil urg', 'itchy scaby spoil', 'iv', 'jol mod pretty reason somewh', 'knock ping pink pinko rap tap', 'leak mict oneself pee peep penny piddl piss reliev spend urin wat wee weew', 'leak mict pass pee penny piss reliev tak urin wee', 'leak mict pass peep piddl puddl reliev tak wat weew', 'leak mict pass peep piddl puddl spend urin wee', 'lookdiff', 'main me pin tre', 'mamill mammill nippl pap teat', 'many', 'mat', 'mat thing', 'me stat', 'me stat tre', 'meal repast', 'meas', 'micturit', 'might mighty pow', 'mighty', 'milklik milky', 'mir', 'mod sight', 'monthwh', 'mosquito', 'my', 'myself', 'naus', 'neer nev', 'nev', 'night nox', 'nipplesshould', 'no', 'no nobel', 'non not', 'not', 'now nowaday today', 'nowaday', 'occass', 'of', 'of rec', 'oft oftentim ofttim', 'oft ofttim', 'on', 'opt', 'past retir tim', 'past retir tim yesteryear', 'patch spel whil', 'pharynx throat', 'piec whil', 'poop', 'puberty pubesc', 'rattl selfsam', 'raz', 'repast', 'respect sev vary', 'room', 'sandwich', 'sensationâ€i', 'should', 'shred shredding up', 'sint', 'sneez', 'sniff sniffl whiff', 'sniffl', 'someth', 'sometim', 'spat', 'standup', 'sternut', 'street', 'symptom', 'tak walk', 'them', 'thes', 'they', 'thi', 'thos', 'though', 'throat', 'througout', 'uncanny weird', 'uncomfort', 'uncontrol', 'urin', 'useless', 'vagin', 'vas vessel watercraft', 'vis', 'washroom', 'we', 'weekend', 'what', 'when', 'wher', 'wheref why', 'would', 'yellowgreen', 'yesterday', 'you']\n",
      "classes_details [{'class': 'Common Cold', 'treatments': 'Stay hydrated;rest;sooth a sore throat using saltwater gargle;take over the counter cold and cough medications\\n', 'danger level': '1', 'synonyms': 'Upper Respiratory Tract;Nose and Throat Infection'}, {'class': 'Allergies', 'treatments': 'Remove the cause of allergy; \\notherwise take Antihistamines to relieve sneezing;Decongestants to relieve congestions in nasal membranes;Anti inflamary agents to reduce;Allergy shots', 'danger level': '1', 'synonyms': 'Allergic Reaction'}, {'class': 'Conjunctivitis', 'treatments': 'See a doctor;Bacterial cases can be treated with antibiotic eye drops;Allergic reactions can be treated with other eye dro', 'danger level': '1', 'synonyms': 'Pink Eye'}, {'class': 'E Coli Infection', 'treatments': 'Visit a hospital immediately', 'danger level': '1', 'synonyms': ''}, {'class': 'Chlamydia', 'treatments': 'See a professional doctor;Use Azithromycin or Doxycycline', 'danger level': '4', 'synonyms': 'Sexually Transmitted Infection (STI)'}, {'class': 'Hepatitis B', 'treatments': 'See a doctor immediately;Take antiviral drugs', 'danger level': '6', 'synonyms': 'Herpes'}, {'class': 'West Nile Virus', 'treatments': 'Please see a healthcare professional;There are no specific treatments for West Nile Virus in humans', 'danger level': '8', 'synonyms': ''}, {'class': 'Chickenpox', 'treatments': 'Drink plenty of fluid;Take Tylenol;Consult with a doctor', 'danger level': '2', 'synonyms': 'Varicella Zoster Virus'}, {'class': 'Alzheimer', 'treatments': 'Cholinesterase inhibitors;Memantine(Namenda);Please consult with a professional doctor', 'danger level': '4', 'synonyms': ''}, {'class': 'Pregnancy', 'treatments': 'You are pregnant! Remember to take your prenatal vitamin;quit smoking;stop drinking alcohol;cut down on caffeine; avoid hazardous foods;eat well and sleep well!', 'danger level': '2', 'synonyms': ''}, {'class': 'Eating Disorders including Anorexia & Bulimia Nervosa', 'treatments': 'Seek help from a therapist, physician, and nutritionist', 'danger level': '3', 'synonyms': ''}, {'class': 'Coronary Heart Disease', 'treatments': 'quit smoking;have a healthy diet;exercise regularly;consult with a doctor', 'danger level': '7', 'synonyms': ''}, {'class': 'Breast Cancer', 'treatments': 'You may have breast cancer, please consult with a doctor and conduct further diagnostics at your local hospital', 'danger level': '10', 'synonyms': ''}, {'class': 'Prostate Cancer', 'treatments': 'You may have prostate cancer, please consult with a doctor and conduct further diagnostics at your local hospital', 'danger level': '10', 'synonyms': ''}]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "words = []\n",
    "words_total = []\n",
    "classes = []\n",
    "classes_details = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "\n",
    "# loop through each description per symptom\n",
    "for pattern in training_data:\n",
    "    symptoms = pattern['sentence']\n",
    "    word_list = symptoms.split(';')\n",
    "    print(\"WORD_LIST:\",word_list)\n",
    "    word_list_new = []\n",
    "    for word in word_list:\n",
    "        word = remove_punctuation(word)\n",
    "        word = word.strip().lower()\n",
    "        word_list_new.append(word)\n",
    "    print(\"WORD_LIST_NEW:\",word_list_new)\n",
    "    joined_symps = ' '.join(s for s in word_list_new)\n",
    "    print(\"joined_symps:\",joined_symps)\n",
    "    \n",
    "    # tokenize each word in the description\n",
    "    w = nltk.word_tokenize(joined_symps)\n",
    "    # add to our words list\n",
    "    words.extend(w)\n",
    "    # add to documents in our corpus\n",
    "    documents.append((w, pattern['class']))\n",
    "    # add to our classes list\n",
    "    if pattern['class'] not in classes:\n",
    "        classes.append(pattern['class'])\n",
    "        classes_details.append({\"class\":pattern['class'], \"synonyms\":pattern['synonyms'],\n",
    "                                \"treatments\":pattern['treatments'], \"danger level\":pattern['danger level']})\n",
    "    \n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# remove duplicates\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print (len(documents), \"documents\", documents)\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)\n",
    "print (\"classes_details\", classes_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENIZED WORDS:  ['aloh hawai hello hi howdoyoudo howdy hullo stat', 'ac an i iodin on singl un', 'acceiv bear bir caus consum del expery feat get giv got hav hold in induc ingest let mak own person possess receiv rich stim suff sustain tak throw wealthy', 'fluid runny', 'horn intrud nos nozzl nuzzl olfact org pok pry scent wind', 'al altogeth complet entir tot whol', 'clip clock dimend four met prison sent term tim', 'an iodin singl', 'dont', 'a acknowledg at away bang be bed bonk cogn do eff fuck go hay hump intercours intim it jazz know laid lie liv lov off out recogn rol screw sex sleep the togeth with', 'what', 'act along answ arrang bash behav brawl coif coiff com doct doh dress execut exerc far mak man of osteopathy out perform pract serv set ut', 'about almost approxim around astir clos just less mor most near nigh or rough so som to virt wellnigh', 'inform technolog', 'an iodin singl', 'sniff sniffl whiff', 'altogeth entir whol', 'clock four prison term', '', 'by past prec retir tens yesteryear', 'few', 'clar day daylight daytim hour jr mean period shepard sid sol twentyfo year', 'it technolog', 'acquir aim amaz ar arrest back baffl beat becom beget begin bewild bring bug buzz capt catch com contract convey develop down draw driv dumbfound engend fath fetch find fix flummox for fuck gen go gravel grow induc int mak moth myst nonpl obtain off out pay perplex pos produc puzzl scram sir skin start stick stupefy sustain und vex', 'id rattl real selfsam very', 'annoy both bothersom chaf devil gal gravel irrit nark nettl nettlesom pesky pest plaguey plaguy rag ril teas vex vexaty', 'an and matchless nonpareil peerless un unit unmatch unr unrival', 'my', 'naus', 'bar barricad blank block blockad chok deflect embarrass forget freez halt hind immobl imp jam kibosh lug obstruct obt occlud off parry plug stop stuff stymy up us', 'an iodin singl', 'couldnt', 'brea breath hint plac spac spel', 'dec good ord prop right way', '', 'pharynx throat', 'ach anct bru dam detry distress harm hurt ind injury offend pain scath smart spit traum wound', 'aft afterward lat subsequ', 'cold coldblood common dusty frigid inhum insens low stal temp', 'bath cascad exhibit lav rain show', 'cobbl conclud dea end fin go in last lastplac lowest net out point shoemak stag surv termin utmost', 'dark night nighttim nox', 'an iodin singl', 'non not', 'as certain enough indisput shoot sur trust', '', 'along arrang behav caus coif com doct dress exerc get man osteopathy perform serv suff ut', 'almost around clos less most nigh rough som virt', 'it', 'it', 'feel fing flav flavo look palp property sens smel spirit tactil ton', 'alik car comp correspond ilk lik sam simil wish', 'in loc plac respect that ther thith', 'someth', '', 'throat', 'iv', 'been compr constitut cost embody eq ex follow mak repres', 'dry iron juiceless prohibit teetot wry', 'cough', '', 'respect sev vary', 'day daytim jr period sid twentyfo', 'already', 'an iodin singl', 'of rec', 'acquir break build educ evolv exply form germin grow highlydevelop modern origin prep recrudesc ris spring train upr', 'check circumst condit consid disciplin precondit qual shap spec stip train', '', 'ask cal command demand expect involv necessit nee post requir want', 'buy frequ haunt patron shop spons', 'washroom', 'breez cal chaff chat chew chitch clav confab fat gossip impos inflict inspect jaw nat see sojourn to travel visit', 'througout', 'day daytim jr period sid twentyfo', 'an iodin singl', 'hent indee soh then thent theref thu thus', 'ban bor commonplac exhaust fag fatigu hackney jad oldh outwear pal play run sap shopworn stock threadbare timeworn tir trit upon wear weary wellworn', 'thi', 'auror break cockcrow dawn daybreak dayspr first forenoon light morn sunr sunup tim', 'every', 'bingl exclud individ singl undivid unmarry', 'an i matchless on peerless un unmatch unrival', '', 'brawn brawny hefty musc musclem muscul sinew tissu', 'anct dam distress hurt injury pain smart suff traum', 'hent so soh then theref thus', 'deal gre lot much oft very', 'altogeth entir whol', '', 'artic articulatio cigaret join joint junct marijuan reef roast spliff', 'acet acid dour fals fer glow glum moody moros offkey rancid saturnin sour sul tart turn work', 'sint', 'cobbl dea fin hold last liv lowest on point stag surv up utmost', 'break dawn daybreak first good light of sunr the', 'an iodin singl', 'feel fing flavo palp sens spirit ton', 'hent so sol then theref thus', 'coldblood dusty inhum low stal', 'altogeth entir whol', 'clock four prison term', 'an iodin singl', 'sneez', 'altogeth entir whol', 'clock four prison term']\n",
      "TOKENIZED WORDS:  ['afterward on subsequ', 'assocy footbal socc', 'bet biz crippl gag gam gamey gamy gimpy gritty lam mettlesom plan plot punt secret spunky stak', 'yesterday', 'an iodin singl', 'ascertain bas bump chant constitut detect determin discov encount est feel found ground hap institut launch lin observ oneself plant recov regain retriev rul set wit', 'belittl dimin humbl littl min minusc modest pockets smal smallsc', 'blow bulg chant demot dislodg downstair excresc extrud gib gibbos hump jut kick knock promin protrud protub releg swel', '', 'bark clamb cut hid par peel pelt scramble scrape shin shinny sput struggle tegu', 'extrem utmost uttermost', 'itch itchy', 'when', 'an iodin singl', 'as bet dal diddl fiddl flirt mak meet perform playact recr repres roleplay spiel toy trifl wag wreak', '', '', 'acquaint admir boost champ friend protagon quak support', 'now nowaday today', 'it', 'feel felt fing mat mattup sens', 'car correspond lik sam the wish', 'an iodin singl', 'bear caus del feat giv had hold induc let own receiv suff tak', '', '', 'cent ey eyebal heart middl ocul opt', 'an iodin singl', 'battl blazon clam cry egregy exclaim exig flagr glar gross hol hollo insist inst outcry ral rank scream shout squal tear voc war watchword weep yel', 'uncontrol', '', 'in intrud nozzl olfact pok scent', 'bad belong blend bucket buy cash chip conk croak dead deceas depart die drop exit expir extend fail farm fit funct ghost giveup going in kick lead leav locomot loss mov on op pass per plump pop process releas rifl short sled snuff sound surv travel way', 'hoosy inch indian inward', 'are ballpark common green mungo park parkland', 'ear oth', 'day daytim jr period sid twentyfo', 'an iodin singl', 'back celebr continu fresh hold keep kept maintain observ op preserv prev restrain retain sav stay unbrok', 'sniffl', 'ful intact integr stal', 'clock four prison term', 'sometim', 'an iodin singl', 'could', 'not', 'breath intim plac spel', 'don through', 'an i nonpareil peerless un unmatch unrival', '', 'in nos nuzzl org pry wind', 'it', 'feel fing flavo palp sens spirit ton', 'car correspond lik sam the', 'it', 'bar blank blockad clos deflect embarrass freez hind imp kibosh obstruct occlud off parry stop stymy us', '', 'an iodin singl', 'air the walk walkto', 'ballpark common lot mungo parkland', '', 'break dawn daybreak first good morn sunr the', 'an iodin singl', 'back continu go keep maintain on preserv process restrain sav sustain unbrok', 'sternut', 'approxim clos less or round som', '', 'bloom blossom effloresc flow flush heyday peak prim', 'ev eventid fiftyfifty level regul stil tied yet', 'though', 'an iodin singl', 'not', 'coldblood dusty inhum low stal', 'gard menagery zoo zoolog', 'nowaday', 'it', 'feel find fing mattup sens', 'car correspond lik sam the', 'an iodin singl', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'feath fledg plum squ', '', '', 'throat', 'ful integr tot', 'day daytim jr period sid twentyfo', 'it', 'genuin rattl tru', 'antsy fidg fret', 'an iodin singl', '', '', 'ful integr tot', 'day daytim jr period sid twentyfo', 'an iodin singl', 'beak blam cle cul fault foot nibbl peck pick piec pluck plunk', 'blossom flow heyday prim', 'nowaday', 'charact compon constitu contribut disunit divid funct out part perc piec port reg sect sep shar split tak up voic', '', 'bridg deal hand handwrit help hir mit ov pass paw reach script', '', 'adjoin advert affect allud com concern contact disturb doe extend fey haveto impact match mov partak pertain reach ref rel riv stir tinct ting tint touch with', '', 'blossom flow heyday prim', 'becam get suit', 'conceit egot intumesc puff selfconceit swol swollenhead tumefy tumesc vain wel', 'an iodin singl', 'bar hard scanty scarc', 'bend bow creas crimp crook crouch curv deform dext flex fold plic stoop twist', 'it', 'it', 'feel fing flavo palp sens spirit ton', 'asleep benumb blunt dul numb']\n",
      "TOKENIZED WORDS:  ['an iodin singl', 'abid appeas bid delay detain on outrid persist put quel remain rest rid stick', 'improv upward', '', 'broad ful fullofthemoon moon phas replet to wax wid widecut', 'night nox', 'abras attrit bray comminut cranch craunch crunch detrit dig drudg grat grind lab labo mash moil toil travail', 'cab chop drudg hack lit machin nag plug polit tax taxicab wardheel whoop writ', 'compass due earl frederick guilford magnet n nor north northward second', 'cast cont design envid extern fant fig im lab out pict plan project propos see send task undertak vis', 'adjac follow fut next success', 'break dawn daybreak first good morn sunr the', '', 'ey heart ocul', 'bright bril burn hop lust prom shiny undim vivid', 'bloodr bolshevik bolshy carmin cer cherry cherryr crimson ink marx red redfac ruby rubyr ruddy scarlet viol', 'altogeth entir whol', '', 'ancestry blood bloodlin desc of par pedigr proflig rak rakehel rip rou stemm', 'vas vessel watercraft', '', 'blood desc of par proflig rakehel rou stock', 'vis', '', 'ey heart ocul', 'genuin rattl tru', 'fidg itchy', '', 'break dawn daybreak first good morn sunr the', '', 'an iodin singl', 'ar awak fir heat ignit inflam rous up wak wok', 'improv upward', 'it', 'knock ping pink pinko rap tap', '', 'an iodin singl', 'across ad ass attend byword catch com constru control dat discov ens escort examin fant find go hear in ins interpret into learn look meet pick pow proverb reckon regard saw steady through understand view visit watch wit word', 'it', 'don with', 'mir', 'afterward on', 'drown flo liquid nai swim', '', 'biot commun district interest resid', 'flo nai', 'billiard consort kitty pocket pond pool puddl synd', '', 'calend hebdomad week workweek', 'an iodin singl', 'feel find mat palp up', 'bit caut combust down electrocut fir incin stak sting sunburn up', 'adept aesthes dat esthes facul geni hotshot impress mav mavin senty star superst virtuoso whiz whizz wiz wizard', '', 'ey heart ocul', 'an iodin singl', 'cal cri exclaim hollo outcry shout weep', '']\n",
      "TOKENIZED WORDS:  ['an iodin singl', 'bad belong break bucket cash chok conk dead depart down drop exit extend farm funct ghost giveup going in kick lead locomot mov on out per pop rifl short sound surv travel way went', 'begrim bemir che col contamin dingy dirty filthy foul grim illgot lousy markedup muddy soil sordid unc unsport unsportsmanlik', 'eat eatery hous resta', 'adenin amp angstrom antiophthalm axerophthol deoxyadenosin fact group monophosph typ vitamin', '', 'day daytim jr period sid twentyfo', 'ago agon', 'an iodin singl', 'beg com get lead out start', 'feel fing flavo palp sens spirit ton', 'airhead dizzy emptyhead featherbrain giddy lighthead sil vertigin woozy', '', 'break dawn daybreak first good morn sunr the', 'ab init initio', 'afterward bel form on postery prevy subsequ tardy ultery', 'an iodin singl', 'barf cast cat chuck disgorg honk puk purg regorg regurgit retch sick spew spu up upchuck vomit', 'everyth', 'closet extinct forbid kayo kod out prohibit proscrib stun taboo tabu verbot', 'afterward on', 'corrod deplet exhaust fee out rust up wip', 'approxim clos less or so to', 'street', 'food intellect nour nutry solid thought', 'an iodin singl', 'begin bulg depart embark get going jump off origin part protrud start startl up', 'feel fing flavo palp sens spirit ton', 'dizzy featherbrain lighthead vertigin', 'debl decrepit faint fall feebl frail imperfect infirm rick sapless unacc washy watery weak', 'bor down exhaust fatigu jad out outwear play sap stock timeworn trit wear wellworn', 'altogeth entir whol', 'clock four prison term', 'furtherm is moreov', 'an iodin singl', 'beg com get off set', 'apprehend becharm beguil bewitch catch charm coll custody enam enamo ench ent fascin gimmick grab haul hitch in match overhear overtak pinch snap snatch tak trant trip view with', 'febr febril fev pyrex', 'halfway midway', 'don with', 'hebdomad workweek', 'also besid excess likew ov too', 'an iodin singl', 'bad belong break buy chip conk dead depart down end exit extend farm funct ghost giveup going in kick lead locomot mov on out per pop rifl short sound surv travel way work', '', '', 'alot', 'ext gre sir than thoma', 'oft oftentim ofttim', 'of', 'an iodin singl', 'across ascertain ass byword catch com constru dat discov ens escort expery fig get hear in interpret it learn look meet pick pow proverb reckon run saw steady through understand view visit wind wit', 'blood desc of par proflig rakehel rou stock', '', 'bathroom can commod crap gut john pot potty privy sew stool throne toilet', 'an iodin singl', 'believ cerebr cogit conceiv guess imagin intend opin recal reckon recollect rememb suppos think', 'an iodin singl', 'might mighty pow', 'beryll constitut embody ex glucin mak repres', 'poop', 'blood desc of par proflig rakehel rou stock', '', 'abdom bear bel breadbasket brook digest out stand stomach support tol tum tummy vent', 'anct dam distress hurt injury pain smart suff wound', 'afterward on', 'meal repast', 'amus cury fishy funny laugh mirth od peculi que remark rum rummy shady singul story suspect suspicy', 'degust rel sampl savo smack tast try', 'angl fish pisc', 'civil cult educ school schoolhouse schooling schooltime sho', 'cafeter', 'it', 'anct dam distress hurt injury pain smart suff wound', 'hent so sol thent thu', 'deal gre much pract', '', 'an iodin singl', 'hard scanty', 'standup', 'consecut direct flat fullstreng heterosex neat squ straight straightaway straightforward tru unb unbow uncoil', 'an iodin singl', 'also besid likew too', '', 'afterward on', '', 'repast', 'alway const forev incess invary perpet', 'bor down fag hackney oldh outwear play sap stock timeworn trit wear wellworn']\n",
      "TOKENIZED WORDS:  ['an iodin singl', 'across ascertain attend car check consid constru dat discov ens escort expery fig get hear in interpret it lin machin out pict project proverb reckon run see steady through understand view visit wind word', 'approxim clos less or so to', 'yellowgreen', 'acquit arc assoil clear discharg dismiss dismit dispatch drop eject elect emit empty exculp exon expel fre lib must out outpo releas sack spark unload vent waiv', 'abstemy alight caloriefr christ cle dismount easy fal get idl illum illumin level lightcol lightsom lit loos lowc lumin luminos perch promiscu rady scant short slut sourc spark sparkl swoon twinkl uncloud unhors vis wanton within', 'blee haemorrh hemorrh leech phlebotomise phlebotomize shed', 'between betwixt tween', '', 'catamen ful geolog mens menstru period stop', 'assocy nurs', 'cas episod occas', 'bit caut cut electrocut glow incin stak sunburn up', 'adept champ esthes facul hotshot mav sens senty superst whiz wiz', 'patch spel whil', 'leak mict oneself pee peep penny piddl piss reliev spend urin wat wee weew', 'thos', 'symptom', 'chlamydia', 'adenin angstrom axerophthol fact monophosph unit vitamin', '', 'beam broadcast carry channel conduc famil genet heredit impart inherit transf transmiss transmit transport', 'infect', 'hawai hi howdy stat', 'an iodin singl', 'of', 'feel go know receiv through', 'bit caut cut electrocut glow off sting the', 'adept champ esthes facul hotshot mav sens star virtuoso whizz wizard', 'piec whil', 'leak mict pass pee penny piss reliev tak urin wee', 'buy haunt shop', 'micturit', 'adenin angstrom axerophthol fact monophosph unit', '', 'arc clear discharg dismit down eject emit exculp exon fir fre lib off outpo releas sack spark vent', '', 'eventid flush level regul tied', 'difficul difficult troubl', 'leak mict pass peep piddl puddl reliev tak wat weew', 'an iodin singl', 'of', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'ail both cark discommod disoblig disord disquiet distract fuss hassl incommod inconveny out perturb problem troubl unh upset worry', 'bad belong break buy chip conk dead depart down end expir fail fit get ghost giveup going in kick lead liv locomot low mov on out per pop releas run sled sound surv travel way', '', '', 'becaus', '', 'genit ven', 'area aren country domain expans field orbit sphere surfac', 'anct dam distress hurt injury pain smart suff weak', 'dur', 'urin', 'they', '', 'bad bangup beau bul clotheshors cork crack crestless dandy dud fash fop gre groovy keen nifty peachy plat sheik slapup smash tumefy up wav', 'improv upward', 'arbit every haphazard indiscrimin random way which willynil']\n",
      "TOKENIZED WORDS:  ['hi howdy', 'an iodin singl', 'bear caus del feat giv had hold induc let own receiv suff tak', '', 'bit caut cut electrocut glow off sting the', 'sensationâ€i', 'believ cerebr conceiv guess ide intellect mean ment opin persuas recal recollect retriev suppos thought view', 'it', 'cystit', 'an iodin singl', 'bad belong break buy chip conk dead depart down end expir fail fit get giv giveup going in kick lead locomot mov on out per pop rifl short sound surv travel way work', '', 'bushel church doc doct dr furb md medico mend on phys repair soph touch', 'he hel', 'afford apply cav collaps commit consecr ded devot eas fal found gav gift grant hand hold in leav mov op pass pres rend return sacr turn way yield', 'main me pin tre', 'approxim clos less or so to', 'antibiot drug', 'it', 'didnt', 'adam bad belong biscuit break buy chip conk crist dead depart disco drop ecstasy exit extend farm fling gam ghost giveup going hug it last liv low off op pass plump process run snuff spel surv tour turn way whirl x xtc', 'asid by off outsid', 'adenin angstrom axerophthol fact monophosph unit', '', 'day daytim jr period sid twentyfo', 'afterward bel form on prevy subsequ ultery', 'an iodin singl', 'acquir amaz arrest baffl becom begin bring buzz catch com convey down driv engend fath find flummox fuck get got grow induc let moth nonpl off out perplex produc receiv scram sir start stim stupefy sustain und', 'approxim clos less or so to', 'bleb blist bull scald ves whip', '', 'vagin', 'altogeth entir whol', 'on', '', 'backtalk brim lip mou rim sass talk', '', '', 'blist scald whip', 'an iodin singl', 'believ cerebr conceiv guess intend opin reckon rememb suppos up', 'an iodin singl', 'acquir amaz arrest baffl becom begin bring buzz catch com convey down driv engend fath find flummox fuck get got grow induc let moth nonpl off out perplex produc receiv set sir start stim suff tak und', 'approxim clos less or so to', 'clamb hid peel scramble shin skin sput tegu', 'aggrav both discomfort excit innerv piqu provoc sor vex', 'raz', 'bit caut cut fir incin mark sting sunt tan', 'but equit exact fair hard mer on scarc simply upright', 'approxim clos less or so to', '', 'itchy rub scaby scratch spoil urg', '', 'doubl duply echo ingemin ov recapit reduply reit rep repetit reply repr retel', 'cas erst erstwhil in ont', '', '', 'monthwh', '']\n",
      "TOKENIZED WORDS:  ['an iodin singl', 'bring chaff chew clav down fat impos inflict jaw see the travel', 'camp hobo jungl', '', 'day daytim jr period sid twentyfo', 'agon', 'an iodin singl', 'acquir amaz arrest baffl becom begin bring buzz catch com convey down driv engend fath find flummox fuck get got grow induc let moth nonpl off out perplex produc receiv set skin stick stupefy sustain und', 'foreign strange unknown unus', 'burn chomp insect morsel prick pung racy seiz sharp snack tee', '', 'it', '', 'away becom belong blend bucket cash chok crack crist dead depart disco drop ecstasy exit extend farm fling gam ghost giveup going hug it last liv low off op pass plump process run snuff spel surv tour turn way work x', 'away for out outsid', '', 'day daytim jr period sid twentyfo', 'afterward bel form on prevy subsequ ultery', 'an iodin singl', 'beg com get off set', 'ar build educ exply germin grow modern prep recrudesc spring up upr', 'febril pyrex', 'brawny musc muscul tissu', 'helpless impuiss', '', 'should', 'an iodin singl', 'along arrang behav caus coiff do doh execut far mak of out pract set ut', 'an iodin singl', 'across attain break chant describ disclos distinct divulg expos fal get hap ident key let lin nam observ out rev strike unwrap upon word', 'eldritch uncanny unearth weird wyrd', 'mosquito', 'burn col insect prick racy sharp sting tee', '', 'deal hand help man mit ov paw reach turn', '', 'along arrang caus coiff depart doe energy exerc get man out pract set', 'not', 'away becom biscuit break buy chip conk crist dead depart disco drop ecstasy exit extend farm fling gam ghost giveup going hug it last liv low off op pass plump process run snuff spel surv tour turn way work xtc', 'away for out', 'altern els instead rath', 'it', 'bangup bul cork crestless dud fop gre intumesc keen nifty peachy puff sheik smash tumefy up wel', '', 'deal hand help man on pass play script', 'benumb dead dul', '', 'room', 'angl bir consum expery get got ha has hold in ingest mak possess stim sustain throw', 'uncanny weird', 'burn col morsel pung seiz snack tee', 'his', 'bon brow forehead front os', 'hel', 'becharm bewitch catch caught enam ench fascin grab hold of overhear pick tak trip view with', 'febril pyrex', 'compr cost eq evergreen follow mak repres up wa was washington', 'baffl bedevil befuddl bemus blur brok confound confus dis discombob disconcert disconnect disjoint flurry fox fuddl garbl illog jumbl lost maz mix mixedup obnubl obsc put scat sea unconnect upset', 'almost around clos less most nigh rough som virt', '', 'beleagu besieg bord circumv environ fent hem milieu palisad ring skirt smoth surround wal', 'hel', 'feel go know receiv through', 'mod sight', 'casual depr expir going ink pass personnel releas', 'accru autumn capit crepusc dec declend declin decreas descend devolv dip downfal downslop dusk evenfal flow gloam hang light nightfal pin precipit settl spil strike surrend tumbl twilight', '', 'occass', 'coma comatos', '', '', 'an iodin singl', 'along arrang behav caus coiff do doh execut far mak of out pract set ut', 'almost around clos less most nigh rough som virt', 'him', 'an iodin singl', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'child fry kid min nestl nip pockets shav smallsc tiddl tik tyk ven youngst', 'cephalalg head headach worry', 'be cat disgorg emes puk regorg retch spew throw upchuck', 'an iodin singl', 'across ascertain attend car check consid control determin encount ens escort expery fig get hear in interpret it lin machin out pict project real regard saw steady through understand view visit wind word', 'bolshevik carmin cherry crimson ink marx redfac ruby ruddy viol', 'blizzard rash roseol', 'altogeth entir whol', 'complet end oer termin', '', 'backbon backrest backward bind book column cov dors endors gag hind indors plump punt rach rear rearward second spin support vertebr', 'breast bureau chest dress pect thorax', 'abid bel brook end put stand stomach support tum up']\n",
      "TOKENIZED WORDS:  ['hey', 'in on point that thith', 'an iodin singl', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'been constitut embody ex liv person up', 'believ feel fing flavo intuit not palp smel tact ton touch', 'extrem high sup', 'big defect forg highrisk regret risky sorry spoil spoilt tough uncollect unfit unsound', 'of', '', 'febril pyrex', 'ar build educ exply germin grow modern prep recrudesc spring underdevelop upr', 'past retir tim yesteryear', '', 'day daytim jr period sid twentyfo', 'an iodin singl', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'been constitut embody ex liv person up', 'acquir amaz arrest baffl becom begin bring buzz catch com convey down driv engend fath find flummox fuck get gravel hav int mak myst obtain on pay pos puzzl scram sir start stim suff tak vex', '', 'blist scald whip', 'effloresc rash skin', '', 'body consist eubst person soundbox structure torso trunk', 'of', '', 'backtalk cav fiss mou mouthpiec rim speak ut verb', 'angl bir consum expery get got has hold in ingest mak possess stim sustain throw', 'as besid too', 'been constitut embody ex liv person up', 'rattl selfsam', 'afflict huffy mad raw sensit tend', 'adenin angstrom axerophthol fact monophosph unit', '', 'ber billet com direct grad hom ident invest lay lieu off ord piazz plaz pos posit property rang rat seat set sit situ spot stead target topograph', 'in on point that thith', 'angl bir consum expery get got has hold in ingest mak possess stim sustain throw', 'been constitut embody ex liv person up', 'itchy', 'altogeth entir whol', 'complet end ov', '', 'consist eubst person soundbox torso', 'an iodin singl', 'bad belong break buy chip conk dead depart down end expir fail fit get giv go hold it last liv low off op pass plump process run snuff start surv travel way work', '', 'admir boost friend quak', 'busy domicy famy firm hous household mand men planet sign the zodiac', 'cobbl dea fin hold last liv net out shoemak stop termin utmost', 'night nox', 'ceas conclud fin stop', 'improv upward', 'ar build educ exply germin grow modern prep recrudesc spring underdevelop upr', 'febril pyrex', 'bolshevik carmin cherry crimson ink marx redfac ruby ruddy viol', 'apart ber bit blem blob blot dappl daub descry discern distinct espy fleck mac musc musca out pick pip point post situ slur smear smirch smudg speckl spot spotlight spy stain tel touch volit', 'alabam alumin camell dixy of', 'complet end ov', '', 'consist eubst person soundbox torso', 'an iodin singl', '', 'eventid flush out regul tied', 'corrod eat fee out rust up wip', 'anyth', '', 'an iodin singl', 'bear caus del feat giv had hold induc let own receiv suff tak', 'no nobel', 'appet appetit', 'begin bulg depart embark get going jump off origin part protrud start tak', 'acquir amaz arrest baffl becom begin bring buzz catch com convey down driv engend fath find flummox fuck get gravel hav int mak myst obtain on pay pos puzzl scram sir start stim suff tak vex', 'fidg itchy', 'altogeth entir whol', 'complet end ov', '', 'consist eubst person soundbox torso', '', 'backtalk fiss mou or sass speak ut', 'angl bir consum expery get got has hold in ingest mak possess stim sustain throw', 'been constitut embody ex liv person up', 'expery find flav impress intuit not palp smel tact ton', 'big forg regret sorry spoil tough unfit', 'of', 'an iodin singl', 'neer nev', 'feel fing flavo palp sens spirit ton', 'car correspond lik sam the', 'corrod eat fee out rust up wip', '', 'brawny musc muscul tissu', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'as likew wel', 'been constitut embody ex liv person up', 'ext mor than to', 'huffy pain raw sor tend', 'of', 'eventid flush out stil yet', '', 'an iodin singl', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'not', 'been constitut embody ex liv person up', '', 'gym gymnas', '', 'clamb hid peel scramble shin skin struggle', 'angl bir consum expery get got has hold in ingest mak possess stim sustain throw', 'get suit', 'ext mor than to', 'fidg itchy', 'of', 'blist scald whip', 'effloresc roseol', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'appear look seem', 'altogeth entir whol', 'complet end ov', '', 'consist eubst person soundbox torso', 'of', '', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'altogeth entir whol', 'begin bulg depart embark get going jump off origin part protrud start tak', 'itchy scaby spoil', '', 'arm blazonry branch coat fort gird impl limb munit sleev subdivid system war weapon weaponry', 'hamst tendon', 'as likew wel', 'begin bulg depart embark get going jump off origin part protrud start tak', 'hurt lanct smart yearn yen', 'an iodin singl', 'conceiv think', 'an iodin singl', 'crataeg engl hawthorn laevigat may oxycanth whitethorn', 'beryll constitut embody ex glucin mak repres', 'ar build educ exply germin grow modern prep recrudesc spring underdevelop upr', 'febril pyrex', '', '', '', 'emin gamy gear height highpitch highschool luxury mellow school seny up', '', 'cobbl dea fin hold last liv net out shoemak stop termin utmost', 'meas', 'bolshevik carmin cherry crimson ink marx redfac ruby ruddy viol', 'ber bit blob dappl descry distinct fleck mac musc off patch pip point post situ smear smudg spot spy tel touch', 'ap attract invok sympathet', 'altogeth entir whol', 'complet end ov', '', 'branch fort limb sleev system weapon', '', 'expery find flav impress look opin sens spirit tact ton', 'an quit rath', 'uncomfort', 'but exact good just mer on scarc upright', '', 'break dawn daybreak first good morn sunr the', 'an iodin singl', 'ceas conclud fin stop', 'improv upward', 'ar build educ exply germin grow modern prep recrudesc spring underdevelop upr', 'febril pyrex', '', 'backtalk fiss mou or sass talk verb', 'angl bir consum expery get got has hold in ingest mak possess stim sustain throw', 'feel find mat palp up', 'rattl selfsam', 'huffy pain sensit tend', '', '', 'admir boost friend quak', 'gabriel', 'an iodin singl', 'bad belong break buy chip conk dead depart down end expir fail fit get giv go hold it last liv low off op pass plump process run snuff start the up went', '', 'film flick mot motionpict movingpict movy pic show', 'an iodin singl', 'ceas conclud fin stop', 'improv upward', 'ar build educ exply germin grow modern prep recrudesc spring underdevelop upr', 'effloresc roseol', 'altogeth entir whol', 'complet end ov', '', 'consist eubst person soundbox torso', '', 'becom bend cal chang deform flex mov on plough plow revers rick sprain turn twist wrench wrick', '', 'fidg itchy', 'ber bit blob dappl descry distinct fleck mac musc off patch pip point post situ smear smudg spot spy tel touch', 'afterward bel form on prevy subsequ ultery', '', 'night nox', 'an iodin singl', 'as likew wel', 'appra assess calc delib evalu mens out quant valu', '', '', 'bear caus del feat giv had hold induc let own receiv suff tak', 'ar build educ exply germin grow mak modern prep recrudesc spring up', 'febril pyrex', '', 'break dawn daybreak first good morn sunr the', 'an iodin singl', 'not', 'eventid flush out stil yet', 'act actu be displac impress incit mak motil process prompt propel reloc strike', 'adv comfort easy fountainhead intim subst wel wellspr', '', '', 'brawny musc muscul tissu', 'achy lanct smart yearn', '']\n",
      "TOKENIZED WORDS:  ['ic', 'cal commem commend recal rememb think', 'citizenry hoi mass multitud peopl pollo unwash', 'alleg av enjoin enount enunt out pronount read say stat suppos', '', 'acquir amaz arrest baffl becom begin bring buzz catch com convey down driv engend fath find flummox fuck get got grow induc let moth nonpl off out perplex produc receiv set skin stick stupefy sustain und', 'angry fury tempestu wild', '', 'an iodin singl', 'along arrang behav caus coiff do doh execut far mak of out pract set ut', 'feel fing flavo palp sens spirit ton', 'fury tempestu', 'although', 'an iodin singl', '', 'acknowledg away be bonk do expery get hav hay in intercours it know lie lov off out rol sex the with', 'wheref why', 'an iodin singl', '', 'cal commend recal rememb think', 'her', 'facesh', 'authorit cruc import sign', 'me stat tre', 'an iodin singl', '', 'not', '', 'anim biograph hist lif lifesp lifetim sent sprightliness', 'me stat', 'cal commend recal rememb think', '', 'aspect bold brass cheek confront express fac font fount grimac hum nerv pres typefac', 'wher', 'along arrang caus coiff did dress exerc get man perform serv suff', 'an iodin singl', 'abs allow alon behind bequea entrust farewel forget giv impart leav leavetak on part provid pul result up wil', '', 'bank box camb cant coin company deposit institut money rely swear', 'annot banknot bil distinct emin fed govern greenback let mark mus observ preemin promiss reserv tak', 'it', 'but exact good just now prec simply', '', '', 'an iodin singl', 'cal recal recov rememb think', 'them', '', 'along arrang caus coiff did dress exerc get man perform serv suff', 'you', 'artic av enount ord pronount say stat tel', '', 'address speak ut', 'brassy cheap flash flashy gar gaudy gimcrack loud meretricy tacky tatty tawdry trashy', '', 'brawny musc muscul tissu', 'as likew wel', 'anct hurt offend smart suff', 'hurt pin suff yearn', 'an rath', 'admin allot band batch bunch caboodl circ deal destiny dish dispens distribut dol fat flock fortun gre hat heap lot luck mess mickl mint mountain muckl parcel passel pil plenty pot raft shel sight slew spat stack sum tidy wad', 'it', 'as likew wel', 'uncanny weird', '', '', 'bring fath fem fuss get overprotect sir', 'abod domicil dwel habit hous intern intery nat nurs plat', 'cobbl dea fin hold last liv net out shoemak stop termin utmost', 'weekend', 'we', 'cook dev dispos educ fain gear groom inclin mak prep ready train', 'adult big bigheart boast bount bounty brag braggart braggy cockahoop crow enceint freehand ful fullgrown gravid grown grownup handsom heavy larg lib magnanim openhand selfaggrand vaingl vaunt', 'din party', 'an iodin singl', 'bear caus del feat giv had hold induc let own receiv suff tak', 'no', 'appetit', 'compl discharg fil in nail', 'chor job project task undertak', 'along arrang caus coiff do doing execut far mak out pract set', 'affair thing', 'angl bir consum expery get got has hold in ingest mak possess stim sustain throw', 'been constitut embody ex liv person up', 'rattl selfsam', 'hard unm', 'of', 'an iodin singl', 'appear seem', 'not', 'cal commend recal rememb think', 'mat thing', 'an iodin singl', 'along arrang caus coiff did dress exerc get man perform serv suff', '', 'past retir tim', 'along arrang caus coiff do dress exerc get man perform serv suff', 'mat', 'an iodin singl', 'faimili', '', 'angl bir consum expery get got has hold in ingest mak possess stim sustain throw', 'been constitut embody ex liv person up', 'befuddl blur confus disconcert flurry fuddl mix obsc perplex puzzl up', 'feel find mat palp up', 'closet extinct kayo kod out proscrib taboo the verbot', 'ber blank commit grad ident invest lieu off out piazz plaz pos post put rang rat send sho sit spac stat stead topograph', 'it', 'appear look seem', '', 'spat', 'opt', 'dop facul gag goddess grass gumpt hors jan locowee mary out pot sen senty sess sign skunk smok wit', 'as likew wel', 'decreas fal']\n",
      "TOKENIZED WORDS:  ['', 'backbon backward bind column dors for gam indors plump punt rear second stak up vertebr', 'anct dam distress hurt injury pain smart suff wound', 'hent so sol thent thu', 'deal gre much pract', 'but exact good just now prec simply', 'tak walk', 'approxim clos less or round som', '', 'ankl joint mort talocr', 'egot puff swel swol tumefy up vain', 'also besid fault ov too', '', 'babbl bag bean blab lect mou out peach publ sing spil tattl town verb', 'me stat', 'but exact good just now prec simply', 'away becom biscuit break buy chip conk crist dead depart disco drop ecstasy exit extend farm fling gam ghost giveup going hug it last liv low off op pass plump process run snuff spel surv tour turn way work xtc', 'away for out', 'allow away behind depart entrust farewel forget giv impart leav of out pass provid result up', 'me stat', 'entir lon sol solit solo unaccompany uneq unequal unparallel', '', 'flow geolog menstru period stop', 'deep lat prevy tardy', 'away past', 'deuceac ii leash tercet tern terzetto three threesome tierc trey triad trin trio triplet troik', 'daysit', 'almost near virt', 'nev', 'deep lat prevy tardy', '', 'fac meat posit slop', 'cramp hamp strangle', 'hent so sol thent thu', 'ardu backbreak difficult gruel heavy intemp knockout operos pun strong surd toilsom unvo voiceless', 'it', 'feel find mat palp up', 'car correspond lik sam the', 'individ person somebody someon soul', 'shred shredding up', 'me stat', 'at bottom deep heart indo insid inward privileg within', 'closet extinct kayo kod out proscrib taboo the']\n",
      "TOKENIZED WORDS:  ['no', 'count issu subject top weigh', 'how', 'fiddl footl lilliput minusc niggl petty picayun piffl slight triv', 'an iodin singl', 'corrod eat fee out rust up wip', 'an iodin singl', 'acquir amaz arrest baffl becom begin bring buzz catch com convey down driv engend fath find flummox fuck get gravel hav int mak myst obtain on pay pos puzzl scram sir start stim suff tak vex', 'fatty fertil juicy rich', 'for intellect nutry thought', '', 'enemy foe foem opposit', 'an iodin singl', 'declin defy deny food garb refus reject resist scraps up wast', 'ced fal resign up', '', 'many', 'gram kiloc kilogram nutrit', 'in inch indian stat', '', 'sandwich', 'bad complaint il inauspicy omin poor', 'but exact good just now prec simply', 'acquir admit adopt ask assum away cal chart choos claim conduc consum contain convey demand driv eng fil for guid hir in involv lead leas mak nee occupy on pack payoff post read remov rent return select strike study submit subscrib to up withdraw', 'appl mal orchard pumil', 'els or', 'an iodin singl', 'feel fing flavo palp sens spirit ton', 'hent so sol thent thu', 'coldblood dusty inhum low stal', 'eventid flush out stil yet', '', 'an iodin singl', 'artic bear bust cloth don end fal get haby hold jad off out put thin upon vest weary', 'addit duply extr redund spar spear supererog superflu supernum surpl', 'cak coat pel', 'an iodin singl', 'be cat disgorg honk pur regorg retch sanct sco spew throw upchuck', 'myself', 'blank cleanl cleans fair hous housec jerk mak pick plumb scaveng sport sportsmanlik sporty strip uncloud uncontamin uninfect unobject whit', 'nowaday', 'cas erstwhil in ont', 'ext mor than to', 'but exact good just now prec simply', 'car correspond lik sam the', 'an iodin singl', 'const eer forev invary', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'dem exceiv leav omit tak', 'in on point that thith', 'adenin angstrom axerophthol fact monophosph unit', 'bit caut cut electrocut glow off sting the', 'adept champ esthes facul hotshot mav sens star virtuoso whizz wizard', 'belt blu bolt cut depress devo dispirit downcast downheart downward drink finetun gloom grim h john kil l land lowspirit mast off pat pil pol pour push refin the toss', '', 'throat', 'an iodin singl', 'conject estim gaug guess guesswork hazard hypothes inf judg pretend shot suppos supposit surm vent', 'if', '', 'dam leontyn monet pric tol', 'an iodin singl', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'ant compens earn mak pay remun sal wag']\n",
      "TOKENIZED WORDS:  ['of', '', 'bureau draw of pect', 'expery find flav impress look opin sens spirit tactil touch', 'jol mod pretty reason somewh', 'besot blind blotto cockey compress crock drunk fast load mingy mis nasty pieey pixil plast rig slop slosh soak sous sozzl squiffy stiff stringent taut tight wet', 'eventid flush out stil yet', 'benumb dead numb', 'astatin', 'clock four multiply sent tim', 'an iodin singl', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'get suit', 'alight caloriefr cle dismount easy fal get ignit illum in light lightcol lightsom loos lumin off perch rady short sourc sparkl trip twinkl uncloud up wak wanton within', 'direct guid head maneuv manoeuv manoeuvr ste', 'dizzy featherbrain lighthead vertigin', '', 'almost degr highest near the virt', '', 'day daytim jr period sid twentyfo', 'depict fork furn giv in interl out pict rend rendit show supply transl turn vert', 'me stat', 'depl lam pity sad', 'useless', 'an iodin singl', 'as likew wel', 'oft ofttim', 'be cat disgorg emet naus purg regurgit sick spu up vomit', 'expiery', 'heartburn pyros', '', 'hell', 'abomin atrocy aw dread pain terr unspeak', '', 'genuin rattl tru', 'car disquiet disturb occupy vex', '', 'an iodin singl', 'mighty', 'bear caus del feat giv hav in ingest mak person receiv stim sustain throw', 'dang grav griev lifethr sery sob unplay', 'diseas', '', 'dyspeps indigest stomach', 'not', 'aid assist avail facilit oneself serv']\n",
      "TOKENIZED WORDS:  ['in on point that thith', 'adenin angstrom axerophthol fact monophosph unit', 'bundl clump clust crew crowd gang togeth', 'bolshevik carmin cherry crimson ink marx redfac ruby ruddy viol', 'ber bit blob dappl descry distinct fleck mac musc off patch pip point post situ smear smudg spot spy tel touch', '', 'bureau draw of thorax', 'an iodin singl', '', 'believ cerebr conceiv guess intend opin reckon rememb suppos up', '', 'contact hold of through', 'any whatev whatsoev', 'allergenswhy', 'habit wont', '', 'away becom biscuit break buy chip conk crist dead depart disco drop ecstasy exit extend farm fling gam ghost giveup going hug it last liv low off op pass plump process run snuff spel surv tour turn way work xtc', 'away for out', 'in on point that thith', 'dimin littl min modest smal', 'bit flak minut mo mom numb routin scrap snatch turn', 'milklik milky', 'album andrew ash b blanch bloodless caucas d dickson dougla e edward eg elwyn flannel gabardin harold lilywhit livid martind ovalbumin patrick riv snowy stanford t theod twee vict whitehot', 'flu limpid liqu melt smoo swim', 'adv amount approach climax der descend fal follow forthcom hail issu numb occ orgasm sex up upcom', 'closet extinct kayo kod out proscrib taboo the', '', 'nipplesshould', 'an iodin singl', 'beryll constitut embody ex glucin mak repres', 'com doe imply occupy pertain rel touch worry', 'charact constitu depart disunit for off part person port rol sect set shar start the voic', '', 'boob bosom chest knock meat summit tit titty', 'feel find mat palp up', 'bit chip flak minut mom numb prick routin second snatch sting turn', 'hardalmost', 'chunky lumpy', 'would', '', 'away becom biscuit break buy chip conk crist dead depart disco drop ecstasy exit extend farm fling gam ghost giveup going hug it last liv low off op pass plump process run snuff spel surv tour turn way work xtc', 'away for out', 'afterward on', 'puberty pubesc', 'it', 'but exact good just now prec simply', 'com go occ occur on pass tak', 'me stat', 'nowaday', '', '', 'mamill mammill nippl pap teat', 'lookdiff', '', 'conject divin hypothec hypothet opin presuppos say socal suppos the', 'beryll constitut embody ex glucin mak repres', 'bury deepset dip drop go laps pass recess settl sink slid slump subsid sunk']\n",
      "TOKENIZED WORDS:  ['', 'been constitut embody ex liv person up', 'expery find flav impress look opin sens spirit tactil touch', 'car correspond lik sam the', 'an iodin singl', 'cal for involv nee requir requisit want', 'away becom biscuit break buy chip conk crist dead depart disco drop ecstasy exit extend farm fling gam ghost giveup going hug it last liv low off op pass plump process run snuff spel surv tour turn way work xtc', '', '', 'ext mor than to', 'ext mor than to', 'deal gre oft ofttim', 'thes', 'day daytim jr period sid twentyfo', '', 'begin down get getgo kickoff off offset out outset root showtim start', 'aw dir fear fearsom fright hor horrend terr', '', '', 'actu head jaunt misstep on set slip stumbl travel trig tripup up', '', 'day daytim jr period sid twentyfo', '', 'it', 'anct dam distress hurt injury pain smart suff wound', 'leak mict pass peep piddl puddl spend urin wee', '', 'an iodin singl', 'bad belong break buy chip conk dead depart down end expir fail fit get giv go hold it last liv low off op pass plump process run snuff start the up went', 'leak mict pass peep piddl puddl spend urin wee', '', 'break dawn daybreak first good morn sunr the', 'in on point that thith', 'blood desc of par proflig rakehel rou stock', '', 'leak mict pass peep piddl puddl spend urin wee', '', '', 'amiss awry faul haywir illtim improp incorrect leg unseason untim wrong', '', 'me stat', 'bear caus del feat giv hav in ingest mak possess stim sustain throw', 'ar gend pract turn urg', '', '', 'girl girlfriend lady', '', 'bear caus del feat giv had hold induc let own receiv suff tak', 'anct dam distress hurt injury pain smart suff weak', 'hent so sol thent thu', 'deal gre much pract']\n"
     ]
    }
   ],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    print(\"TOKENIZED WORDS: \",pattern_words)\n",
    "    # stem each word\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# create train and test lists\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: AA8AZL\n",
      "Log directory: tflearn_logs/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 14\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 1  | time: 0.148s\n",
      "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m2.37510\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 002 | loss: 2.37510 - acc: 0.0643 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m2.59353\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 003 | loss: 2.59353 - acc: 0.0117 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m2.62914\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 004 | loss: 2.62914 - acc: 0.0565 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m2.61529\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 005 | loss: 2.61529 - acc: 0.0174 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m2.59751\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 006 | loss: 2.59751 - acc: 0.1440 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m2.57435\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 007 | loss: 2.57435 - acc: 0.4004 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m2.61429\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 008 | loss: 2.61429 - acc: 0.1752 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m2.55010\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 009 | loss: 2.55010 - acc: 0.6119 -- iter: 14/14\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m2.50223\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 010 | loss: 2.50223 - acc: 0.8059 -- iter: 14/14\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m2.54979\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 011 | loss: 2.54979 - acc: 0.4918 -- iter: 14/14\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m2.57033\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 012 | loss: 2.57033 - acc: 0.3348 -- iter: 14/14\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m2.46219\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 013 | loss: 2.46219 - acc: 0.6199 -- iter: 14/14\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m2.38053\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 014 | loss: 2.38053 - acc: 0.7754 -- iter: 14/14\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m2.52167\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 015 | loss: 2.52167 - acc: 0.4720 -- iter: 14/14\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m2.62974\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 016 | loss: 2.62974 - acc: 0.2950 -- iter: 14/14\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m2.43946\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 017 | loss: 2.43946 - acc: 0.5488 -- iter: 14/14\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m2.53706\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 018 | loss: 2.53706 - acc: 0.3588 -- iter: 14/14\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m2.35962\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 019 | loss: 2.35962 - acc: 0.5725 -- iter: 14/14\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m2.48956\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 020 | loss: 2.48956 - acc: 0.3885 -- iter: 14/14\n",
      "--\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m2.30703\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 021 | loss: 2.30703 - acc: 0.5783 -- iter: 14/14\n",
      "--\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m2.16708\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 022 | loss: 2.16708 - acc: 0.7048 -- iter: 14/14\n",
      "--\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m2.30590\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 023 | loss: 2.30590 - acc: 0.5209 -- iter: 14/14\n",
      "--\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m2.38925\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 024 | loss: 2.38925 - acc: 0.3945 -- iter: 14/14\n",
      "--\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m2.18998\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 025 | loss: 2.18998 - acc: 0.5596 -- iter: 14/14\n",
      "--\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m2.40585\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 026 | loss: 2.40585 - acc: 0.4493 -- iter: 14/14\n",
      "--\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m2.18832\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 027 | loss: 2.18832 - acc: 0.5909 -- iter: 14/14\n",
      "--\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m2.40488\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 028 | loss: 2.40488 - acc: 0.4432 -- iter: 14/14\n",
      "--\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m2.17824\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 029 | loss: 2.17824 - acc: 0.5786 -- iter: 14/14\n",
      "--\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m2.37027\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 030 | loss: 2.37027 - acc: 0.4754 -- iter: 14/14\n",
      "--\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m2.14450\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 031 | loss: 2.14450 - acc: 0.5965 -- iter: 14/14\n",
      "--\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m2.30337\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 032 | loss: 2.30337 - acc: 0.4783 -- iter: 14/14\n",
      "--\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m2.08625\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 033 | loss: 2.08625 - acc: 0.5929 -- iter: 14/14\n",
      "--\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m2.25717\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 034 | loss: 2.25717 - acc: 0.4811 -- iter: 14/14\n",
      "--\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m2.04472\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 035 | loss: 2.04472 - acc: 0.5897 -- iter: 14/14\n",
      "--\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m2.28097\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 036 | loss: 2.28097 - acc: 0.4837 -- iter: 14/14\n",
      "--\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m2.06029\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 037 | loss: 2.06029 - acc: 0.5870 -- iter: 14/14\n",
      "--\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m1.88043\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 038 | loss: 1.88043 - acc: 0.6678 -- iter: 14/14\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m1.72950\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 039 | loss: 1.72950 - acc: 0.7314 -- iter: 14/14\n",
      "--\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m1.59895\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 040 | loss: 1.59895 - acc: 0.7818 -- iter: 14/14\n",
      "--\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m1.48267\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 041 | loss: 1.48267 - acc: 0.8218 -- iter: 14/14\n",
      "--\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m1.72685\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 042 | loss: 1.72685 - acc: 0.6996 -- iter: 14/14\n",
      "--\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m1.57094\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 043 | loss: 1.57094 - acc: 0.7526 -- iter: 14/14\n",
      "--\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m1.43541\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 044 | loss: 1.43541 - acc: 0.7954 -- iter: 14/14\n",
      "--\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m1.31525\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 045 | loss: 1.31525 - acc: 0.8302 -- iter: 14/14\n",
      "--\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m1.20684\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 046 | loss: 1.20684 - acc: 0.8585 -- iter: 14/14\n",
      "--\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m1.10764\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 047 | loss: 1.10764 - acc: 0.8816 -- iter: 14/14\n",
      "--\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m1.55293\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 048 | loss: 1.55293 - acc: 0.7399 -- iter: 14/14\n",
      "--\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m1.38659\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 049 | loss: 1.38659 - acc: 0.7810 -- iter: 14/14\n",
      "--\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m1.84289\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 050 | loss: 1.84289 - acc: 0.6598 -- iter: 14/14\n",
      "--\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m1.62933\u001b[0m\u001b[0m | time: 0.014s\n",
      "| Adam | epoch: 051 | loss: 1.62933 - acc: 0.7117 -- iter: 14/14\n",
      "--\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m1.44835\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 052 | loss: 1.44835 - acc: 0.7550 -- iter: 14/14\n",
      "--\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m1.29372\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 053 | loss: 1.29372 - acc: 0.7911 -- iter: 14/14\n",
      "--\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m1.66901\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 054 | loss: 1.66901 - acc: 0.6866 -- iter: 14/14\n",
      "--\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m1.48329\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 055 | loss: 1.48329 - acc: 0.7314 -- iter: 14/14\n",
      "--\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m1.32519\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 056 | loss: 1.32519 - acc: 0.7692 -- iter: 14/14\n",
      "--\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m1.18962\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 057 | loss: 1.18962 - acc: 0.8011 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m1.68434\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 058 | loss: 1.68434 - acc: 0.6919 -- iter: 14/14\n",
      "--\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m1.50292\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 059 | loss: 1.50292 - acc: 0.7333 -- iter: 14/14\n",
      "--\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m1.34804\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 060 | loss: 1.34804 - acc: 0.7686 -- iter: 14/14\n",
      "--\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m1.21488\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 061 | loss: 1.21488 - acc: 0.7988 -- iter: 14/14\n",
      "--\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m1.65281\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 062 | loss: 1.65281 - acc: 0.6961 -- iter: 14/14\n",
      "--\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m1.48426\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 063 | loss: 1.48426 - acc: 0.7346 -- iter: 14/14\n",
      "--\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m1.79442\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 064 | loss: 1.79442 - acc: 0.6606 -- iter: 14/14\n",
      "--\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m1.61483\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 065 | loss: 1.61483 - acc: 0.7025 -- iter: 14/14\n",
      "--\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m1.46050\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 066 | loss: 1.46050 - acc: 0.7387 -- iter: 14/14\n",
      "--\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m1.32702\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 067 | loss: 1.32702 - acc: 0.7700 -- iter: 14/14\n",
      "--\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m1.21068\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 068 | loss: 1.21068 - acc: 0.7973 -- iter: 14/14\n",
      "--\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m1.10842\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 069 | loss: 1.10842 - acc: 0.8209 -- iter: 14/14\n",
      "--\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m1.01774\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 070 | loss: 1.01774 - acc: 0.8416 -- iter: 14/14\n",
      "--\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m0.93659\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 071 | loss: 0.93659 - acc: 0.8597 -- iter: 14/14\n",
      "--\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m1.25653\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 072 | loss: 1.25653 - acc: 0.7790 -- iter: 14/14\n",
      "--\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m1.14837\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 073 | loss: 1.14837 - acc: 0.8036 -- iter: 14/14\n",
      "--\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m1.50046\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 074 | loss: 1.50046 - acc: 0.7232 -- iter: 14/14\n",
      "--\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m1.36877\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 075 | loss: 1.36877 - acc: 0.7532 -- iter: 14/14\n",
      "--\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m1.25325\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 076 | loss: 1.25325 - acc: 0.7797 -- iter: 14/14\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m1.15131\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 077 | loss: 1.15131 - acc: 0.8030 -- iter: 14/14\n",
      "--\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m1.06077\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 078 | loss: 1.06077 - acc: 0.8236 -- iter: 14/14\n",
      "--\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m0.97977\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 079 | loss: 0.97977 - acc: 0.8419 -- iter: 14/14\n",
      "--\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m1.29636\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 080 | loss: 1.29636 - acc: 0.7704 -- iter: 14/14\n",
      "--\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m1.19251\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 081 | loss: 1.19251 - acc: 0.7936 -- iter: 14/14\n",
      "--\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m1.48986\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 082 | loss: 1.48986 - acc: 0.7214 -- iter: 14/14\n",
      "--\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m1.36891\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 083 | loss: 1.36891 - acc: 0.7492 -- iter: 14/14\n",
      "--\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m1.62538\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 084 | loss: 1.62538 - acc: 0.6815 -- iter: 14/14\n",
      "--\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m1.49384\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 085 | loss: 1.49384 - acc: 0.7133 -- iter: 14/14\n",
      "--\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m1.75711\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 086 | loss: 1.75711 - acc: 0.6491 -- iter: 14/14\n",
      "--\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m1.61710\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 087 | loss: 1.61710 - acc: 0.6842 -- iter: 14/14\n",
      "--\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m1.49345\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 088 | loss: 1.49345 - acc: 0.7158 -- iter: 14/14\n",
      "--\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m1.38374\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 089 | loss: 1.38374 - acc: 0.7442 -- iter: 14/14\n",
      "--\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m1.61783\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 090 | loss: 1.61783 - acc: 0.6769 -- iter: 14/14\n",
      "--\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m1.49868\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 091 | loss: 1.49868 - acc: 0.7092 -- iter: 14/14\n",
      "--\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m1.77568\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 092 | loss: 1.77568 - acc: 0.6383 -- iter: 14/14\n",
      "--\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m1.64498\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 093 | loss: 1.64498 - acc: 0.6745 -- iter: 14/14\n",
      "--\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m1.52922\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 094 | loss: 1.52922 - acc: 0.7070 -- iter: 14/14\n",
      "--\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m1.42589\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 095 | loss: 1.42589 - acc: 0.7363 -- iter: 14/14\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m1.65361\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 096 | loss: 1.65361 - acc: 0.6698 -- iter: 14/14\n",
      "--\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m1.53914\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 097 | loss: 1.53914 - acc: 0.7029 -- iter: 14/14\n",
      "--\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m1.74198\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 098 | loss: 1.74198 - acc: 0.6326 -- iter: 14/14\n",
      "--\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m1.62089\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 099 | loss: 1.62089 - acc: 0.6693 -- iter: 14/14\n",
      "--\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m1.84224\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 100 | loss: 1.84224 - acc: 0.6024 -- iter: 14/14\n",
      "--\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m1.71411\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 101 | loss: 1.71411 - acc: 0.6421 -- iter: 14/14\n",
      "--\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m1.59992\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 102 | loss: 1.59992 - acc: 0.6779 -- iter: 14/14\n",
      "--\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m1.49718\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 103 | loss: 1.49718 - acc: 0.7101 -- iter: 14/14\n",
      "--\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m1.69991\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 104 | loss: 1.69991 - acc: 0.6391 -- iter: 14/14\n",
      "--\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m1.58665\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 105 | loss: 1.58665 - acc: 0.6752 -- iter: 14/14\n",
      "--\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m1.79302\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 106 | loss: 1.79302 - acc: 0.6077 -- iter: 14/14\n",
      "--\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m1.67089\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 107 | loss: 1.67089 - acc: 0.6469 -- iter: 14/14\n",
      "--\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m1.88924\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 108 | loss: 1.88924 - acc: 0.5822 -- iter: 14/14\n",
      "--\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m1.75897\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 109 | loss: 1.75897 - acc: 0.6240 -- iter: 14/14\n",
      "--\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m1.93941\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 110 | loss: 1.93941 - acc: 0.5616 -- iter: 14/14\n",
      "--\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m1.80638\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 111 | loss: 1.80638 - acc: 0.6054 -- iter: 14/14\n",
      "--\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m1.68737\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 112 | loss: 1.68737 - acc: 0.6449 -- iter: 14/14\n",
      "--\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m1.57994\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 113 | loss: 1.57994 - acc: 0.6804 -- iter: 14/14\n",
      "--\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m1.75470\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 114 | loss: 1.75470 - acc: 0.6124 -- iter: 14/14\n",
      "--\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m1.63941\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 115 | loss: 1.63941 - acc: 0.6511 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m1.82966\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 116 | loss: 1.82966 - acc: 0.5932 -- iter: 14/14\n",
      "--\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m1.70661\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 117 | loss: 1.70661 - acc: 0.6338 -- iter: 14/14\n",
      "--\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m1.84216\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 118 | loss: 1.84216 - acc: 0.5776 -- iter: 14/14\n",
      "--\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m1.71811\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 119 | loss: 1.71811 - acc: 0.6198 -- iter: 14/14\n",
      "--\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m1.87110\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 120 | loss: 1.87110 - acc: 0.5650 -- iter: 14/14\n",
      "--\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m1.74472\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 121 | loss: 1.74472 - acc: 0.6085 -- iter: 14/14\n",
      "--\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m1.86170\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 122 | loss: 1.86170 - acc: 0.5619 -- iter: 14/14\n",
      "--\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m1.73683\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 123 | loss: 1.73683 - acc: 0.6057 -- iter: 14/14\n",
      "--\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m1.90759\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 124 | loss: 1.90759 - acc: 0.5452 -- iter: 14/14\n",
      "--\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m1.77869\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 125 | loss: 1.77869 - acc: 0.5907 -- iter: 14/14\n",
      "--\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m1.66251\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 126 | loss: 1.66251 - acc: 0.6316 -- iter: 14/14\n",
      "--\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m1.55684\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 127 | loss: 1.55684 - acc: 0.6684 -- iter: 14/14\n",
      "--\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m1.45980\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 128 | loss: 1.45980 - acc: 0.7016 -- iter: 14/14\n",
      "--\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m1.36986\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 129 | loss: 1.36986 - acc: 0.7314 -- iter: 14/14\n",
      "--\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m1.28575\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 130 | loss: 1.28575 - acc: 0.7583 -- iter: 14/14\n",
      "--\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m1.20649\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 131 | loss: 1.20649 - acc: 0.7825 -- iter: 14/14\n",
      "--\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m1.46118\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 132 | loss: 1.46118 - acc: 0.7114 -- iter: 14/14\n",
      "--\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m1.35854\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 133 | loss: 1.35854 - acc: 0.7402 -- iter: 14/14\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m1.60222\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 134 | loss: 1.60222 - acc: 0.6805 -- iter: 14/14\n",
      "--\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m1.48235\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 135 | loss: 1.48235 - acc: 0.7124 -- iter: 14/14\n",
      "--\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m1.68204\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 136 | loss: 1.68204 - acc: 0.6483 -- iter: 14/14\n",
      "--\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m1.55319\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 137 | loss: 1.55319 - acc: 0.6835 -- iter: 14/14\n",
      "--\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m1.43686\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 138 | loss: 1.43686 - acc: 0.7151 -- iter: 14/14\n",
      "--\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m1.33130\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 139 | loss: 1.33130 - acc: 0.7436 -- iter: 14/14\n",
      "--\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m1.23499\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 140 | loss: 1.23499 - acc: 0.7693 -- iter: 14/14\n",
      "--\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m1.14664\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 141 | loss: 1.14664 - acc: 0.7923 -- iter: 14/14\n",
      "--\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m1.06516\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 142 | loss: 1.06516 - acc: 0.8131 -- iter: 14/14\n",
      "--\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m0.98966\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 143 | loss: 0.98966 - acc: 0.8318 -- iter: 14/14\n",
      "--\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m1.29656\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 144 | loss: 1.29656 - acc: 0.7558 -- iter: 14/14\n",
      "--\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m1.19487\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 145 | loss: 1.19487 - acc: 0.7802 -- iter: 14/14\n",
      "--\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m1.50949\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 146 | loss: 1.50949 - acc: 0.7022 -- iter: 14/14\n",
      "--\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m1.38589\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 147 | loss: 1.38589 - acc: 0.7320 -- iter: 14/14\n",
      "--\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m1.62406\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 148 | loss: 1.62406 - acc: 0.6659 -- iter: 14/14\n",
      "--\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m1.49023\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 149 | loss: 1.49023 - acc: 0.6993 -- iter: 14/14\n",
      "--\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m1.37053\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 150 | loss: 1.37053 - acc: 0.7294 -- iter: 14/14\n",
      "--\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m1.26309\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 151 | loss: 1.26309 - acc: 0.7564 -- iter: 14/14\n",
      "--\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m1.52816\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 152 | loss: 1.52816 - acc: 0.6808 -- iter: 14/14\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m1.40595\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 153 | loss: 1.40595 - acc: 0.7127 -- iter: 14/14\n",
      "--\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m1.67513\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 154 | loss: 1.67513 - acc: 0.6414 -- iter: 14/14\n",
      "--\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m1.54083\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 155 | loss: 1.54083 - acc: 0.6773 -- iter: 14/14\n",
      "--\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m1.76713\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 156 | loss: 1.76713 - acc: 0.6167 -- iter: 14/14\n",
      "--\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m1.62763\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 157 | loss: 1.62763 - acc: 0.6550 -- iter: 14/14\n",
      "--\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m1.50400\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 158 | loss: 1.50400 - acc: 0.6895 -- iter: 14/14\n",
      "--\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m1.39389\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 159 | loss: 1.39389 - acc: 0.7206 -- iter: 14/14\n",
      "--\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m1.53951\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 160 | loss: 1.53951 - acc: 0.6771 -- iter: 14/14\n",
      "--\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m1.42739\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 161 | loss: 1.42739 - acc: 0.7094 -- iter: 14/14\n",
      "--\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m1.32683\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 162 | loss: 1.32683 - acc: 0.7384 -- iter: 14/14\n",
      "--\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m1.23598\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 163 | loss: 1.23598 - acc: 0.7646 -- iter: 14/14\n",
      "--\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m1.15325\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 164 | loss: 1.15325 - acc: 0.7881 -- iter: 14/14\n",
      "--\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m1.07730\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 165 | loss: 1.07730 - acc: 0.8093 -- iter: 14/14\n",
      "--\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m1.33225\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 166 | loss: 1.33225 - acc: 0.7355 -- iter: 14/14\n",
      "--\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m1.23608\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 167 | loss: 1.23608 - acc: 0.7620 -- iter: 14/14\n",
      "--\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m1.14862\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 168 | loss: 1.14862 - acc: 0.7858 -- iter: 14/14\n",
      "--\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m1.06859\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 169 | loss: 1.06859 - acc: 0.8072 -- iter: 14/14\n",
      "--\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.99491\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 170 | loss: 0.99491 - acc: 0.8265 -- iter: 14/14\n",
      "--\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.92670\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 171 | loss: 0.92670 - acc: 0.8438 -- iter: 14/14\n",
      "--\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.86324\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 172 | loss: 0.86324 - acc: 0.8595 -- iter: 14/14\n",
      "--\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.80395\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 173 | loss: 0.80395 - acc: 0.8735 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.74839\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 174 | loss: 0.74839 - acc: 0.8862 -- iter: 14/14\n",
      "--\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.69621\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 175 | loss: 0.69621 - acc: 0.8975 -- iter: 14/14\n",
      "--\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m1.06197\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 176 | loss: 1.06197 - acc: 0.8149 -- iter: 14/14\n",
      "--\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.97557\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 177 | loss: 0.97557 - acc: 0.8334 -- iter: 14/14\n",
      "--\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m1.27690\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 178 | loss: 1.27690 - acc: 0.7644 -- iter: 14/14\n",
      "--\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m1.16830\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 179 | loss: 1.16830 - acc: 0.7879 -- iter: 14/14\n",
      "--\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m1.44290\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 180 | loss: 1.44290 - acc: 0.7163 -- iter: 14/14\n",
      "--\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m1.31863\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 181 | loss: 1.31863 - acc: 0.7447 -- iter: 14/14\n",
      "--\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m1.20751\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 182 | loss: 1.20751 - acc: 0.7702 -- iter: 14/14\n",
      "--\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m1.10796\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 183 | loss: 1.10796 - acc: 0.7932 -- iter: 14/14\n",
      "--\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m1.42093\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 184 | loss: 1.42093 - acc: 0.7210 -- iter: 14/14\n",
      "--\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m1.30151\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 185 | loss: 1.30151 - acc: 0.7489 -- iter: 14/14\n",
      "--\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m1.52734\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 186 | loss: 1.52734 - acc: 0.6954 -- iter: 14/14\n",
      "--\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m1.40001\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 187 | loss: 1.40001 - acc: 0.7259 -- iter: 14/14\n",
      "--\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m1.28681\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 188 | loss: 1.28681 - acc: 0.7533 -- iter: 14/14\n",
      "--\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m1.18589\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 189 | loss: 1.18589 - acc: 0.7780 -- iter: 14/14\n",
      "--\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m1.09558\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 190 | loss: 1.09558 - acc: 0.8002 -- iter: 14/14\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m1.01440\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 191 | loss: 1.01440 - acc: 0.8202 -- iter: 14/14\n",
      "--\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m1.32386\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 192 | loss: 1.32386 - acc: 0.7381 -- iter: 14/14\n",
      "--\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m1.22043\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 193 | loss: 1.22043 - acc: 0.7643 -- iter: 14/14\n",
      "--\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m1.12776\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 194 | loss: 1.12776 - acc: 0.7879 -- iter: 14/14\n",
      "--\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m1.04433\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 195 | loss: 1.04433 - acc: 0.8091 -- iter: 14/14\n",
      "--\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.96881\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 196 | loss: 0.96881 - acc: 0.8282 -- iter: 14/14\n",
      "--\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.90006\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 197 | loss: 0.90006 - acc: 0.8454 -- iter: 14/14\n",
      "--\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m1.26160\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 198 | loss: 1.26160 - acc: 0.7608 -- iter: 14/14\n",
      "--\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m1.16292\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 199 | loss: 1.16292 - acc: 0.7848 -- iter: 14/14\n",
      "--\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m1.07413\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 200 | loss: 1.07413 - acc: 0.8063 -- iter: 14/14\n",
      "--\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.99387\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 201 | loss: 0.99387 - acc: 0.8257 -- iter: 14/14\n",
      "--\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.92098\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 202 | loss: 0.92098 - acc: 0.8431 -- iter: 14/14\n",
      "--\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.85444\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 203 | loss: 0.85444 - acc: 0.8588 -- iter: 14/14\n",
      "--\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m1.13433\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 204 | loss: 1.13433 - acc: 0.7872 -- iter: 14/14\n",
      "--\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m1.04524\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 205 | loss: 1.04524 - acc: 0.8085 -- iter: 14/14\n",
      "--\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m1.34576\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 206 | loss: 1.34576 - acc: 0.7276 -- iter: 14/14\n",
      "--\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m1.23607\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 207 | loss: 1.23607 - acc: 0.7549 -- iter: 14/14\n",
      "--\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m1.46285\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 208 | loss: 1.46285 - acc: 0.7008 -- iter: 14/14\n",
      "--\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m1.34340\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 209 | loss: 1.34340 - acc: 0.7307 -- iter: 14/14\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m1.63226\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 210 | loss: 1.63226 - acc: 0.6576 -- iter: 14/14\n",
      "--\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m1.49926\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 211 | loss: 1.49926 - acc: 0.6919 -- iter: 14/14\n",
      "--\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m1.72395\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 212 | loss: 1.72395 - acc: 0.6298 -- iter: 14/14\n",
      "--\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m1.58677\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 213 | loss: 1.58677 - acc: 0.6669 -- iter: 14/14\n",
      "--\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m1.82898\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 214 | loss: 1.82898 - acc: 0.6002 -- iter: 14/14\n",
      "--\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m1.68736\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 215 | loss: 1.68736 - acc: 0.6402 -- iter: 14/14\n",
      "--\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m1.90792\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 216 | loss: 1.90792 - acc: 0.5761 -- iter: 14/14\n",
      "--\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m1.76507\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 217 | loss: 1.76507 - acc: 0.6185 -- iter: 14/14\n",
      "--\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m1.94266\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 218 | loss: 1.94266 - acc: 0.5638 -- iter: 14/14\n",
      "--\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m1.80317\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 219 | loss: 1.80317 - acc: 0.6074 -- iter: 14/14\n",
      "--\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m1.95191\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 220 | loss: 1.95191 - acc: 0.5538 -- iter: 14/14\n",
      "--\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m1.81811\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 221 | loss: 1.81811 - acc: 0.5984 -- iter: 14/14\n",
      "--\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m1.99227\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 222 | loss: 1.99227 - acc: 0.5600 -- iter: 14/14\n",
      "--\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m1.86017\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 223 | loss: 1.86017 - acc: 0.6040 -- iter: 14/14\n",
      "--\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m1.74313\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 224 | loss: 1.74313 - acc: 0.6436 -- iter: 14/14\n",
      "--\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m1.63834\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 225 | loss: 1.63834 - acc: 0.6793 -- iter: 14/14\n",
      "--\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m1.78689\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 226 | loss: 1.78689 - acc: 0.6256 -- iter: 14/14\n",
      "--\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m1.67754\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 227 | loss: 1.67754 - acc: 0.6631 -- iter: 14/14\n",
      "--\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m1.57846\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 228 | loss: 1.57846 - acc: 0.6968 -- iter: 14/14\n",
      "--\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m1.48762\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 229 | loss: 1.48762 - acc: 0.7271 -- iter: 14/14\n",
      "--\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m1.67617\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 230 | loss: 1.67617 - acc: 0.6544 -- iter: 14/14\n",
      "--\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m1.57215\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 231 | loss: 1.57215 - acc: 0.6889 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m1.74732\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 232 | loss: 1.74732 - acc: 0.6200 -- iter: 14/14\n",
      "--\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m1.63401\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 233 | loss: 1.63401 - acc: 0.6580 -- iter: 14/14\n",
      "--\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m1.53075\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 234 | loss: 1.53075 - acc: 0.6922 -- iter: 14/14\n",
      "--\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m1.43579\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 235 | loss: 1.43579 - acc: 0.7230 -- iter: 14/14\n",
      "--\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m1.64833\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 236 | loss: 1.64833 - acc: 0.6507 -- iter: 14/14\n",
      "--\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m1.53779\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 237 | loss: 1.53779 - acc: 0.6856 -- iter: 14/14\n",
      "--\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m1.43648\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 238 | loss: 1.43648 - acc: 0.7171 -- iter: 14/14\n",
      "--\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m1.34294\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 239 | loss: 1.34294 - acc: 0.7454 -- iter: 14/14\n",
      "--\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m1.25595\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 240 | loss: 1.25595 - acc: 0.7708 -- iter: 14/14\n",
      "--\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m1.17456\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 241 | loss: 1.17456 - acc: 0.7937 -- iter: 14/14\n",
      "--\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m1.33925\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 242 | loss: 1.33925 - acc: 0.7429 -- iter: 14/14\n",
      "--\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m1.24417\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 243 | loss: 1.24417 - acc: 0.7686 -- iter: 14/14\n",
      "--\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m1.51689\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 244 | loss: 1.51689 - acc: 0.6918 -- iter: 14/14\n",
      "--\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m1.40096\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 245 | loss: 1.40096 - acc: 0.7226 -- iter: 14/14\n",
      "--\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m1.63396\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 246 | loss: 1.63396 - acc: 0.6575 -- iter: 14/14\n",
      "--\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m1.50529\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 247 | loss: 1.50529 - acc: 0.6917 -- iter: 14/14\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m1.69724\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 248 | loss: 1.69724 - acc: 0.6369 -- iter: 14/14\n",
      "--\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m1.56260\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 249 | loss: 1.56260 - acc: 0.6732 -- iter: 14/14\n",
      "--\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m1.78232\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 250 | loss: 1.78232 - acc: 0.6201 -- iter: 14/14\n",
      "--\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m1.64071\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 251 | loss: 1.64071 - acc: 0.6581 -- iter: 14/14\n",
      "--\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m1.83659\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 252 | loss: 1.83659 - acc: 0.5995 -- iter: 14/14\n",
      "--\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m1.69223\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 253 | loss: 1.69223 - acc: 0.6395 -- iter: 14/14\n",
      "--\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m1.86082\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 254 | loss: 1.86082 - acc: 0.5898 -- iter: 14/14\n",
      "--\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m1.71743\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 255 | loss: 1.71743 - acc: 0.6309 -- iter: 14/14\n",
      "--\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m1.86231\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 256 | loss: 1.86231 - acc: 0.5821 -- iter: 14/14\n",
      "--\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m1.72249\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 257 | loss: 1.72249 - acc: 0.6239 -- iter: 14/14\n",
      "--\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m1.90018\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 258 | loss: 1.90018 - acc: 0.5686 -- iter: 14/14\n",
      "--\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m1.76068\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 259 | loss: 1.76068 - acc: 0.6117 -- iter: 14/14\n",
      "--\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m1.63680\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 260 | loss: 1.63680 - acc: 0.6506 -- iter: 14/14\n",
      "--\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m1.52605\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 261 | loss: 1.52605 - acc: 0.6855 -- iter: 14/14\n",
      "--\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m1.42625\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 262 | loss: 1.42625 - acc: 0.7170 -- iter: 14/14\n",
      "--\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m1.33550\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 263 | loss: 1.33550 - acc: 0.7453 -- iter: 14/14\n",
      "--\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m1.48341\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 264 | loss: 1.48341 - acc: 0.6922 -- iter: 14/14\n",
      "--\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m1.38464\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 265 | loss: 1.38464 - acc: 0.7230 -- iter: 14/14\n",
      "--\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m1.29438\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 266 | loss: 1.29438 - acc: 0.7507 -- iter: 14/14\n",
      "--\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m1.21120\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 267 | loss: 1.21120 - acc: 0.7756 -- iter: 14/14\n",
      "--\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m1.43661\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 268 | loss: 1.43661 - acc: 0.7052 -- iter: 14/14\n",
      "--\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m1.33589\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 269 | loss: 1.33589 - acc: 0.7347 -- iter: 14/14\n",
      "--\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m1.55074\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 270 | loss: 1.55074 - acc: 0.6683 -- iter: 14/14\n",
      "--\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m1.43687\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 271 | loss: 1.43687 - acc: 0.7015 -- iter: 14/14\n",
      "--\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m1.33354\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 272 | loss: 1.33354 - acc: 0.7314 -- iter: 14/14\n",
      "--\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m1.23923\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 273 | loss: 1.23923 - acc: 0.7582 -- iter: 14/14\n",
      "--\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m1.46262\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 274 | loss: 1.46262 - acc: 0.6895 -- iter: 14/14\n",
      "--\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m1.35327\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 275 | loss: 1.35327 - acc: 0.7206 -- iter: 14/14\n",
      "--\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m1.25400\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 276 | loss: 1.25400 - acc: 0.7485 -- iter: 14/14\n",
      "--\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m1.16342\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 277 | loss: 1.16342 - acc: 0.7737 -- iter: 14/14\n",
      "--\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m1.08036\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 278 | loss: 1.08036 - acc: 0.7963 -- iter: 14/14\n",
      "--\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m1.00380\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 279 | loss: 1.00380 - acc: 0.8167 -- iter: 14/14\n",
      "--\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.93294\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 280 | loss: 0.93294 - acc: 0.8350 -- iter: 14/14\n",
      "--\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.86710\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 281 | loss: 0.86710 - acc: 0.8515 -- iter: 14/14\n",
      "--\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.80574\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 282 | loss: 0.80574 - acc: 0.8664 -- iter: 14/14\n",
      "--\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.74842\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 283 | loss: 0.74842 - acc: 0.8797 -- iter: 14/14\n",
      "--\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m1.03803\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 284 | loss: 1.03803 - acc: 0.8060 -- iter: 14/14\n",
      "--\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.95464\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 285 | loss: 0.95464 - acc: 0.8254 -- iter: 14/14\n",
      "--\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.87866\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 286 | loss: 0.87866 - acc: 0.8429 -- iter: 14/14\n",
      "--\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.80928\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 287 | loss: 0.80928 - acc: 0.8586 -- iter: 14/14\n",
      "--\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.74579\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 288 | loss: 0.74579 - acc: 0.8727 -- iter: 14/14\n",
      "--\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.68755\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 289 | loss: 0.68755 - acc: 0.8855 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m1.03041\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 290 | loss: 1.03041 - acc: 0.8041 -- iter: 14/14\n",
      "--\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.94247\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 291 | loss: 0.94247 - acc: 0.8237 -- iter: 14/14\n",
      "--\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m1.30082\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 292 | loss: 1.30082 - acc: 0.7484 -- iter: 14/14\n",
      "--\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m1.18621\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 293 | loss: 1.18621 - acc: 0.7736 -- iter: 14/14\n",
      "--\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m1.08353\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 294 | loss: 1.08353 - acc: 0.7962 -- iter: 14/14\n",
      "--\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.99139\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 295 | loss: 0.99139 - acc: 0.8166 -- iter: 14/14\n",
      "--\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m1.36800\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 296 | loss: 1.36800 - acc: 0.7349 -- iter: 14/14\n",
      "--\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m1.24854\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 297 | loss: 1.24854 - acc: 0.7615 -- iter: 14/14\n",
      "--\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m1.54204\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 298 | loss: 1.54204 - acc: 0.6924 -- iter: 14/14\n",
      "--\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m1.40774\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 299 | loss: 1.40774 - acc: 0.7232 -- iter: 14/14\n",
      "--\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m1.75128\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 300 | loss: 1.75128 - acc: 0.6509 -- iter: 14/14\n",
      "--\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m1.60012\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 301 | loss: 1.60012 - acc: 0.6858 -- iter: 14/14\n",
      "--\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m1.80831\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 302 | loss: 1.80831 - acc: 0.6315 -- iter: 14/14\n",
      "--\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m1.65678\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 303 | loss: 1.65678 - acc: 0.6684 -- iter: 14/14\n",
      "--\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m1.93275\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 304 | loss: 1.93275 - acc: 0.6015 -- iter: 14/14\n",
      "--\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m1.77521\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 305 | loss: 1.77521 - acc: 0.6414 -- iter: 14/14\n",
      "--\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m1.94480\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 306 | loss: 1.94480 - acc: 0.5844 -- iter: 14/14\n",
      "--\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m1.79340\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 307 | loss: 1.79340 - acc: 0.6259 -- iter: 14/14\n",
      "--\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m1.97320\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 308 | loss: 1.97320 - acc: 0.5705 -- iter: 14/14\n",
      "--\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m1.82658\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 309 | loss: 1.82658 - acc: 0.6134 -- iter: 14/14\n",
      "--\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m1.69787\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 310 | loss: 1.69787 - acc: 0.6521 -- iter: 14/14\n",
      "--\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m1.58418\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 311 | loss: 1.58418 - acc: 0.6869 -- iter: 14/14\n",
      "--\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m1.73692\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 312 | loss: 1.73692 - acc: 0.6253 -- iter: 14/14\n",
      "--\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m1.62249\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 313 | loss: 1.62249 - acc: 0.6628 -- iter: 14/14\n",
      "--\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m1.52049\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 314 | loss: 1.52049 - acc: 0.6965 -- iter: 14/14\n",
      "--\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m1.42865\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 315 | loss: 1.42865 - acc: 0.7269 -- iter: 14/14\n",
      "--\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m1.62126\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 316 | loss: 1.62126 - acc: 0.6613 -- iter: 14/14\n",
      "--\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m1.51862\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 317 | loss: 1.51862 - acc: 0.6952 -- iter: 14/14\n",
      "--\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m1.71867\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 318 | loss: 1.71867 - acc: 0.6257 -- iter: 14/14\n",
      "--\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m1.60606\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 319 | loss: 1.60606 - acc: 0.6631 -- iter: 14/14\n",
      "--\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m1.50424\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 320 | loss: 1.50424 - acc: 0.6968 -- iter: 14/14\n",
      "--\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m1.41129\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 321 | loss: 1.41129 - acc: 0.7271 -- iter: 14/14\n",
      "--\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m1.60723\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 322 | loss: 1.60723 - acc: 0.6544 -- iter: 14/14\n",
      "--\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m1.50146\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 323 | loss: 1.50146 - acc: 0.6890 -- iter: 14/14\n",
      "--\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m1.63254\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 324 | loss: 1.63254 - acc: 0.6415 -- iter: 14/14\n",
      "--\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m1.52277\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 325 | loss: 1.52277 - acc: 0.6773 -- iter: 14/14\n",
      "--\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m1.42301\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 326 | loss: 1.42301 - acc: 0.7096 -- iter: 14/14\n",
      "--\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m1.33157\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 327 | loss: 1.33157 - acc: 0.7387 -- iter: 14/14\n",
      "--\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m1.51229\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 328 | loss: 1.51229 - acc: 0.6791 -- iter: 14/14\n",
      "--\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m1.40883\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 329 | loss: 1.40883 - acc: 0.7112 -- iter: 14/14\n",
      "--\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m1.62379\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 330 | loss: 1.62379 - acc: 0.6472 -- iter: 14/14\n",
      "--\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m1.50760\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 331 | loss: 1.50760 - acc: 0.6825 -- iter: 14/14\n",
      "--\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m1.70769\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 332 | loss: 1.70769 - acc: 0.6214 -- iter: 14/14\n",
      "--\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m1.58273\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 333 | loss: 1.58273 - acc: 0.6592 -- iter: 14/14\n",
      "--\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m1.76315\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 334 | loss: 1.76315 - acc: 0.6005 -- iter: 14/14\n",
      "--\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m1.63313\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 335 | loss: 1.63313 - acc: 0.6404 -- iter: 14/14\n",
      "--\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m1.87074\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 336 | loss: 1.87074 - acc: 0.5764 -- iter: 14/14\n",
      "--\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m1.73150\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 337 | loss: 1.73150 - acc: 0.6187 -- iter: 14/14\n",
      "--\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m1.60682\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 338 | loss: 1.60682 - acc: 0.6569 -- iter: 14/14\n",
      "--\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m1.49449\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 339 | loss: 1.49449 - acc: 0.6912 -- iter: 14/14\n",
      "--\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m1.66885\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 340 | loss: 1.66885 - acc: 0.6363 -- iter: 14/14\n",
      "--\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m1.54963\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 341 | loss: 1.54963 - acc: 0.6727 -- iter: 14/14\n",
      "--\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m1.75035\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 342 | loss: 1.75035 - acc: 0.6054 -- iter: 14/14\n",
      "--\n",
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m1.62317\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 343 | loss: 1.62317 - acc: 0.6449 -- iter: 14/14\n",
      "--\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m1.50868\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 344 | loss: 1.50868 - acc: 0.6804 -- iter: 14/14\n",
      "--\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m1.40492\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 345 | loss: 1.40492 - acc: 0.7124 -- iter: 14/14\n",
      "--\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m1.60330\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 346 | loss: 1.60330 - acc: 0.6483 -- iter: 14/14\n",
      "--\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m1.48880\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 347 | loss: 1.48880 - acc: 0.6834 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m1.66316\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 348 | loss: 1.66316 - acc: 0.6294 -- iter: 14/14\n",
      "--\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m1.54253\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 349 | loss: 1.54253 - acc: 0.6664 -- iter: 14/14\n",
      "--\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m1.43373\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 350 | loss: 1.43373 - acc: 0.6998 -- iter: 14/14\n",
      "--\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m1.33493\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 351 | loss: 1.33493 - acc: 0.7298 -- iter: 14/14\n",
      "--\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m1.51035\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 352 | loss: 1.51035 - acc: 0.6711 -- iter: 14/14\n",
      "--\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m1.40216\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 353 | loss: 1.40216 - acc: 0.7040 -- iter: 14/14\n",
      "--\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m1.57994\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 354 | loss: 1.57994 - acc: 0.6550 -- iter: 14/14\n",
      "--\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m1.46402\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 355 | loss: 1.46402 - acc: 0.6895 -- iter: 14/14\n",
      "--\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m1.63819\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 356 | loss: 1.63819 - acc: 0.6349 -- iter: 14/14\n",
      "--\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m1.51648\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 357 | loss: 1.51648 - acc: 0.6714 -- iter: 14/14\n",
      "--\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m1.73848\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 358 | loss: 1.73848 - acc: 0.6042 -- iter: 14/14\n",
      "--\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m1.60784\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 359 | loss: 1.60784 - acc: 0.6438 -- iter: 14/14\n",
      "--\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m1.81863\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 360 | loss: 1.81863 - acc: 0.5794 -- iter: 14/14\n",
      "--\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m1.68217\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 361 | loss: 1.68217 - acc: 0.6215 -- iter: 14/14\n",
      "--\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m1.56025\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 362 | loss: 1.56025 - acc: 0.6593 -- iter: 14/14\n",
      "--\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m1.45062\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 363 | loss: 1.45062 - acc: 0.6934 -- iter: 14/14\n",
      "--\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m1.69090\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 364 | loss: 1.69090 - acc: 0.6241 -- iter: 14/14\n",
      "--\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m1.56818\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 365 | loss: 1.56818 - acc: 0.6617 -- iter: 14/14\n",
      "--\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m1.75643\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 366 | loss: 1.75643 - acc: 0.6026 -- iter: 14/14\n",
      "--\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m1.62795\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 367 | loss: 1.62795 - acc: 0.6424 -- iter: 14/14\n",
      "--\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m1.78176\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 368 | loss: 1.78176 - acc: 0.5853 -- iter: 14/14\n",
      "--\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m1.65212\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 369 | loss: 1.65212 - acc: 0.6268 -- iter: 14/14\n",
      "--\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m1.81900\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 370 | loss: 1.81900 - acc: 0.5712 -- iter: 14/14\n",
      "--\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m1.68753\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 371 | loss: 1.68753 - acc: 0.6141 -- iter: 14/14\n",
      "--\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m1.86658\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 372 | loss: 1.86658 - acc: 0.5670 -- iter: 14/14\n",
      "--\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m1.73266\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 373 | loss: 1.73266 - acc: 0.6103 -- iter: 14/14\n",
      "--\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m1.84812\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 374 | loss: 1.84812 - acc: 0.5707 -- iter: 14/14\n",
      "--\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m1.71818\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 375 | loss: 1.71818 - acc: 0.6136 -- iter: 14/14\n",
      "--\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m1.91073\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 376 | loss: 1.91073 - acc: 0.5594 -- iter: 14/14\n",
      "--\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m1.77661\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 377 | loss: 1.77661 - acc: 0.6035 -- iter: 14/14\n",
      "--\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m1.65653\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 378 | loss: 1.65653 - acc: 0.6431 -- iter: 14/14\n",
      "--\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m1.54817\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 379 | loss: 1.54817 - acc: 0.6788 -- iter: 14/14\n",
      "--\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m1.74039\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 380 | loss: 1.74039 - acc: 0.6181 -- iter: 14/14\n",
      "--\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m1.62253\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 381 | loss: 1.62253 - acc: 0.6563 -- iter: 14/14\n",
      "--\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m1.79236\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 382 | loss: 1.79236 - acc: 0.5978 -- iter: 14/14\n",
      "--\n",
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m1.66882\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 383 | loss: 1.66882 - acc: 0.6380 -- iter: 14/14\n",
      "--\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m1.55715\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 384 | loss: 1.55715 - acc: 0.6742 -- iter: 14/14\n",
      "--\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m1.45539\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 385 | loss: 1.45539 - acc: 0.7068 -- iter: 14/14\n",
      "--\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m1.67574\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 386 | loss: 1.67574 - acc: 0.6361 -- iter: 14/14\n",
      "--\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m1.55962\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 387 | loss: 1.55962 - acc: 0.6725 -- iter: 14/14\n",
      "--\n",
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m1.45385\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 388 | loss: 1.45385 - acc: 0.7052 -- iter: 14/14\n",
      "--\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m1.35680\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 389 | loss: 1.35680 - acc: 0.7347 -- iter: 14/14\n",
      "--\n",
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m1.26716\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 390 | loss: 1.26716 - acc: 0.7612 -- iter: 14/14\n",
      "--\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m1.18382\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 391 | loss: 1.18382 - acc: 0.7851 -- iter: 14/14\n",
      "--\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m1.43623\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 392 | loss: 1.43623 - acc: 0.7066 -- iter: 14/14\n",
      "--\n",
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m1.33191\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 393 | loss: 1.33191 - acc: 0.7359 -- iter: 14/14\n",
      "--\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m1.23645\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 394 | loss: 1.23645 - acc: 0.7624 -- iter: 14/14\n",
      "--\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m1.14867\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 395 | loss: 1.14867 - acc: 0.7861 -- iter: 14/14\n",
      "--\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m1.40755\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 396 | loss: 1.40755 - acc: 0.7075 -- iter: 14/14\n",
      "--\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m1.29993\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 397 | loss: 1.29993 - acc: 0.7368 -- iter: 14/14\n",
      "--\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m1.20211\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 398 | loss: 1.20211 - acc: 0.7631 -- iter: 14/14\n",
      "--\n",
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m1.11281\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 399 | loss: 1.11281 - acc: 0.7868 -- iter: 14/14\n",
      "--\n",
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m1.38166\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 400 | loss: 1.38166 - acc: 0.7224 -- iter: 14/14\n",
      "--\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m1.27262\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 401 | loss: 1.27262 - acc: 0.7501 -- iter: 14/14\n",
      "--\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m1.17387\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 402 | loss: 1.17387 - acc: 0.7751 -- iter: 14/14\n",
      "--\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m1.08410\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 403 | loss: 1.08410 - acc: 0.7976 -- iter: 14/14\n",
      "--\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m1.00217\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 404 | loss: 1.00217 - acc: 0.8179 -- iter: 14/14\n",
      "--\n",
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m0.92714\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 405 | loss: 0.92714 - acc: 0.8361 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m0.85817\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 406 | loss: 0.85817 - acc: 0.8525 -- iter: 14/14\n",
      "--\n",
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.79459\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 407 | loss: 0.79459 - acc: 0.8672 -- iter: 14/14\n",
      "--\n",
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m1.18077\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 408 | loss: 1.18077 - acc: 0.7805 -- iter: 14/14\n",
      "--\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m1.08306\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 409 | loss: 1.08306 - acc: 0.8024 -- iter: 14/14\n",
      "--\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m0.99464\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 410 | loss: 0.99464 - acc: 0.8222 -- iter: 14/14\n",
      "--\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m0.91441\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 411 | loss: 0.91441 - acc: 0.8400 -- iter: 14/14\n",
      "--\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m1.24855\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 412 | loss: 1.24855 - acc: 0.7631 -- iter: 14/14\n",
      "--\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m1.14240\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 413 | loss: 1.14240 - acc: 0.7868 -- iter: 14/14\n",
      "--\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m1.04689\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 414 | loss: 1.04689 - acc: 0.8081 -- iter: 14/14\n",
      "--\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.96078\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 415 | loss: 0.96078 - acc: 0.8273 -- iter: 14/14\n",
      "--\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m1.25772\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 416 | loss: 1.25772 - acc: 0.7589 -- iter: 14/14\n",
      "--\n",
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m1.15070\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 417 | loss: 1.15070 - acc: 0.7830 -- iter: 14/14\n",
      "--\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m1.05466\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 418 | loss: 1.05466 - acc: 0.8047 -- iter: 14/14\n",
      "--\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.96829\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 419 | loss: 0.96829 - acc: 0.8242 -- iter: 14/14\n",
      "--\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m0.89043\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 420 | loss: 0.89043 - acc: 0.8418 -- iter: 14/14\n",
      "--\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m0.82005\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 421 | loss: 0.82005 - acc: 0.8576 -- iter: 14/14\n",
      "--\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m1.17686\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 422 | loss: 1.17686 - acc: 0.7790 -- iter: 14/14\n",
      "--\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m1.07787\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 423 | loss: 1.07787 - acc: 0.8011 -- iter: 14/14\n",
      "--\n",
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m0.98905\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 424 | loss: 0.98905 - acc: 0.8210 -- iter: 14/14\n",
      "--\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m0.90916\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 425 | loss: 0.90916 - acc: 0.8389 -- iter: 14/14\n",
      "--\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m1.25531\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 426 | loss: 1.25531 - acc: 0.7621 -- iter: 14/14\n",
      "--\n",
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m1.14942\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 427 | loss: 1.14942 - acc: 0.7859 -- iter: 14/14\n",
      "--\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m1.47781\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 428 | loss: 1.47781 - acc: 0.7073 -- iter: 14/14\n",
      "--\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m1.35173\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 429 | loss: 1.35173 - acc: 0.7366 -- iter: 14/14\n",
      "--\n",
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m1.64493\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 430 | loss: 1.64493 - acc: 0.6701 -- iter: 14/14\n",
      "--\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m1.50561\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 431 | loss: 1.50561 - acc: 0.7031 -- iter: 14/14\n",
      "--\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m1.38210\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 432 | loss: 1.38210 - acc: 0.7328 -- iter: 14/14\n",
      "--\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m1.27236\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 433 | loss: 1.27236 - acc: 0.7595 -- iter: 14/14\n",
      "--\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m1.17456\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 434 | loss: 1.17456 - acc: 0.7835 -- iter: 14/14\n",
      "--\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m1.08703\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 435 | loss: 1.08703 - acc: 0.8052 -- iter: 14/14\n",
      "--\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m1.37978\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 436 | loss: 1.37978 - acc: 0.7247 -- iter: 14/14\n",
      "--\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m1.27301\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 437 | loss: 1.27301 - acc: 0.7522 -- iter: 14/14\n",
      "--\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m1.51293\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 438 | loss: 1.51293 - acc: 0.6841 -- iter: 14/14\n",
      "--\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m1.39530\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 439 | loss: 1.39530 - acc: 0.7157 -- iter: 14/14\n",
      "--\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m1.62567\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 440 | loss: 1.62567 - acc: 0.6513 -- iter: 14/14\n",
      "--\n",
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m1.50016\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 441 | loss: 1.50016 - acc: 0.6862 -- iter: 14/14\n",
      "--\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m1.38878\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 442 | loss: 1.38878 - acc: 0.7175 -- iter: 14/14\n",
      "--\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m1.28946\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 443 | loss: 1.28946 - acc: 0.7458 -- iter: 14/14\n",
      "--\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m1.51318\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 444 | loss: 1.51318 - acc: 0.6784 -- iter: 14/14\n",
      "--\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m1.40307\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 445 | loss: 1.40307 - acc: 0.7105 -- iter: 14/14\n",
      "--\n",
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m1.62596\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 446 | loss: 1.62596 - acc: 0.6395 -- iter: 14/14\n",
      "--\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m1.50706\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 447 | loss: 1.50706 - acc: 0.6755 -- iter: 14/14\n",
      "--\n",
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m1.72487\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 448 | loss: 1.72487 - acc: 0.6080 -- iter: 14/14\n",
      "--\n",
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m1.59930\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 449 | loss: 1.59930 - acc: 0.6472 -- iter: 14/14\n",
      "--\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m1.80095\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 450 | loss: 1.80095 - acc: 0.5825 -- iter: 14/14\n",
      "--\n",
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m1.67153\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 451 | loss: 1.67153 - acc: 0.6242 -- iter: 14/14\n",
      "--\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m1.85111\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 452 | loss: 1.85111 - acc: 0.5618 -- iter: 14/14\n",
      "--\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m1.72079\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 453 | loss: 1.72079 - acc: 0.6056 -- iter: 14/14\n",
      "--\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m1.60509\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 454 | loss: 1.60509 - acc: 0.6450 -- iter: 14/14\n",
      "--\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m1.50153\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 455 | loss: 1.50153 - acc: 0.6805 -- iter: 14/14\n",
      "--\n",
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m1.40795\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 456 | loss: 1.40795 - acc: 0.7125 -- iter: 14/14\n",
      "--\n",
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m1.32253\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 457 | loss: 1.32253 - acc: 0.7412 -- iter: 14/14\n",
      "--\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m1.24373\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 458 | loss: 1.24373 - acc: 0.7671 -- iter: 14/14\n",
      "--\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m1.17030\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 459 | loss: 1.17030 - acc: 0.7904 -- iter: 14/14\n",
      "--\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m1.41328\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 460 | loss: 1.41328 - acc: 0.7114 -- iter: 14/14\n",
      "--\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m1.31859\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 461 | loss: 1.31859 - acc: 0.7402 -- iter: 14/14\n",
      "--\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m1.55797\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 462 | loss: 1.55797 - acc: 0.6662 -- iter: 14/14\n",
      "--\n",
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m1.44650\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 463 | loss: 1.44650 - acc: 0.6996 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m1.60383\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 464 | loss: 1.60383 - acc: 0.6439 -- iter: 14/14\n",
      "--\n",
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m1.48656\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 465 | loss: 1.48656 - acc: 0.6795 -- iter: 14/14\n",
      "--\n",
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m1.38028\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 466 | loss: 1.38028 - acc: 0.7116 -- iter: 14/14\n",
      "--\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m1.28338\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 467 | loss: 1.28338 - acc: 0.7404 -- iter: 14/14\n",
      "--\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m1.49694\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 468 | loss: 1.49694 - acc: 0.6807 -- iter: 14/14\n",
      "--\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m1.38628\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 469 | loss: 1.38628 - acc: 0.7126 -- iter: 14/14\n",
      "--\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m1.57584\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 470 | loss: 1.57584 - acc: 0.6628 -- iter: 14/14\n",
      "--\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m1.45643\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 471 | loss: 1.45643 - acc: 0.6965 -- iter: 14/14\n",
      "--\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m1.34851\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 472 | loss: 1.34851 - acc: 0.7268 -- iter: 14/14\n",
      "--\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m1.25049\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 473 | loss: 1.25049 - acc: 0.7542 -- iter: 14/14\n",
      "--\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m1.51263\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 474 | loss: 1.51263 - acc: 0.6787 -- iter: 14/14\n",
      "--\n",
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m1.39703\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 475 | loss: 1.39703 - acc: 0.7109 -- iter: 14/14\n",
      "--\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m1.58854\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 476 | loss: 1.58854 - acc: 0.6469 -- iter: 14/14\n",
      "--\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m1.46576\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 477 | loss: 1.46576 - acc: 0.6822 -- iter: 14/14\n",
      "--\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m1.71201\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 478 | loss: 1.71201 - acc: 0.6140 -- iter: 14/14\n",
      "--\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m1.57856\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 479 | loss: 1.57856 - acc: 0.6526 -- iter: 14/14\n",
      "--\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m1.81177\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 480 | loss: 1.81177 - acc: 0.5873 -- iter: 14/14\n",
      "--\n",
      "Training Step: 481  | total loss: \u001b[1m\u001b[32m1.67129\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 481 | loss: 1.67129 - acc: 0.6286 -- iter: 14/14\n",
      "--\n",
      "Training Step: 482  | total loss: \u001b[1m\u001b[32m1.54625\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 482 | loss: 1.54625 - acc: 0.6657 -- iter: 14/14\n",
      "--\n",
      "Training Step: 483  | total loss: \u001b[1m\u001b[32m1.43441\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 483 | loss: 1.43441 - acc: 0.6992 -- iter: 14/14\n",
      "--\n",
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m1.33376\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 484 | loss: 1.33376 - acc: 0.7293 -- iter: 14/14\n",
      "--\n",
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m1.24256\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 485 | loss: 1.24256 - acc: 0.7563 -- iter: 14/14\n",
      "--\n",
      "Training Step: 486  | total loss: \u001b[1m\u001b[32m1.51635\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 486 | loss: 1.51635 - acc: 0.6807 -- iter: 14/14\n",
      "--\n",
      "Training Step: 487  | total loss: \u001b[1m\u001b[32m1.40601\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 487 | loss: 1.40601 - acc: 0.7126 -- iter: 14/14\n",
      "--\n",
      "Training Step: 488  | total loss: \u001b[1m\u001b[32m1.30635\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 488 | loss: 1.30635 - acc: 0.7414 -- iter: 14/14\n",
      "--\n",
      "Training Step: 489  | total loss: \u001b[1m\u001b[32m1.21573\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 489 | loss: 1.21573 - acc: 0.7672 -- iter: 14/14\n",
      "--\n",
      "Training Step: 490  | total loss: \u001b[1m\u001b[32m1.48468\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 490 | loss: 1.48468 - acc: 0.6905 -- iter: 14/14\n",
      "--\n",
      "Training Step: 491  | total loss: \u001b[1m\u001b[32m1.37488\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 491 | loss: 1.37488 - acc: 0.7215 -- iter: 14/14\n",
      "--\n",
      "Training Step: 492  | total loss: \u001b[1m\u001b[32m1.27556\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 492 | loss: 1.27556 - acc: 0.7493 -- iter: 14/14\n",
      "--\n",
      "Training Step: 493  | total loss: \u001b[1m\u001b[32m1.18516\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 493 | loss: 1.18516 - acc: 0.7744 -- iter: 14/14\n",
      "--\n",
      "Training Step: 494  | total loss: \u001b[1m\u001b[32m1.44338\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 494 | loss: 1.44338 - acc: 0.6969 -- iter: 14/14\n",
      "--\n",
      "Training Step: 495  | total loss: \u001b[1m\u001b[32m1.33474\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 495 | loss: 1.33474 - acc: 0.7272 -- iter: 14/14\n",
      "--\n",
      "Training Step: 496  | total loss: \u001b[1m\u001b[32m1.23642\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 496 | loss: 1.23642 - acc: 0.7545 -- iter: 14/14\n",
      "--\n",
      "Training Step: 497  | total loss: \u001b[1m\u001b[32m1.14697\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 497 | loss: 1.14697 - acc: 0.7791 -- iter: 14/14\n",
      "--\n",
      "Training Step: 498  | total loss: \u001b[1m\u001b[32m1.39895\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 498 | loss: 1.39895 - acc: 0.7083 -- iter: 14/14\n",
      "--\n",
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m1.29190\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 499 | loss: 1.29190 - acc: 0.7375 -- iter: 14/14\n",
      "--\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m1.56442\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 500 | loss: 1.56442 - acc: 0.6637 -- iter: 14/14\n",
      "--\n",
      "Training Step: 501  | total loss: \u001b[1m\u001b[32m1.44113\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 501 | loss: 1.44113 - acc: 0.6974 -- iter: 14/14\n",
      "--\n",
      "Training Step: 502  | total loss: \u001b[1m\u001b[32m1.69207\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 502 | loss: 1.69207 - acc: 0.6276 -- iter: 14/14\n",
      "--\n",
      "Training Step: 503  | total loss: \u001b[1m\u001b[32m1.55783\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 503 | loss: 1.55783 - acc: 0.6649 -- iter: 14/14\n",
      "--\n",
      "Training Step: 504  | total loss: \u001b[1m\u001b[32m1.76506\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 504 | loss: 1.76506 - acc: 0.6055 -- iter: 14/14\n",
      "--\n",
      "Training Step: 505  | total loss: \u001b[1m\u001b[32m1.62658\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 505 | loss: 1.62658 - acc: 0.6450 -- iter: 14/14\n",
      "--\n",
      "Training Step: 506  | total loss: \u001b[1m\u001b[32m1.81020\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 506 | loss: 1.81020 - acc: 0.5948 -- iter: 14/14\n",
      "--\n",
      "Training Step: 507  | total loss: \u001b[1m\u001b[32m1.67084\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 507 | loss: 1.67084 - acc: 0.6353 -- iter: 14/14\n",
      "--\n",
      "Training Step: 508  | total loss: \u001b[1m\u001b[32m1.80610\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 508 | loss: 1.80610 - acc: 0.5932 -- iter: 14/14\n",
      "--\n",
      "Training Step: 509  | total loss: \u001b[1m\u001b[32m1.67085\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 509 | loss: 1.67085 - acc: 0.6339 -- iter: 14/14\n",
      "--\n",
      "Training Step: 510  | total loss: \u001b[1m\u001b[32m1.55049\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 510 | loss: 1.55049 - acc: 0.6705 -- iter: 14/14\n",
      "--\n",
      "Training Step: 511  | total loss: \u001b[1m\u001b[32m1.44269\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 511 | loss: 1.44269 - acc: 0.7034 -- iter: 14/14\n",
      "--\n",
      "Training Step: 512  | total loss: \u001b[1m\u001b[32m1.65717\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 512 | loss: 1.65717 - acc: 0.6331 -- iter: 14/14\n",
      "--\n",
      "Training Step: 513  | total loss: \u001b[1m\u001b[32m1.53947\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 513 | loss: 1.53947 - acc: 0.6698 -- iter: 14/14\n",
      "--\n",
      "Training Step: 514  | total loss: \u001b[1m\u001b[32m1.43371\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 514 | loss: 1.43371 - acc: 0.7028 -- iter: 14/14\n",
      "--\n",
      "Training Step: 515  | total loss: \u001b[1m\u001b[32m1.33796\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 515 | loss: 1.33796 - acc: 0.7325 -- iter: 14/14\n",
      "--\n",
      "Training Step: 516  | total loss: \u001b[1m\u001b[32m1.56936\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 516 | loss: 1.56936 - acc: 0.6593 -- iter: 14/14\n",
      "--\n",
      "Training Step: 517  | total loss: \u001b[1m\u001b[32m1.45893\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 517 | loss: 1.45893 - acc: 0.6933 -- iter: 14/14\n",
      "--\n",
      "Training Step: 518  | total loss: \u001b[1m\u001b[32m1.68036\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 518 | loss: 1.68036 - acc: 0.6311 -- iter: 14/14\n",
      "--\n",
      "Training Step: 519  | total loss: \u001b[1m\u001b[32m1.55890\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 519 | loss: 1.55890 - acc: 0.6680 -- iter: 14/14\n",
      "--\n",
      "Training Step: 520  | total loss: \u001b[1m\u001b[32m1.44947\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 520 | loss: 1.44947 - acc: 0.7012 -- iter: 14/14\n",
      "--\n",
      "Training Step: 521  | total loss: \u001b[1m\u001b[32m1.35019\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 521 | loss: 1.35019 - acc: 0.7311 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 522  | total loss: \u001b[1m\u001b[32m1.25944\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 522 | loss: 1.25944 - acc: 0.7580 -- iter: 14/14\n",
      "--\n",
      "Training Step: 523  | total loss: \u001b[1m\u001b[32m1.17589\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 523 | loss: 1.17589 - acc: 0.7822 -- iter: 14/14\n",
      "--\n",
      "Training Step: 524  | total loss: \u001b[1m\u001b[32m1.42759\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 524 | loss: 1.42759 - acc: 0.7111 -- iter: 14/14\n",
      "--\n",
      "Training Step: 525  | total loss: \u001b[1m\u001b[32m1.32414\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 525 | loss: 1.32414 - acc: 0.7400 -- iter: 14/14\n",
      "--\n",
      "Training Step: 526  | total loss: \u001b[1m\u001b[32m1.55066\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 526 | loss: 1.55066 - acc: 0.6732 -- iter: 14/14\n",
      "--\n",
      "Training Step: 527  | total loss: \u001b[1m\u001b[32m1.43366\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 527 | loss: 1.43366 - acc: 0.7058 -- iter: 14/14\n",
      "--\n",
      "Training Step: 528  | total loss: \u001b[1m\u001b[32m1.68160\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 528 | loss: 1.68160 - acc: 0.6353 -- iter: 14/14\n",
      "--\n",
      "Training Step: 529  | total loss: \u001b[1m\u001b[32m1.55185\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 529 | loss: 1.55185 - acc: 0.6717 -- iter: 14/14\n",
      "--\n",
      "Training Step: 530  | total loss: \u001b[1m\u001b[32m1.73002\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 530 | loss: 1.73002 - acc: 0.6117 -- iter: 14/14\n",
      "--\n",
      "Training Step: 531  | total loss: \u001b[1m\u001b[32m1.59708\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 531 | loss: 1.59708 - acc: 0.6505 -- iter: 14/14\n",
      "--\n",
      "Training Step: 532  | total loss: \u001b[1m\u001b[32m1.69741\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 532 | loss: 1.69741 - acc: 0.6212 -- iter: 14/14\n",
      "--\n",
      "Training Step: 533  | total loss: \u001b[1m\u001b[32m1.56975\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 533 | loss: 1.56975 - acc: 0.6591 -- iter: 14/14\n",
      "--\n",
      "Training Step: 534  | total loss: \u001b[1m\u001b[32m1.45544\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 534 | loss: 1.45544 - acc: 0.6932 -- iter: 14/14\n",
      "--\n",
      "Training Step: 535  | total loss: \u001b[1m\u001b[32m1.35250\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 535 | loss: 1.35250 - acc: 0.7238 -- iter: 14/14\n",
      "--\n",
      "Training Step: 536  | total loss: \u001b[1m\u001b[32m1.59494\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 536 | loss: 1.59494 - acc: 0.6586 -- iter: 14/14\n",
      "--\n",
      "Training Step: 537  | total loss: \u001b[1m\u001b[32m1.47804\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 537 | loss: 1.47804 - acc: 0.6927 -- iter: 14/14\n",
      "--\n",
      "Training Step: 538  | total loss: \u001b[1m\u001b[32m1.37281\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 538 | loss: 1.37281 - acc: 0.7235 -- iter: 14/14\n",
      "--\n",
      "Training Step: 539  | total loss: \u001b[1m\u001b[32m1.27748\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 539 | loss: 1.27748 - acc: 0.7511 -- iter: 14/14\n",
      "--\n",
      "Training Step: 540  | total loss: \u001b[1m\u001b[32m1.46388\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 540 | loss: 1.46388 - acc: 0.6974 -- iter: 14/14\n",
      "--\n",
      "Training Step: 541  | total loss: \u001b[1m\u001b[32m1.35804\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 541 | loss: 1.35804 - acc: 0.7277 -- iter: 14/14\n",
      "--\n",
      "Training Step: 542  | total loss: \u001b[1m\u001b[32m1.26202\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 542 | loss: 1.26202 - acc: 0.7549 -- iter: 14/14\n",
      "--\n",
      "Training Step: 543  | total loss: \u001b[1m\u001b[32m1.17436\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 543 | loss: 1.17436 - acc: 0.7794 -- iter: 14/14\n",
      "--\n",
      "Training Step: 544  | total loss: \u001b[1m\u001b[32m1.39447\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 544 | loss: 1.39447 - acc: 0.7158 -- iter: 14/14\n",
      "--\n",
      "Training Step: 545  | total loss: \u001b[1m\u001b[32m1.29147\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 545 | loss: 1.29147 - acc: 0.7442 -- iter: 14/14\n",
      "--\n",
      "Training Step: 546  | total loss: \u001b[1m\u001b[32m1.55630\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 546 | loss: 1.55630 - acc: 0.6698 -- iter: 14/14\n",
      "--\n",
      "Training Step: 547  | total loss: \u001b[1m\u001b[32m1.43673\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 547 | loss: 1.43673 - acc: 0.7028 -- iter: 14/14\n",
      "--\n",
      "Training Step: 548  | total loss: \u001b[1m\u001b[32m1.32909\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 548 | loss: 1.32909 - acc: 0.7325 -- iter: 14/14\n",
      "--\n",
      "Training Step: 549  | total loss: \u001b[1m\u001b[32m1.23170\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 549 | loss: 1.23170 - acc: 0.7593 -- iter: 14/14\n",
      "--\n",
      "Training Step: 550  | total loss: \u001b[1m\u001b[32m1.14309\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 550 | loss: 1.14309 - acc: 0.7833 -- iter: 14/14\n",
      "--\n",
      "Training Step: 551  | total loss: \u001b[1m\u001b[32m1.06202\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 551 | loss: 1.06202 - acc: 0.8050 -- iter: 14/14\n",
      "--\n",
      "Training Step: 552  | total loss: \u001b[1m\u001b[32m1.35614\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 552 | loss: 1.35614 - acc: 0.7316 -- iter: 14/14\n",
      "--\n",
      "Training Step: 553  | total loss: \u001b[1m\u001b[32m1.25183\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 553 | loss: 1.25183 - acc: 0.7585 -- iter: 14/14\n",
      "--\n",
      "Training Step: 554  | total loss: \u001b[1m\u001b[32m1.51320\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 554 | loss: 1.51320 - acc: 0.6898 -- iter: 14/14\n",
      "--\n",
      "Training Step: 555  | total loss: \u001b[1m\u001b[32m1.39289\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 555 | loss: 1.39289 - acc: 0.7208 -- iter: 14/14\n",
      "--\n",
      "Training Step: 556  | total loss: \u001b[1m\u001b[32m1.67835\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 556 | loss: 1.67835 - acc: 0.6487 -- iter: 14/14\n",
      "--\n",
      "Training Step: 557  | total loss: \u001b[1m\u001b[32m1.54271\u001b[0m\u001b[0m | time: 0.014s\n",
      "| Adam | epoch: 557 | loss: 1.54271 - acc: 0.6838 -- iter: 14/14\n",
      "--\n",
      "Training Step: 558  | total loss: \u001b[1m\u001b[32m1.75101\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 558 | loss: 1.75101 - acc: 0.6226 -- iter: 14/14\n",
      "--\n",
      "Training Step: 559  | total loss: \u001b[1m\u001b[32m1.61055\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 559 | loss: 1.61055 - acc: 0.6603 -- iter: 14/14\n",
      "--\n",
      "Training Step: 560  | total loss: \u001b[1m\u001b[32m1.80715\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 560 | loss: 1.80715 - acc: 0.6015 -- iter: 14/14\n",
      "--\n",
      "Training Step: 561  | total loss: \u001b[1m\u001b[32m1.66453\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 561 | loss: 1.66453 - acc: 0.6413 -- iter: 14/14\n",
      "--\n",
      "Training Step: 562  | total loss: \u001b[1m\u001b[32m1.76655\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 562 | loss: 1.76655 - acc: 0.6057 -- iter: 14/14\n",
      "--\n",
      "Training Step: 563  | total loss: \u001b[1m\u001b[32m1.63165\u001b[0m\u001b[0m | time: 0.023s\n",
      "| Adam | epoch: 563 | loss: 1.63165 - acc: 0.6452 -- iter: 14/14\n",
      "--\n",
      "Training Step: 564  | total loss: \u001b[1m\u001b[32m1.84884\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 564 | loss: 1.84884 - acc: 0.5807 -- iter: 14/14\n",
      "--\n",
      "Training Step: 565  | total loss: \u001b[1m\u001b[32m1.70959\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 565 | loss: 1.70959 - acc: 0.6226 -- iter: 14/14\n",
      "--\n",
      "Training Step: 566  | total loss: \u001b[1m\u001b[32m1.89440\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 566 | loss: 1.89440 - acc: 0.5603 -- iter: 14/14\n",
      "--\n",
      "Training Step: 567  | total loss: \u001b[1m\u001b[32m1.75511\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 567 | loss: 1.75511 - acc: 0.6043 -- iter: 14/14\n",
      "--\n",
      "Training Step: 568  | total loss: \u001b[1m\u001b[32m1.87136\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 568 | loss: 1.87136 - acc: 0.5653 -- iter: 14/14\n",
      "--\n",
      "Training Step: 569  | total loss: \u001b[1m\u001b[32m1.73872\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 569 | loss: 1.73872 - acc: 0.6088 -- iter: 14/14\n",
      "--\n",
      "Training Step: 570  | total loss: \u001b[1m\u001b[32m1.62082\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 570 | loss: 1.62082 - acc: 0.6479 -- iter: 14/14\n",
      "--\n",
      "Training Step: 571  | total loss: \u001b[1m\u001b[32m1.51518\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 571 | loss: 1.51518 - acc: 0.6831 -- iter: 14/14\n",
      "--\n",
      "Training Step: 572  | total loss: \u001b[1m\u001b[32m1.69879\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 572 | loss: 1.69879 - acc: 0.6219 -- iter: 14/14\n",
      "--\n",
      "Training Step: 573  | total loss: \u001b[1m\u001b[32m1.58568\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 573 | loss: 1.58568 - acc: 0.6597 -- iter: 14/14\n",
      "--\n",
      "Training Step: 574  | total loss: \u001b[1m\u001b[32m1.74317\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 574 | loss: 1.74317 - acc: 0.6009 -- iter: 14/14\n",
      "--\n",
      "Training Step: 575  | total loss: \u001b[1m\u001b[32m1.62616\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 575 | loss: 1.62616 - acc: 0.6408 -- iter: 14/14\n",
      "--\n",
      "Training Step: 576  | total loss: \u001b[1m\u001b[32m1.52064\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 576 | loss: 1.52064 - acc: 0.6767 -- iter: 14/14\n",
      "--\n",
      "Training Step: 577  | total loss: \u001b[1m\u001b[32m1.42461\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 577 | loss: 1.42461 - acc: 0.7091 -- iter: 14/14\n",
      "--\n",
      "Training Step: 578  | total loss: \u001b[1m\u001b[32m1.54605\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 578 | loss: 1.54605 - acc: 0.6667 -- iter: 14/14\n",
      "--\n",
      "Training Step: 579  | total loss: \u001b[1m\u001b[32m1.44483\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 579 | loss: 1.44483 - acc: 0.7001 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 580  | total loss: \u001b[1m\u001b[32m1.60553\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 580 | loss: 1.60553 - acc: 0.6515 -- iter: 14/14\n",
      "--\n",
      "Training Step: 581  | total loss: \u001b[1m\u001b[32m1.49624\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 581 | loss: 1.49624 - acc: 0.6863 -- iter: 14/14\n",
      "--\n",
      "Training Step: 582  | total loss: \u001b[1m\u001b[32m1.68691\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 582 | loss: 1.68691 - acc: 0.6248 -- iter: 14/14\n",
      "--\n",
      "Training Step: 583  | total loss: \u001b[1m\u001b[32m1.56833\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 583 | loss: 1.56833 - acc: 0.6624 -- iter: 14/14\n",
      "--\n",
      "Training Step: 584  | total loss: \u001b[1m\u001b[32m1.72429\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 584 | loss: 1.72429 - acc: 0.6104 -- iter: 14/14\n",
      "--\n",
      "Training Step: 585  | total loss: \u001b[1m\u001b[32m1.60167\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 585 | loss: 1.60167 - acc: 0.6494 -- iter: 14/14\n",
      "--\n",
      "Training Step: 586  | total loss: \u001b[1m\u001b[32m1.78304\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 586 | loss: 1.78304 - acc: 0.5916 -- iter: 14/14\n",
      "--\n",
      "Training Step: 587  | total loss: \u001b[1m\u001b[32m1.65494\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 587 | loss: 1.65494 - acc: 0.6324 -- iter: 14/14\n",
      "--\n",
      "Training Step: 588  | total loss: \u001b[1m\u001b[32m1.83934\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 588 | loss: 1.83934 - acc: 0.5692 -- iter: 14/14\n",
      "--\n",
      "Training Step: 589  | total loss: \u001b[1m\u001b[32m1.70693\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 589 | loss: 1.70693 - acc: 0.6123 -- iter: 14/14\n",
      "--\n",
      "Training Step: 590  | total loss: \u001b[1m\u001b[32m1.87446\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 590 | loss: 1.87446 - acc: 0.5510 -- iter: 14/14\n",
      "--\n",
      "Training Step: 591  | total loss: \u001b[1m\u001b[32m1.74067\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 591 | loss: 1.74067 - acc: 0.5959 -- iter: 14/14\n",
      "--\n",
      "Training Step: 592  | total loss: \u001b[1m\u001b[32m1.62098\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 592 | loss: 1.62098 - acc: 0.6363 -- iter: 14/14\n",
      "--\n",
      "Training Step: 593  | total loss: \u001b[1m\u001b[32m1.51309\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 593 | loss: 1.51309 - acc: 0.6727 -- iter: 14/14\n",
      "--\n",
      "Training Step: 594  | total loss: \u001b[1m\u001b[32m1.70501\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 594 | loss: 1.70501 - acc: 0.6126 -- iter: 14/14\n",
      "--\n",
      "Training Step: 595  | total loss: \u001b[1m\u001b[32m1.58796\u001b[0m\u001b[0m | time: 0.022s\n",
      "| Adam | epoch: 595 | loss: 1.58796 - acc: 0.6513 -- iter: 14/14\n",
      "--\n",
      "Training Step: 596  | total loss: \u001b[1m\u001b[32m1.72893\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 596 | loss: 1.72893 - acc: 0.6005 -- iter: 14/14\n",
      "--\n",
      "Training Step: 597  | total loss: \u001b[1m\u001b[32m1.60934\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 597 | loss: 1.60934 - acc: 0.6404 -- iter: 14/14\n",
      "--\n",
      "Training Step: 598  | total loss: \u001b[1m\u001b[32m1.78880\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 598 | loss: 1.78880 - acc: 0.5835 -- iter: 14/14\n",
      "--\n",
      "Training Step: 599  | total loss: \u001b[1m\u001b[32m1.66358\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 599 | loss: 1.66358 - acc: 0.6252 -- iter: 14/14\n",
      "--\n",
      "Training Step: 600  | total loss: \u001b[1m\u001b[32m1.55075\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 600 | loss: 1.55075 - acc: 0.6627 -- iter: 14/14\n",
      "--\n",
      "Training Step: 601  | total loss: \u001b[1m\u001b[32m1.44827\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 601 | loss: 1.44827 - acc: 0.6964 -- iter: 14/14\n",
      "--\n",
      "Training Step: 602  | total loss: \u001b[1m\u001b[32m1.63488\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 602 | loss: 1.63488 - acc: 0.6268 -- iter: 14/14\n",
      "--\n",
      "Training Step: 603  | total loss: \u001b[1m\u001b[32m1.52204\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 603 | loss: 1.52204 - acc: 0.6641 -- iter: 14/14\n",
      "--\n",
      "Training Step: 604  | total loss: \u001b[1m\u001b[32m1.72069\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 604 | loss: 1.72069 - acc: 0.5977 -- iter: 14/14\n",
      "--\n",
      "Training Step: 605  | total loss: \u001b[1m\u001b[32m1.59864\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 605 | loss: 1.59864 - acc: 0.6379 -- iter: 14/14\n",
      "--\n",
      "Training Step: 606  | total loss: \u001b[1m\u001b[32m1.48839\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 606 | loss: 1.48839 - acc: 0.6741 -- iter: 14/14\n",
      "--\n",
      "Training Step: 607  | total loss: \u001b[1m\u001b[32m1.38804\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 607 | loss: 1.38804 - acc: 0.7067 -- iter: 14/14\n",
      "--\n",
      "Training Step: 608  | total loss: \u001b[1m\u001b[32m1.62975\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 608 | loss: 1.62975 - acc: 0.6432 -- iter: 14/14\n",
      "--\n",
      "Training Step: 609  | total loss: \u001b[1m\u001b[32m1.51328\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 609 | loss: 1.51328 - acc: 0.6789 -- iter: 14/14\n",
      "--\n",
      "Training Step: 610  | total loss: \u001b[1m\u001b[32m1.40753\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 610 | loss: 1.40753 - acc: 0.7110 -- iter: 14/14\n",
      "--\n",
      "Training Step: 611  | total loss: \u001b[1m\u001b[32m1.31084\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 611 | loss: 1.31084 - acc: 0.7399 -- iter: 14/14\n",
      "--\n",
      "Training Step: 612  | total loss: \u001b[1m\u001b[32m1.22183\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 612 | loss: 1.22183 - acc: 0.7659 -- iter: 14/14\n",
      "--\n",
      "Training Step: 613  | total loss: \u001b[1m\u001b[32m1.13937\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 613 | loss: 1.13937 - acc: 0.7893 -- iter: 14/14\n",
      "--\n",
      "Training Step: 614  | total loss: \u001b[1m\u001b[32m1.06255\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 614 | loss: 1.06255 - acc: 0.8104 -- iter: 14/14\n",
      "--\n",
      "Training Step: 615  | total loss: \u001b[1m\u001b[32m0.99066\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 615 | loss: 0.99066 - acc: 0.8293 -- iter: 14/14\n",
      "--\n",
      "Training Step: 616  | total loss: \u001b[1m\u001b[32m1.29069\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 616 | loss: 1.29069 - acc: 0.7464 -- iter: 14/14\n",
      "--\n",
      "Training Step: 617  | total loss: \u001b[1m\u001b[32m1.19188\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 617 | loss: 1.19188 - acc: 0.7718 -- iter: 14/14\n",
      "--\n",
      "Training Step: 618  | total loss: \u001b[1m\u001b[32m1.45798\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 618 | loss: 1.45798 - acc: 0.7017 -- iter: 14/14\n",
      "--\n",
      "Training Step: 619  | total loss: \u001b[1m\u001b[32m1.34075\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 619 | loss: 1.34075 - acc: 0.7316 -- iter: 14/14\n",
      "--\n",
      "Training Step: 620  | total loss: \u001b[1m\u001b[32m1.59953\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 620 | loss: 1.59953 - acc: 0.6655 -- iter: 14/14\n",
      "--\n",
      "Training Step: 621  | total loss: \u001b[1m\u001b[32m1.46833\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 621 | loss: 1.46833 - acc: 0.6990 -- iter: 14/14\n",
      "--\n",
      "Training Step: 622  | total loss: \u001b[1m\u001b[32m1.71734\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 622 | loss: 1.71734 - acc: 0.6362 -- iter: 14/14\n",
      "--\n",
      "Training Step: 623  | total loss: \u001b[1m\u001b[32m1.57601\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 623 | loss: 1.57601 - acc: 0.6726 -- iter: 14/14\n",
      "--\n",
      "Training Step: 624  | total loss: \u001b[1m\u001b[32m1.76013\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 624 | loss: 1.76013 - acc: 0.6125 -- iter: 14/14\n",
      "--\n",
      "Training Step: 625  | total loss: \u001b[1m\u001b[32m1.61733\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 625 | loss: 1.61733 - acc: 0.6512 -- iter: 14/14\n",
      "--\n",
      "Training Step: 626  | total loss: \u001b[1m\u001b[32m1.49015\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 626 | loss: 1.49015 - acc: 0.6861 -- iter: 14/14\n",
      "--\n",
      "Training Step: 627  | total loss: \u001b[1m\u001b[32m1.37646\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 627 | loss: 1.37646 - acc: 0.7175 -- iter: 14/14\n",
      "--\n",
      "Training Step: 628  | total loss: \u001b[1m\u001b[32m1.65903\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 628 | loss: 1.65903 - acc: 0.6458 -- iter: 14/14\n",
      "--\n",
      "Training Step: 629  | total loss: \u001b[1m\u001b[32m1.53011\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 629 | loss: 1.53011 - acc: 0.6812 -- iter: 14/14\n",
      "--\n",
      "Training Step: 630  | total loss: \u001b[1m\u001b[32m1.76012\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 630 | loss: 1.76012 - acc: 0.6131 -- iter: 14/14\n",
      "--\n",
      "Training Step: 631  | total loss: \u001b[1m\u001b[32m1.62397\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 631 | loss: 1.62397 - acc: 0.6518 -- iter: 14/14\n",
      "--\n",
      "Training Step: 632  | total loss: \u001b[1m\u001b[32m1.83666\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 632 | loss: 1.83666 - acc: 0.5866 -- iter: 14/14\n",
      "--\n",
      "Training Step: 633  | total loss: \u001b[1m\u001b[32m1.69666\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 633 | loss: 1.69666 - acc: 0.6279 -- iter: 14/14\n",
      "--\n",
      "Training Step: 634  | total loss: \u001b[1m\u001b[32m1.90693\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 634 | loss: 1.90693 - acc: 0.5651 -- iter: 14/14\n",
      "--\n",
      "Training Step: 635  | total loss: \u001b[1m\u001b[32m1.76436\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 635 | loss: 1.76436 - acc: 0.6086 -- iter: 14/14\n",
      "--\n",
      "Training Step: 636  | total loss: \u001b[1m\u001b[32m1.94907\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 636 | loss: 1.94907 - acc: 0.5478 -- iter: 14/14\n",
      "--\n",
      "Training Step: 637  | total loss: \u001b[1m\u001b[32m1.80732\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 637 | loss: 1.80732 - acc: 0.5930 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 638  | total loss: \u001b[1m\u001b[32m1.99724\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 638 | loss: 1.99724 - acc: 0.5337 -- iter: 14/14\n",
      "--\n",
      "Training Step: 639  | total loss: \u001b[1m\u001b[32m1.85597\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 639 | loss: 1.85597 - acc: 0.5803 -- iter: 14/14\n",
      "--\n",
      "Training Step: 640  | total loss: \u001b[1m\u001b[32m2.00616\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 640 | loss: 2.00616 - acc: 0.5294 -- iter: 14/14\n",
      "--\n",
      "Training Step: 641  | total loss: \u001b[1m\u001b[32m1.86890\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 641 | loss: 1.86890 - acc: 0.5765 -- iter: 14/14\n",
      "--\n",
      "Training Step: 642  | total loss: \u001b[1m\u001b[32m2.01077\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 642 | loss: 2.01077 - acc: 0.5260 -- iter: 14/14\n",
      "--\n",
      "Training Step: 643  | total loss: \u001b[1m\u001b[32m1.87721\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 643 | loss: 1.87721 - acc: 0.5734 -- iter: 14/14\n",
      "--\n",
      "Training Step: 644  | total loss: \u001b[1m\u001b[32m1.96336\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 644 | loss: 1.96336 - acc: 0.5375 -- iter: 14/14\n",
      "--\n",
      "Training Step: 645  | total loss: \u001b[1m\u001b[32m1.83765\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 645 | loss: 1.83765 - acc: 0.5837 -- iter: 14/14\n",
      "--\n",
      "Training Step: 646  | total loss: \u001b[1m\u001b[32m1.99416\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 646 | loss: 1.99416 - acc: 0.5254 -- iter: 14/14\n",
      "--\n",
      "Training Step: 647  | total loss: \u001b[1m\u001b[32m1.86755\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 647 | loss: 1.86755 - acc: 0.5728 -- iter: 14/14\n",
      "--\n",
      "Training Step: 648  | total loss: \u001b[1m\u001b[32m2.00330\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 648 | loss: 2.00330 - acc: 0.5155 -- iter: 14/14\n",
      "--\n",
      "Training Step: 649  | total loss: \u001b[1m\u001b[32m1.87740\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 649 | loss: 1.87740 - acc: 0.5640 -- iter: 14/14\n",
      "--\n",
      "Training Step: 650  | total loss: \u001b[1m\u001b[32m1.99943\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 650 | loss: 1.99943 - acc: 0.5147 -- iter: 14/14\n",
      "--\n",
      "Training Step: 651  | total loss: \u001b[1m\u001b[32m1.87476\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 651 | loss: 1.87476 - acc: 0.5633 -- iter: 14/14\n",
      "--\n",
      "Training Step: 652  | total loss: \u001b[1m\u001b[32m1.99607\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 652 | loss: 1.99607 - acc: 0.5141 -- iter: 14/14\n",
      "--\n",
      "Training Step: 653  | total loss: \u001b[1m\u001b[32m1.87186\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 653 | loss: 1.87186 - acc: 0.5627 -- iter: 14/14\n",
      "--\n",
      "Training Step: 654  | total loss: \u001b[1m\u001b[32m1.97215\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 654 | loss: 1.97215 - acc: 0.5135 -- iter: 14/14\n",
      "--\n",
      "Training Step: 655  | total loss: \u001b[1m\u001b[32m1.85006\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 655 | loss: 1.85006 - acc: 0.5622 -- iter: 14/14\n",
      "--\n",
      "Training Step: 656  | total loss: \u001b[1m\u001b[32m1.96112\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 656 | loss: 1.96112 - acc: 0.5131 -- iter: 14/14\n",
      "--\n",
      "Training Step: 657  | total loss: \u001b[1m\u001b[32m1.83942\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 657 | loss: 1.83942 - acc: 0.5618 -- iter: 14/14\n",
      "--\n",
      "Training Step: 658  | total loss: \u001b[1m\u001b[32m1.96666\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 658 | loss: 1.96666 - acc: 0.5128 -- iter: 14/14\n",
      "--\n",
      "Training Step: 659  | total loss: \u001b[1m\u001b[32m1.84336\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 659 | loss: 1.84336 - acc: 0.5615 -- iter: 14/14\n",
      "--\n",
      "Training Step: 660  | total loss: \u001b[1m\u001b[32m2.02575\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 660 | loss: 2.02575 - acc: 0.5053 -- iter: 14/14\n",
      "--\n",
      "Training Step: 661  | total loss: \u001b[1m\u001b[32m1.89577\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 661 | loss: 1.89577 - acc: 0.5548 -- iter: 14/14\n",
      "--\n",
      "Training Step: 662  | total loss: \u001b[1m\u001b[32m2.03134\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 662 | loss: 2.03134 - acc: 0.5065 -- iter: 14/14\n",
      "--\n",
      "Training Step: 663  | total loss: \u001b[1m\u001b[32m1.90030\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 663 | loss: 1.90030 - acc: 0.5558 -- iter: 14/14\n",
      "--\n",
      "Training Step: 664  | total loss: \u001b[1m\u001b[32m2.05178\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 664 | loss: 2.05178 - acc: 0.5002 -- iter: 14/14\n",
      "--\n",
      "Training Step: 665  | total loss: \u001b[1m\u001b[32m1.91832\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 665 | loss: 1.91832 - acc: 0.5502 -- iter: 14/14\n",
      "--\n",
      "Training Step: 666  | total loss: \u001b[1m\u001b[32m1.79756\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 666 | loss: 1.79756 - acc: 0.5952 -- iter: 14/14\n",
      "--\n",
      "Training Step: 667  | total loss: \u001b[1m\u001b[32m1.68732\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 667 | loss: 1.68732 - acc: 0.6357 -- iter: 14/14\n",
      "--\n",
      "Training Step: 668  | total loss: \u001b[1m\u001b[32m1.58575\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 668 | loss: 1.58575 - acc: 0.6721 -- iter: 14/14\n",
      "--\n",
      "Training Step: 669  | total loss: \u001b[1m\u001b[32m1.49132\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 669 | loss: 1.49132 - acc: 0.7049 -- iter: 14/14\n",
      "--\n",
      "Training Step: 670  | total loss: \u001b[1m\u001b[32m1.66260\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 670 | loss: 1.66260 - acc: 0.6415 -- iter: 14/14\n",
      "--\n",
      "Training Step: 671  | total loss: \u001b[1m\u001b[32m1.55487\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 671 | loss: 1.55487 - acc: 0.6774 -- iter: 14/14\n",
      "--\n",
      "Training Step: 672  | total loss: \u001b[1m\u001b[32m1.76118\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 672 | loss: 1.76118 - acc: 0.6097 -- iter: 14/14\n",
      "--\n",
      "Training Step: 673  | total loss: \u001b[1m\u001b[32m1.63976\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 673 | loss: 1.63976 - acc: 0.6487 -- iter: 14/14\n",
      "--\n",
      "Training Step: 674  | total loss: \u001b[1m\u001b[32m1.83207\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 674 | loss: 1.83207 - acc: 0.5838 -- iter: 14/14\n",
      "--\n",
      "Training Step: 675  | total loss: \u001b[1m\u001b[32m1.70131\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 675 | loss: 1.70131 - acc: 0.6254 -- iter: 14/14\n",
      "--\n",
      "Training Step: 676  | total loss: \u001b[1m\u001b[32m1.90042\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 676 | loss: 1.90042 - acc: 0.5629 -- iter: 14/14\n",
      "--\n",
      "Training Step: 677  | total loss: \u001b[1m\u001b[32m1.76193\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 677 | loss: 1.76193 - acc: 0.6066 -- iter: 14/14\n",
      "--\n",
      "Training Step: 678  | total loss: \u001b[1m\u001b[32m1.92156\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 678 | loss: 1.92156 - acc: 0.5531 -- iter: 14/14\n",
      "--\n",
      "Training Step: 679  | total loss: \u001b[1m\u001b[32m1.78109\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 679 | loss: 1.78109 - acc: 0.5978 -- iter: 14/14\n",
      "--\n",
      "Training Step: 680  | total loss: \u001b[1m\u001b[32m1.65454\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 680 | loss: 1.65454 - acc: 0.6380 -- iter: 14/14\n",
      "--\n",
      "Training Step: 681  | total loss: \u001b[1m\u001b[32m1.53983\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 681 | loss: 1.53983 - acc: 0.6742 -- iter: 14/14\n",
      "--\n",
      "Training Step: 682  | total loss: \u001b[1m\u001b[32m1.77303\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 682 | loss: 1.77303 - acc: 0.6068 -- iter: 14/14\n",
      "--\n",
      "Training Step: 683  | total loss: \u001b[1m\u001b[32m1.64517\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 683 | loss: 1.64517 - acc: 0.6461 -- iter: 14/14\n",
      "--\n",
      "Training Step: 684  | total loss: \u001b[1m\u001b[32m1.52953\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 684 | loss: 1.52953 - acc: 0.6815 -- iter: 14/14\n",
      "--\n",
      "Training Step: 685  | total loss: \u001b[1m\u001b[32m1.42427\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 685 | loss: 1.42427 - acc: 0.7133 -- iter: 14/14\n",
      "--\n",
      "Training Step: 686  | total loss: \u001b[1m\u001b[32m1.32782\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 686 | loss: 1.32782 - acc: 0.7420 -- iter: 14/14\n",
      "--\n",
      "Training Step: 687  | total loss: \u001b[1m\u001b[32m1.23887\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 687 | loss: 1.23887 - acc: 0.7678 -- iter: 14/14\n",
      "--\n",
      "Training Step: 688  | total loss: \u001b[1m\u001b[32m1.49524\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 688 | loss: 1.49524 - acc: 0.6910 -- iter: 14/14\n",
      "--\n",
      "Training Step: 689  | total loss: \u001b[1m\u001b[32m1.38615\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 689 | loss: 1.38615 - acc: 0.7219 -- iter: 14/14\n",
      "--\n",
      "Training Step: 690  | total loss: \u001b[1m\u001b[32m1.65259\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 690 | loss: 1.65259 - acc: 0.6497 -- iter: 14/14\n",
      "--\n",
      "Training Step: 691  | total loss: \u001b[1m\u001b[32m1.52649\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 691 | loss: 1.52649 - acc: 0.6848 -- iter: 14/14\n",
      "--\n",
      "Training Step: 692  | total loss: \u001b[1m\u001b[32m1.74093\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 692 | loss: 1.74093 - acc: 0.6234 -- iter: 14/14\n",
      "--\n",
      "Training Step: 693  | total loss: \u001b[1m\u001b[32m1.60630\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 693 | loss: 1.60630 - acc: 0.6611 -- iter: 14/14\n",
      "--\n",
      "Training Step: 694  | total loss: \u001b[1m\u001b[32m1.80600\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 694 | loss: 1.80600 - acc: 0.5950 -- iter: 14/14\n",
      "--\n",
      "Training Step: 695  | total loss: \u001b[1m\u001b[32m1.66627\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 695 | loss: 1.66627 - acc: 0.6355 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 696  | total loss: \u001b[1m\u001b[32m1.54113\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 696 | loss: 1.54113 - acc: 0.6719 -- iter: 14/14\n",
      "--\n",
      "Training Step: 697  | total loss: \u001b[1m\u001b[32m1.42849\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 697 | loss: 1.42849 - acc: 0.7047 -- iter: 14/14\n",
      "--\n",
      "Training Step: 698  | total loss: \u001b[1m\u001b[32m1.32655\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 698 | loss: 1.32655 - acc: 0.7343 -- iter: 14/14\n",
      "--\n",
      "Training Step: 699  | total loss: \u001b[1m\u001b[32m1.23371\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 699 | loss: 1.23371 - acc: 0.7608 -- iter: 14/14\n",
      "--\n",
      "Training Step: 700  | total loss: \u001b[1m\u001b[32m1.48750\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 700 | loss: 1.48750 - acc: 0.6848 -- iter: 14/14\n",
      "--\n",
      "Training Step: 701  | total loss: \u001b[1m\u001b[32m1.37692\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 701 | loss: 1.37692 - acc: 0.7163 -- iter: 14/14\n",
      "--\n",
      "Training Step: 702  | total loss: \u001b[1m\u001b[32m1.63173\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 702 | loss: 1.63173 - acc: 0.6446 -- iter: 14/14\n",
      "--\n",
      "Training Step: 703  | total loss: \u001b[1m\u001b[32m1.50669\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 703 | loss: 1.50669 - acc: 0.6802 -- iter: 14/14\n",
      "--\n",
      "Training Step: 704  | total loss: \u001b[1m\u001b[32m1.39418\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 704 | loss: 1.39418 - acc: 0.7122 -- iter: 14/14\n",
      "--\n",
      "Training Step: 705  | total loss: \u001b[1m\u001b[32m1.29244\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 705 | loss: 1.29244 - acc: 0.7409 -- iter: 14/14\n",
      "--\n",
      "Training Step: 706  | total loss: \u001b[1m\u001b[32m1.52981\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 706 | loss: 1.52981 - acc: 0.6740 -- iter: 14/14\n",
      "--\n",
      "Training Step: 707  | total loss: \u001b[1m\u001b[32m1.41381\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 707 | loss: 1.41381 - acc: 0.7066 -- iter: 14/14\n",
      "--\n",
      "Training Step: 708  | total loss: \u001b[1m\u001b[32m1.30912\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 708 | loss: 1.30912 - acc: 0.7359 -- iter: 14/14\n",
      "--\n",
      "Training Step: 709  | total loss: \u001b[1m\u001b[32m1.21415\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 709 | loss: 1.21415 - acc: 0.7623 -- iter: 14/14\n",
      "--\n",
      "Training Step: 710  | total loss: \u001b[1m\u001b[32m1.12754\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 710 | loss: 1.12754 - acc: 0.7861 -- iter: 14/14\n",
      "--\n",
      "Training Step: 711  | total loss: \u001b[1m\u001b[32m1.04814\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 711 | loss: 1.04814 - acc: 0.8075 -- iter: 14/14\n",
      "--\n",
      "Training Step: 712  | total loss: \u001b[1m\u001b[32m0.97497\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 712 | loss: 0.97497 - acc: 0.8267 -- iter: 14/14\n",
      "--\n",
      "Training Step: 713  | total loss: \u001b[1m\u001b[32m0.90723\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 713 | loss: 0.90723 - acc: 0.8441 -- iter: 14/14\n",
      "--\n",
      "Training Step: 714  | total loss: \u001b[1m\u001b[32m0.84428\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 714 | loss: 0.84428 - acc: 0.8597 -- iter: 14/14\n",
      "--\n",
      "Training Step: 715  | total loss: \u001b[1m\u001b[32m0.78558\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 715 | loss: 0.78558 - acc: 0.8737 -- iter: 14/14\n",
      "--\n",
      "Training Step: 716  | total loss: \u001b[1m\u001b[32m1.12886\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 716 | loss: 1.12886 - acc: 0.7863 -- iter: 14/14\n",
      "--\n",
      "Training Step: 717  | total loss: \u001b[1m\u001b[32m1.03902\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 717 | loss: 1.03902 - acc: 0.8077 -- iter: 14/14\n",
      "--\n",
      "Training Step: 718  | total loss: \u001b[1m\u001b[32m1.33852\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 718 | loss: 1.33852 - acc: 0.7341 -- iter: 14/14\n",
      "--\n",
      "Training Step: 719  | total loss: \u001b[1m\u001b[32m1.22711\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 719 | loss: 1.22711 - acc: 0.7607 -- iter: 14/14\n",
      "--\n",
      "Training Step: 720  | total loss: \u001b[1m\u001b[32m1.53162\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 720 | loss: 1.53162 - acc: 0.6846 -- iter: 14/14\n",
      "--\n",
      "Training Step: 721  | total loss: \u001b[1m\u001b[32m1.40192\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 721 | loss: 1.40192 - acc: 0.7161 -- iter: 14/14\n",
      "--\n",
      "Training Step: 722  | total loss: \u001b[1m\u001b[32m1.69293\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 722 | loss: 1.69293 - acc: 0.6445 -- iter: 14/14\n",
      "--\n",
      "Training Step: 723  | total loss: \u001b[1m\u001b[32m1.54962\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 723 | loss: 1.54962 - acc: 0.6801 -- iter: 14/14\n",
      "--\n",
      "Training Step: 724  | total loss: \u001b[1m\u001b[32m1.82932\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 724 | loss: 1.82932 - acc: 0.6121 -- iter: 14/14\n",
      "--\n",
      "Training Step: 725  | total loss: \u001b[1m\u001b[32m1.67630\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 725 | loss: 1.67630 - acc: 0.6509 -- iter: 14/14\n",
      "--\n",
      "Training Step: 726  | total loss: \u001b[1m\u001b[32m1.54060\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 726 | loss: 1.54060 - acc: 0.6858 -- iter: 14/14\n",
      "--\n",
      "Training Step: 727  | total loss: \u001b[1m\u001b[32m1.41996\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 727 | loss: 1.41996 - acc: 0.7172 -- iter: 14/14\n",
      "--\n",
      "Training Step: 728  | total loss: \u001b[1m\u001b[32m1.62676\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 728 | loss: 1.62676 - acc: 0.6598 -- iter: 14/14\n",
      "--\n",
      "Training Step: 729  | total loss: \u001b[1m\u001b[32m1.50026\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 729 | loss: 1.50026 - acc: 0.6938 -- iter: 14/14\n",
      "--\n",
      "Training Step: 730  | total loss: \u001b[1m\u001b[32m1.38763\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 730 | loss: 1.38763 - acc: 0.7244 -- iter: 14/14\n",
      "--\n",
      "Training Step: 731  | total loss: \u001b[1m\u001b[32m1.28686\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 731 | loss: 1.28686 - acc: 0.7520 -- iter: 14/14\n",
      "--\n",
      "Training Step: 732  | total loss: \u001b[1m\u001b[32m1.54252\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 732 | loss: 1.54252 - acc: 0.6839 -- iter: 14/14\n",
      "--\n",
      "Training Step: 733  | total loss: \u001b[1m\u001b[32m1.42752\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 733 | loss: 1.42752 - acc: 0.7155 -- iter: 14/14\n",
      "--\n",
      "Training Step: 734  | total loss: \u001b[1m\u001b[32m1.65848\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 734 | loss: 1.65848 - acc: 0.6511 -- iter: 14/14\n",
      "--\n",
      "Training Step: 735  | total loss: \u001b[1m\u001b[32m1.53412\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 735 | loss: 1.53412 - acc: 0.6860 -- iter: 14/14\n",
      "--\n",
      "Training Step: 736  | total loss: \u001b[1m\u001b[32m1.42314\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 736 | loss: 1.42314 - acc: 0.7174 -- iter: 14/14\n",
      "--\n",
      "Training Step: 737  | total loss: \u001b[1m\u001b[32m1.32350\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 737 | loss: 1.32350 - acc: 0.7457 -- iter: 14/14\n",
      "--\n",
      "Training Step: 738  | total loss: \u001b[1m\u001b[32m1.53802\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 738 | loss: 1.53802 - acc: 0.6782 -- iter: 14/14\n",
      "--\n",
      "Training Step: 739  | total loss: \u001b[1m\u001b[32m1.42723\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 739 | loss: 1.42723 - acc: 0.7104 -- iter: 14/14\n",
      "--\n",
      "Training Step: 740  | total loss: \u001b[1m\u001b[32m1.63326\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 740 | loss: 1.63326 - acc: 0.6465 -- iter: 14/14\n",
      "--\n",
      "Training Step: 741  | total loss: \u001b[1m\u001b[32m1.51422\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 741 | loss: 1.51422 - acc: 0.6819 -- iter: 14/14\n",
      "--\n",
      "Training Step: 742  | total loss: \u001b[1m\u001b[32m1.65857\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 742 | loss: 1.65857 - acc: 0.6351 -- iter: 14/14\n",
      "--\n",
      "Training Step: 743  | total loss: \u001b[1m\u001b[32m1.53877\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 743 | loss: 1.53877 - acc: 0.6716 -- iter: 14/14\n",
      "--\n",
      "Training Step: 744  | total loss: \u001b[1m\u001b[32m1.72742\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 744 | loss: 1.72742 - acc: 0.6116 -- iter: 14/14\n",
      "--\n",
      "Training Step: 745  | total loss: \u001b[1m\u001b[32m1.60282\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 745 | loss: 1.60282 - acc: 0.6504 -- iter: 14/14\n",
      "--\n",
      "Training Step: 746  | total loss: \u001b[1m\u001b[32m1.81855\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 746 | loss: 1.81855 - acc: 0.5854 -- iter: 14/14\n",
      "--\n",
      "Training Step: 747  | total loss: \u001b[1m\u001b[32m1.68755\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 747 | loss: 1.68755 - acc: 0.6268 -- iter: 14/14\n",
      "--\n",
      "Training Step: 748  | total loss: \u001b[1m\u001b[32m1.87350\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 748 | loss: 1.87350 - acc: 0.5642 -- iter: 14/14\n",
      "--\n",
      "Training Step: 749  | total loss: \u001b[1m\u001b[32m1.74032\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 749 | loss: 1.74032 - acc: 0.6077 -- iter: 14/14\n",
      "--\n",
      "Training Step: 750  | total loss: \u001b[1m\u001b[32m1.87723\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 750 | loss: 1.87723 - acc: 0.5613 -- iter: 14/14\n",
      "--\n",
      "Training Step: 751  | total loss: \u001b[1m\u001b[32m1.74702\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 751 | loss: 1.74702 - acc: 0.6051 -- iter: 14/14\n",
      "--\n",
      "Training Step: 752  | total loss: \u001b[1m\u001b[32m1.63087\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 752 | loss: 1.63087 - acc: 0.6446 -- iter: 14/14\n",
      "--\n",
      "Training Step: 753  | total loss: \u001b[1m\u001b[32m1.52635\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 753 | loss: 1.52635 - acc: 0.6802 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 754  | total loss: \u001b[1m\u001b[32m1.72444\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 754 | loss: 1.72444 - acc: 0.6193 -- iter: 14/14\n",
      "--\n",
      "Training Step: 755  | total loss: \u001b[1m\u001b[32m1.60991\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 755 | loss: 1.60991 - acc: 0.6574 -- iter: 14/14\n",
      "--\n",
      "Training Step: 756  | total loss: \u001b[1m\u001b[32m1.76783\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 756 | loss: 1.76783 - acc: 0.5988 -- iter: 14/14\n",
      "--\n",
      "Training Step: 757  | total loss: \u001b[1m\u001b[32m1.64876\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 757 | loss: 1.64876 - acc: 0.6389 -- iter: 14/14\n",
      "--\n",
      "Training Step: 758  | total loss: \u001b[1m\u001b[32m1.54116\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 758 | loss: 1.54116 - acc: 0.6750 -- iter: 14/14\n",
      "--\n",
      "Training Step: 759  | total loss: \u001b[1m\u001b[32m1.44305\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 759 | loss: 1.44305 - acc: 0.7075 -- iter: 14/14\n",
      "--\n",
      "Training Step: 760  | total loss: \u001b[1m\u001b[32m1.35279\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 760 | loss: 1.35279 - acc: 0.7367 -- iter: 14/14\n",
      "--\n",
      "Training Step: 761  | total loss: \u001b[1m\u001b[32m1.26902\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 761 | loss: 1.26902 - acc: 0.7631 -- iter: 14/14\n",
      "--\n",
      "Training Step: 762  | total loss: \u001b[1m\u001b[32m1.45314\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 762 | loss: 1.45314 - acc: 0.7011 -- iter: 14/14\n",
      "--\n",
      "Training Step: 763  | total loss: \u001b[1m\u001b[32m1.35481\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 763 | loss: 1.35481 - acc: 0.7309 -- iter: 14/14\n",
      "--\n",
      "Training Step: 764  | total loss: \u001b[1m\u001b[32m1.50684\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 764 | loss: 1.50684 - acc: 0.6793 -- iter: 14/14\n",
      "--\n",
      "Training Step: 765  | total loss: \u001b[1m\u001b[32m1.40007\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 765 | loss: 1.40007 - acc: 0.7114 -- iter: 14/14\n",
      "--\n",
      "Training Step: 766  | total loss: \u001b[1m\u001b[32m1.30244\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 766 | loss: 1.30244 - acc: 0.7402 -- iter: 14/14\n",
      "--\n",
      "Training Step: 767  | total loss: \u001b[1m\u001b[32m1.21263\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 767 | loss: 1.21263 - acc: 0.7662 -- iter: 14/14\n",
      "--\n",
      "Training Step: 768  | total loss: \u001b[1m\u001b[32m1.12954\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 768 | loss: 1.12954 - acc: 0.7896 -- iter: 14/14\n",
      "--\n",
      "Training Step: 769  | total loss: \u001b[1m\u001b[32m1.05227\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 769 | loss: 1.05227 - acc: 0.8106 -- iter: 14/14\n",
      "--\n",
      "Training Step: 770  | total loss: \u001b[1m\u001b[32m1.32051\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 770 | loss: 1.32051 - acc: 0.7296 -- iter: 14/14\n",
      "--\n",
      "Training Step: 771  | total loss: \u001b[1m\u001b[32m1.22044\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 771 | loss: 1.22044 - acc: 0.7566 -- iter: 14/14\n",
      "--\n",
      "Training Step: 772  | total loss: \u001b[1m\u001b[32m1.48271\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 772 | loss: 1.48271 - acc: 0.6881 -- iter: 14/14\n",
      "--\n",
      "Training Step: 773  | total loss: \u001b[1m\u001b[32m1.36494\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 773 | loss: 1.36494 - acc: 0.7193 -- iter: 14/14\n",
      "--\n",
      "Training Step: 774  | total loss: \u001b[1m\u001b[32m1.57000\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 774 | loss: 1.57000 - acc: 0.6616 -- iter: 14/14\n",
      "--\n",
      "Training Step: 775  | total loss: \u001b[1m\u001b[32m1.44362\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 775 | loss: 1.44362 - acc: 0.6955 -- iter: 14/14\n",
      "--\n",
      "Training Step: 776  | total loss: \u001b[1m\u001b[32m1.64224\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 776 | loss: 1.64224 - acc: 0.6402 -- iter: 14/14\n",
      "--\n",
      "Training Step: 777  | total loss: \u001b[1m\u001b[32m1.50995\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 777 | loss: 1.50995 - acc: 0.6762 -- iter: 14/14\n",
      "--\n",
      "Training Step: 778  | total loss: \u001b[1m\u001b[32m1.69572\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 778 | loss: 1.69572 - acc: 0.6300 -- iter: 14/14\n",
      "--\n",
      "Training Step: 779  | total loss: \u001b[1m\u001b[32m1.56022\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 779 | loss: 1.56022 - acc: 0.6670 -- iter: 14/14\n",
      "--\n",
      "Training Step: 780  | total loss: \u001b[1m\u001b[32m1.80832\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 780 | loss: 1.80832 - acc: 0.6003 -- iter: 14/14\n",
      "--\n",
      "Training Step: 781  | total loss: \u001b[1m\u001b[32m1.66461\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 781 | loss: 1.66461 - acc: 0.6403 -- iter: 14/14\n",
      "--\n",
      "Training Step: 782  | total loss: \u001b[1m\u001b[32m1.53671\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 782 | loss: 1.53671 - acc: 0.6762 -- iter: 14/14\n",
      "--\n",
      "Training Step: 783  | total loss: \u001b[1m\u001b[32m1.42235\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 783 | loss: 1.42235 - acc: 0.7086 -- iter: 14/14\n",
      "--\n",
      "Training Step: 784  | total loss: \u001b[1m\u001b[32m1.31952\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 784 | loss: 1.31952 - acc: 0.7378 -- iter: 14/14\n",
      "--\n",
      "Training Step: 785  | total loss: \u001b[1m\u001b[32m1.22649\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 785 | loss: 1.22649 - acc: 0.7640 -- iter: 14/14\n",
      "--\n",
      "Training Step: 786  | total loss: \u001b[1m\u001b[32m1.48393\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 786 | loss: 1.48393 - acc: 0.6876 -- iter: 14/14\n",
      "--\n",
      "Training Step: 787  | total loss: \u001b[1m\u001b[32m1.37382\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 787 | loss: 1.37382 - acc: 0.7188 -- iter: 14/14\n",
      "--\n",
      "Training Step: 788  | total loss: \u001b[1m\u001b[32m1.60831\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 788 | loss: 1.60831 - acc: 0.6541 -- iter: 14/14\n",
      "--\n",
      "Training Step: 789  | total loss: \u001b[1m\u001b[32m1.48643\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 789 | loss: 1.48643 - acc: 0.6887 -- iter: 14/14\n",
      "--\n",
      "Training Step: 790  | total loss: \u001b[1m\u001b[32m1.37700\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 790 | loss: 1.37700 - acc: 0.7198 -- iter: 14/14\n",
      "--\n",
      "Training Step: 791  | total loss: \u001b[1m\u001b[32m1.27820\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 791 | loss: 1.27820 - acc: 0.7478 -- iter: 14/14\n",
      "--\n",
      "Training Step: 792  | total loss: \u001b[1m\u001b[32m1.50235\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 792 | loss: 1.50235 - acc: 0.6802 -- iter: 14/14\n",
      "--\n",
      "Training Step: 793  | total loss: \u001b[1m\u001b[32m1.39053\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 793 | loss: 1.39053 - acc: 0.7122 -- iter: 14/14\n",
      "--\n",
      "Training Step: 794  | total loss: \u001b[1m\u001b[32m1.28969\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 794 | loss: 1.28969 - acc: 0.7410 -- iter: 14/14\n",
      "--\n",
      "Training Step: 795  | total loss: \u001b[1m\u001b[32m1.19821\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 795 | loss: 1.19821 - acc: 0.7669 -- iter: 14/14\n",
      "--\n",
      "Training Step: 796  | total loss: \u001b[1m\u001b[32m1.42056\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 796 | loss: 1.42056 - acc: 0.7045 -- iter: 14/14\n",
      "--\n",
      "Training Step: 797  | total loss: \u001b[1m\u001b[32m1.31483\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 797 | loss: 1.31483 - acc: 0.7340 -- iter: 14/14\n",
      "--\n",
      "Training Step: 798  | total loss: \u001b[1m\u001b[32m1.54961\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 798 | loss: 1.54961 - acc: 0.6606 -- iter: 14/14\n",
      "--\n",
      "Training Step: 799  | total loss: \u001b[1m\u001b[32m1.43118\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 799 | loss: 1.43118 - acc: 0.6945 -- iter: 14/14\n",
      "--\n",
      "Training Step: 800  | total loss: \u001b[1m\u001b[32m1.63258\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 800 | loss: 1.63258 - acc: 0.6322 -- iter: 14/14\n",
      "--\n",
      "Training Step: 801  | total loss: \u001b[1m\u001b[32m1.50713\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 801 | loss: 1.50713 - acc: 0.6690 -- iter: 14/14\n",
      "--\n",
      "Training Step: 802  | total loss: \u001b[1m\u001b[32m1.75083\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 802 | loss: 1.75083 - acc: 0.6021 -- iter: 14/14\n",
      "--\n",
      "Training Step: 803  | total loss: \u001b[1m\u001b[32m1.61581\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 803 | loss: 1.61581 - acc: 0.6419 -- iter: 14/14\n",
      "--\n",
      "Training Step: 804  | total loss: \u001b[1m\u001b[32m1.49532\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 804 | loss: 1.49532 - acc: 0.6777 -- iter: 14/14\n",
      "--\n",
      "Training Step: 805  | total loss: \u001b[1m\u001b[32m1.38721\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 805 | loss: 1.38721 - acc: 0.7099 -- iter: 14/14\n",
      "--\n",
      "Training Step: 806  | total loss: \u001b[1m\u001b[32m1.59179\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 806 | loss: 1.59179 - acc: 0.6461 -- iter: 14/14\n",
      "--\n",
      "Training Step: 807  | total loss: \u001b[1m\u001b[32m1.47470\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 807 | loss: 1.47470 - acc: 0.6815 -- iter: 14/14\n",
      "--\n",
      "Training Step: 808  | total loss: \u001b[1m\u001b[32m1.36960\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 808 | loss: 1.36960 - acc: 0.7133 -- iter: 14/14\n",
      "--\n",
      "Training Step: 809  | total loss: \u001b[1m\u001b[32m1.27463\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 809 | loss: 1.27463 - acc: 0.7420 -- iter: 14/14\n",
      "--\n",
      "Training Step: 810  | total loss: \u001b[1m\u001b[32m1.18824\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 810 | loss: 1.18824 - acc: 0.7678 -- iter: 14/14\n",
      "--\n",
      "Training Step: 811  | total loss: \u001b[1m\u001b[32m1.10908\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 811 | loss: 1.10908 - acc: 0.7910 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 812  | total loss: \u001b[1m\u001b[32m1.03603\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 812 | loss: 1.03603 - acc: 0.8119 -- iter: 14/14\n",
      "--\n",
      "Training Step: 813  | total loss: \u001b[1m\u001b[32m0.96819\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 813 | loss: 0.96819 - acc: 0.8307 -- iter: 14/14\n",
      "--\n",
      "Training Step: 814  | total loss: \u001b[1m\u001b[32m1.24477\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 814 | loss: 1.24477 - acc: 0.7548 -- iter: 14/14\n",
      "--\n",
      "Training Step: 815  | total loss: \u001b[1m\u001b[32m1.15293\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 815 | loss: 1.15293 - acc: 0.7793 -- iter: 14/14\n",
      "--\n",
      "Training Step: 816  | total loss: \u001b[1m\u001b[32m1.38062\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 816 | loss: 1.38062 - acc: 0.7157 -- iter: 14/14\n",
      "--\n",
      "Training Step: 817  | total loss: \u001b[1m\u001b[32m1.27399\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 817 | loss: 1.27399 - acc: 0.7441 -- iter: 14/14\n",
      "--\n",
      "Training Step: 818  | total loss: \u001b[1m\u001b[32m1.17759\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 818 | loss: 1.17759 - acc: 0.7697 -- iter: 14/14\n",
      "--\n",
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m1.09004\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 819 | loss: 1.09004 - acc: 0.7927 -- iter: 14/14\n",
      "--\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m1.01018\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 820 | loss: 1.01018 - acc: 0.8135 -- iter: 14/14\n",
      "--\n",
      "Training Step: 821  | total loss: \u001b[1m\u001b[32m0.93699\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 821 | loss: 0.93699 - acc: 0.8321 -- iter: 14/14\n",
      "--\n",
      "Training Step: 822  | total loss: \u001b[1m\u001b[32m0.86964\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 822 | loss: 0.86964 - acc: 0.8489 -- iter: 14/14\n",
      "--\n",
      "Training Step: 823  | total loss: \u001b[1m\u001b[32m0.80742\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 823 | loss: 0.80742 - acc: 0.8640 -- iter: 14/14\n",
      "--\n",
      "Training Step: 824  | total loss: \u001b[1m\u001b[32m0.74976\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 824 | loss: 0.74976 - acc: 0.8776 -- iter: 14/14\n",
      "--\n",
      "Training Step: 825  | total loss: \u001b[1m\u001b[32m0.69617\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 825 | loss: 0.69617 - acc: 0.8898 -- iter: 14/14\n",
      "--\n",
      "Training Step: 826  | total loss: \u001b[1m\u001b[32m1.09667\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 826 | loss: 1.09667 - acc: 0.8009 -- iter: 14/14\n",
      "--\n",
      "Training Step: 827  | total loss: \u001b[1m\u001b[32m1.00625\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 827 | loss: 1.00625 - acc: 0.8208 -- iter: 14/14\n",
      "--\n",
      "Training Step: 828  | total loss: \u001b[1m\u001b[32m0.92427\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 828 | loss: 0.92427 - acc: 0.8387 -- iter: 14/14\n",
      "--\n",
      "Training Step: 829  | total loss: \u001b[1m\u001b[32m0.84975\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 829 | loss: 0.84975 - acc: 0.8548 -- iter: 14/14\n",
      "--\n",
      "Training Step: 830  | total loss: \u001b[1m\u001b[32m1.17838\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 830 | loss: 1.17838 - acc: 0.7765 -- iter: 14/14\n",
      "--\n",
      "Training Step: 831  | total loss: \u001b[1m\u001b[32m1.07786\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 831 | loss: 1.07786 - acc: 0.7988 -- iter: 14/14\n",
      "--\n",
      "Training Step: 832  | total loss: \u001b[1m\u001b[32m0.98742\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 832 | loss: 0.98742 - acc: 0.8190 -- iter: 14/14\n",
      "--\n",
      "Training Step: 833  | total loss: \u001b[1m\u001b[32m0.90589\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 833 | loss: 0.90589 - acc: 0.8371 -- iter: 14/14\n",
      "--\n",
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m1.26439\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 834 | loss: 1.26439 - acc: 0.7534 -- iter: 14/14\n",
      "--\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m1.15550\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 835 | loss: 1.15550 - acc: 0.7780 -- iter: 14/14\n",
      "--\n",
      "Training Step: 836  | total loss: \u001b[1m\u001b[32m1.05791\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 836 | loss: 1.05791 - acc: 0.8002 -- iter: 14/14\n",
      "--\n",
      "Training Step: 837  | total loss: \u001b[1m\u001b[32m0.97026\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 837 | loss: 0.97026 - acc: 0.8202 -- iter: 14/14\n",
      "--\n",
      "Training Step: 838  | total loss: \u001b[1m\u001b[32m1.27743\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 838 | loss: 1.27743 - acc: 0.7525 -- iter: 14/14\n",
      "--\n",
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m1.16867\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 839 | loss: 1.16867 - acc: 0.7772 -- iter: 14/14\n",
      "--\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m1.50638\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 840 | loss: 1.50638 - acc: 0.6995 -- iter: 14/14\n",
      "--\n",
      "Training Step: 841  | total loss: \u001b[1m\u001b[32m1.37692\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 841 | loss: 1.37692 - acc: 0.7295 -- iter: 14/14\n",
      "--\n",
      "Training Step: 842  | total loss: \u001b[1m\u001b[32m1.61408\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 842 | loss: 1.61408 - acc: 0.6709 -- iter: 14/14\n",
      "--\n",
      "Training Step: 843  | total loss: \u001b[1m\u001b[32m1.47735\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 843 | loss: 1.47735 - acc: 0.7038 -- iter: 14/14\n",
      "--\n",
      "Training Step: 844  | total loss: \u001b[1m\u001b[32m1.78070\u001b[0m\u001b[0m | time: 0.034s\n",
      "| Adam | epoch: 844 | loss: 1.78070 - acc: 0.6334 -- iter: 14/14\n",
      "--\n",
      "Training Step: 845  | total loss: \u001b[1m\u001b[32m1.63215\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 845 | loss: 1.63215 - acc: 0.6701 -- iter: 14/14\n",
      "--\n",
      "Training Step: 846  | total loss: \u001b[1m\u001b[32m1.86603\u001b[0m\u001b[0m | time: 0.027s\n",
      "| Adam | epoch: 846 | loss: 1.86603 - acc: 0.6031 -- iter: 14/14\n",
      "--\n",
      "Training Step: 847  | total loss: \u001b[1m\u001b[32m1.71511\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 847 | loss: 1.71511 - acc: 0.6428 -- iter: 14/14\n",
      "--\n",
      "Training Step: 848  | total loss: \u001b[1m\u001b[32m1.91981\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 848 | loss: 1.91981 - acc: 0.5856 -- iter: 14/14\n",
      "--\n",
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m1.77058\u001b[0m\u001b[0m | time: 0.020s\n",
      "| Adam | epoch: 849 | loss: 1.77058 - acc: 0.6271 -- iter: 14/14\n",
      "--\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m1.93413\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 850 | loss: 1.93413 - acc: 0.5715 -- iter: 14/14\n",
      "--\n",
      "Training Step: 851  | total loss: \u001b[1m\u001b[32m1.79096\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 851 | loss: 1.79096 - acc: 0.6143 -- iter: 14/14\n",
      "--\n",
      "Training Step: 852  | total loss: \u001b[1m\u001b[32m1.98774\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 852 | loss: 1.98774 - acc: 0.5529 -- iter: 14/14\n",
      "--\n",
      "Training Step: 853  | total loss: \u001b[1m\u001b[32m1.84667\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 853 | loss: 1.84667 - acc: 0.5976 -- iter: 14/14\n",
      "--\n",
      "Training Step: 854  | total loss: \u001b[1m\u001b[32m1.97181\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 854 | loss: 1.97181 - acc: 0.5521 -- iter: 14/14\n",
      "--\n",
      "Training Step: 855  | total loss: \u001b[1m\u001b[32m1.83899\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 855 | loss: 1.83899 - acc: 0.5969 -- iter: 14/14\n",
      "--\n",
      "Training Step: 856  | total loss: \u001b[1m\u001b[32m1.87271\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 856 | loss: 1.87271 - acc: 0.5801 -- iter: 14/14\n",
      "--\n",
      "Training Step: 857  | total loss: \u001b[1m\u001b[32m1.75432\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 857 | loss: 1.75432 - acc: 0.6221 -- iter: 14/14\n",
      "--\n",
      "Training Step: 858  | total loss: \u001b[1m\u001b[32m1.64865\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 858 | loss: 1.64865 - acc: 0.6599 -- iter: 14/14\n",
      "--\n",
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m1.55326\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 859 | loss: 1.55326 - acc: 0.6939 -- iter: 14/14\n",
      "--\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m1.67647\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 860 | loss: 1.67647 - acc: 0.6388 -- iter: 14/14\n",
      "--\n",
      "Training Step: 861  | total loss: \u001b[1m\u001b[32m1.57661\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 861 | loss: 1.57661 - acc: 0.6749 -- iter: 14/14\n",
      "--\n",
      "Training Step: 862  | total loss: \u001b[1m\u001b[32m1.74720\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 862 | loss: 1.74720 - acc: 0.6146 -- iter: 14/14\n",
      "--\n",
      "Training Step: 863  | total loss: \u001b[1m\u001b[32m1.63876\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 863 | loss: 1.63876 - acc: 0.6531 -- iter: 14/14\n",
      "--\n",
      "Training Step: 864  | total loss: \u001b[1m\u001b[32m1.79930\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 864 | loss: 1.79930 - acc: 0.5949 -- iter: 14/14\n",
      "--\n",
      "Training Step: 865  | total loss: \u001b[1m\u001b[32m1.68435\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 865 | loss: 1.68435 - acc: 0.6354 -- iter: 14/14\n",
      "--\n",
      "Training Step: 866  | total loss: \u001b[1m\u001b[32m1.82557\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 866 | loss: 1.82557 - acc: 0.5862 -- iter: 14/14\n",
      "--\n",
      "Training Step: 867  | total loss: \u001b[1m\u001b[32m1.70676\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 867 | loss: 1.70676 - acc: 0.6276 -- iter: 14/14\n",
      "--\n",
      "Training Step: 868  | total loss: \u001b[1m\u001b[32m1.86812\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 868 | loss: 1.86812 - acc: 0.5720 -- iter: 14/14\n",
      "--\n",
      "Training Step: 869  | total loss: \u001b[1m\u001b[32m1.74420\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 869 | loss: 1.74420 - acc: 0.6148 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 870  | total loss: \u001b[1m\u001b[32m1.91576\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 870 | loss: 1.91576 - acc: 0.5533 -- iter: 14/14\n",
      "--\n",
      "Training Step: 871  | total loss: \u001b[1m\u001b[32m1.78691\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 871 | loss: 1.78691 - acc: 0.5980 -- iter: 14/14\n",
      "--\n",
      "Training Step: 872  | total loss: \u001b[1m\u001b[32m1.95721\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 872 | loss: 1.95721 - acc: 0.5382 -- iter: 14/14\n",
      "--\n",
      "Training Step: 873  | total loss: \u001b[1m\u001b[32m1.82466\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 873 | loss: 1.82466 - acc: 0.5843 -- iter: 14/14\n",
      "--\n",
      "Training Step: 874  | total loss: \u001b[1m\u001b[32m1.95255\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 874 | loss: 1.95255 - acc: 0.5331 -- iter: 14/14\n",
      "--\n",
      "Training Step: 875  | total loss: \u001b[1m\u001b[32m1.82104\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 875 | loss: 1.82104 - acc: 0.5797 -- iter: 14/14\n",
      "--\n",
      "Training Step: 876  | total loss: \u001b[1m\u001b[32m1.70242\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 876 | loss: 1.70242 - acc: 0.6218 -- iter: 14/14\n",
      "--\n",
      "Training Step: 877  | total loss: \u001b[1m\u001b[32m1.59449\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 877 | loss: 1.59449 - acc: 0.6596 -- iter: 14/14\n",
      "--\n",
      "Training Step: 878  | total loss: \u001b[1m\u001b[32m1.49542\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 878 | loss: 1.49542 - acc: 0.6936 -- iter: 14/14\n",
      "--\n",
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m1.40367\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 879 | loss: 1.40367 - acc: 0.7243 -- iter: 14/14\n",
      "--\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m1.60180\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 880 | loss: 1.60180 - acc: 0.6518 -- iter: 14/14\n",
      "--\n",
      "Training Step: 881  | total loss: \u001b[1m\u001b[32m1.49468\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 881 | loss: 1.49468 - acc: 0.6867 -- iter: 14/14\n",
      "--\n",
      "Training Step: 882  | total loss: \u001b[1m\u001b[32m1.61393\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 882 | loss: 1.61393 - acc: 0.6466 -- iter: 14/14\n",
      "--\n",
      "Training Step: 883  | total loss: \u001b[1m\u001b[32m1.50209\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 883 | loss: 1.50209 - acc: 0.6819 -- iter: 14/14\n",
      "--\n",
      "Training Step: 884  | total loss: \u001b[1m\u001b[32m1.67393\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 884 | loss: 1.67393 - acc: 0.6280 -- iter: 14/14\n",
      "--\n",
      "Training Step: 885  | total loss: \u001b[1m\u001b[32m1.55356\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 885 | loss: 1.55356 - acc: 0.6652 -- iter: 14/14\n",
      "--\n",
      "Training Step: 886  | total loss: \u001b[1m\u001b[32m1.70351\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 886 | loss: 1.70351 - acc: 0.6130 -- iter: 14/14\n",
      "--\n",
      "Training Step: 887  | total loss: \u001b[1m\u001b[32m1.57884\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 887 | loss: 1.57884 - acc: 0.6517 -- iter: 14/14\n",
      "--\n",
      "Training Step: 888  | total loss: \u001b[1m\u001b[32m1.80335\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 888 | loss: 1.80335 - acc: 0.5865 -- iter: 14/14\n",
      "--\n",
      "Training Step: 889  | total loss: \u001b[1m\u001b[32m1.66864\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 889 | loss: 1.66864 - acc: 0.6279 -- iter: 14/14\n",
      "--\n",
      "Training Step: 890  | total loss: \u001b[1m\u001b[32m1.54737\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 890 | loss: 1.54737 - acc: 0.6651 -- iter: 14/14\n",
      "--\n",
      "Training Step: 891  | total loss: \u001b[1m\u001b[32m1.43755\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 891 | loss: 1.43755 - acc: 0.6986 -- iter: 14/14\n",
      "--\n",
      "Training Step: 892  | total loss: \u001b[1m\u001b[32m1.65359\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 892 | loss: 1.65359 - acc: 0.6358 -- iter: 14/14\n",
      "--\n",
      "Training Step: 893  | total loss: \u001b[1m\u001b[32m1.53194\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 893 | loss: 1.53194 - acc: 0.6723 -- iter: 14/14\n",
      "--\n",
      "Training Step: 894  | total loss: \u001b[1m\u001b[32m1.75299\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 894 | loss: 1.75299 - acc: 0.6050 -- iter: 14/14\n",
      "--\n",
      "Training Step: 895  | total loss: \u001b[1m\u001b[32m1.62153\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 895 | loss: 1.62153 - acc: 0.6445 -- iter: 14/14\n",
      "--\n",
      "Training Step: 896  | total loss: \u001b[1m\u001b[32m1.50324\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 896 | loss: 1.50324 - acc: 0.6801 -- iter: 14/14\n",
      "--\n",
      "Training Step: 897  | total loss: \u001b[1m\u001b[32m1.39618\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 897 | loss: 1.39618 - acc: 0.7121 -- iter: 14/14\n",
      "--\n",
      "Training Step: 898  | total loss: \u001b[1m\u001b[32m1.62295\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 898 | loss: 1.62295 - acc: 0.6409 -- iter: 14/14\n",
      "--\n",
      "Training Step: 899  | total loss: \u001b[1m\u001b[32m1.50285\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 899 | loss: 1.50285 - acc: 0.6768 -- iter: 14/14\n",
      "--\n",
      "Training Step: 900  | total loss: \u001b[1m\u001b[32m1.39426\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 900 | loss: 1.39426 - acc: 0.7091 -- iter: 14/14\n",
      "--\n",
      "Training Step: 901  | total loss: \u001b[1m\u001b[32m1.29550\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 901 | loss: 1.29550 - acc: 0.7382 -- iter: 14/14\n",
      "--\n",
      "Training Step: 902  | total loss: \u001b[1m\u001b[32m1.20514\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 902 | loss: 1.20514 - acc: 0.7644 -- iter: 14/14\n",
      "--\n",
      "Training Step: 903  | total loss: \u001b[1m\u001b[32m1.12197\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 903 | loss: 1.12197 - acc: 0.7879 -- iter: 14/14\n",
      "--\n",
      "Training Step: 904  | total loss: \u001b[1m\u001b[32m1.04500\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 904 | loss: 1.04500 - acc: 0.8091 -- iter: 14/14\n",
      "--\n",
      "Training Step: 905  | total loss: \u001b[1m\u001b[32m0.97344\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 905 | loss: 0.97344 - acc: 0.8282 -- iter: 14/14\n",
      "--\n",
      "Training Step: 906  | total loss: \u001b[1m\u001b[32m0.90663\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 906 | loss: 0.90663 - acc: 0.8454 -- iter: 14/14\n",
      "--\n",
      "Training Step: 907  | total loss: \u001b[1m\u001b[32m0.84407\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 907 | loss: 0.84407 - acc: 0.8609 -- iter: 14/14\n",
      "--\n",
      "Training Step: 908  | total loss: \u001b[1m\u001b[32m0.78537\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 908 | loss: 0.78537 - acc: 0.8748 -- iter: 14/14\n",
      "--\n",
      "Training Step: 909  | total loss: \u001b[1m\u001b[32m0.73023\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 909 | loss: 0.73023 - acc: 0.8873 -- iter: 14/14\n",
      "--\n",
      "Training Step: 910  | total loss: \u001b[1m\u001b[32m1.06544\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 910 | loss: 1.06544 - acc: 0.8057 -- iter: 14/14\n",
      "--\n",
      "Training Step: 911  | total loss: \u001b[1m\u001b[32m0.97921\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 911 | loss: 0.97921 - acc: 0.8251 -- iter: 14/14\n",
      "--\n",
      "Training Step: 912  | total loss: \u001b[1m\u001b[32m1.29190\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 912 | loss: 1.29190 - acc: 0.7498 -- iter: 14/14\n",
      "--\n",
      "Training Step: 913  | total loss: \u001b[1m\u001b[32m1.18207\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 913 | loss: 1.18207 - acc: 0.7748 -- iter: 14/14\n",
      "--\n",
      "Training Step: 914  | total loss: \u001b[1m\u001b[32m1.51806\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 914 | loss: 1.51806 - acc: 0.6973 -- iter: 14/14\n",
      "--\n",
      "Training Step: 915  | total loss: \u001b[1m\u001b[32m1.38630\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 915 | loss: 1.38630 - acc: 0.7276 -- iter: 14/14\n",
      "--\n",
      "Training Step: 916  | total loss: \u001b[1m\u001b[32m1.66059\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 916 | loss: 1.66059 - acc: 0.6620 -- iter: 14/14\n",
      "--\n",
      "Training Step: 917  | total loss: \u001b[1m\u001b[32m1.51667\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 917 | loss: 1.51667 - acc: 0.6958 -- iter: 14/14\n",
      "--\n",
      "Training Step: 918  | total loss: \u001b[1m\u001b[32m1.78639\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 918 | loss: 1.78639 - acc: 0.6333 -- iter: 14/14\n",
      "--\n",
      "Training Step: 919  | total loss: \u001b[1m\u001b[32m1.63320\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 919 | loss: 1.63320 - acc: 0.6700 -- iter: 14/14\n",
      "--\n",
      "Training Step: 920  | total loss: \u001b[1m\u001b[32m1.88034\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 920 | loss: 1.88034 - acc: 0.6030 -- iter: 14/14\n",
      "--\n",
      "Training Step: 921  | total loss: \u001b[1m\u001b[32m1.72233\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 921 | loss: 1.72233 - acc: 0.6427 -- iter: 14/14\n",
      "--\n",
      "Training Step: 922  | total loss: \u001b[1m\u001b[32m1.95759\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 922 | loss: 1.95759 - acc: 0.5784 -- iter: 14/14\n",
      "--\n",
      "Training Step: 923  | total loss: \u001b[1m\u001b[32m1.79774\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 923 | loss: 1.79774 - acc: 0.6206 -- iter: 14/14\n",
      "--\n",
      "Training Step: 924  | total loss: \u001b[1m\u001b[32m1.65677\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 924 | loss: 1.65677 - acc: 0.6585 -- iter: 14/14\n",
      "--\n",
      "Training Step: 925  | total loss: \u001b[1m\u001b[32m1.53208\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 925 | loss: 1.53208 - acc: 0.6927 -- iter: 14/14\n",
      "--\n",
      "Training Step: 926  | total loss: \u001b[1m\u001b[32m1.68197\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 926 | loss: 1.68197 - acc: 0.6520 -- iter: 14/14\n",
      "--\n",
      "Training Step: 927  | total loss: \u001b[1m\u001b[32m1.55830\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 927 | loss: 1.55830 - acc: 0.6868 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 928  | total loss: \u001b[1m\u001b[32m1.77933\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 928 | loss: 1.77933 - acc: 0.6181 -- iter: 14/14\n",
      "--\n",
      "Training Step: 929  | total loss: \u001b[1m\u001b[32m1.64968\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 929 | loss: 1.64968 - acc: 0.6563 -- iter: 14/14\n",
      "--\n",
      "Training Step: 930  | total loss: \u001b[1m\u001b[32m1.81049\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 930 | loss: 1.81049 - acc: 0.5978 -- iter: 14/14\n",
      "--\n",
      "Training Step: 931  | total loss: \u001b[1m\u001b[32m1.68167\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 931 | loss: 1.68167 - acc: 0.6380 -- iter: 14/14\n",
      "--\n",
      "Training Step: 932  | total loss: \u001b[1m\u001b[32m1.86735\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 932 | loss: 1.86735 - acc: 0.5742 -- iter: 14/14\n",
      "--\n",
      "Training Step: 933  | total loss: \u001b[1m\u001b[32m1.73674\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 933 | loss: 1.73674 - acc: 0.6168 -- iter: 14/14\n",
      "--\n",
      "Training Step: 934  | total loss: \u001b[1m\u001b[32m1.87795\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 934 | loss: 1.87795 - acc: 0.5623 -- iter: 14/14\n",
      "--\n",
      "Training Step: 935  | total loss: \u001b[1m\u001b[32m1.75000\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 935 | loss: 1.75000 - acc: 0.6060 -- iter: 14/14\n",
      "--\n",
      "Training Step: 936  | total loss: \u001b[1m\u001b[32m1.88916\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 936 | loss: 1.88916 - acc: 0.5597 -- iter: 14/14\n",
      "--\n",
      "Training Step: 937  | total loss: \u001b[1m\u001b[32m1.76330\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 937 | loss: 1.76330 - acc: 0.6037 -- iter: 14/14\n",
      "--\n",
      "Training Step: 938  | total loss: \u001b[1m\u001b[32m1.65092\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 938 | loss: 1.65092 - acc: 0.6434 -- iter: 14/14\n",
      "--\n",
      "Training Step: 939  | total loss: \u001b[1m\u001b[32m1.54966\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 939 | loss: 1.54966 - acc: 0.6790 -- iter: 14/14\n",
      "--\n",
      "Training Step: 940  | total loss: \u001b[1m\u001b[32m1.71736\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 940 | loss: 1.71736 - acc: 0.6183 -- iter: 14/14\n",
      "--\n",
      "Training Step: 941  | total loss: \u001b[1m\u001b[32m1.60860\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 941 | loss: 1.60860 - acc: 0.6564 -- iter: 14/14\n",
      "--\n",
      "Training Step: 942  | total loss: \u001b[1m\u001b[32m1.50995\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 942 | loss: 1.50995 - acc: 0.6908 -- iter: 14/14\n",
      "--\n",
      "Training Step: 943  | total loss: \u001b[1m\u001b[32m1.41958\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 943 | loss: 1.41958 - acc: 0.7217 -- iter: 14/14\n",
      "--\n",
      "Training Step: 944  | total loss: \u001b[1m\u001b[32m1.62742\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 944 | loss: 1.62742 - acc: 0.6496 -- iter: 14/14\n",
      "--\n",
      "Training Step: 945  | total loss: \u001b[1m\u001b[32m1.52211\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 945 | loss: 1.52211 - acc: 0.6846 -- iter: 14/14\n",
      "--\n",
      "Training Step: 946  | total loss: \u001b[1m\u001b[32m1.71196\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 946 | loss: 1.71196 - acc: 0.6161 -- iter: 14/14\n",
      "--\n",
      "Training Step: 947  | total loss: \u001b[1m\u001b[32m1.59622\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 947 | loss: 1.59622 - acc: 0.6545 -- iter: 14/14\n",
      "--\n",
      "Training Step: 948  | total loss: \u001b[1m\u001b[32m1.49096\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 948 | loss: 1.49096 - acc: 0.6891 -- iter: 14/14\n",
      "--\n",
      "Training Step: 949  | total loss: \u001b[1m\u001b[32m1.39448\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 949 | loss: 1.39448 - acc: 0.7202 -- iter: 14/14\n",
      "--\n",
      "Training Step: 950  | total loss: \u001b[1m\u001b[32m1.30535\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 950 | loss: 1.30535 - acc: 0.7481 -- iter: 14/14\n",
      "--\n",
      "Training Step: 951  | total loss: \u001b[1m\u001b[32m1.22241\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 951 | loss: 1.22241 - acc: 0.7733 -- iter: 14/14\n",
      "--\n",
      "Training Step: 952  | total loss: \u001b[1m\u001b[32m1.49594\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 952 | loss: 1.49594 - acc: 0.6960 -- iter: 14/14\n",
      "--\n",
      "Training Step: 953  | total loss: \u001b[1m\u001b[32m1.38953\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 953 | loss: 1.38953 - acc: 0.7264 -- iter: 14/14\n",
      "--\n",
      "Training Step: 954  | total loss: \u001b[1m\u001b[32m1.29195\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 954 | loss: 1.29195 - acc: 0.7538 -- iter: 14/14\n",
      "--\n",
      "Training Step: 955  | total loss: \u001b[1m\u001b[32m1.20201\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 955 | loss: 1.20201 - acc: 0.7784 -- iter: 14/14\n",
      "--\n",
      "Training Step: 956  | total loss: \u001b[1m\u001b[32m1.11870\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 956 | loss: 1.11870 - acc: 0.8005 -- iter: 14/14\n",
      "--\n",
      "Training Step: 957  | total loss: \u001b[1m\u001b[32m1.04120\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 957 | loss: 1.04120 - acc: 0.8205 -- iter: 14/14\n",
      "--\n",
      "Training Step: 958  | total loss: \u001b[1m\u001b[32m1.32119\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 958 | loss: 1.32119 - acc: 0.7456 -- iter: 14/14\n",
      "--\n",
      "Training Step: 959  | total loss: \u001b[1m\u001b[32m1.21969\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 959 | loss: 1.21969 - acc: 0.7710 -- iter: 14/14\n",
      "--\n",
      "Training Step: 960  | total loss: \u001b[1m\u001b[32m1.44518\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 960 | loss: 1.44518 - acc: 0.7082 -- iter: 14/14\n",
      "--\n",
      "Training Step: 961  | total loss: \u001b[1m\u001b[32m1.32958\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 961 | loss: 1.32958 - acc: 0.7374 -- iter: 14/14\n",
      "--\n",
      "Training Step: 962  | total loss: \u001b[1m\u001b[32m1.22494\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 962 | loss: 1.22494 - acc: 0.7636 -- iter: 14/14\n",
      "--\n",
      "Training Step: 963  | total loss: \u001b[1m\u001b[32m1.12993\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 963 | loss: 1.12993 - acc: 0.7873 -- iter: 14/14\n",
      "--\n",
      "Training Step: 964  | total loss: \u001b[1m\u001b[32m1.38942\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 964 | loss: 1.38942 - acc: 0.7228 -- iter: 14/14\n",
      "--\n",
      "Training Step: 965  | total loss: \u001b[1m\u001b[32m1.27703\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 965 | loss: 1.27703 - acc: 0.7506 -- iter: 14/14\n",
      "--\n",
      "Training Step: 966  | total loss: \u001b[1m\u001b[32m1.17569\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 966 | loss: 1.17569 - acc: 0.7755 -- iter: 14/14\n",
      "--\n",
      "Training Step: 967  | total loss: \u001b[1m\u001b[32m1.08401\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 967 | loss: 1.08401 - acc: 0.7980 -- iter: 14/14\n",
      "--\n",
      "Training Step: 968  | total loss: \u001b[1m\u001b[32m1.00080\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 968 | loss: 1.00080 - acc: 0.8182 -- iter: 14/14\n",
      "--\n",
      "Training Step: 969  | total loss: \u001b[1m\u001b[32m0.92501\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 969 | loss: 0.92501 - acc: 0.8363 -- iter: 14/14\n",
      "--\n",
      "Training Step: 970  | total loss: \u001b[1m\u001b[32m1.26191\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 970 | loss: 1.26191 - acc: 0.7598 -- iter: 14/14\n",
      "--\n",
      "Training Step: 971  | total loss: \u001b[1m\u001b[32m1.15900\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 971 | loss: 1.15900 - acc: 0.7839 -- iter: 14/14\n",
      "--\n",
      "Training Step: 972  | total loss: \u001b[1m\u001b[32m1.37622\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 972 | loss: 1.37622 - acc: 0.7269 -- iter: 14/14\n",
      "--\n",
      "Training Step: 973  | total loss: \u001b[1m\u001b[32m1.26232\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 973 | loss: 1.26232 - acc: 0.7542 -- iter: 14/14\n",
      "--\n",
      "Training Step: 974  | total loss: \u001b[1m\u001b[32m1.54579\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 974 | loss: 1.54579 - acc: 0.6859 -- iter: 14/14\n",
      "--\n",
      "Training Step: 975  | total loss: \u001b[1m\u001b[32m1.41665\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 975 | loss: 1.41665 - acc: 0.7173 -- iter: 14/14\n",
      "--\n",
      "Training Step: 976  | total loss: \u001b[1m\u001b[32m1.62391\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 976 | loss: 1.62391 - acc: 0.6599 -- iter: 14/14\n",
      "--\n",
      "Training Step: 977  | total loss: \u001b[1m\u001b[32m1.48990\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 977 | loss: 1.48990 - acc: 0.6939 -- iter: 14/14\n",
      "--\n",
      "Training Step: 978  | total loss: \u001b[1m\u001b[32m1.37074\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 978 | loss: 1.37074 - acc: 0.7245 -- iter: 14/14\n",
      "--\n",
      "Training Step: 979  | total loss: \u001b[1m\u001b[32m1.26449\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 979 | loss: 1.26449 - acc: 0.7521 -- iter: 14/14\n",
      "--\n",
      "Training Step: 980  | total loss: \u001b[1m\u001b[32m1.16936\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 980 | loss: 1.16936 - acc: 0.7769 -- iter: 14/14\n",
      "--\n",
      "Training Step: 981  | total loss: \u001b[1m\u001b[32m1.08380\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 981 | loss: 1.08380 - acc: 0.7992 -- iter: 14/14\n",
      "--\n",
      "Training Step: 982  | total loss: \u001b[1m\u001b[32m1.27540\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 982 | loss: 1.27540 - acc: 0.7407 -- iter: 14/14\n",
      "--\n",
      "Training Step: 983  | total loss: \u001b[1m\u001b[32m1.17942\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 983 | loss: 1.17942 - acc: 0.7666 -- iter: 14/14\n",
      "--\n",
      "Training Step: 984  | total loss: \u001b[1m\u001b[32m1.45001\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 984 | loss: 1.45001 - acc: 0.6900 -- iter: 14/14\n",
      "--\n",
      "Training Step: 985  | total loss: \u001b[1m\u001b[32m1.33794\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 985 | loss: 1.33794 - acc: 0.7210 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 986  | total loss: \u001b[1m\u001b[32m1.51829\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 986 | loss: 1.51829 - acc: 0.6703 -- iter: 14/14\n",
      "--\n",
      "Training Step: 987  | total loss: \u001b[1m\u001b[32m1.40169\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 987 | loss: 1.40169 - acc: 0.7033 -- iter: 14/14\n",
      "--\n",
      "Training Step: 988  | total loss: \u001b[1m\u001b[32m1.29770\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 988 | loss: 1.29770 - acc: 0.7329 -- iter: 14/14\n",
      "--\n",
      "Training Step: 989  | total loss: \u001b[1m\u001b[32m1.20447\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 989 | loss: 1.20447 - acc: 0.7596 -- iter: 14/14\n",
      "--\n",
      "Training Step: 990  | total loss: \u001b[1m\u001b[32m1.41646\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 990 | loss: 1.41646 - acc: 0.7051 -- iter: 14/14\n",
      "--\n",
      "Training Step: 991  | total loss: \u001b[1m\u001b[32m1.31171\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 991 | loss: 1.31171 - acc: 0.7346 -- iter: 14/14\n",
      "--\n",
      "Training Step: 992  | total loss: \u001b[1m\u001b[32m1.21743\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 992 | loss: 1.21743 - acc: 0.7611 -- iter: 14/14\n",
      "--\n",
      "Training Step: 993  | total loss: \u001b[1m\u001b[32m1.13203\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 993 | loss: 1.13203 - acc: 0.7850 -- iter: 14/14\n",
      "--\n",
      "Training Step: 994  | total loss: \u001b[1m\u001b[32m1.05418\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 994 | loss: 1.05418 - acc: 0.8065 -- iter: 14/14\n",
      "--\n",
      "Training Step: 995  | total loss: \u001b[1m\u001b[32m0.98275\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 995 | loss: 0.98275 - acc: 0.8259 -- iter: 14/14\n",
      "--\n",
      "Training Step: 996  | total loss: \u001b[1m\u001b[32m1.24907\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 996 | loss: 1.24907 - acc: 0.7504 -- iter: 14/14\n",
      "--\n",
      "Training Step: 997  | total loss: \u001b[1m\u001b[32m1.15624\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 997 | loss: 1.15624 - acc: 0.7754 -- iter: 14/14\n",
      "--\n",
      "Training Step: 998  | total loss: \u001b[1m\u001b[32m1.46573\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 998 | loss: 1.46573 - acc: 0.6978 -- iter: 14/14\n",
      "--\n",
      "Training Step: 999  | total loss: \u001b[1m\u001b[32m1.35121\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 999 | loss: 1.35121 - acc: 0.7281 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1000  | total loss: \u001b[1m\u001b[32m1.55348\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1000 | loss: 1.55348 - acc: 0.6695 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1001  | total loss: \u001b[1m\u001b[32m1.43137\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1001 | loss: 1.43137 - acc: 0.7026 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1002  | total loss: \u001b[1m\u001b[32m1.64656\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1002 | loss: 1.64656 - acc: 0.6466 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1003  | total loss: \u001b[1m\u001b[32m1.51712\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1003 | loss: 1.51712 - acc: 0.6820 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1004  | total loss: \u001b[1m\u001b[32m1.75100\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1004 | loss: 1.75100 - acc: 0.6138 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1005  | total loss: \u001b[1m\u001b[32m1.61414\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1005 | loss: 1.61414 - acc: 0.6524 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1006  | total loss: \u001b[1m\u001b[32m1.49241\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1006 | loss: 1.49241 - acc: 0.6871 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1007  | total loss: \u001b[1m\u001b[32m1.38361\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1007 | loss: 1.38361 - acc: 0.7184 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1008  | total loss: \u001b[1m\u001b[32m1.28580\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1008 | loss: 1.28580 - acc: 0.7466 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1009  | total loss: \u001b[1m\u001b[32m1.19730\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1009 | loss: 1.19730 - acc: 0.7719 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1010  | total loss: \u001b[1m\u001b[32m1.46938\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1010 | loss: 1.46938 - acc: 0.6947 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1011  | total loss: \u001b[1m\u001b[32m1.36186\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1011 | loss: 1.36186 - acc: 0.7253 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1012  | total loss: \u001b[1m\u001b[32m1.61440\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1012 | loss: 1.61440 - acc: 0.6527 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1013  | total loss: \u001b[1m\u001b[32m1.49315\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1013 | loss: 1.49315 - acc: 0.6875 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1014  | total loss: \u001b[1m\u001b[32m1.38439\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1014 | loss: 1.38439 - acc: 0.7187 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1015  | total loss: \u001b[1m\u001b[32m1.28628\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1015 | loss: 1.28628 - acc: 0.7468 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1016  | total loss: \u001b[1m\u001b[32m1.19720\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1016 | loss: 1.19720 - acc: 0.7722 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1017  | total loss: \u001b[1m\u001b[32m1.11578\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1017 | loss: 1.11578 - acc: 0.7949 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1018  | total loss: \u001b[1m\u001b[32m1.04088\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1018 | loss: 1.04088 - acc: 0.8154 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1019  | total loss: \u001b[1m\u001b[32m0.97153\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1019 | loss: 0.97153 - acc: 0.8339 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1020  | total loss: \u001b[1m\u001b[32m1.26747\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1020 | loss: 1.26747 - acc: 0.7577 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1021  | total loss: \u001b[1m\u001b[32m1.17245\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1021 | loss: 1.17245 - acc: 0.7819 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1022  | total loss: \u001b[1m\u001b[32m1.08578\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1022 | loss: 1.08578 - acc: 0.8037 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1023  | total loss: \u001b[1m\u001b[32m1.00639\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1023 | loss: 1.00639 - acc: 0.8233 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1024  | total loss: \u001b[1m\u001b[32m1.32751\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1024 | loss: 1.32751 - acc: 0.7410 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1025  | total loss: \u001b[1m\u001b[32m1.22220\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1025 | loss: 1.22220 - acc: 0.7669 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1026  | total loss: \u001b[1m\u001b[32m1.39799\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1026 | loss: 1.39799 - acc: 0.7188 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1027  | total loss: \u001b[1m\u001b[32m1.28551\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1027 | loss: 1.28551 - acc: 0.7469 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1028  | total loss: \u001b[1m\u001b[32m1.54905\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1028 | loss: 1.54905 - acc: 0.6794 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1029  | total loss: \u001b[1m\u001b[32m1.42261\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1029 | loss: 1.42261 - acc: 0.7114 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1030  | total loss: \u001b[1m\u001b[32m1.66413\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1030 | loss: 1.66413 - acc: 0.6474 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1031  | total loss: \u001b[1m\u001b[32m1.52862\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1031 | loss: 1.52862 - acc: 0.6827 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1032  | total loss: \u001b[1m\u001b[32m1.40785\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1032 | loss: 1.40785 - acc: 0.7144 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1033  | total loss: \u001b[1m\u001b[32m1.29983\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1033 | loss: 1.29983 - acc: 0.7430 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1034  | total loss: \u001b[1m\u001b[32m1.20277\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1034 | loss: 1.20277 - acc: 0.7687 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1035  | total loss: \u001b[1m\u001b[32m1.11512\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1035 | loss: 1.11512 - acc: 0.7918 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1036  | total loss: \u001b[1m\u001b[32m1.03552\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1036 | loss: 1.03552 - acc: 0.8126 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1037  | total loss: \u001b[1m\u001b[32m0.96281\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1037 | loss: 0.96281 - acc: 0.8314 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1038  | total loss: \u001b[1m\u001b[32m1.23442\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1038 | loss: 1.23442 - acc: 0.7554 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1039  | total loss: \u001b[1m\u001b[32m1.14029\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1039 | loss: 1.14029 - acc: 0.7798 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1040  | total loss: \u001b[1m\u001b[32m1.05503\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1040 | loss: 1.05503 - acc: 0.8018 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1041  | total loss: \u001b[1m\u001b[32m0.97745\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1041 | loss: 0.97745 - acc: 0.8217 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1042  | total loss: \u001b[1m\u001b[32m0.90655\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1042 | loss: 0.90655 - acc: 0.8395 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1043  | total loss: \u001b[1m\u001b[32m0.84145\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1043 | loss: 0.84145 - acc: 0.8555 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1044  | total loss: \u001b[1m\u001b[32m1.18539\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1044 | loss: 1.18539 - acc: 0.7700 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1045  | total loss: \u001b[1m\u001b[32m1.09085\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1045 | loss: 1.09085 - acc: 0.7930 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1046  | total loss: \u001b[1m\u001b[32m1.00535\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1046 | loss: 1.00535 - acc: 0.8137 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1047  | total loss: \u001b[1m\u001b[32m0.92777\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1047 | loss: 0.92777 - acc: 0.8323 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1048  | total loss: \u001b[1m\u001b[32m1.23843\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1048 | loss: 1.23843 - acc: 0.7562 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1049  | total loss: \u001b[1m\u001b[32m1.13700\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1049 | loss: 1.13700 - acc: 0.7806 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1050  | total loss: \u001b[1m\u001b[32m1.42616\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1050 | loss: 1.42616 - acc: 0.7097 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1051  | total loss: \u001b[1m\u001b[32m1.30690\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1051 | loss: 1.30690 - acc: 0.7387 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1052  | total loss: \u001b[1m\u001b[32m1.20020\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1052 | loss: 1.20020 - acc: 0.7649 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1053  | total loss: \u001b[1m\u001b[32m1.10448\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1053 | loss: 1.10448 - acc: 0.7884 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1054  | total loss: \u001b[1m\u001b[32m1.34406\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1054 | loss: 1.34406 - acc: 0.7167 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1055  | total loss: \u001b[1m\u001b[32m1.23500\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1055 | loss: 1.23500 - acc: 0.7450 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1056  | total loss: \u001b[1m\u001b[32m1.51065\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1056 | loss: 1.51065 - acc: 0.6705 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1057  | total loss: \u001b[1m\u001b[32m1.38739\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1057 | loss: 1.38739 - acc: 0.7035 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1058  | total loss: \u001b[1m\u001b[32m1.27783\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1058 | loss: 1.27783 - acc: 0.7331 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1059  | total loss: \u001b[1m\u001b[32m1.18012\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1059 | loss: 1.18012 - acc: 0.7598 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1060  | total loss: \u001b[1m\u001b[32m1.45279\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1060 | loss: 1.45279 - acc: 0.6910 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1061  | total loss: \u001b[1m\u001b[32m1.33959\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1061 | loss: 1.33959 - acc: 0.7219 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1062  | total loss: \u001b[1m\u001b[32m1.57560\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1062 | loss: 1.57560 - acc: 0.6568 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1063  | total loss: \u001b[1m\u001b[32m1.45307\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1063 | loss: 1.45307 - acc: 0.6911 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1064  | total loss: \u001b[1m\u001b[32m1.34410\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1064 | loss: 1.34410 - acc: 0.7220 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1065  | total loss: \u001b[1m\u001b[32m1.24670\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1065 | loss: 1.24670 - acc: 0.7498 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1066  | total loss: \u001b[1m\u001b[32m1.51004\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1066 | loss: 1.51004 - acc: 0.6748 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1067  | total loss: \u001b[1m\u001b[32m1.39748\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1067 | loss: 1.39748 - acc: 0.7074 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1068  | total loss: \u001b[1m\u001b[32m1.63562\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1068 | loss: 1.63562 - acc: 0.6366 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1069  | total loss: \u001b[1m\u001b[32m1.51296\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1069 | loss: 1.51296 - acc: 0.6730 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1070  | total loss: \u001b[1m\u001b[32m1.70958\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1070 | loss: 1.70958 - acc: 0.6128 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1071  | total loss: \u001b[1m\u001b[32m1.58280\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1071 | loss: 1.58280 - acc: 0.6515 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1072  | total loss: \u001b[1m\u001b[32m1.79907\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1072 | loss: 1.79907 - acc: 0.5864 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1073  | total loss: \u001b[1m\u001b[32m1.66728\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1073 | loss: 1.66728 - acc: 0.6277 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1074  | total loss: \u001b[1m\u001b[32m1.82227\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1074 | loss: 1.82227 - acc: 0.5792 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1075  | total loss: \u001b[1m\u001b[32m1.69204\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1075 | loss: 1.69204 - acc: 0.6213 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1076  | total loss: \u001b[1m\u001b[32m1.86909\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1076 | loss: 1.86909 - acc: 0.5592 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1077  | total loss: \u001b[1m\u001b[32m1.73796\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1077 | loss: 1.73796 - acc: 0.6033 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1078  | total loss: \u001b[1m\u001b[32m1.92854\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1078 | loss: 1.92854 - acc: 0.5429 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1079  | total loss: \u001b[1m\u001b[32m1.79544\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1079 | loss: 1.79544 - acc: 0.5886 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1080  | total loss: \u001b[1m\u001b[32m1.89172\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1080 | loss: 1.89172 - acc: 0.5512 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1081  | total loss: \u001b[1m\u001b[32m1.76584\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1081 | loss: 1.76584 - acc: 0.5961 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1082  | total loss: \u001b[1m\u001b[32m1.91869\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1082 | loss: 1.91869 - acc: 0.5436 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1083  | total loss: \u001b[1m\u001b[32m1.79296\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1083 | loss: 1.79296 - acc: 0.5893 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1084  | total loss: \u001b[1m\u001b[32m1.91025\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1084 | loss: 1.91025 - acc: 0.5446 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1085  | total loss: \u001b[1m\u001b[32m1.78757\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1085 | loss: 1.78757 - acc: 0.5902 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1086  | total loss: \u001b[1m\u001b[32m1.67751\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1086 | loss: 1.67751 - acc: 0.6311 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1087  | total loss: \u001b[1m\u001b[32m1.57773\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1087 | loss: 1.57773 - acc: 0.6680 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1088  | total loss: \u001b[1m\u001b[32m1.77428\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1088 | loss: 1.77428 - acc: 0.6012 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1089  | total loss: \u001b[1m\u001b[32m1.66294\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1089 | loss: 1.66294 - acc: 0.6411 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1090  | total loss: \u001b[1m\u001b[32m1.56157\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1090 | loss: 1.56157 - acc: 0.6770 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1091  | total loss: \u001b[1m\u001b[32m1.46832\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1091 | loss: 1.46832 - acc: 0.7093 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1092  | total loss: \u001b[1m\u001b[32m1.66322\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1092 | loss: 1.66322 - acc: 0.6455 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1093  | total loss: \u001b[1m\u001b[32m1.55583\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1093 | loss: 1.55583 - acc: 0.6810 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1094  | total loss: \u001b[1m\u001b[32m1.71766\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1094 | loss: 1.71766 - acc: 0.6271 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1095  | total loss: \u001b[1m\u001b[32m1.60201\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1095 | loss: 1.60201 - acc: 0.6644 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1096  | total loss: \u001b[1m\u001b[32m1.78231\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1096 | loss: 1.78231 - acc: 0.6051 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1097  | total loss: \u001b[1m\u001b[32m1.65828\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1097 | loss: 1.65828 - acc: 0.6446 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1098  | total loss: \u001b[1m\u001b[32m1.54558\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1098 | loss: 1.54558 - acc: 0.6802 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1099  | total loss: \u001b[1m\u001b[32m1.44244\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1099 | loss: 1.44244 - acc: 0.7121 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1100  | total loss: \u001b[1m\u001b[32m1.63924\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1100 | loss: 1.63924 - acc: 0.6552 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1101  | total loss: \u001b[1m\u001b[32m1.52354\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1101 | loss: 1.52354 - acc: 0.6897 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1102  | total loss: \u001b[1m\u001b[32m1.65989\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1102 | loss: 1.65989 - acc: 0.6422 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1103  | total loss: \u001b[1m\u001b[32m1.54014\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1103 | loss: 1.54014 - acc: 0.6779 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1104  | total loss: \u001b[1m\u001b[32m1.77465\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1104 | loss: 1.77465 - acc: 0.6101 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1105  | total loss: \u001b[1m\u001b[32m1.64276\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1105 | loss: 1.64276 - acc: 0.6491 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1106  | total loss: \u001b[1m\u001b[32m1.85820\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1106 | loss: 1.85820 - acc: 0.5842 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1107  | total loss: \u001b[1m\u001b[32m1.71854\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1107 | loss: 1.71854 - acc: 0.6258 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1108  | total loss: \u001b[1m\u001b[32m1.93352\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1108 | loss: 1.93352 - acc: 0.5632 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1109  | total loss: \u001b[1m\u001b[32m1.78790\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1109 | loss: 1.78790 - acc: 0.6069 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1110  | total loss: \u001b[1m\u001b[32m1.94083\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1110 | loss: 1.94083 - acc: 0.5533 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1111  | total loss: \u001b[1m\u001b[32m1.79670\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1111 | loss: 1.79670 - acc: 0.5980 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1112  | total loss: \u001b[1m\u001b[32m1.66775\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1112 | loss: 1.66775 - acc: 0.6382 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1113  | total loss: \u001b[1m\u001b[32m1.55162\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1113 | loss: 1.55162 - acc: 0.6744 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1114  | total loss: \u001b[1m\u001b[32m1.73287\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1114 | loss: 1.73287 - acc: 0.6141 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1115  | total loss: \u001b[1m\u001b[32m1.60981\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1115 | loss: 1.60981 - acc: 0.6527 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1116  | total loss: \u001b[1m\u001b[32m1.49868\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1116 | loss: 1.49868 - acc: 0.6874 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1117  | total loss: \u001b[1m\u001b[32m1.39761\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1117 | loss: 1.39761 - acc: 0.7187 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1118  | total loss: \u001b[1m\u001b[32m1.61437\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1118 | loss: 1.61437 - acc: 0.6468 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1119  | total loss: \u001b[1m\u001b[32m1.49982\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1119 | loss: 1.49982 - acc: 0.6821 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1120  | total loss: \u001b[1m\u001b[32m1.69419\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1120 | loss: 1.69419 - acc: 0.6211 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1121  | total loss: \u001b[1m\u001b[32m1.57110\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1121 | loss: 1.57110 - acc: 0.6590 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1122  | total loss: \u001b[1m\u001b[32m1.45998\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1122 | loss: 1.45998 - acc: 0.6931 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1123  | total loss: \u001b[1m\u001b[32m1.35901\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1123 | loss: 1.35901 - acc: 0.7237 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1124  | total loss: \u001b[1m\u001b[32m1.26665\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1124 | loss: 1.26665 - acc: 0.7514 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1125  | total loss: \u001b[1m\u001b[32m1.18161\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1125 | loss: 1.18161 - acc: 0.7762 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1126  | total loss: \u001b[1m\u001b[32m1.40707\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1126 | loss: 1.40707 - acc: 0.7129 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1127  | total loss: \u001b[1m\u001b[32m1.30481\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1127 | loss: 1.30481 - acc: 0.7416 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1128  | total loss: \u001b[1m\u001b[32m1.52443\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1128 | loss: 1.52443 - acc: 0.6746 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1129  | total loss: \u001b[1m\u001b[32m1.40900\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1129 | loss: 1.40900 - acc: 0.7071 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1130  | total loss: \u001b[1m\u001b[32m1.62583\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1130 | loss: 1.62583 - acc: 0.6436 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1131  | total loss: \u001b[1m\u001b[32m1.50025\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1131 | loss: 1.50025 - acc: 0.6792 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1132  | total loss: \u001b[1m\u001b[32m1.71032\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1132 | loss: 1.71032 - acc: 0.6113 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1133  | total loss: \u001b[1m\u001b[32m1.57749\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1133 | loss: 1.57749 - acc: 0.6502 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1134  | total loss: \u001b[1m\u001b[32m1.80086\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1134 | loss: 1.80086 - acc: 0.5851 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1135  | total loss: \u001b[1m\u001b[32m1.66117\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1135 | loss: 1.66117 - acc: 0.6266 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1136  | total loss: \u001b[1m\u001b[32m1.81173\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1136 | loss: 1.81173 - acc: 0.5854 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1137  | total loss: \u001b[1m\u001b[32m1.67355\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1137 | loss: 1.67355 - acc: 0.6269 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1138  | total loss: \u001b[1m\u001b[32m1.83422\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1138 | loss: 1.83422 - acc: 0.5785 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1139  | total loss: \u001b[1m\u001b[32m1.69649\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1139 | loss: 1.69649 - acc: 0.6206 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1140  | total loss: \u001b[1m\u001b[32m1.57351\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1140 | loss: 1.57351 - acc: 0.6585 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1141  | total loss: \u001b[1m\u001b[32m1.46303\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1141 | loss: 1.46303 - acc: 0.6927 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1142  | total loss: \u001b[1m\u001b[32m1.65969\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1142 | loss: 1.65969 - acc: 0.6306 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1143  | total loss: \u001b[1m\u001b[32m1.54075\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1143 | loss: 1.54075 - acc: 0.6675 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1144  | total loss: \u001b[1m\u001b[32m1.75326\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1144 | loss: 1.75326 - acc: 0.6008 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1145  | total loss: \u001b[1m\u001b[32m1.62601\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1145 | loss: 1.62601 - acc: 0.6407 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1146  | total loss: \u001b[1m\u001b[32m1.77825\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1146 | loss: 1.77825 - acc: 0.5909 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1147  | total loss: \u001b[1m\u001b[32m1.65013\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1147 | loss: 1.65013 - acc: 0.6318 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1148  | total loss: \u001b[1m\u001b[32m1.53527\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1148 | loss: 1.53527 - acc: 0.6686 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1149  | total loss: \u001b[1m\u001b[32m1.43161\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1149 | loss: 1.43161 - acc: 0.7018 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1150  | total loss: \u001b[1m\u001b[32m1.59070\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1150 | loss: 1.59070 - acc: 0.6459 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1151  | total loss: \u001b[1m\u001b[32m1.48063\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1151 | loss: 1.48063 - acc: 0.6813 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1152  | total loss: \u001b[1m\u001b[32m1.62740\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1152 | loss: 1.62740 - acc: 0.6417 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1153  | total loss: \u001b[1m\u001b[32m1.51322\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1153 | loss: 1.51322 - acc: 0.6776 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1154  | total loss: \u001b[1m\u001b[32m1.40992\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1154 | loss: 1.40992 - acc: 0.7098 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1155  | total loss: \u001b[1m\u001b[32m1.31578\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1155 | loss: 1.31578 - acc: 0.7388 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1156  | total loss: \u001b[1m\u001b[32m1.22936\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1156 | loss: 1.22936 - acc: 0.7649 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1157  | total loss: \u001b[1m\u001b[32m1.14944\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1157 | loss: 1.14944 - acc: 0.7884 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1158  | total loss: \u001b[1m\u001b[32m1.38929\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1158 | loss: 1.38929 - acc: 0.7096 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1159  | total loss: \u001b[1m\u001b[32m1.29005\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1159 | loss: 1.29005 - acc: 0.7386 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1160  | total loss: \u001b[1m\u001b[32m1.55049\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1160 | loss: 1.55049 - acc: 0.6648 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1161  | total loss: \u001b[1m\u001b[32m1.43387\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1161 | loss: 1.43387 - acc: 0.6983 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1162  | total loss: \u001b[1m\u001b[32m1.32845\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1162 | loss: 1.32845 - acc: 0.7285 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1163  | total loss: \u001b[1m\u001b[32m1.23264\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1163 | loss: 1.23264 - acc: 0.7556 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1164  | total loss: \u001b[1m\u001b[32m1.47797\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1164 | loss: 1.47797 - acc: 0.6872 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1165  | total loss: \u001b[1m\u001b[32m1.36583\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1165 | loss: 1.36583 - acc: 0.7185 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1166  | total loss: \u001b[1m\u001b[32m1.26438\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1166 | loss: 1.26438 - acc: 0.7466 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1167  | total loss: \u001b[1m\u001b[32m1.17212\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1167 | loss: 1.17212 - acc: 0.7720 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1168  | total loss: \u001b[1m\u001b[32m1.41029\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1168 | loss: 1.41029 - acc: 0.7091 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1169  | total loss: \u001b[1m\u001b[32m1.30200\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1169 | loss: 1.30200 - acc: 0.7382 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1170  | total loss: \u001b[1m\u001b[32m1.54332\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1170 | loss: 1.54332 - acc: 0.6715 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1171  | total loss: \u001b[1m\u001b[32m1.42182\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1171 | loss: 1.42182 - acc: 0.7043 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1172  | total loss: \u001b[1m\u001b[32m1.68930\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1172 | loss: 1.68930 - acc: 0.6339 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1173  | total loss: \u001b[1m\u001b[32m1.55471\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1173 | loss: 1.55471 - acc: 0.6705 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1174  | total loss: \u001b[1m\u001b[32m1.43438\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1174 | loss: 1.43438 - acc: 0.7035 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1175  | total loss: \u001b[1m\u001b[32m1.32631\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1175 | loss: 1.32631 - acc: 0.7331 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1176  | total loss: \u001b[1m\u001b[32m1.22879\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1176 | loss: 1.22879 - acc: 0.7598 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1177  | total loss: \u001b[1m\u001b[32m1.14028\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1177 | loss: 1.14028 - acc: 0.7838 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1178  | total loss: \u001b[1m\u001b[32m1.43111\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1178 | loss: 1.43111 - acc: 0.7054 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1179  | total loss: \u001b[1m\u001b[32m1.32154\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1179 | loss: 1.32154 - acc: 0.7349 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1180  | total loss: \u001b[1m\u001b[32m1.56019\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1180 | loss: 1.56019 - acc: 0.6685 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1181  | total loss: \u001b[1m\u001b[32m1.43832\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1181 | loss: 1.43832 - acc: 0.7017 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1182  | total loss: \u001b[1m\u001b[32m1.32892\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1182 | loss: 1.32892 - acc: 0.7315 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1183  | total loss: \u001b[1m\u001b[32m1.23026\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1183 | loss: 1.23026 - acc: 0.7584 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1184  | total loss: \u001b[1m\u001b[32m1.14083\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1184 | loss: 1.14083 - acc: 0.7825 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1185  | total loss: \u001b[1m\u001b[32m1.05933\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1185 | loss: 1.05933 - acc: 0.8043 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1186  | total loss: \u001b[1m\u001b[32m1.33443\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1186 | loss: 1.33443 - acc: 0.7310 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1187  | total loss: \u001b[1m\u001b[32m1.23219\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1187 | loss: 1.23219 - acc: 0.7579 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1188  | total loss: \u001b[1m\u001b[32m1.13970\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1188 | loss: 1.13970 - acc: 0.7821 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1189  | total loss: \u001b[1m\u001b[32m1.05565\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1189 | loss: 1.05565 - acc: 0.8039 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1190  | total loss: \u001b[1m\u001b[32m1.35821\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1190 | loss: 1.35821 - acc: 0.7235 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1191  | total loss: \u001b[1m\u001b[32m1.25145\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1191 | loss: 1.25145 - acc: 0.7512 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1192  | total loss: \u001b[1m\u001b[32m1.15521\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1192 | loss: 1.15521 - acc: 0.7760 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1193  | total loss: \u001b[1m\u001b[32m1.06810\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1193 | loss: 1.06810 - acc: 0.7984 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1194  | total loss: \u001b[1m\u001b[32m1.32110\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1194 | loss: 1.32110 - acc: 0.7329 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1195  | total loss: \u001b[1m\u001b[32m1.21675\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1195 | loss: 1.21675 - acc: 0.7596 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1196  | total loss: \u001b[1m\u001b[32m1.12262\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1196 | loss: 1.12262 - acc: 0.7836 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1197  | total loss: \u001b[1m\u001b[32m1.03739\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1197 | loss: 1.03739 - acc: 0.8053 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1198  | total loss: \u001b[1m\u001b[32m1.31704\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1198 | loss: 1.31704 - acc: 0.7319 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1199  | total loss: \u001b[1m\u001b[32m1.21190\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1199 | loss: 1.21190 - acc: 0.7587 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1200  | total loss: \u001b[1m\u001b[32m1.47277\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1200 | loss: 1.47277 - acc: 0.6900 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1201  | total loss: \u001b[1m\u001b[32m1.35306\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1201 | loss: 1.35306 - acc: 0.7210 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1202  | total loss: \u001b[1m\u001b[32m1.24594\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1202 | loss: 1.24594 - acc: 0.7489 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1203  | total loss: \u001b[1m\u001b[32m1.14976\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1203 | loss: 1.14976 - acc: 0.7740 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1204  | total loss: \u001b[1m\u001b[32m1.06305\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1204 | loss: 1.06305 - acc: 0.7966 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1205  | total loss: \u001b[1m\u001b[32m0.98453\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1205 | loss: 0.98453 - acc: 0.8169 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1206  | total loss: \u001b[1m\u001b[32m0.91310\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1206 | loss: 0.91310 - acc: 0.8352 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1207  | total loss: \u001b[1m\u001b[32m0.84781\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1207 | loss: 0.84781 - acc: 0.8517 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1208  | total loss: \u001b[1m\u001b[32m1.18281\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1208 | loss: 1.18281 - acc: 0.7665 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1209  | total loss: \u001b[1m\u001b[32m1.08943\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1209 | loss: 1.08943 - acc: 0.7899 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1210  | total loss: \u001b[1m\u001b[32m1.38360\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1210 | loss: 1.38360 - acc: 0.7109 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1211  | total loss: \u001b[1m\u001b[32m1.27078\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1211 | loss: 1.27078 - acc: 0.7398 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1212  | total loss: \u001b[1m\u001b[32m1.16976\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1212 | loss: 1.16976 - acc: 0.7658 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1213  | total loss: \u001b[1m\u001b[32m1.07901\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1213 | loss: 1.07901 - acc: 0.7892 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1214  | total loss: \u001b[1m\u001b[32m1.40570\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1214 | loss: 1.40570 - acc: 0.7103 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1215  | total loss: \u001b[1m\u001b[32m1.29224\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1215 | loss: 1.29224 - acc: 0.7393 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1216  | total loss: \u001b[1m\u001b[32m1.46408\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1216 | loss: 1.46408 - acc: 0.6939 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1217  | total loss: \u001b[1m\u001b[32m1.34672\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1217 | loss: 1.34672 - acc: 0.7245 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1218  | total loss: \u001b[1m\u001b[32m1.54758\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1218 | loss: 1.54758 - acc: 0.6664 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1219  | total loss: \u001b[1m\u001b[32m1.42454\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1219 | loss: 1.42454 - acc: 0.6997 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1220  | total loss: \u001b[1m\u001b[32m1.31508\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1220 | loss: 1.31508 - acc: 0.7298 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1221  | total loss: \u001b[1m\u001b[32m1.21731\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1221 | loss: 1.21731 - acc: 0.7568 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1222  | total loss: \u001b[1m\u001b[32m1.12952\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1222 | loss: 1.12952 - acc: 0.7811 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1223  | total loss: \u001b[1m\u001b[32m1.05022\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1223 | loss: 1.05022 - acc: 0.8030 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1224  | total loss: \u001b[1m\u001b[32m0.97812\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1224 | loss: 0.97812 - acc: 0.8227 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1225  | total loss: \u001b[1m\u001b[32m0.91212\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1225 | loss: 0.91212 - acc: 0.8404 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1226  | total loss: \u001b[1m\u001b[32m1.24420\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1226 | loss: 1.24420 - acc: 0.7564 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1227  | total loss: \u001b[1m\u001b[32m1.15023\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1227 | loss: 1.15023 - acc: 0.7807 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1228  | total loss: \u001b[1m\u001b[32m1.06529\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1228 | loss: 1.06529 - acc: 0.8027 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1229  | total loss: \u001b[1m\u001b[32m0.98811\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1229 | loss: 0.98811 - acc: 0.8224 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1230  | total loss: \u001b[1m\u001b[32m0.91761\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1230 | loss: 0.91761 - acc: 0.8402 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1231  | total loss: \u001b[1m\u001b[32m0.85287\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1231 | loss: 0.85287 - acc: 0.8561 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1232  | total loss: \u001b[1m\u001b[32m0.79315\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1232 | loss: 0.79315 - acc: 0.8705 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1233  | total loss: \u001b[1m\u001b[32m0.73783\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1233 | loss: 0.73783 - acc: 0.8835 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1234  | total loss: \u001b[1m\u001b[32m1.07082\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1234 | loss: 1.07082 - acc: 0.8023 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1235  | total loss: \u001b[1m\u001b[32m0.98563\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1235 | loss: 0.98563 - acc: 0.8220 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1236  | total loss: \u001b[1m\u001b[32m0.90831\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1236 | loss: 0.90831 - acc: 0.8398 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1237  | total loss: \u001b[1m\u001b[32m0.83794\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1237 | loss: 0.83794 - acc: 0.8559 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1238  | total loss: \u001b[1m\u001b[32m1.13661\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1238 | loss: 1.13661 - acc: 0.7774 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1239  | total loss: \u001b[1m\u001b[32m1.04266\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1239 | loss: 1.04266 - acc: 0.7997 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1240  | total loss: \u001b[1m\u001b[32m1.38455\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1240 | loss: 1.38455 - acc: 0.7197 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1241  | total loss: \u001b[1m\u001b[32m1.26673\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1241 | loss: 1.26673 - acc: 0.7477 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1242  | total loss: \u001b[1m\u001b[32m1.53134\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1242 | loss: 1.53134 - acc: 0.6872 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1243  | total loss: \u001b[1m\u001b[32m1.40118\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1243 | loss: 1.40118 - acc: 0.7185 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1244  | total loss: \u001b[1m\u001b[32m1.66230\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1244 | loss: 1.66230 - acc: 0.6538 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1245  | total loss: \u001b[1m\u001b[32m1.52277\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1245 | loss: 1.52277 - acc: 0.6884 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1246  | total loss: \u001b[1m\u001b[32m1.79176\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1246 | loss: 1.79176 - acc: 0.6196 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1247  | total loss: \u001b[1m\u001b[32m1.64442\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1247 | loss: 1.64442 - acc: 0.6576 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1248  | total loss: \u001b[1m\u001b[32m1.86025\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1248 | loss: 1.86025 - acc: 0.5990 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1249  | total loss: \u001b[1m\u001b[32m1.71230\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1249 | loss: 1.71230 - acc: 0.6391 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1250  | total loss: \u001b[1m\u001b[32m1.88467\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1250 | loss: 1.88467 - acc: 0.5895 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1251  | total loss: \u001b[1m\u001b[32m1.74092\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1251 | loss: 1.74092 - acc: 0.6305 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1252  | total loss: \u001b[1m\u001b[32m1.92294\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1252 | loss: 1.92294 - acc: 0.5675 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1253  | total loss: \u001b[1m\u001b[32m1.78209\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1253 | loss: 1.78209 - acc: 0.6107 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1254  | total loss: \u001b[1m\u001b[32m1.96471\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1254 | loss: 1.96471 - acc: 0.5568 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1255  | total loss: \u001b[1m\u001b[32m1.82631\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1255 | loss: 1.82631 - acc: 0.6011 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1256  | total loss: \u001b[1m\u001b[32m1.98308\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1256 | loss: 1.98308 - acc: 0.5410 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1257  | total loss: \u001b[1m\u001b[32m1.84911\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1257 | loss: 1.84911 - acc: 0.5869 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1258  | total loss: \u001b[1m\u001b[32m2.01261\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1258 | loss: 2.01261 - acc: 0.5354 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1259  | total loss: \u001b[1m\u001b[32m1.88120\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1259 | loss: 1.88120 - acc: 0.5818 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1260  | total loss: \u001b[1m\u001b[32m1.76466\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1260 | loss: 1.76466 - acc: 0.6236 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1261  | total loss: \u001b[1m\u001b[32m1.66018\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1261 | loss: 1.66018 - acc: 0.6613 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1262  | total loss: \u001b[1m\u001b[32m1.56537\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1262 | loss: 1.56537 - acc: 0.6952 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1263  | total loss: \u001b[1m\u001b[32m1.47823\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1263 | loss: 1.47823 - acc: 0.7256 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1264  | total loss: \u001b[1m\u001b[32m1.39712\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1264 | loss: 1.39712 - acc: 0.7531 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1265  | total loss: \u001b[1m\u001b[32m1.32073\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1265 | loss: 1.32073 - acc: 0.7778 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1266  | total loss: \u001b[1m\u001b[32m1.24806\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1266 | loss: 1.24806 - acc: 0.8000 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1267  | total loss: \u001b[1m\u001b[32m1.17839\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1267 | loss: 1.17839 - acc: 0.8200 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1268  | total loss: \u001b[1m\u001b[32m1.39434\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1268 | loss: 1.39434 - acc: 0.7451 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1269  | total loss: \u001b[1m\u001b[32m1.30268\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1269 | loss: 1.30268 - acc: 0.7706 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1270  | total loss: \u001b[1m\u001b[32m1.52913\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1270 | loss: 1.52913 - acc: 0.7007 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1271  | total loss: \u001b[1m\u001b[32m1.41924\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1271 | loss: 1.41924 - acc: 0.7306 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1272  | total loss: \u001b[1m\u001b[32m1.65366\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1272 | loss: 1.65366 - acc: 0.6576 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1273  | total loss: \u001b[1m\u001b[32m1.52891\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1273 | loss: 1.52891 - acc: 0.6918 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1274  | total loss: \u001b[1m\u001b[32m1.75493\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1274 | loss: 1.75493 - acc: 0.6226 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1275  | total loss: \u001b[1m\u001b[32m1.61962\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1275 | loss: 1.61962 - acc: 0.6604 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1276  | total loss: \u001b[1m\u001b[32m1.49772\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1276 | loss: 1.49772 - acc: 0.6943 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1277  | total loss: \u001b[1m\u001b[32m1.38737\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1277 | loss: 1.38737 - acc: 0.7249 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1278  | total loss: \u001b[1m\u001b[32m1.28696\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1278 | loss: 1.28696 - acc: 0.7524 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1279  | total loss: \u001b[1m\u001b[32m1.19512\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1279 | loss: 1.19512 - acc: 0.7772 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1280  | total loss: \u001b[1m\u001b[32m1.47512\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1280 | loss: 1.47512 - acc: 0.6995 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1281  | total loss: \u001b[1m\u001b[32m1.36232\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1281 | loss: 1.36232 - acc: 0.7295 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1282  | total loss: \u001b[1m\u001b[32m1.58097\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1282 | loss: 1.58097 - acc: 0.6708 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1283  | total loss: \u001b[1m\u001b[32m1.45719\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1283 | loss: 1.45719 - acc: 0.7038 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1284  | total loss: \u001b[1m\u001b[32m1.65281\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1284 | loss: 1.65281 - acc: 0.6405 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1285  | total loss: \u001b[1m\u001b[32m1.52278\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1285 | loss: 1.52278 - acc: 0.6765 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1286  | total loss: \u001b[1m\u001b[32m1.70351\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1286 | loss: 1.70351 - acc: 0.6231 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1287  | total loss: \u001b[1m\u001b[32m1.57034\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1287 | loss: 1.57034 - acc: 0.6608 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1288  | total loss: \u001b[1m\u001b[32m1.75188\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1288 | loss: 1.75188 - acc: 0.6019 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1289  | total loss: \u001b[1m\u001b[32m1.61642\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1289 | loss: 1.61642 - acc: 0.6417 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1290  | total loss: \u001b[1m\u001b[32m1.76583\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1290 | loss: 1.76583 - acc: 0.5918 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1291  | total loss: \u001b[1m\u001b[32m1.63185\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1291 | loss: 1.63185 - acc: 0.6326 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1292  | total loss: \u001b[1m\u001b[32m1.51233\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1292 | loss: 1.51233 - acc: 0.6694 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1293  | total loss: \u001b[1m\u001b[32m1.40504\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1293 | loss: 1.40504 - acc: 0.7024 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1294  | total loss: \u001b[1m\u001b[32m1.65447\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1294 | loss: 1.65447 - acc: 0.6322 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1295  | total loss: \u001b[1m\u001b[32m1.53339\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1295 | loss: 1.53339 - acc: 0.6690 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1296  | total loss: \u001b[1m\u001b[32m1.74011\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1296 | loss: 1.74011 - acc: 0.6092 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1297  | total loss: \u001b[1m\u001b[32m1.61178\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1297 | loss: 1.61178 - acc: 0.6483 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1298  | total loss: \u001b[1m\u001b[32m1.80697\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1298 | loss: 1.80697 - acc: 0.5906 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1299  | total loss: \u001b[1m\u001b[32m1.67393\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1299 | loss: 1.67393 - acc: 0.6315 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1300  | total loss: \u001b[1m\u001b[32m1.85736\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1300 | loss: 1.85736 - acc: 0.5755 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1301  | total loss: \u001b[1m\u001b[32m1.72177\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1301 | loss: 1.72177 - acc: 0.6180 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1302  | total loss: \u001b[1m\u001b[32m1.84862\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1302 | loss: 1.84862 - acc: 0.5776 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1303  | total loss: \u001b[1m\u001b[32m1.71643\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1303 | loss: 1.71643 - acc: 0.6198 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1304  | total loss: \u001b[1m\u001b[32m1.92636\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1304 | loss: 1.92636 - acc: 0.5579 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1305  | total loss: \u001b[1m\u001b[32m1.78896\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1305 | loss: 1.78896 - acc: 0.6021 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1306  | total loss: \u001b[1m\u001b[32m1.96499\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1306 | loss: 1.96499 - acc: 0.5419 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1307  | total loss: \u001b[1m\u001b[32m1.82657\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1307 | loss: 1.82657 - acc: 0.5877 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1308  | total loss: \u001b[1m\u001b[32m1.99069\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1308 | loss: 1.99069 - acc: 0.5289 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1309  | total loss: \u001b[1m\u001b[32m1.85254\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1309 | loss: 1.85254 - acc: 0.5760 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1310  | total loss: \u001b[1m\u001b[32m2.00781\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1310 | loss: 2.00781 - acc: 0.5184 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1311  | total loss: \u001b[1m\u001b[32m1.87068\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1311 | loss: 1.87068 - acc: 0.5666 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1312  | total loss: \u001b[1m\u001b[32m1.96821\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1312 | loss: 1.96821 - acc: 0.5242 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1313  | total loss: \u001b[1m\u001b[32m1.83724\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1313 | loss: 1.83724 - acc: 0.5718 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1314  | total loss: \u001b[1m\u001b[32m1.71972\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1314 | loss: 1.71972 - acc: 0.6146 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1315  | total loss: \u001b[1m\u001b[32m1.61331\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1315 | loss: 1.61331 - acc: 0.6531 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1316  | total loss: \u001b[1m\u001b[32m1.75378\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1316 | loss: 1.75378 - acc: 0.5950 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1317  | total loss: \u001b[1m\u001b[32m1.64204\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1317 | loss: 1.64204 - acc: 0.6355 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1318  | total loss: \u001b[1m\u001b[32m1.54019\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1318 | loss: 1.54019 - acc: 0.6719 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1319  | total loss: \u001b[1m\u001b[32m1.44652\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1319 | loss: 1.44652 - acc: 0.7047 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1320  | total loss: \u001b[1m\u001b[32m1.35959\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1320 | loss: 1.35959 - acc: 0.7343 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1321  | total loss: \u001b[1m\u001b[32m1.27826\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1321 | loss: 1.27826 - acc: 0.7608 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1322  | total loss: \u001b[1m\u001b[32m1.48310\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1322 | loss: 1.48310 - acc: 0.6919 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1323  | total loss: \u001b[1m\u001b[32m1.38397\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1323 | loss: 1.38397 - acc: 0.7227 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1324  | total loss: \u001b[1m\u001b[32m1.61130\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1324 | loss: 1.61130 - acc: 0.6504 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1325  | total loss: \u001b[1m\u001b[32m1.49607\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1325 | loss: 1.49607 - acc: 0.6854 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1326  | total loss: \u001b[1m\u001b[32m1.71422\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1326 | loss: 1.71422 - acc: 0.6240 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1327  | total loss: \u001b[1m\u001b[32m1.58719\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1327 | loss: 1.58719 - acc: 0.6616 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1328  | total loss: \u001b[1m\u001b[32m1.75224\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1328 | loss: 1.75224 - acc: 0.6169 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1329  | total loss: \u001b[1m\u001b[32m1.62100\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1329 | loss: 1.62100 - acc: 0.6552 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1330  | total loss: \u001b[1m\u001b[32m1.50255\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1330 | loss: 1.50255 - acc: 0.6897 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1331  | total loss: \u001b[1m\u001b[32m1.39506\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1331 | loss: 1.39506 - acc: 0.7207 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1332  | total loss: \u001b[1m\u001b[32m1.60072\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1332 | loss: 1.60072 - acc: 0.6486 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1333  | total loss: \u001b[1m\u001b[32m1.48198\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1333 | loss: 1.48198 - acc: 0.6838 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1334  | total loss: \u001b[1m\u001b[32m1.37448\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1334 | loss: 1.37448 - acc: 0.7154 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1335  | total loss: \u001b[1m\u001b[32m1.27663\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1335 | loss: 1.27663 - acc: 0.7438 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1336  | total loss: \u001b[1m\u001b[32m1.18707\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1336 | loss: 1.18707 - acc: 0.7695 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1337  | total loss: \u001b[1m\u001b[32m1.10465\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1337 | loss: 1.10465 - acc: 0.7925 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1338  | total loss: \u001b[1m\u001b[32m1.38391\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1338 | loss: 1.38391 - acc: 0.7133 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1339  | total loss: \u001b[1m\u001b[32m1.27914\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1339 | loss: 1.27914 - acc: 0.7419 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1340  | total loss: \u001b[1m\u001b[32m1.18386\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1340 | loss: 1.18386 - acc: 0.7677 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1341  | total loss: \u001b[1m\u001b[32m1.09683\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1341 | loss: 1.09683 - acc: 0.7910 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1342  | total loss: \u001b[1m\u001b[32m1.01699\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1342 | loss: 1.01699 - acc: 0.8119 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1343  | total loss: \u001b[1m\u001b[32m0.94344\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1343 | loss: 0.94344 - acc: 0.8307 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1344  | total loss: \u001b[1m\u001b[32m0.87545\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1344 | loss: 0.87545 - acc: 0.8476 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1345  | total loss: \u001b[1m\u001b[32m0.81241\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1345 | loss: 0.81241 - acc: 0.8629 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1346  | total loss: \u001b[1m\u001b[32m1.15930\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1346 | loss: 1.15930 - acc: 0.7766 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1347  | total loss: \u001b[1m\u001b[32m1.06543\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1347 | loss: 1.06543 - acc: 0.7989 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1348  | total loss: \u001b[1m\u001b[32m0.98019\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1348 | loss: 0.98019 - acc: 0.8190 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1349  | total loss: \u001b[1m\u001b[32m0.90259\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1349 | loss: 0.90259 - acc: 0.8371 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1350  | total loss: \u001b[1m\u001b[32m0.83178\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1350 | loss: 0.83178 - acc: 0.8534 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1351  | total loss: \u001b[1m\u001b[32m0.76700\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1351 | loss: 0.76700 - acc: 0.8681 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1352  | total loss: \u001b[1m\u001b[32m1.08960\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1352 | loss: 1.08960 - acc: 0.7955 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1353  | total loss: \u001b[1m\u001b[32m0.99780\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1353 | loss: 0.99780 - acc: 0.8160 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1354  | total loss: \u001b[1m\u001b[32m0.91490\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1354 | loss: 0.91490 - acc: 0.8344 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1355  | total loss: \u001b[1m\u001b[32m0.83988\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1355 | loss: 0.83988 - acc: 0.8510 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1356  | total loss: \u001b[1m\u001b[32m0.77185\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1356 | loss: 0.77185 - acc: 0.8659 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1357  | total loss: \u001b[1m\u001b[32m0.71004\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1357 | loss: 0.71004 - acc: 0.8793 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1358  | total loss: \u001b[1m\u001b[32m1.06785\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1358 | loss: 1.06785 - acc: 0.7985 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1359  | total loss: \u001b[1m\u001b[32m0.97596\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1359 | loss: 0.97596 - acc: 0.8186 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1360  | total loss: \u001b[1m\u001b[32m1.28463\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1360 | loss: 1.28463 - acc: 0.7511 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1361  | total loss: \u001b[1m\u001b[32m1.17186\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1361 | loss: 1.17186 - acc: 0.7760 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1362  | total loss: \u001b[1m\u001b[32m1.07095\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1362 | loss: 1.07095 - acc: 0.7984 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1363  | total loss: \u001b[1m\u001b[32m0.98055\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1363 | loss: 0.98055 - acc: 0.8185 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1364  | total loss: \u001b[1m\u001b[32m1.28164\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1364 | loss: 1.28164 - acc: 0.7438 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1365  | total loss: \u001b[1m\u001b[32m1.17136\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1365 | loss: 1.17136 - acc: 0.7694 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1366  | total loss: \u001b[1m\u001b[32m1.46263\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1366 | loss: 1.46263 - acc: 0.7068 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1367  | total loss: \u001b[1m\u001b[32m1.33657\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1367 | loss: 1.33657 - acc: 0.7361 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1368  | total loss: \u001b[1m\u001b[32m1.56987\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1368 | loss: 1.56987 - acc: 0.6768 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1369  | total loss: \u001b[1m\u001b[32m1.43654\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1369 | loss: 1.43654 - acc: 0.7091 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1370  | total loss: \u001b[1m\u001b[32m1.31835\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1370 | loss: 1.31835 - acc: 0.7382 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1371  | total loss: \u001b[1m\u001b[32m1.21336\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1371 | loss: 1.21336 - acc: 0.7644 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1372  | total loss: \u001b[1m\u001b[32m1.50413\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1372 | loss: 1.50413 - acc: 0.6951 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1373  | total loss: \u001b[1m\u001b[32m1.38347\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1373 | loss: 1.38347 - acc: 0.7256 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1374  | total loss: \u001b[1m\u001b[32m1.62744\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1374 | loss: 1.62744 - acc: 0.6602 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1375  | total loss: \u001b[1m\u001b[32m1.49820\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1375 | loss: 1.49820 - acc: 0.6941 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1376  | total loss: \u001b[1m\u001b[32m1.38359\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1376 | loss: 1.38359 - acc: 0.7247 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1377  | total loss: \u001b[1m\u001b[32m1.28150\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1377 | loss: 1.28150 - acc: 0.7523 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1378  | total loss: \u001b[1m\u001b[32m1.52421\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1378 | loss: 1.52421 - acc: 0.6770 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1379  | total loss: \u001b[1m\u001b[32m1.41015\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1379 | loss: 1.41015 - acc: 0.7093 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1380  | total loss: \u001b[1m\u001b[32m1.62242\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1380 | loss: 1.62242 - acc: 0.6527 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1381  | total loss: \u001b[1m\u001b[32m1.50138\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1381 | loss: 1.50138 - acc: 0.6874 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1382  | total loss: \u001b[1m\u001b[32m1.39356\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1382 | loss: 1.39356 - acc: 0.7187 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1383  | total loss: \u001b[1m\u001b[32m1.29692\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1383 | loss: 1.29692 - acc: 0.7468 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1384  | total loss: \u001b[1m\u001b[32m1.50780\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1384 | loss: 1.50780 - acc: 0.6864 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1385  | total loss: \u001b[1m\u001b[32m1.40019\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1385 | loss: 1.40019 - acc: 0.7178 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1386  | total loss: \u001b[1m\u001b[32m1.60723\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1386 | loss: 1.60723 - acc: 0.6531 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1387  | total loss: \u001b[1m\u001b[32m1.49104\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1387 | loss: 1.49104 - acc: 0.6878 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1388  | total loss: \u001b[1m\u001b[32m1.70857\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1388 | loss: 1.70857 - acc: 0.6190 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1389  | total loss: \u001b[1m\u001b[32m1.58451\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1389 | loss: 1.58451 - acc: 0.6571 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1390  | total loss: \u001b[1m\u001b[32m1.47372\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1390 | loss: 1.47372 - acc: 0.6914 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1391  | total loss: \u001b[1m\u001b[32m1.37409\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1391 | loss: 1.37409 - acc: 0.7223 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1392  | total loss: \u001b[1m\u001b[32m1.54417\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1392 | loss: 1.54417 - acc: 0.6715 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1393  | total loss: \u001b[1m\u001b[32m1.43715\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1393 | loss: 1.43715 - acc: 0.7043 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1394  | total loss: \u001b[1m\u001b[32m1.34042\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1394 | loss: 1.34042 - acc: 0.7339 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1395  | total loss: \u001b[1m\u001b[32m1.25231\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1395 | loss: 1.25231 - acc: 0.7605 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1396  | total loss: \u001b[1m\u001b[32m1.48922\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1396 | loss: 1.48922 - acc: 0.6845 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1397  | total loss: \u001b[1m\u001b[32m1.38439\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1397 | loss: 1.38439 - acc: 0.7160 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1398  | total loss: \u001b[1m\u001b[32m1.55936\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1398 | loss: 1.55936 - acc: 0.6658 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1399  | total loss: \u001b[1m\u001b[32m1.44664\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1399 | loss: 1.44664 - acc: 0.6993 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1400  | total loss: \u001b[1m\u001b[32m1.64550\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1400 | loss: 1.64550 - acc: 0.6293 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1401  | total loss: \u001b[1m\u001b[32m1.52421\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1401 | loss: 1.52421 - acc: 0.6664 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1402  | total loss: \u001b[1m\u001b[32m1.76591\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1402 | loss: 1.76591 - acc: 0.5998 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1403  | total loss: \u001b[1m\u001b[32m1.63378\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1403 | loss: 1.63378 - acc: 0.6398 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1404  | total loss: \u001b[1m\u001b[32m1.84926\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1404 | loss: 1.84926 - acc: 0.5758 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1405  | total loss: \u001b[1m\u001b[32m1.71100\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1405 | loss: 1.71100 - acc: 0.6182 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1406  | total loss: \u001b[1m\u001b[32m1.58749\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1406 | loss: 1.58749 - acc: 0.6564 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1407  | total loss: \u001b[1m\u001b[32m1.47649\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1407 | loss: 1.47649 - acc: 0.6908 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1408  | total loss: \u001b[1m\u001b[32m1.69902\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1408 | loss: 1.69902 - acc: 0.6217 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1409  | total loss: \u001b[1m\u001b[32m1.57706\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1409 | loss: 1.57706 - acc: 0.6595 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1410  | total loss: \u001b[1m\u001b[32m1.78924\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1410 | loss: 1.78924 - acc: 0.6007 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1411  | total loss: \u001b[1m\u001b[32m1.65936\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1411 | loss: 1.65936 - acc: 0.6406 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1412  | total loss: \u001b[1m\u001b[32m1.54279\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1412 | loss: 1.54279 - acc: 0.6766 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1413  | total loss: \u001b[1m\u001b[32m1.43749\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1413 | loss: 1.43749 - acc: 0.7089 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1414  | total loss: \u001b[1m\u001b[32m1.67236\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1414 | loss: 1.67236 - acc: 0.6380 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1415  | total loss: \u001b[1m\u001b[32m1.55344\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1415 | loss: 1.55344 - acc: 0.6742 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1416  | total loss: \u001b[1m\u001b[32m1.71590\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1416 | loss: 1.71590 - acc: 0.6282 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1417  | total loss: \u001b[1m\u001b[32m1.59288\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1417 | loss: 1.59288 - acc: 0.6654 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1418  | total loss: \u001b[1m\u001b[32m1.79262\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1418 | loss: 1.79262 - acc: 0.5989 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1419  | total loss: \u001b[1m\u001b[32m1.66285\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1419 | loss: 1.66285 - acc: 0.6390 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1420  | total loss: \u001b[1m\u001b[32m1.85254\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1420 | loss: 1.85254 - acc: 0.5751 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1421  | total loss: \u001b[1m\u001b[32m1.71846\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1421 | loss: 1.71846 - acc: 0.6176 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1422  | total loss: \u001b[1m\u001b[32m1.86814\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1422 | loss: 1.86814 - acc: 0.5701 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1423  | total loss: \u001b[1m\u001b[32m1.73433\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1423 | loss: 1.73433 - acc: 0.6131 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1424  | total loss: \u001b[1m\u001b[32m1.93766\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1424 | loss: 1.93766 - acc: 0.5518 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1425  | total loss: \u001b[1m\u001b[32m1.79894\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1425 | loss: 1.79894 - acc: 0.5966 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1426  | total loss: \u001b[1m\u001b[32m1.92619\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1426 | loss: 1.92619 - acc: 0.5512 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1427  | total loss: \u001b[1m\u001b[32m1.79080\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1427 | loss: 1.79080 - acc: 0.5961 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1428  | total loss: \u001b[1m\u001b[32m1.89540\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1428 | loss: 1.89540 - acc: 0.5579 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1429  | total loss: \u001b[1m\u001b[32m1.76491\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1429 | loss: 1.76491 - acc: 0.6021 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1430  | total loss: \u001b[1m\u001b[32m1.93360\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1430 | loss: 1.93360 - acc: 0.5419 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1431  | total loss: \u001b[1m\u001b[32m1.80102\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1431 | loss: 1.80102 - acc: 0.5877 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1432  | total loss: \u001b[1m\u001b[32m1.87092\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1432 | loss: 1.87092 - acc: 0.5575 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1433  | total loss: \u001b[1m\u001b[32m1.74594\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1433 | loss: 1.74594 - acc: 0.6018 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1434  | total loss: \u001b[1m\u001b[32m1.91375\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1434 | loss: 1.91375 - acc: 0.5416 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1435  | total loss: \u001b[1m\u001b[32m1.78559\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1435 | loss: 1.78559 - acc: 0.5874 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1436  | total loss: \u001b[1m\u001b[32m1.91102\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1436 | loss: 1.91102 - acc: 0.5287 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1437  | total loss: \u001b[1m\u001b[32m1.78442\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1437 | loss: 1.78442 - acc: 0.5758 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1438  | total loss: \u001b[1m\u001b[32m1.92773\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1438 | loss: 1.92773 - acc: 0.5254 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1439  | total loss: \u001b[1m\u001b[32m1.80066\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1439 | loss: 1.80066 - acc: 0.5728 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1440  | total loss: \u001b[1m\u001b[32m1.93178\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1440 | loss: 1.93178 - acc: 0.5227 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1441  | total loss: \u001b[1m\u001b[32m1.80540\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1441 | loss: 1.80540 - acc: 0.5704 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1442  | total loss: \u001b[1m\u001b[32m1.69163\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1442 | loss: 1.69163 - acc: 0.6134 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1443  | total loss: \u001b[1m\u001b[32m1.58825\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1443 | loss: 1.58825 - acc: 0.6521 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1444  | total loss: \u001b[1m\u001b[32m1.49338\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1444 | loss: 1.49338 - acc: 0.6868 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1445  | total loss: \u001b[1m\u001b[32m1.40544\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1445 | loss: 1.40544 - acc: 0.7182 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1446  | total loss: \u001b[1m\u001b[32m1.32316\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1446 | loss: 1.32316 - acc: 0.7463 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1447  | total loss: \u001b[1m\u001b[32m1.24554\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1447 | loss: 1.24554 - acc: 0.7717 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1448  | total loss: \u001b[1m\u001b[32m1.46138\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1448 | loss: 1.46138 - acc: 0.6945 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1449  | total loss: \u001b[1m\u001b[32m1.36384\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1449 | loss: 1.36384 - acc: 0.7251 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1450  | total loss: \u001b[1m\u001b[32m1.54676\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1450 | loss: 1.54676 - acc: 0.6669 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1451  | total loss: \u001b[1m\u001b[32m1.43675\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1451 | loss: 1.43675 - acc: 0.7002 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1452  | total loss: \u001b[1m\u001b[32m1.57611\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1452 | loss: 1.57611 - acc: 0.6516 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1453  | total loss: \u001b[1m\u001b[32m1.46066\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1453 | loss: 1.46066 - acc: 0.6864 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1454  | total loss: \u001b[1m\u001b[32m1.35555\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1454 | loss: 1.35555 - acc: 0.7178 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1455  | total loss: \u001b[1m\u001b[32m1.25933\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1455 | loss: 1.25933 - acc: 0.7460 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1456  | total loss: \u001b[1m\u001b[32m1.47363\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1456 | loss: 1.47363 - acc: 0.6857 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1457  | total loss: \u001b[1m\u001b[32m1.36298\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1457 | loss: 1.36298 - acc: 0.7171 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1458  | total loss: \u001b[1m\u001b[32m1.59427\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1458 | loss: 1.59427 - acc: 0.6526 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1459  | total loss: \u001b[1m\u001b[32m1.47063\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1459 | loss: 1.47063 - acc: 0.6873 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1460  | total loss: \u001b[1m\u001b[32m1.35906\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1460 | loss: 1.35906 - acc: 0.7186 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1461  | total loss: \u001b[1m\u001b[32m1.25791\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1461 | loss: 1.25791 - acc: 0.7467 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1462  | total loss: \u001b[1m\u001b[32m1.45353\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1462 | loss: 1.45353 - acc: 0.6935 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1463  | total loss: \u001b[1m\u001b[32m1.34190\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1463 | loss: 1.34190 - acc: 0.7241 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1464  | total loss: \u001b[1m\u001b[32m1.59436\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1464 | loss: 1.59436 - acc: 0.6589 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1465  | total loss: \u001b[1m\u001b[32m1.46903\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1465 | loss: 1.46903 - acc: 0.6930 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1466  | total loss: \u001b[1m\u001b[32m1.64756\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1466 | loss: 1.64756 - acc: 0.6380 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1467  | total loss: \u001b[1m\u001b[32m1.51826\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1467 | loss: 1.51826 - acc: 0.6742 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1468  | total loss: \u001b[1m\u001b[32m1.75860\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1468 | loss: 1.75860 - acc: 0.6139 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1469  | total loss: \u001b[1m\u001b[32m1.62031\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1469 | loss: 1.62031 - acc: 0.6525 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1470  | total loss: \u001b[1m\u001b[32m1.77733\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1470 | loss: 1.77733 - acc: 0.5944 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1471  | total loss: \u001b[1m\u001b[32m1.63994\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1471 | loss: 1.63994 - acc: 0.6350 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1472  | total loss: \u001b[1m\u001b[32m1.83054\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1472 | loss: 1.83054 - acc: 0.5857 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1473  | total loss: \u001b[1m\u001b[32m1.69087\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1473 | loss: 1.69087 - acc: 0.6272 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1474  | total loss: \u001b[1m\u001b[32m1.88046\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1474 | loss: 1.88046 - acc: 0.5716 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1475  | total loss: \u001b[1m\u001b[32m1.73916\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1475 | loss: 1.73916 - acc: 0.6144 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1476  | total loss: \u001b[1m\u001b[32m1.88750\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1476 | loss: 1.88750 - acc: 0.5673 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1477  | total loss: \u001b[1m\u001b[32m1.74895\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1477 | loss: 1.74895 - acc: 0.6105 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1478  | total loss: \u001b[1m\u001b[32m1.88962\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1478 | loss: 1.88962 - acc: 0.5566 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1479  | total loss: \u001b[1m\u001b[32m1.75416\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1479 | loss: 1.75416 - acc: 0.6010 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1480  | total loss: \u001b[1m\u001b[32m1.91239\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1480 | loss: 1.91239 - acc: 0.5480 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1481  | total loss: \u001b[1m\u001b[32m1.77797\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1481 | loss: 1.77797 - acc: 0.5932 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1482  | total loss: \u001b[1m\u001b[32m1.65817\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1482 | loss: 1.65817 - acc: 0.6339 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1483  | total loss: \u001b[1m\u001b[32m1.55058\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1483 | loss: 1.55058 - acc: 0.6705 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1484  | total loss: \u001b[1m\u001b[32m1.45313\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1484 | loss: 1.45313 - acc: 0.7035 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1485  | total loss: \u001b[1m\u001b[32m1.36404\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1485 | loss: 1.36404 - acc: 0.7331 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1486  | total loss: \u001b[1m\u001b[32m1.50091\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1486 | loss: 1.50091 - acc: 0.6884 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1487  | total loss: \u001b[1m\u001b[32m1.40383\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1487 | loss: 1.40383 - acc: 0.7195 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1488  | total loss: \u001b[1m\u001b[32m1.64313\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1488 | loss: 1.64313 - acc: 0.6476 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1489  | total loss: \u001b[1m\u001b[32m1.52976\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1489 | loss: 1.52976 - acc: 0.6828 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1490  | total loss: \u001b[1m\u001b[32m1.42678\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1490 | loss: 1.42678 - acc: 0.7145 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1491  | total loss: \u001b[1m\u001b[32m1.33254\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1491 | loss: 1.33254 - acc: 0.7431 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1492  | total loss: \u001b[1m\u001b[32m1.55685\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1492 | loss: 1.55685 - acc: 0.6688 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1493  | total loss: \u001b[1m\u001b[32m1.44694\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1493 | loss: 1.44694 - acc: 0.7019 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1494  | total loss: \u001b[1m\u001b[32m1.67162\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1494 | loss: 1.67162 - acc: 0.6317 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1495  | total loss: \u001b[1m\u001b[32m1.54925\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1495 | loss: 1.54925 - acc: 0.6685 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1496  | total loss: \u001b[1m\u001b[32m1.43870\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1496 | loss: 1.43870 - acc: 0.7017 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1497  | total loss: \u001b[1m\u001b[32m1.33821\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1497 | loss: 1.33821 - acc: 0.7315 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1498  | total loss: \u001b[1m\u001b[32m1.55422\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1498 | loss: 1.55422 - acc: 0.6655 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1499  | total loss: \u001b[1m\u001b[32m1.44056\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1499 | loss: 1.44056 - acc: 0.6990 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1500  | total loss: \u001b[1m\u001b[32m1.60328\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1500 | loss: 1.60328 - acc: 0.6505 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1501  | total loss: \u001b[1m\u001b[32m1.48431\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1501 | loss: 1.48431 - acc: 0.6854 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1502  | total loss: \u001b[1m\u001b[32m1.67382\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1502 | loss: 1.67382 - acc: 0.6240 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1503  | total loss: \u001b[1m\u001b[32m1.54834\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1503 | loss: 1.54834 - acc: 0.6616 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1504  | total loss: \u001b[1m\u001b[32m1.43560\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1504 | loss: 1.43560 - acc: 0.6955 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1505  | total loss: \u001b[1m\u001b[32m1.33372\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1505 | loss: 1.33372 - acc: 0.7259 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1506  | total loss: \u001b[1m\u001b[32m1.55138\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1506 | loss: 1.55138 - acc: 0.6605 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1507  | total loss: \u001b[1m\u001b[32m1.43722\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1507 | loss: 1.43722 - acc: 0.6944 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1508  | total loss: \u001b[1m\u001b[32m1.68102\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1508 | loss: 1.68102 - acc: 0.6250 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1509  | total loss: \u001b[1m\u001b[32m1.55445\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1509 | loss: 1.55445 - acc: 0.6625 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1510  | total loss: \u001b[1m\u001b[32m1.44078\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1510 | loss: 1.44078 - acc: 0.6962 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1511  | total loss: \u001b[1m\u001b[32m1.33810\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1511 | loss: 1.33810 - acc: 0.7266 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1512  | total loss: \u001b[1m\u001b[32m1.56985\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1512 | loss: 1.56985 - acc: 0.6611 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1513  | total loss: \u001b[1m\u001b[32m1.45361\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1513 | loss: 1.45361 - acc: 0.6950 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1514  | total loss: \u001b[1m\u001b[32m1.70713\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1514 | loss: 1.70713 - acc: 0.6255 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1515  | total loss: \u001b[1m\u001b[32m1.57783\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1515 | loss: 1.57783 - acc: 0.6629 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1516  | total loss: \u001b[1m\u001b[32m1.78513\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1516 | loss: 1.78513 - acc: 0.6038 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1517  | total loss: \u001b[1m\u001b[32m1.64982\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1517 | loss: 1.64982 - acc: 0.6434 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1518  | total loss: \u001b[1m\u001b[32m1.83131\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1518 | loss: 1.83131 - acc: 0.5862 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1519  | total loss: \u001b[1m\u001b[32m1.69378\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1519 | loss: 1.69378 - acc: 0.6276 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1520  | total loss: \u001b[1m\u001b[32m1.86878\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1520 | loss: 1.86878 - acc: 0.5791 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1521  | total loss: \u001b[1m\u001b[32m1.73015\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1521 | loss: 1.73015 - acc: 0.6212 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1522  | total loss: \u001b[1m\u001b[32m1.87871\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1522 | loss: 1.87871 - acc: 0.5662 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1523  | total loss: \u001b[1m\u001b[32m1.74171\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1523 | loss: 1.74171 - acc: 0.6096 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1524  | total loss: \u001b[1m\u001b[32m1.88463\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1524 | loss: 1.88463 - acc: 0.5629 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1525  | total loss: \u001b[1m\u001b[32m1.74953\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1525 | loss: 1.74953 - acc: 0.6066 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1526  | total loss: \u001b[1m\u001b[32m1.90132\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1526 | loss: 1.90132 - acc: 0.5603 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1527  | total loss: \u001b[1m\u001b[32m1.76687\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1527 | loss: 1.76687 - acc: 0.6042 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1528  | total loss: \u001b[1m\u001b[32m1.92423\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1528 | loss: 1.92423 - acc: 0.5510 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1529  | total loss: \u001b[1m\u001b[32m1.78974\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1529 | loss: 1.78974 - acc: 0.5959 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1530  | total loss: \u001b[1m\u001b[32m1.95283\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1530 | loss: 1.95283 - acc: 0.5363 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1531  | total loss: \u001b[1m\u001b[32m1.81776\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1531 | loss: 1.81776 - acc: 0.5826 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1532  | total loss: \u001b[1m\u001b[32m1.97091\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1532 | loss: 1.97091 - acc: 0.5315 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1533  | total loss: \u001b[1m\u001b[32m1.83628\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1533 | loss: 1.83628 - acc: 0.5784 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1534  | total loss: \u001b[1m\u001b[32m1.99467\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1534 | loss: 1.99467 - acc: 0.5205 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1535  | total loss: \u001b[1m\u001b[32m1.85990\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1535 | loss: 1.85990 - acc: 0.5685 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1536  | total loss: \u001b[1m\u001b[32m1.73921\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1536 | loss: 1.73921 - acc: 0.6116 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1537  | total loss: \u001b[1m\u001b[32m1.63021\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1537 | loss: 1.63021 - acc: 0.6505 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1538  | total loss: \u001b[1m\u001b[32m1.80109\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1538 | loss: 1.80109 - acc: 0.5926 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1539  | total loss: \u001b[1m\u001b[32m1.68458\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1539 | loss: 1.68458 - acc: 0.6333 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1540  | total loss: \u001b[1m\u001b[32m1.57877\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1540 | loss: 1.57877 - acc: 0.6700 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1541  | total loss: \u001b[1m\u001b[32m1.48182\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1541 | loss: 1.48182 - acc: 0.7030 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1542  | total loss: \u001b[1m\u001b[32m1.67888\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1542 | loss: 1.67888 - acc: 0.6327 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1543  | total loss: \u001b[1m\u001b[32m1.56846\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1543 | loss: 1.56846 - acc: 0.6694 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1544  | total loss: \u001b[1m\u001b[32m1.75798\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1544 | loss: 1.75798 - acc: 0.6096 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1545  | total loss: \u001b[1m\u001b[32m1.63736\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1545 | loss: 1.63736 - acc: 0.6487 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1546  | total loss: \u001b[1m\u001b[32m1.78018\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1546 | loss: 1.78018 - acc: 0.5909 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1547  | total loss: \u001b[1m\u001b[32m1.65604\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1547 | loss: 1.65604 - acc: 0.6318 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1548  | total loss: \u001b[1m\u001b[32m1.54352\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1548 | loss: 1.54352 - acc: 0.6687 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1549  | total loss: \u001b[1m\u001b[32m1.44079\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1549 | loss: 1.44079 - acc: 0.7018 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1550  | total loss: \u001b[1m\u001b[32m1.34634\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1550 | loss: 1.34634 - acc: 0.7316 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1551  | total loss: \u001b[1m\u001b[32m1.25889\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1551 | loss: 1.25889 - acc: 0.7584 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1552  | total loss: \u001b[1m\u001b[32m1.17742\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1552 | loss: 1.17742 - acc: 0.7826 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1553  | total loss: \u001b[1m\u001b[32m1.10109\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1553 | loss: 1.10109 - acc: 0.8043 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1554  | total loss: \u001b[1m\u001b[32m1.02926\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1554 | loss: 1.02926 - acc: 0.8239 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1555  | total loss: \u001b[1m\u001b[32m0.96145\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1555 | loss: 0.96145 - acc: 0.8415 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1556  | total loss: \u001b[1m\u001b[32m0.89729\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1556 | loss: 0.89729 - acc: 0.8574 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1557  | total loss: \u001b[1m\u001b[32m0.83652\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1557 | loss: 0.83652 - acc: 0.8716 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1558  | total loss: \u001b[1m\u001b[32m0.77894\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1558 | loss: 0.77894 - acc: 0.8845 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1559  | total loss: \u001b[1m\u001b[32m0.72444\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1559 | loss: 0.72444 - acc: 0.8960 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1560  | total loss: \u001b[1m\u001b[32m1.02210\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1560 | loss: 1.02210 - acc: 0.8207 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1561  | total loss: \u001b[1m\u001b[32m0.93950\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1561 | loss: 0.93950 - acc: 0.8386 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1562  | total loss: \u001b[1m\u001b[32m0.86385\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1562 | loss: 0.86385 - acc: 0.8548 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1563  | total loss: \u001b[1m\u001b[32m0.79450\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1563 | loss: 0.79450 - acc: 0.8693 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1564  | total loss: \u001b[1m\u001b[32m0.73085\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1564 | loss: 0.73085 - acc: 0.8824 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1565  | total loss: \u001b[1m\u001b[32m0.67239\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1565 | loss: 0.67239 - acc: 0.8941 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1566  | total loss: \u001b[1m\u001b[32m1.03204\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1566 | loss: 1.03204 - acc: 0.8119 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1567  | total loss: \u001b[1m\u001b[32m0.94203\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1567 | loss: 0.94203 - acc: 0.8307 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1568  | total loss: \u001b[1m\u001b[32m0.86065\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1568 | loss: 0.86065 - acc: 0.8476 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1569  | total loss: \u001b[1m\u001b[32m0.78699\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1569 | loss: 0.78699 - acc: 0.8628 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1570  | total loss: \u001b[1m\u001b[32m1.18292\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1570 | loss: 1.18292 - acc: 0.7837 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1571  | total loss: \u001b[1m\u001b[32m1.07680\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1571 | loss: 1.07680 - acc: 0.8053 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1572  | total loss: \u001b[1m\u001b[32m1.42061\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1572 | loss: 1.42061 - acc: 0.7319 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1573  | total loss: \u001b[1m\u001b[32m1.29160\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1573 | loss: 1.29160 - acc: 0.7587 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1574  | total loss: \u001b[1m\u001b[32m1.59104\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1574 | loss: 1.59104 - acc: 0.6900 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1575  | total loss: \u001b[1m\u001b[32m1.44692\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1575 | loss: 1.44692 - acc: 0.7210 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1576  | total loss: \u001b[1m\u001b[32m1.77337\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1576 | loss: 1.77337 - acc: 0.6489 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1577  | total loss: \u001b[1m\u001b[32m1.61409\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1577 | loss: 1.61409 - acc: 0.6840 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1578  | total loss: \u001b[1m\u001b[32m1.47245\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1578 | loss: 1.47245 - acc: 0.7156 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1579  | total loss: \u001b[1m\u001b[32m1.34639\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1579 | loss: 1.34639 - acc: 0.7441 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1580  | total loss: \u001b[1m\u001b[32m1.60398\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1580 | loss: 1.60398 - acc: 0.6839 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1581  | total loss: \u001b[1m\u001b[32m1.46794\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1581 | loss: 1.46794 - acc: 0.7155 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1582  | total loss: \u001b[1m\u001b[32m1.72420\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1582 | loss: 1.72420 - acc: 0.6511 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1583  | total loss: \u001b[1m\u001b[32m1.58053\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1583 | loss: 1.58053 - acc: 0.6860 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1584  | total loss: \u001b[1m\u001b[32m1.45344\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1584 | loss: 1.45344 - acc: 0.7174 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1585  | total loss: \u001b[1m\u001b[32m1.34074\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1585 | loss: 1.34074 - acc: 0.7457 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1586  | total loss: \u001b[1m\u001b[32m1.24043\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1586 | loss: 1.24043 - acc: 0.7711 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1587  | total loss: \u001b[1m\u001b[32m1.15072\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1587 | loss: 1.15072 - acc: 0.7940 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1588  | total loss: \u001b[1m\u001b[32m1.44535\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1588 | loss: 1.44535 - acc: 0.7146 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1589  | total loss: \u001b[1m\u001b[32m1.33640\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1589 | loss: 1.33640 - acc: 0.7431 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1590  | total loss: \u001b[1m\u001b[32m1.23895\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1590 | loss: 1.23895 - acc: 0.7688 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1591  | total loss: \u001b[1m\u001b[32m1.15129\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1591 | loss: 1.15129 - acc: 0.7919 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1592  | total loss: \u001b[1m\u001b[32m1.07194\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1592 | loss: 1.07194 - acc: 0.8127 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1593  | total loss: \u001b[1m\u001b[32m0.99963\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1593 | loss: 0.99963 - acc: 0.8315 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1594  | total loss: \u001b[1m\u001b[32m1.27156\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1594 | loss: 1.27156 - acc: 0.7555 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1595  | total loss: \u001b[1m\u001b[32m1.17783\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1595 | loss: 1.17783 - acc: 0.7799 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1596  | total loss: \u001b[1m\u001b[32m1.44698\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1596 | loss: 1.44698 - acc: 0.7091 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1597  | total loss: \u001b[1m\u001b[32m1.33565\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1597 | loss: 1.33565 - acc: 0.7382 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1598  | total loss: \u001b[1m\u001b[32m1.56468\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1598 | loss: 1.56468 - acc: 0.6715 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1599  | total loss: \u001b[1m\u001b[32m1.44284\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1599 | loss: 1.44284 - acc: 0.7043 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1600  | total loss: \u001b[1m\u001b[32m1.67401\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1600 | loss: 1.67401 - acc: 0.6411 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1601  | total loss: \u001b[1m\u001b[32m1.54359\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1601 | loss: 1.54359 - acc: 0.6769 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1602  | total loss: \u001b[1m\u001b[32m1.77008\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1602 | loss: 1.77008 - acc: 0.6093 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1603  | total loss: \u001b[1m\u001b[32m1.63331\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1603 | loss: 1.63331 - acc: 0.6483 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1604  | total loss: \u001b[1m\u001b[32m1.81464\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1604 | loss: 1.81464 - acc: 0.5906 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1605  | total loss: \u001b[1m\u001b[32m1.67725\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1605 | loss: 1.67725 - acc: 0.6316 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1606  | total loss: \u001b[1m\u001b[32m1.55518\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1606 | loss: 1.55518 - acc: 0.6684 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1607  | total loss: \u001b[1m\u001b[32m1.44612\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1607 | loss: 1.44612 - acc: 0.7016 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1608  | total loss: \u001b[1m\u001b[32m1.64241\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1608 | loss: 1.64241 - acc: 0.6457 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1609  | total loss: \u001b[1m\u001b[32m1.52564\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1609 | loss: 1.52564 - acc: 0.6811 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1610  | total loss: \u001b[1m\u001b[32m1.73092\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1610 | loss: 1.73092 - acc: 0.6130 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1611  | total loss: \u001b[1m\u001b[32m1.60687\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1611 | loss: 1.60687 - acc: 0.6517 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1612  | total loss: \u001b[1m\u001b[32m1.49577\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1612 | loss: 1.49577 - acc: 0.6865 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1613  | total loss: \u001b[1m\u001b[32m1.39559\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1613 | loss: 1.39559 - acc: 0.7179 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1614  | total loss: \u001b[1m\u001b[32m1.30456\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1614 | loss: 1.30456 - acc: 0.7461 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1615  | total loss: \u001b[1m\u001b[32m1.22118\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1615 | loss: 1.22118 - acc: 0.7715 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1616  | total loss: \u001b[1m\u001b[32m1.42832\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1616 | loss: 1.42832 - acc: 0.7086 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1617  | total loss: \u001b[1m\u001b[32m1.32977\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1617 | loss: 1.32977 - acc: 0.7378 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1618  | total loss: \u001b[1m\u001b[32m1.23972\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1618 | loss: 1.23972 - acc: 0.7640 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1619  | total loss: \u001b[1m\u001b[32m1.15691\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1619 | loss: 1.15691 - acc: 0.7876 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1620  | total loss: \u001b[1m\u001b[32m1.41234\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1620 | loss: 1.41234 - acc: 0.7088 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1621  | total loss: \u001b[1m\u001b[32m1.30951\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1621 | loss: 1.30951 - acc: 0.7379 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1622  | total loss: \u001b[1m\u001b[32m1.54794\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1622 | loss: 1.54794 - acc: 0.6784 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1623  | total loss: \u001b[1m\u001b[32m1.43057\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1623 | loss: 1.43057 - acc: 0.7106 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1624  | total loss: \u001b[1m\u001b[32m1.67067\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1624 | loss: 1.67067 - acc: 0.6395 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1625  | total loss: \u001b[1m\u001b[32m1.54145\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1625 | loss: 1.54145 - acc: 0.6756 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1626  | total loss: \u001b[1m\u001b[32m1.42538\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1626 | loss: 1.42538 - acc: 0.7080 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1627  | total loss: \u001b[1m\u001b[32m1.32064\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1627 | loss: 1.32064 - acc: 0.7372 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1628  | total loss: \u001b[1m\u001b[32m1.53518\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1628 | loss: 1.53518 - acc: 0.6706 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1629  | total loss: \u001b[1m\u001b[32m1.41908\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1629 | loss: 1.41908 - acc: 0.7036 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1630  | total loss: \u001b[1m\u001b[32m1.31446\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1630 | loss: 1.31446 - acc: 0.7332 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1631  | total loss: \u001b[1m\u001b[32m1.21968\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1631 | loss: 1.21968 - acc: 0.7599 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1632  | total loss: \u001b[1m\u001b[32m1.13338\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1632 | loss: 1.13338 - acc: 0.7839 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1633  | total loss: \u001b[1m\u001b[32m1.05436\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1633 | loss: 1.05436 - acc: 0.8055 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1634  | total loss: \u001b[1m\u001b[32m1.30549\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1634 | loss: 1.30549 - acc: 0.7321 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1635  | total loss: \u001b[1m\u001b[32m1.20717\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1635 | loss: 1.20717 - acc: 0.7589 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1636  | total loss: \u001b[1m\u001b[32m1.47368\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1636 | loss: 1.47368 - acc: 0.6902 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1637  | total loss: \u001b[1m\u001b[32m1.35808\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1637 | loss: 1.35808 - acc: 0.7211 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1638  | total loss: \u001b[1m\u001b[32m1.56922\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1638 | loss: 1.56922 - acc: 0.6633 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1639  | total loss: \u001b[1m\u001b[32m1.44502\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1639 | loss: 1.44502 - acc: 0.6970 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1640  | total loss: \u001b[1m\u001b[32m1.33377\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1640 | loss: 1.33377 - acc: 0.7273 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1641  | total loss: \u001b[1m\u001b[32m1.23373\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1641 | loss: 1.23373 - acc: 0.7546 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1642  | total loss: \u001b[1m\u001b[32m1.51330\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1642 | loss: 1.51330 - acc: 0.6791 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1643  | total loss: \u001b[1m\u001b[32m1.39585\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1643 | loss: 1.39585 - acc: 0.7112 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1644  | total loss: \u001b[1m\u001b[32m1.29052\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1644 | loss: 1.29052 - acc: 0.7401 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1645  | total loss: \u001b[1m\u001b[32m1.19560\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1645 | loss: 1.19560 - acc: 0.7661 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1646  | total loss: \u001b[1m\u001b[32m1.46409\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1646 | loss: 1.46409 - acc: 0.6895 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1647  | total loss: \u001b[1m\u001b[32m1.35197\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1647 | loss: 1.35197 - acc: 0.7205 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1648  | total loss: \u001b[1m\u001b[32m1.25125\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1648 | loss: 1.25125 - acc: 0.7485 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1649  | total loss: \u001b[1m\u001b[32m1.16028\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1649 | loss: 1.16028 - acc: 0.7736 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1650  | total loss: \u001b[1m\u001b[32m1.40264\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1650 | loss: 1.40264 - acc: 0.7034 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1651  | total loss: \u001b[1m\u001b[32m1.29623\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1651 | loss: 1.29623 - acc: 0.7331 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1652  | total loss: \u001b[1m\u001b[32m1.20038\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1652 | loss: 1.20038 - acc: 0.7597 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1653  | total loss: \u001b[1m\u001b[32m1.11360\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1653 | loss: 1.11360 - acc: 0.7838 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1654  | total loss: \u001b[1m\u001b[32m1.03458\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1654 | loss: 1.03458 - acc: 0.8054 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1655  | total loss: \u001b[1m\u001b[32m0.96221\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1655 | loss: 0.96221 - acc: 0.8249 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1656  | total loss: \u001b[1m\u001b[32m1.26736\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1656 | loss: 1.26736 - acc: 0.7424 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1657  | total loss: \u001b[1m\u001b[32m1.17005\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1657 | loss: 1.17005 - acc: 0.7681 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1658  | total loss: \u001b[1m\u001b[32m1.43222\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1658 | loss: 1.43222 - acc: 0.6985 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1659  | total loss: \u001b[1m\u001b[32m1.31862\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1659 | loss: 1.31862 - acc: 0.7286 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1660  | total loss: \u001b[1m\u001b[32m1.21663\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1660 | loss: 1.21663 - acc: 0.7558 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1661  | total loss: \u001b[1m\u001b[32m1.12470\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1661 | loss: 1.12470 - acc: 0.7802 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1662  | total loss: \u001b[1m\u001b[32m1.33266\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1662 | loss: 1.33266 - acc: 0.7236 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1663  | total loss: \u001b[1m\u001b[32m1.22901\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1663 | loss: 1.22901 - acc: 0.7512 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1664  | total loss: \u001b[1m\u001b[32m1.53436\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1664 | loss: 1.53436 - acc: 0.6761 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1665  | total loss: \u001b[1m\u001b[32m1.41176\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1665 | loss: 1.41176 - acc: 0.7085 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1666  | total loss: \u001b[1m\u001b[32m1.30213\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1666 | loss: 1.30213 - acc: 0.7376 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1667  | total loss: \u001b[1m\u001b[32m1.20371\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1667 | loss: 1.20371 - acc: 0.7639 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1668  | total loss: \u001b[1m\u001b[32m1.43612\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1668 | loss: 1.43612 - acc: 0.7018 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1669  | total loss: \u001b[1m\u001b[32m1.32491\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1669 | loss: 1.32491 - acc: 0.7316 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1670  | total loss: \u001b[1m\u001b[32m1.58897\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1670 | loss: 1.58897 - acc: 0.6584 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1671  | total loss: \u001b[1m\u001b[32m1.46434\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1671 | loss: 1.46434 - acc: 0.6926 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1672  | total loss: \u001b[1m\u001b[32m1.68996\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1672 | loss: 1.68996 - acc: 0.6233 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1673  | total loss: \u001b[1m\u001b[32m1.55843\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1673 | loss: 1.55843 - acc: 0.6610 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1674  | total loss: \u001b[1m\u001b[32m1.74584\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1674 | loss: 1.74584 - acc: 0.6092 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1675  | total loss: \u001b[1m\u001b[32m1.61272\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1675 | loss: 1.61272 - acc: 0.6483 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1676  | total loss: \u001b[1m\u001b[32m1.79662\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1676 | loss: 1.79662 - acc: 0.5906 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1677  | total loss: \u001b[1m\u001b[32m1.66268\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1677 | loss: 1.66268 - acc: 0.6315 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1678  | total loss: \u001b[1m\u001b[32m1.82621\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1678 | loss: 1.82621 - acc: 0.5827 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1679  | total loss: \u001b[1m\u001b[32m1.69365\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1679 | loss: 1.69365 - acc: 0.6244 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1680  | total loss: \u001b[1m\u001b[32m1.57598\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1680 | loss: 1.57598 - acc: 0.6620 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1681  | total loss: \u001b[1m\u001b[32m1.47074\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1681 | loss: 1.47074 - acc: 0.6958 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1682  | total loss: \u001b[1m\u001b[32m1.69949\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1682 | loss: 1.69949 - acc: 0.6262 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1683  | total loss: \u001b[1m\u001b[32m1.58263\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1683 | loss: 1.58263 - acc: 0.6636 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1684  | total loss: \u001b[1m\u001b[32m1.47749\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1684 | loss: 1.47749 - acc: 0.6972 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1685  | total loss: \u001b[1m\u001b[32m1.38209\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1685 | loss: 1.38209 - acc: 0.7275 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1686  | total loss: \u001b[1m\u001b[32m1.57937\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1686 | loss: 1.57937 - acc: 0.6619 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1687  | total loss: \u001b[1m\u001b[32m1.47204\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1687 | loss: 1.47204 - acc: 0.6957 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1688  | total loss: \u001b[1m\u001b[32m1.61772\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1688 | loss: 1.61772 - acc: 0.6476 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1689  | total loss: \u001b[1m\u001b[32m1.50553\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1689 | loss: 1.50553 - acc: 0.6828 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1690  | total loss: \u001b[1m\u001b[32m1.68673\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1690 | loss: 1.68673 - acc: 0.6288 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1691  | total loss: \u001b[1m\u001b[32m1.56716\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1691 | loss: 1.56716 - acc: 0.6659 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1692  | total loss: \u001b[1m\u001b[32m1.73779\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1692 | loss: 1.73779 - acc: 0.6065 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1693  | total loss: \u001b[1m\u001b[32m1.61330\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1693 | loss: 1.61330 - acc: 0.6458 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1694  | total loss: \u001b[1m\u001b[32m1.78536\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1694 | loss: 1.78536 - acc: 0.5884 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1695  | total loss: \u001b[1m\u001b[32m1.65702\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1695 | loss: 1.65702 - acc: 0.6295 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1696  | total loss: \u001b[1m\u001b[32m1.80825\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1696 | loss: 1.80825 - acc: 0.5809 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1697  | total loss: \u001b[1m\u001b[32m1.67891\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1697 | loss: 1.67891 - acc: 0.6228 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1698  | total loss: \u001b[1m\u001b[32m1.84063\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1698 | loss: 1.84063 - acc: 0.5677 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1699  | total loss: \u001b[1m\u001b[32m1.70959\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1699 | loss: 1.70959 - acc: 0.6109 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1700  | total loss: \u001b[1m\u001b[32m1.87081\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1700 | loss: 1.87081 - acc: 0.5569 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1701  | total loss: \u001b[1m\u001b[32m1.73856\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1701 | loss: 1.73856 - acc: 0.6012 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1702  | total loss: \u001b[1m\u001b[32m1.62000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1702 | loss: 1.62000 - acc: 0.6411 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1703  | total loss: \u001b[1m\u001b[32m1.51289\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1703 | loss: 1.51289 - acc: 0.6770 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1704  | total loss: \u001b[1m\u001b[32m1.72858\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1704 | loss: 1.72858 - acc: 0.6093 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1705  | total loss: \u001b[1m\u001b[32m1.60965\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1705 | loss: 1.60965 - acc: 0.6484 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1706  | total loss: \u001b[1m\u001b[32m1.79238\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1706 | loss: 1.79238 - acc: 0.5835 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1707  | total loss: \u001b[1m\u001b[32m1.66715\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1707 | loss: 1.66715 - acc: 0.6252 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1708  | total loss: \u001b[1m\u001b[32m1.80212\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1708 | loss: 1.80212 - acc: 0.5770 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1709  | total loss: \u001b[1m\u001b[32m1.67664\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1709 | loss: 1.67664 - acc: 0.6193 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1710  | total loss: \u001b[1m\u001b[32m1.79613\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1710 | loss: 1.79613 - acc: 0.5859 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1711  | total loss: \u001b[1m\u001b[32m1.67197\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1711 | loss: 1.67197 - acc: 0.6273 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1712  | total loss: \u001b[1m\u001b[32m1.56004\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1712 | loss: 1.56004 - acc: 0.6646 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1713  | total loss: \u001b[1m\u001b[32m1.45834\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1713 | loss: 1.45834 - acc: 0.6981 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1714  | total loss: \u001b[1m\u001b[32m1.63739\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1714 | loss: 1.63739 - acc: 0.6355 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1715  | total loss: \u001b[1m\u001b[32m1.52576\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1715 | loss: 1.52576 - acc: 0.6719 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1716  | total loss: \u001b[1m\u001b[32m1.71947\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1716 | loss: 1.71947 - acc: 0.6119 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1717  | total loss: \u001b[1m\u001b[32m1.59817\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1717 | loss: 1.59817 - acc: 0.6507 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1718  | total loss: \u001b[1m\u001b[32m1.76831\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1718 | loss: 1.76831 - acc: 0.5928 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1719  | total loss: \u001b[1m\u001b[32m1.64135\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1719 | loss: 1.64135 - acc: 0.6335 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1720  | total loss: \u001b[1m\u001b[32m1.84551\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1720 | loss: 1.84551 - acc: 0.5701 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1721  | total loss: \u001b[1m\u001b[32m1.71094\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1721 | loss: 1.71094 - acc: 0.6131 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1722  | total loss: \u001b[1m\u001b[32m1.58973\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1722 | loss: 1.58973 - acc: 0.6518 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1723  | total loss: \u001b[1m\u001b[32m1.47985\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1723 | loss: 1.47985 - acc: 0.6866 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1724  | total loss: \u001b[1m\u001b[32m1.64063\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1724 | loss: 1.64063 - acc: 0.6394 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1725  | total loss: \u001b[1m\u001b[32m1.52395\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1725 | loss: 1.52395 - acc: 0.6755 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1726  | total loss: \u001b[1m\u001b[32m1.71359\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1726 | loss: 1.71359 - acc: 0.6150 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1727  | total loss: \u001b[1m\u001b[32m1.58894\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1727 | loss: 1.58894 - acc: 0.6535 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1728  | total loss: \u001b[1m\u001b[32m1.47635\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1728 | loss: 1.47635 - acc: 0.6882 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1729  | total loss: \u001b[1m\u001b[32m1.37405\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1729 | loss: 1.37405 - acc: 0.7194 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1730  | total loss: \u001b[1m\u001b[32m1.58535\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1730 | loss: 1.58535 - acc: 0.6474 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1731  | total loss: \u001b[1m\u001b[32m1.47044\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1731 | loss: 1.47044 - acc: 0.6827 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1732  | total loss: \u001b[1m\u001b[32m1.67027\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1732 | loss: 1.67027 - acc: 0.6216 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1733  | total loss: \u001b[1m\u001b[32m1.54634\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1733 | loss: 1.54634 - acc: 0.6594 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1734  | total loss: \u001b[1m\u001b[32m1.74418\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1734 | loss: 1.74418 - acc: 0.6006 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1735  | total loss: \u001b[1m\u001b[32m1.61334\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1735 | loss: 1.61334 - acc: 0.6405 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1736  | total loss: \u001b[1m\u001b[32m1.82443\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1736 | loss: 1.82443 - acc: 0.5765 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1737  | total loss: \u001b[1m\u001b[32m1.68705\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1737 | loss: 1.68705 - acc: 0.6188 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1738  | total loss: \u001b[1m\u001b[32m1.82343\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1738 | loss: 1.82343 - acc: 0.5784 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1739  | total loss: \u001b[1m\u001b[32m1.68818\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1739 | loss: 1.68818 - acc: 0.6205 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1740  | total loss: \u001b[1m\u001b[32m1.56711\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1740 | loss: 1.56711 - acc: 0.6585 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1741  | total loss: \u001b[1m\u001b[32m1.45809\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1741 | loss: 1.45809 - acc: 0.6926 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1742  | total loss: \u001b[1m\u001b[32m1.35925\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1742 | loss: 1.35925 - acc: 0.7234 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1743  | total loss: \u001b[1m\u001b[32m1.26900\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1743 | loss: 1.26900 - acc: 0.7510 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1744  | total loss: \u001b[1m\u001b[32m1.45024\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1744 | loss: 1.45024 - acc: 0.6902 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1745  | total loss: \u001b[1m\u001b[32m1.34845\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1745 | loss: 1.34845 - acc: 0.7212 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1746  | total loss: \u001b[1m\u001b[32m1.59007\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1746 | loss: 1.59007 - acc: 0.6491 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1747  | total loss: \u001b[1m\u001b[32m1.47314\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1747 | loss: 1.47314 - acc: 0.6842 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1748  | total loss: \u001b[1m\u001b[32m1.36736\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1748 | loss: 1.36736 - acc: 0.7158 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1749  | total loss: \u001b[1m\u001b[32m1.27110\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1749 | loss: 1.27110 - acc: 0.7442 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1750  | total loss: \u001b[1m\u001b[32m1.18298\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1750 | loss: 1.18298 - acc: 0.7698 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1751  | total loss: \u001b[1m\u001b[32m1.10185\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1751 | loss: 1.10185 - acc: 0.7928 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1752  | total loss: \u001b[1m\u001b[32m1.38768\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1752 | loss: 1.38768 - acc: 0.7135 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1753  | total loss: \u001b[1m\u001b[32m1.28339\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1753 | loss: 1.28339 - acc: 0.7422 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1754  | total loss: \u001b[1m\u001b[32m1.54930\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1754 | loss: 1.54930 - acc: 0.6679 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1755  | total loss: \u001b[1m\u001b[32m1.42820\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1755 | loss: 1.42820 - acc: 0.7011 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1756  | total loss: \u001b[1m\u001b[32m1.65457\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1756 | loss: 1.65457 - acc: 0.6382 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1757  | total loss: \u001b[1m\u001b[32m1.52383\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1757 | loss: 1.52383 - acc: 0.6744 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1758  | total loss: \u001b[1m\u001b[32m1.74849\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1758 | loss: 1.74849 - acc: 0.6069 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1759  | total loss: \u001b[1m\u001b[32m1.61038\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1759 | loss: 1.61038 - acc: 0.6462 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1760  | total loss: \u001b[1m\u001b[32m1.83420\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1760 | loss: 1.83420 - acc: 0.5816 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1761  | total loss: \u001b[1m\u001b[32m1.69049\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1761 | loss: 1.69049 - acc: 0.6234 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1762  | total loss: \u001b[1m\u001b[32m1.91189\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1762 | loss: 1.91189 - acc: 0.5611 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1763  | total loss: \u001b[1m\u001b[32m1.76430\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1763 | loss: 1.76430 - acc: 0.6050 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1764  | total loss: \u001b[1m\u001b[32m1.97836\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1764 | loss: 1.97836 - acc: 0.5445 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1765  | total loss: \u001b[1m\u001b[32m1.82881\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1765 | loss: 1.82881 - acc: 0.5900 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1766  | total loss: \u001b[1m\u001b[32m1.69623\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1766 | loss: 1.69623 - acc: 0.6310 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1767  | total loss: \u001b[1m\u001b[32m1.57802\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1767 | loss: 1.57802 - acc: 0.6679 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1768  | total loss: \u001b[1m\u001b[32m1.47188\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1768 | loss: 1.47188 - acc: 0.7011 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1769  | total loss: \u001b[1m\u001b[32m1.37584\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1769 | loss: 1.37584 - acc: 0.7310 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1770  | total loss: \u001b[1m\u001b[32m1.61740\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1770 | loss: 1.61740 - acc: 0.6579 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1771  | total loss: \u001b[1m\u001b[32m1.50564\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1771 | loss: 1.50564 - acc: 0.6921 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1772  | total loss: \u001b[1m\u001b[32m1.71522\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1772 | loss: 1.71522 - acc: 0.6301 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1773  | total loss: \u001b[1m\u001b[32m1.59338\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1773 | loss: 1.59338 - acc: 0.6671 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1774  | total loss: \u001b[1m\u001b[32m1.76711\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1774 | loss: 1.76711 - acc: 0.6075 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1775  | total loss: \u001b[1m\u001b[32m1.64054\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1775 | loss: 1.64054 - acc: 0.6467 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1776  | total loss: \u001b[1m\u001b[32m1.52665\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1776 | loss: 1.52665 - acc: 0.6821 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1777  | total loss: \u001b[1m\u001b[32m1.42351\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1777 | loss: 1.42351 - acc: 0.7139 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1778  | total loss: \u001b[1m\u001b[32m1.32943\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1778 | loss: 1.32943 - acc: 0.7425 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1779  | total loss: \u001b[1m\u001b[32m1.24301\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1779 | loss: 1.24301 - acc: 0.7682 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1780  | total loss: \u001b[1m\u001b[32m1.48208\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1780 | loss: 1.48208 - acc: 0.6914 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1781  | total loss: \u001b[1m\u001b[32m1.37751\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1781 | loss: 1.37751 - acc: 0.7223 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1782  | total loss: \u001b[1m\u001b[32m1.60114\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1782 | loss: 1.60114 - acc: 0.6500 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1783  | total loss: \u001b[1m\u001b[32m1.48352\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1783 | loss: 1.48352 - acc: 0.6850 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1784  | total loss: \u001b[1m\u001b[32m1.62041\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1784 | loss: 1.62041 - acc: 0.6380 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1785  | total loss: \u001b[1m\u001b[32m1.50069\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1785 | loss: 1.50069 - acc: 0.6742 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1786  | total loss: \u001b[1m\u001b[32m1.39268\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1786 | loss: 1.39268 - acc: 0.7067 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1787  | total loss: \u001b[1m\u001b[32m1.29469\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1787 | loss: 1.29469 - acc: 0.7361 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1788  | total loss: \u001b[1m\u001b[32m1.53757\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1788 | loss: 1.53757 - acc: 0.6625 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1789  | total loss: \u001b[1m\u001b[32m1.42380\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1789 | loss: 1.42380 - acc: 0.6962 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1790  | total loss: \u001b[1m\u001b[32m1.63940\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1790 | loss: 1.63940 - acc: 0.6337 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1791  | total loss: \u001b[1m\u001b[32m1.51558\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1791 | loss: 1.51558 - acc: 0.6704 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1792  | total loss: \u001b[1m\u001b[32m1.40426\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1792 | loss: 1.40426 - acc: 0.7033 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1793  | total loss: \u001b[1m\u001b[32m1.30365\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1793 | loss: 1.30365 - acc: 0.7330 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1794  | total loss: \u001b[1m\u001b[32m1.21218\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1794 | loss: 1.21218 - acc: 0.7597 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1795  | total loss: \u001b[1m\u001b[32m1.12853\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1795 | loss: 1.12853 - acc: 0.7837 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1796  | total loss: \u001b[1m\u001b[32m1.05160\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1796 | loss: 1.05160 - acc: 0.8054 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1797  | total loss: \u001b[1m\u001b[32m0.98044\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1797 | loss: 0.98044 - acc: 0.8248 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1798  | total loss: \u001b[1m\u001b[32m0.91430\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1798 | loss: 0.91430 - acc: 0.8423 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1799  | total loss: \u001b[1m\u001b[32m0.85258\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1799 | loss: 0.85258 - acc: 0.8581 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1800  | total loss: \u001b[1m\u001b[32m1.19663\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1800 | loss: 1.19663 - acc: 0.7723 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1801  | total loss: \u001b[1m\u001b[32m1.10355\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1801 | loss: 1.10355 - acc: 0.7951 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1802  | total loss: \u001b[1m\u001b[32m1.39875\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1802 | loss: 1.39875 - acc: 0.7227 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1803  | total loss: \u001b[1m\u001b[32m1.28446\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1803 | loss: 1.28446 - acc: 0.7504 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1804  | total loss: \u001b[1m\u001b[32m1.58918\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1804 | loss: 1.58918 - acc: 0.6825 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1805  | total loss: \u001b[1m\u001b[32m1.45653\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1805 | loss: 1.45653 - acc: 0.7143 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1806  | total loss: \u001b[1m\u001b[32m1.72117\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1806 | loss: 1.72117 - acc: 0.6428 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1807  | total loss: \u001b[1m\u001b[32m1.57745\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1807 | loss: 1.57745 - acc: 0.6786 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1808  | total loss: \u001b[1m\u001b[32m1.79040\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1808 | loss: 1.79040 - acc: 0.6179 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1809  | total loss: \u001b[1m\u001b[32m1.64312\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1809 | loss: 1.64312 - acc: 0.6561 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1810  | total loss: \u001b[1m\u001b[32m1.86682\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1810 | loss: 1.86682 - acc: 0.5905 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1811  | total loss: \u001b[1m\u001b[32m1.71649\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1811 | loss: 1.71649 - acc: 0.6314 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1812  | total loss: \u001b[1m\u001b[32m1.92207\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1812 | loss: 1.92207 - acc: 0.5683 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1813  | total loss: \u001b[1m\u001b[32m1.77170\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1813 | loss: 1.77170 - acc: 0.6114 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1814  | total loss: \u001b[1m\u001b[32m1.94724\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1814 | loss: 1.94724 - acc: 0.5574 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1815  | total loss: \u001b[1m\u001b[32m1.80012\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1815 | loss: 1.80012 - acc: 0.6017 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1816  | total loss: \u001b[1m\u001b[32m1.97076\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1816 | loss: 1.97076 - acc: 0.5487 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1817  | total loss: \u001b[1m\u001b[32m1.82714\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1817 | loss: 1.82714 - acc: 0.5938 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1818  | total loss: \u001b[1m\u001b[32m1.70025\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1818 | loss: 1.70025 - acc: 0.6344 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1819  | total loss: \u001b[1m\u001b[32m1.58739\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1819 | loss: 1.58739 - acc: 0.6710 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1820  | total loss: \u001b[1m\u001b[32m1.78136\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1820 | loss: 1.78136 - acc: 0.6039 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1821  | total loss: \u001b[1m\u001b[32m1.66225\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1821 | loss: 1.66225 - acc: 0.6435 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1822  | total loss: \u001b[1m\u001b[32m1.55554\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1822 | loss: 1.55554 - acc: 0.6791 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1823  | total loss: \u001b[1m\u001b[32m1.45906\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1823 | loss: 1.45906 - acc: 0.7112 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1824  | total loss: \u001b[1m\u001b[32m1.61303\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1824 | loss: 1.61303 - acc: 0.6615 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1825  | total loss: \u001b[1m\u001b[32m1.50920\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1825 | loss: 1.50920 - acc: 0.6954 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1826  | total loss: \u001b[1m\u001b[32m1.69689\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1826 | loss: 1.69689 - acc: 0.6258 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1827  | total loss: \u001b[1m\u001b[32m1.58365\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1827 | loss: 1.58365 - acc: 0.6633 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1828  | total loss: \u001b[1m\u001b[32m1.76850\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1828 | loss: 1.76850 - acc: 0.6041 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1829  | total loss: \u001b[1m\u001b[32m1.64792\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1829 | loss: 1.64792 - acc: 0.6437 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1830  | total loss: \u001b[1m\u001b[32m1.80770\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1830 | loss: 1.80770 - acc: 0.5864 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1831  | total loss: \u001b[1m\u001b[32m1.68368\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1831 | loss: 1.68368 - acc: 0.6278 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1832  | total loss: \u001b[1m\u001b[32m1.84308\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1832 | loss: 1.84308 - acc: 0.5722 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1833  | total loss: \u001b[1m\u001b[32m1.71630\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1833 | loss: 1.71630 - acc: 0.6149 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1834  | total loss: \u001b[1m\u001b[32m1.60216\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1834 | loss: 1.60216 - acc: 0.6535 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1835  | total loss: \u001b[1m\u001b[32m1.49858\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1835 | loss: 1.49858 - acc: 0.6881 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1836  | total loss: \u001b[1m\u001b[32m1.40380\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1836 | loss: 1.40380 - acc: 0.7193 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1837  | total loss: \u001b[1m\u001b[32m1.31634\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1837 | loss: 1.31634 - acc: 0.7474 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1838  | total loss: \u001b[1m\u001b[32m1.49631\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1838 | loss: 1.49631 - acc: 0.6869 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1839  | total loss: \u001b[1m\u001b[32m1.39555\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1839 | loss: 1.39555 - acc: 0.7182 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1840  | total loss: \u001b[1m\u001b[32m1.30295\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1840 | loss: 1.30295 - acc: 0.7464 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1841  | total loss: \u001b[1m\u001b[32m1.21730\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1841 | loss: 1.21730 - acc: 0.7718 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1842  | total loss: \u001b[1m\u001b[32m1.13761\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1842 | loss: 1.13761 - acc: 0.7946 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1843  | total loss: \u001b[1m\u001b[32m1.06310\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1843 | loss: 1.06310 - acc: 0.8151 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1844  | total loss: \u001b[1m\u001b[32m1.32411\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1844 | loss: 1.32411 - acc: 0.7408 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1845  | total loss: \u001b[1m\u001b[32m1.22654\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1845 | loss: 1.22654 - acc: 0.7667 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1846  | total loss: \u001b[1m\u001b[32m1.13700\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1846 | loss: 1.13700 - acc: 0.7900 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1847  | total loss: \u001b[1m\u001b[32m1.05451\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1847 | loss: 1.05451 - acc: 0.8110 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1848  | total loss: \u001b[1m\u001b[32m0.97825\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1848 | loss: 0.97825 - acc: 0.8299 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1849  | total loss: \u001b[1m\u001b[32m0.90757\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1849 | loss: 0.90757 - acc: 0.8469 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1850  | total loss: \u001b[1m\u001b[32m1.10695\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1850 | loss: 1.10695 - acc: 0.7979 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1851  | total loss: \u001b[1m\u001b[32m1.02014\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1851 | loss: 1.02014 - acc: 0.8181 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1852  | total loss: \u001b[1m\u001b[32m1.29482\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1852 | loss: 1.29482 - acc: 0.7506 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1853  | total loss: \u001b[1m\u001b[32m1.18770\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1853 | loss: 1.18770 - acc: 0.7756 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1854  | total loss: \u001b[1m\u001b[32m1.49957\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1854 | loss: 1.49957 - acc: 0.6980 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1855  | total loss: \u001b[1m\u001b[32m1.37216\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1855 | loss: 1.37216 - acc: 0.7282 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1856  | total loss: \u001b[1m\u001b[32m1.66976\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1856 | loss: 1.66976 - acc: 0.6625 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1857  | total loss: \u001b[1m\u001b[32m1.52694\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1857 | loss: 1.52694 - acc: 0.6963 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1858  | total loss: \u001b[1m\u001b[32m1.39936\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1858 | loss: 1.39936 - acc: 0.7266 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1859  | total loss: \u001b[1m\u001b[32m1.28517\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1859 | loss: 1.28517 - acc: 0.7540 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1860  | total loss: \u001b[1m\u001b[32m1.55778\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1860 | loss: 1.55778 - acc: 0.6786 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1861  | total loss: \u001b[1m\u001b[32m1.42947\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1861 | loss: 1.42947 - acc: 0.7107 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1862  | total loss: \u001b[1m\u001b[32m1.66477\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1862 | loss: 1.66477 - acc: 0.6468 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1863  | total loss: \u001b[1m\u001b[32m1.52885\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1863 | loss: 1.52885 - acc: 0.6821 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1864  | total loss: \u001b[1m\u001b[32m1.80946\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1864 | loss: 1.80946 - acc: 0.6139 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1865  | total loss: \u001b[1m\u001b[32m1.66339\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1865 | loss: 1.66339 - acc: 0.6525 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1866  | total loss: \u001b[1m\u001b[32m1.88075\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1866 | loss: 1.88075 - acc: 0.5873 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1867  | total loss: \u001b[1m\u001b[32m1.73295\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1867 | loss: 1.73295 - acc: 0.6285 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1868  | total loss: \u001b[1m\u001b[32m1.60242\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1868 | loss: 1.60242 - acc: 0.6657 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1869  | total loss: \u001b[1m\u001b[32m1.48664\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1869 | loss: 1.48664 - acc: 0.6991 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1870  | total loss: \u001b[1m\u001b[32m1.69734\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1870 | loss: 1.69734 - acc: 0.6363 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1871  | total loss: \u001b[1m\u001b[32m1.57480\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1871 | loss: 1.57480 - acc: 0.6727 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1872  | total loss: \u001b[1m\u001b[32m1.46547\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1872 | loss: 1.46547 - acc: 0.7054 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1873  | total loss: \u001b[1m\u001b[32m1.36722\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1873 | loss: 1.36722 - acc: 0.7349 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1874  | total loss: \u001b[1m\u001b[32m1.60914\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1874 | loss: 1.60914 - acc: 0.6614 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1875  | total loss: \u001b[1m\u001b[32m1.49658\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1875 | loss: 1.49658 - acc: 0.6953 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1876  | total loss: \u001b[1m\u001b[32m1.70320\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1876 | loss: 1.70320 - acc: 0.6329 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1877  | total loss: \u001b[1m\u001b[32m1.58189\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1877 | loss: 1.58189 - acc: 0.6696 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1878  | total loss: \u001b[1m\u001b[32m1.47271\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1878 | loss: 1.47271 - acc: 0.7026 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1879  | total loss: \u001b[1m\u001b[32m1.37375\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1879 | loss: 1.37375 - acc: 0.7324 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1880  | total loss: \u001b[1m\u001b[32m1.56529\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1880 | loss: 1.56529 - acc: 0.6663 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1881  | total loss: \u001b[1m\u001b[32m1.45571\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1881 | loss: 1.45571 - acc: 0.6996 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1882  | total loss: \u001b[1m\u001b[32m1.65589\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1882 | loss: 1.65589 - acc: 0.6368 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1883  | total loss: \u001b[1m\u001b[32m1.53713\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1883 | loss: 1.53713 - acc: 0.6731 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1884  | total loss: \u001b[1m\u001b[32m1.43012\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1884 | loss: 1.43012 - acc: 0.7058 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1885  | total loss: \u001b[1m\u001b[32m1.33308\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1885 | loss: 1.33308 - acc: 0.7352 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1886  | total loss: \u001b[1m\u001b[32m1.55177\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1886 | loss: 1.55177 - acc: 0.6689 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1887  | total loss: \u001b[1m\u001b[32m1.44125\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1887 | loss: 1.44125 - acc: 0.7020 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1888  | total loss: \u001b[1m\u001b[32m1.65348\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1888 | loss: 1.65348 - acc: 0.6389 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1889  | total loss: \u001b[1m\u001b[32m1.53268\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1889 | loss: 1.53268 - acc: 0.6750 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1890  | total loss: \u001b[1m\u001b[32m1.42381\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1890 | loss: 1.42381 - acc: 0.7075 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1891  | total loss: \u001b[1m\u001b[32m1.32509\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1891 | loss: 1.32509 - acc: 0.7368 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1892  | total loss: \u001b[1m\u001b[32m1.23497\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1892 | loss: 1.23497 - acc: 0.7631 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1893  | total loss: \u001b[1m\u001b[32m1.15217\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1893 | loss: 1.15217 - acc: 0.7868 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1894  | total loss: \u001b[1m\u001b[32m1.36656\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1894 | loss: 1.36656 - acc: 0.7224 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1895  | total loss: \u001b[1m\u001b[32m1.26775\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1895 | loss: 1.26775 - acc: 0.7502 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1896  | total loss: \u001b[1m\u001b[32m1.17761\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1896 | loss: 1.17761 - acc: 0.7751 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1897  | total loss: \u001b[1m\u001b[32m1.09495\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1897 | loss: 1.09495 - acc: 0.7976 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1898  | total loss: \u001b[1m\u001b[32m1.33529\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1898 | loss: 1.33529 - acc: 0.7321 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1899  | total loss: \u001b[1m\u001b[32m1.23446\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1899 | loss: 1.23446 - acc: 0.7589 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1900  | total loss: \u001b[1m\u001b[32m1.48910\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1900 | loss: 1.48910 - acc: 0.6902 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1901  | total loss: \u001b[1m\u001b[32m1.37214\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1901 | loss: 1.37214 - acc: 0.7212 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1902  | total loss: \u001b[1m\u001b[32m1.26664\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1902 | loss: 1.26664 - acc: 0.7490 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1903  | total loss: \u001b[1m\u001b[32m1.17109\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1903 | loss: 1.17109 - acc: 0.7741 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1904  | total loss: \u001b[1m\u001b[32m1.45455\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1904 | loss: 1.45455 - acc: 0.6967 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1905  | total loss: \u001b[1m\u001b[32m1.33968\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1905 | loss: 1.33968 - acc: 0.7271 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1906  | total loss: \u001b[1m\u001b[32m1.53478\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1906 | loss: 1.53478 - acc: 0.6758 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1907  | total loss: \u001b[1m\u001b[32m1.41268\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1907 | loss: 1.41268 - acc: 0.7082 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1908  | total loss: \u001b[1m\u001b[32m1.66498\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1908 | loss: 1.66498 - acc: 0.6374 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1909  | total loss: \u001b[1m\u001b[32m1.53179\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1909 | loss: 1.53179 - acc: 0.6736 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1910  | total loss: \u001b[1m\u001b[32m1.75539\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1910 | loss: 1.75539 - acc: 0.6134 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1911  | total loss: \u001b[1m\u001b[32m1.61622\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1911 | loss: 1.61622 - acc: 0.6521 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1912  | total loss: \u001b[1m\u001b[32m1.78821\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1912 | loss: 1.78821 - acc: 0.6012 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1913  | total loss: \u001b[1m\u001b[32m1.64954\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1913 | loss: 1.64954 - acc: 0.6410 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1914  | total loss: \u001b[1m\u001b[32m1.52640\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1914 | loss: 1.52640 - acc: 0.6769 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1915  | total loss: \u001b[1m\u001b[32m1.41652\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1915 | loss: 1.41652 - acc: 0.7092 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1916  | total loss: \u001b[1m\u001b[32m1.62740\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1916 | loss: 1.62740 - acc: 0.6455 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1917  | total loss: \u001b[1m\u001b[32m1.50903\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1917 | loss: 1.50903 - acc: 0.6809 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1918  | total loss: \u001b[1m\u001b[32m1.71246\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1918 | loss: 1.71246 - acc: 0.6200 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1919  | total loss: \u001b[1m\u001b[32m1.58790\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1919 | loss: 1.58790 - acc: 0.6580 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1920  | total loss: \u001b[1m\u001b[32m1.76250\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1920 | loss: 1.76250 - acc: 0.6065 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1921  | total loss: \u001b[1m\u001b[32m1.63540\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1921 | loss: 1.63540 - acc: 0.6458 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1922  | total loss: \u001b[1m\u001b[32m1.52170\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1922 | loss: 1.52170 - acc: 0.6812 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1923  | total loss: \u001b[1m\u001b[32m1.41925\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1923 | loss: 1.41925 - acc: 0.7131 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1924  | total loss: \u001b[1m\u001b[32m1.62706\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1924 | loss: 1.62706 - acc: 0.6418 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1925  | total loss: \u001b[1m\u001b[32m1.51354\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1925 | loss: 1.51354 - acc: 0.6776 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1926  | total loss: \u001b[1m\u001b[32m1.41093\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1926 | loss: 1.41093 - acc: 0.7099 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1927  | total loss: \u001b[1m\u001b[32m1.31748\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1927 | loss: 1.31748 - acc: 0.7389 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1928  | total loss: \u001b[1m\u001b[32m1.53013\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1928 | loss: 1.53013 - acc: 0.6721 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1929  | total loss: \u001b[1m\u001b[32m1.42265\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1929 | loss: 1.42265 - acc: 0.7049 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1930  | total loss: \u001b[1m\u001b[32m1.65481\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1930 | loss: 1.65481 - acc: 0.6344 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1931  | total loss: \u001b[1m\u001b[32m1.53395\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1931 | loss: 1.53395 - acc: 0.6710 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1932  | total loss: \u001b[1m\u001b[32m1.72351\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1932 | loss: 1.72351 - acc: 0.6110 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1933  | total loss: \u001b[1m\u001b[32m1.59582\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1933 | loss: 1.59582 - acc: 0.6499 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1934  | total loss: \u001b[1m\u001b[32m1.75169\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1934 | loss: 1.75169 - acc: 0.5992 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1935  | total loss: \u001b[1m\u001b[32m1.62193\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1935 | loss: 1.62193 - acc: 0.6393 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1936  | total loss: \u001b[1m\u001b[32m1.77577\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1936 | loss: 1.77577 - acc: 0.5897 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1937  | total loss: \u001b[1m\u001b[32m1.64493\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1937 | loss: 1.64493 - acc: 0.6307 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1938  | total loss: \u001b[1m\u001b[32m1.52759\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1938 | loss: 1.52759 - acc: 0.6676 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1939  | total loss: \u001b[1m\u001b[32m1.42174\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1939 | loss: 1.42174 - acc: 0.7009 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1940  | total loss: \u001b[1m\u001b[32m1.64343\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1940 | loss: 1.64343 - acc: 0.6308 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1941  | total loss: \u001b[1m\u001b[32m1.52560\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1941 | loss: 1.52560 - acc: 0.6677 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1942  | total loss: \u001b[1m\u001b[32m1.69538\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1942 | loss: 1.69538 - acc: 0.6224 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1943  | total loss: \u001b[1m\u001b[32m1.57289\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1943 | loss: 1.57289 - acc: 0.6601 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1944  | total loss: \u001b[1m\u001b[32m1.72592\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1944 | loss: 1.72592 - acc: 0.6084 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1945  | total loss: \u001b[1m\u001b[32m1.60131\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1945 | loss: 1.60131 - acc: 0.6476 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1946  | total loss: \u001b[1m\u001b[32m1.78436\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1946 | loss: 1.78436 - acc: 0.5899 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1947  | total loss: \u001b[1m\u001b[32m1.65524\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1947 | loss: 1.65524 - acc: 0.6309 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1948  | total loss: \u001b[1m\u001b[32m1.85968\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1948 | loss: 1.85968 - acc: 0.5679 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1949  | total loss: \u001b[1m\u001b[32m1.72496\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1949 | loss: 1.72496 - acc: 0.6111 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1950  | total loss: \u001b[1m\u001b[32m1.89054\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1950 | loss: 1.89054 - acc: 0.5571 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1951  | total loss: \u001b[1m\u001b[32m1.75506\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1951 | loss: 1.75506 - acc: 0.6014 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1952  | total loss: \u001b[1m\u001b[32m1.88830\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1952 | loss: 1.88830 - acc: 0.5555 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1953  | total loss: \u001b[1m\u001b[32m1.75526\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1953 | loss: 1.75526 - acc: 0.6000 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1954  | total loss: \u001b[1m\u001b[32m1.90077\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1954 | loss: 1.90077 - acc: 0.5471 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1955  | total loss: \u001b[1m\u001b[32m1.76852\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1955 | loss: 1.76852 - acc: 0.5924 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1956  | total loss: \u001b[1m\u001b[32m1.94585\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1956 | loss: 1.94585 - acc: 0.5332 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1957  | total loss: \u001b[1m\u001b[32m1.81135\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1957 | loss: 1.81135 - acc: 0.5799 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1958  | total loss: \u001b[1m\u001b[32m1.69103\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1958 | loss: 1.69103 - acc: 0.6219 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1959  | total loss: \u001b[1m\u001b[32m1.58252\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1959 | loss: 1.58252 - acc: 0.6597 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1960  | total loss: \u001b[1m\u001b[32m1.48382\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1960 | loss: 1.48382 - acc: 0.6937 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1961  | total loss: \u001b[1m\u001b[32m1.39321\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1961 | loss: 1.39321 - acc: 0.7243 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1962  | total loss: \u001b[1m\u001b[32m1.30926\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1962 | loss: 1.30926 - acc: 0.7519 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1963  | total loss: \u001b[1m\u001b[32m1.23082\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1963 | loss: 1.23082 - acc: 0.7767 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1964  | total loss: \u001b[1m\u001b[32m1.44323\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1964 | loss: 1.44323 - acc: 0.6990 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1965  | total loss: \u001b[1m\u001b[32m1.34629\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1965 | loss: 1.34629 - acc: 0.7291 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1966  | total loss: \u001b[1m\u001b[32m1.55543\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1966 | loss: 1.55543 - acc: 0.6634 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1967  | total loss: \u001b[1m\u001b[32m1.44399\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1967 | loss: 1.44399 - acc: 0.6970 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1968  | total loss: \u001b[1m\u001b[32m1.62712\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1968 | loss: 1.62712 - acc: 0.6416 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1969  | total loss: \u001b[1m\u001b[32m1.50658\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1969 | loss: 1.50658 - acc: 0.6775 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1970  | total loss: \u001b[1m\u001b[32m1.39717\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1970 | loss: 1.39717 - acc: 0.7097 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1971  | total loss: \u001b[1m\u001b[32m1.29732\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1971 | loss: 1.29732 - acc: 0.7387 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1972  | total loss: \u001b[1m\u001b[32m1.20575\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1972 | loss: 1.20575 - acc: 0.7649 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1973  | total loss: \u001b[1m\u001b[32m1.12133\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1973 | loss: 1.12133 - acc: 0.7884 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1974  | total loss: \u001b[1m\u001b[32m1.35803\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1974 | loss: 1.35803 - acc: 0.7238 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1975  | total loss: \u001b[1m\u001b[32m1.25519\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1975 | loss: 1.25519 - acc: 0.7514 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1976  | total loss: \u001b[1m\u001b[32m1.53402\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1976 | loss: 1.53402 - acc: 0.6763 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1977  | total loss: \u001b[1m\u001b[32m1.41237\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1977 | loss: 1.41237 - acc: 0.7087 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1978  | total loss: \u001b[1m\u001b[32m1.67141\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1978 | loss: 1.67141 - acc: 0.6378 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1979  | total loss: \u001b[1m\u001b[32m1.53656\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1979 | loss: 1.53656 - acc: 0.6740 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1980  | total loss: \u001b[1m\u001b[32m1.72035\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1980 | loss: 1.72035 - acc: 0.6280 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1981  | total loss: \u001b[1m\u001b[32m1.58229\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1981 | loss: 1.58229 - acc: 0.6652 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1982  | total loss: \u001b[1m\u001b[32m1.82301\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1982 | loss: 1.82301 - acc: 0.5987 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1983  | total loss: \u001b[1m\u001b[32m1.67739\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1983 | loss: 1.67739 - acc: 0.6388 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1984  | total loss: \u001b[1m\u001b[32m1.54766\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1984 | loss: 1.54766 - acc: 0.6750 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1985  | total loss: \u001b[1m\u001b[32m1.43163\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1985 | loss: 1.43163 - acc: 0.7075 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1986  | total loss: \u001b[1m\u001b[32m1.61640\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1986 | loss: 1.61640 - acc: 0.6439 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1987  | total loss: \u001b[1m\u001b[32m1.49462\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1987 | loss: 1.49462 - acc: 0.6795 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1988  | total loss: \u001b[1m\u001b[32m1.68541\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1988 | loss: 1.68541 - acc: 0.6258 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1989  | total loss: \u001b[1m\u001b[32m1.55834\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1989 | loss: 1.55834 - acc: 0.6632 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1990  | total loss: \u001b[1m\u001b[32m1.76465\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1990 | loss: 1.76465 - acc: 0.6041 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1991  | total loss: \u001b[1m\u001b[32m1.63183\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1991 | loss: 1.63183 - acc: 0.6436 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1992  | total loss: \u001b[1m\u001b[32m1.82436\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1992 | loss: 1.82436 - acc: 0.5864 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1993  | total loss: \u001b[1m\u001b[32m1.68816\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1993 | loss: 1.68816 - acc: 0.6278 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1994  | total loss: \u001b[1m\u001b[32m1.86292\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1994 | loss: 1.86292 - acc: 0.5721 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1995  | total loss: \u001b[1m\u001b[32m1.72572\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1995 | loss: 1.72572 - acc: 0.6149 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1996  | total loss: \u001b[1m\u001b[32m1.92426\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1996 | loss: 1.92426 - acc: 0.5534 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1997  | total loss: \u001b[1m\u001b[32m1.78407\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1997 | loss: 1.78407 - acc: 0.5981 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1998  | total loss: \u001b[1m\u001b[32m1.65906\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1998 | loss: 1.65906 - acc: 0.6383 -- iter: 14/14\n",
      "--\n",
      "Training Step: 1999  | total loss: \u001b[1m\u001b[32m1.54686\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1999 | loss: 1.54686 - acc: 0.6745 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2000  | total loss: \u001b[1m\u001b[32m1.73775\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2000 | loss: 1.73775 - acc: 0.6070 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2001  | total loss: \u001b[1m\u001b[32m1.61784\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2001 | loss: 1.61784 - acc: 0.6463 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2002  | total loss: \u001b[1m\u001b[32m1.76697\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2002 | loss: 1.76697 - acc: 0.5960 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2003  | total loss: \u001b[1m\u001b[32m1.64471\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2003 | loss: 1.64471 - acc: 0.6364 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2004  | total loss: \u001b[1m\u001b[32m1.80180\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2004 | loss: 1.80180 - acc: 0.5799 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2005  | total loss: \u001b[1m\u001b[32m1.67694\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2005 | loss: 1.67694 - acc: 0.6219 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2006  | total loss: \u001b[1m\u001b[32m1.82545\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2006 | loss: 1.82545 - acc: 0.5740 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2007  | total loss: \u001b[1m\u001b[32m1.69940\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2007 | loss: 1.69940 - acc: 0.6166 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2008  | total loss: \u001b[1m\u001b[32m1.58616\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2008 | loss: 1.58616 - acc: 0.6549 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2009  | total loss: \u001b[1m\u001b[32m1.48365\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2009 | loss: 1.48365 - acc: 0.6894 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2010  | total loss: \u001b[1m\u001b[32m1.68210\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2010 | loss: 1.68210 - acc: 0.6205 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2011  | total loss: \u001b[1m\u001b[32m1.56867\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2011 | loss: 1.56867 - acc: 0.6584 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2012  | total loss: \u001b[1m\u001b[32m1.46580\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2012 | loss: 1.46580 - acc: 0.6926 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2013  | total loss: \u001b[1m\u001b[32m1.37178\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2013 | loss: 1.37178 - acc: 0.7233 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2014  | total loss: \u001b[1m\u001b[32m1.55101\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2014 | loss: 1.55101 - acc: 0.6581 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2015  | total loss: \u001b[1m\u001b[32m1.44564\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2015 | loss: 1.44564 - acc: 0.6923 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2016  | total loss: \u001b[1m\u001b[32m1.66125\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2016 | loss: 1.66125 - acc: 0.6231 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2017  | total loss: \u001b[1m\u001b[32m1.54328\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2017 | loss: 1.54328 - acc: 0.6608 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2018  | total loss: \u001b[1m\u001b[32m1.74781\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2018 | loss: 1.74781 - acc: 0.6019 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2019  | total loss: \u001b[1m\u001b[32m1.62082\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2019 | loss: 1.62082 - acc: 0.6417 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2020  | total loss: \u001b[1m\u001b[32m1.50627\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2020 | loss: 1.50627 - acc: 0.6775 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2021  | total loss: \u001b[1m\u001b[32m1.40230\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2021 | loss: 1.40230 - acc: 0.7098 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2022  | total loss: \u001b[1m\u001b[32m1.58803\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2022 | loss: 1.58803 - acc: 0.6531 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2023  | total loss: \u001b[1m\u001b[32m1.47416\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2023 | loss: 1.47416 - acc: 0.6878 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2024  | total loss: \u001b[1m\u001b[32m1.66605\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2024 | loss: 1.66605 - acc: 0.6261 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2025  | total loss: \u001b[1m\u001b[32m1.54367\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2025 | loss: 1.54367 - acc: 0.6635 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2026  | total loss: \u001b[1m\u001b[32m1.76225\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2026 | loss: 1.76225 - acc: 0.5972 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2027  | total loss: \u001b[1m\u001b[32m1.63052\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2027 | loss: 1.63052 - acc: 0.6374 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2028  | total loss: \u001b[1m\u001b[32m1.79410\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2028 | loss: 1.79410 - acc: 0.5880 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2029  | total loss: \u001b[1m\u001b[32m1.66017\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2029 | loss: 1.66017 - acc: 0.6292 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2030  | total loss: \u001b[1m\u001b[32m1.87150\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2030 | loss: 1.87150 - acc: 0.5734 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2031  | total loss: \u001b[1m\u001b[32m1.73140\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2031 | loss: 1.73140 - acc: 0.6161 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2032  | total loss: \u001b[1m\u001b[32m1.89103\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2032 | loss: 1.89103 - acc: 0.5687 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2033  | total loss: \u001b[1m\u001b[32m1.75092\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2033 | loss: 1.75092 - acc: 0.6119 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2034  | total loss: \u001b[1m\u001b[32m1.90433\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2034 | loss: 1.90433 - acc: 0.5578 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2035  | total loss: \u001b[1m\u001b[32m1.76507\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2035 | loss: 1.76507 - acc: 0.6020 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2036  | total loss: \u001b[1m\u001b[32m1.64046\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2036 | loss: 1.64046 - acc: 0.6418 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2037  | total loss: \u001b[1m\u001b[32m1.52828\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2037 | loss: 1.52828 - acc: 0.6777 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2038  | total loss: \u001b[1m\u001b[32m1.68608\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2038 | loss: 1.68608 - acc: 0.6242 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2039  | total loss: \u001b[1m\u001b[32m1.56880\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2039 | loss: 1.56880 - acc: 0.6618 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2040  | total loss: \u001b[1m\u001b[32m1.46275\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2040 | loss: 1.46275 - acc: 0.6956 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2041  | total loss: \u001b[1m\u001b[32m1.36617\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2041 | loss: 1.36617 - acc: 0.7260 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2042  | total loss: \u001b[1m\u001b[32m1.27759\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2042 | loss: 1.27759 - acc: 0.7534 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2043  | total loss: \u001b[1m\u001b[32m1.19576\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2043 | loss: 1.19576 - acc: 0.7781 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2044  | total loss: \u001b[1m\u001b[32m1.41693\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2044 | loss: 1.41693 - acc: 0.7074 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2045  | total loss: \u001b[1m\u001b[32m1.31754\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2045 | loss: 1.31754 - acc: 0.7367 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2046  | total loss: \u001b[1m\u001b[32m1.53668\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2046 | loss: 1.53668 - acc: 0.6701 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2047  | total loss: \u001b[1m\u001b[32m1.42334\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2047 | loss: 1.42334 - acc: 0.7031 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2048  | total loss: \u001b[1m\u001b[32m1.32047\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2048 | loss: 1.32047 - acc: 0.7328 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2049  | total loss: \u001b[1m\u001b[32m1.22663\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2049 | loss: 1.22663 - acc: 0.7595 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2050  | total loss: \u001b[1m\u001b[32m1.41167\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2050 | loss: 1.41167 - acc: 0.6979 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2051  | total loss: \u001b[1m\u001b[32m1.30665\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2051 | loss: 1.30665 - acc: 0.7281 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2052  | total loss: \u001b[1m\u001b[32m1.57584\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2052 | loss: 1.57584 - acc: 0.6553 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2053  | total loss: \u001b[1m\u001b[32m1.45403\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2053 | loss: 1.45403 - acc: 0.6897 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2054  | total loss: \u001b[1m\u001b[32m1.71966\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2054 | loss: 1.71966 - acc: 0.6208 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2055  | total loss: \u001b[1m\u001b[32m1.58471\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2055 | loss: 1.58471 - acc: 0.6587 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2056  | total loss: \u001b[1m\u001b[32m1.78124\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2056 | loss: 1.78124 - acc: 0.6000 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2057  | total loss: \u001b[1m\u001b[32m1.64244\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2057 | loss: 1.64244 - acc: 0.6400 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2058  | total loss: \u001b[1m\u001b[32m1.82450\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2058 | loss: 1.82450 - acc: 0.5831 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2059  | total loss: \u001b[1m\u001b[32m1.68442\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2059 | loss: 1.68442 - acc: 0.6248 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2060  | total loss: \u001b[1m\u001b[32m1.84862\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2060 | loss: 1.84862 - acc: 0.5766 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2061  | total loss: \u001b[1m\u001b[32m1.70941\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2061 | loss: 1.70941 - acc: 0.6189 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2062  | total loss: \u001b[1m\u001b[32m1.58532\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2062 | loss: 1.58532 - acc: 0.6571 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2063  | total loss: \u001b[1m\u001b[32m1.47409\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2063 | loss: 1.47409 - acc: 0.6913 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2064  | total loss: \u001b[1m\u001b[32m1.71142\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2064 | loss: 1.71142 - acc: 0.6222 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2065  | total loss: \u001b[1m\u001b[32m1.58815\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2065 | loss: 1.58815 - acc: 0.6600 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2066  | total loss: \u001b[1m\u001b[32m1.74357\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2066 | loss: 1.74357 - acc: 0.6154 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2067  | total loss: \u001b[1m\u001b[32m1.61808\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2067 | loss: 1.61808 - acc: 0.6539 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2068  | total loss: \u001b[1m\u001b[32m1.76718\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2068 | loss: 1.76718 - acc: 0.6028 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2069  | total loss: \u001b[1m\u001b[32m1.64062\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2069 | loss: 1.64062 - acc: 0.6425 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2070  | total loss: \u001b[1m\u001b[32m1.52707\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2070 | loss: 1.52707 - acc: 0.6783 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2071  | total loss: \u001b[1m\u001b[32m1.42449\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2071 | loss: 1.42449 - acc: 0.7104 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2072  | total loss: \u001b[1m\u001b[32m1.64282\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2072 | loss: 1.64282 - acc: 0.6465 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2073  | total loss: \u001b[1m\u001b[32m1.52785\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2073 | loss: 1.52785 - acc: 0.6819 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2074  | total loss: \u001b[1m\u001b[32m1.42389\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2074 | loss: 1.42389 - acc: 0.7137 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2075  | total loss: \u001b[1m\u001b[32m1.32924\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2075 | loss: 1.32924 - acc: 0.7423 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2076  | total loss: \u001b[1m\u001b[32m1.55858\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2076 | loss: 1.55858 - acc: 0.6752 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2077  | total loss: \u001b[1m\u001b[32m1.44854\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2077 | loss: 1.44854 - acc: 0.7077 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2078  | total loss: \u001b[1m\u001b[32m1.34861\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2078 | loss: 1.34861 - acc: 0.7369 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2079  | total loss: \u001b[1m\u001b[32m1.25730\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2079 | loss: 1.25730 - acc: 0.7632 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2080  | total loss: \u001b[1m\u001b[32m1.17332\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2080 | loss: 1.17332 - acc: 0.7869 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2081  | total loss: \u001b[1m\u001b[32m1.09563\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2081 | loss: 1.09563 - acc: 0.8082 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2082  | total loss: \u001b[1m\u001b[32m1.02335\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2082 | loss: 1.02335 - acc: 0.8274 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2083  | total loss: \u001b[1m\u001b[32m0.95580\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2083 | loss: 0.95580 - acc: 0.8447 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2084  | total loss: \u001b[1m\u001b[32m1.24755\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2084 | loss: 1.24755 - acc: 0.7673 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2085  | total loss: \u001b[1m\u001b[32m1.15383\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2085 | loss: 1.15383 - acc: 0.7906 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2086  | total loss: \u001b[1m\u001b[32m1.40996\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2086 | loss: 1.40996 - acc: 0.7187 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2087  | total loss: \u001b[1m\u001b[32m1.29834\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2087 | loss: 1.29834 - acc: 0.7468 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2088  | total loss: \u001b[1m\u001b[32m1.59557\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2088 | loss: 1.59557 - acc: 0.6721 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2089  | total loss: \u001b[1m\u001b[32m1.46544\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2089 | loss: 1.46544 - acc: 0.7049 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2090  | total loss: \u001b[1m\u001b[32m1.65334\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2090 | loss: 1.65334 - acc: 0.6487 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2091  | total loss: \u001b[1m\u001b[32m1.51876\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2091 | loss: 1.51876 - acc: 0.6838 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2092  | total loss: \u001b[1m\u001b[32m1.72289\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2092 | loss: 1.72289 - acc: 0.6226 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2093  | total loss: \u001b[1m\u001b[32m1.58361\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2093 | loss: 1.58361 - acc: 0.6603 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2094  | total loss: \u001b[1m\u001b[32m1.45935\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2094 | loss: 1.45935 - acc: 0.6943 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2095  | total loss: \u001b[1m\u001b[32m1.34810\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2095 | loss: 1.34810 - acc: 0.7249 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2096  | total loss: \u001b[1m\u001b[32m1.24805\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2096 | loss: 1.24805 - acc: 0.7524 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2097  | total loss: \u001b[1m\u001b[32m1.15765\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2097 | loss: 1.15765 - acc: 0.7772 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2098  | total loss: \u001b[1m\u001b[32m1.40532\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2098 | loss: 1.40532 - acc: 0.7066 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2099  | total loss: \u001b[1m\u001b[32m1.29866\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2099 | loss: 1.29866 - acc: 0.7359 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2100  | total loss: \u001b[1m\u001b[32m1.54563\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2100 | loss: 1.54563 - acc: 0.6695 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2101  | total loss: \u001b[1m\u001b[32m1.42555\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2101 | loss: 1.42555 - acc: 0.7025 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2102  | total loss: \u001b[1m\u001b[32m1.63552\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2102 | loss: 1.63552 - acc: 0.6466 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2103  | total loss: \u001b[1m\u001b[32m1.50800\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2103 | loss: 1.50800 - acc: 0.6819 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2104  | total loss: \u001b[1m\u001b[32m1.39391\u001b[0m\u001b[0m | time: 0.025s\n",
      "| Adam | epoch: 2104 | loss: 1.39391 - acc: 0.7137 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2105  | total loss: \u001b[1m\u001b[32m1.29136\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2105 | loss: 1.29136 - acc: 0.7423 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2106  | total loss: \u001b[1m\u001b[32m1.53778\u001b[0m\u001b[0m | time: 0.034s\n",
      "| Adam | epoch: 2106 | loss: 1.53778 - acc: 0.6752 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2107  | total loss: \u001b[1m\u001b[32m1.42123\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2107 | loss: 1.42123 - acc: 0.7077 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2108  | total loss: \u001b[1m\u001b[32m1.65203\u001b[0m\u001b[0m | time: 0.016s\n",
      "| Adam | epoch: 2108 | loss: 1.65203 - acc: 0.6370 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2109  | total loss: \u001b[1m\u001b[32m1.52558\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2109 | loss: 1.52558 - acc: 0.6733 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2110  | total loss: \u001b[1m\u001b[32m1.73498\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2110 | loss: 1.73498 - acc: 0.6059 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2111  | total loss: \u001b[1m\u001b[32m1.60277\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2111 | loss: 1.60277 - acc: 0.6453 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2112  | total loss: \u001b[1m\u001b[32m1.81322\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2112 | loss: 1.81322 - acc: 0.5808 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2113  | total loss: \u001b[1m\u001b[32m1.67651\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2113 | loss: 1.67651 - acc: 0.6227 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2114  | total loss: \u001b[1m\u001b[32m1.86267\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2114 | loss: 1.86267 - acc: 0.5676 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2115  | total loss: \u001b[1m\u001b[32m1.72483\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2115 | loss: 1.72483 - acc: 0.6108 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2116  | total loss: \u001b[1m\u001b[32m1.91218\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2116 | loss: 1.91218 - acc: 0.5498 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2117  | total loss: \u001b[1m\u001b[32m1.77355\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2117 | loss: 1.77355 - acc: 0.5948 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2118  | total loss: \u001b[1m\u001b[32m1.65048\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2118 | loss: 1.65048 - acc: 0.6353 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2119  | total loss: \u001b[1m\u001b[32m1.54049\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2119 | loss: 1.54049 - acc: 0.6718 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2120  | total loss: \u001b[1m\u001b[32m1.44143\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2120 | loss: 1.44143 - acc: 0.7046 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2121  | total loss: \u001b[1m\u001b[32m1.35143\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2121 | loss: 1.35143 - acc: 0.7341 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2122  | total loss: \u001b[1m\u001b[32m1.26891\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2122 | loss: 1.26891 - acc: 0.7607 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2123  | total loss: \u001b[1m\u001b[32m1.19257\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2123 | loss: 1.19257 - acc: 0.7846 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2124  | total loss: \u001b[1m\u001b[32m1.40001\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2124 | loss: 1.40001 - acc: 0.7133 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2125  | total loss: \u001b[1m\u001b[32m1.30673\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2125 | loss: 1.30673 - acc: 0.7420 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2126  | total loss: \u001b[1m\u001b[32m1.22102\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2126 | loss: 1.22102 - acc: 0.7678 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2127  | total loss: \u001b[1m\u001b[32m1.14171\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2127 | loss: 1.14171 - acc: 0.7910 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2128  | total loss: \u001b[1m\u001b[32m1.41041\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2128 | loss: 1.41041 - acc: 0.7119 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2129  | total loss: \u001b[1m\u001b[32m1.30875\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2129 | loss: 1.30875 - acc: 0.7407 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2130  | total loss: \u001b[1m\u001b[32m1.21589\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2130 | loss: 1.21589 - acc: 0.7666 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2131  | total loss: \u001b[1m\u001b[32m1.13063\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2131 | loss: 1.13063 - acc: 0.7900 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2132  | total loss: \u001b[1m\u001b[32m1.05199\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2132 | loss: 1.05199 - acc: 0.8110 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2133  | total loss: \u001b[1m\u001b[32m0.97913\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2133 | loss: 0.97913 - acc: 0.8299 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2134  | total loss: \u001b[1m\u001b[32m0.91137\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2134 | loss: 0.91137 - acc: 0.8469 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2135  | total loss: \u001b[1m\u001b[32m0.84816\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2135 | loss: 0.84816 - acc: 0.8622 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2136  | total loss: \u001b[1m\u001b[32m1.14364\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2136 | loss: 1.14364 - acc: 0.7831 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2137  | total loss: \u001b[1m\u001b[32m1.05403\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2137 | loss: 1.05403 - acc: 0.8048 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2138  | total loss: \u001b[1m\u001b[32m0.97228\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2138 | loss: 0.97228 - acc: 0.8243 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2139  | total loss: \u001b[1m\u001b[32m0.89750\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2139 | loss: 0.89750 - acc: 0.8419 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2140  | total loss: \u001b[1m\u001b[32m0.82893\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2140 | loss: 0.82893 - acc: 0.8577 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2141  | total loss: \u001b[1m\u001b[32m0.76591\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2141 | loss: 0.76591 - acc: 0.8719 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2142  | total loss: \u001b[1m\u001b[32m1.14783\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2142 | loss: 1.14783 - acc: 0.7847 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2143  | total loss: \u001b[1m\u001b[32m1.05131\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2143 | loss: 1.05131 - acc: 0.8063 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2144  | total loss: \u001b[1m\u001b[32m1.34755\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2144 | loss: 1.34755 - acc: 0.7399 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2145  | total loss: \u001b[1m\u001b[32m1.23107\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2145 | loss: 1.23107 - acc: 0.7659 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2146  | total loss: \u001b[1m\u001b[32m1.12648\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2146 | loss: 1.12648 - acc: 0.7893 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2147  | total loss: \u001b[1m\u001b[32m1.03240\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2147 | loss: 1.03240 - acc: 0.8104 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2148  | total loss: \u001b[1m\u001b[32m0.94762\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2148 | loss: 0.94762 - acc: 0.8294 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2149  | total loss: \u001b[1m\u001b[32m0.87102\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2149 | loss: 0.87102 - acc: 0.8464 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2150  | total loss: \u001b[1m\u001b[32m1.23364\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2150 | loss: 1.23364 - acc: 0.7618 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2151  | total loss: \u001b[1m\u001b[32m1.12848\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2151 | loss: 1.12848 - acc: 0.7856 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2152  | total loss: \u001b[1m\u001b[32m1.03408\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2152 | loss: 1.03408 - acc: 0.8070 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2153  | total loss: \u001b[1m\u001b[32m0.94916\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2153 | loss: 0.94916 - acc: 0.8263 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2154  | total loss: \u001b[1m\u001b[32m0.87259\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2154 | loss: 0.87259 - acc: 0.8437 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2155  | total loss: \u001b[1m\u001b[32m0.80335\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2155 | loss: 0.80335 - acc: 0.8593 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2156  | total loss: \u001b[1m\u001b[32m0.74057\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2156 | loss: 0.74057 - acc: 0.8734 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2157  | total loss: \u001b[1m\u001b[32m0.68349\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2157 | loss: 0.68349 - acc: 0.8861 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2158  | total loss: \u001b[1m\u001b[32m1.00841\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2158 | loss: 1.00841 - acc: 0.8117 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2159  | total loss: \u001b[1m\u001b[32m0.92398\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2159 | loss: 0.92398 - acc: 0.8306 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2160  | total loss: \u001b[1m\u001b[32m1.30780\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2160 | loss: 1.30780 - acc: 0.7475 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2161  | total loss: \u001b[1m\u001b[32m1.19420\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2161 | loss: 1.19420 - acc: 0.7728 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2162  | total loss: \u001b[1m\u001b[32m1.54331\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2162 | loss: 1.54331 - acc: 0.6955 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2163  | total loss: \u001b[1m\u001b[32m1.40817\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2163 | loss: 1.40817 - acc: 0.7259 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2164  | total loss: \u001b[1m\u001b[32m1.71998\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2164 | loss: 1.71998 - acc: 0.6533 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2165  | total loss: \u001b[1m\u001b[32m1.57039\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2165 | loss: 1.57039 - acc: 0.6880 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2166  | total loss: \u001b[1m\u001b[32m1.75733\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2166 | loss: 1.75733 - acc: 0.6406 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2167  | total loss: \u001b[1m\u001b[32m1.60818\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2167 | loss: 1.60818 - acc: 0.6766 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2168  | total loss: \u001b[1m\u001b[32m1.78772\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2168 | loss: 1.78772 - acc: 0.6303 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2169  | total loss: \u001b[1m\u001b[32m1.64039\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2169 | loss: 1.64039 - acc: 0.6673 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2170  | total loss: \u001b[1m\u001b[32m1.84878\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2170 | loss: 1.84878 - acc: 0.6077 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2171  | total loss: \u001b[1m\u001b[32m1.70092\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2171 | loss: 1.70092 - acc: 0.6469 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2172  | total loss: \u001b[1m\u001b[32m1.57044\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2172 | loss: 1.57044 - acc: 0.6823 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2173  | total loss: \u001b[1m\u001b[32m1.45486\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2173 | loss: 1.45486 - acc: 0.7140 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2174  | total loss: \u001b[1m\u001b[32m1.63678\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2174 | loss: 1.63678 - acc: 0.6498 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2175  | total loss: \u001b[1m\u001b[32m1.51776\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2175 | loss: 1.51776 - acc: 0.6848 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2176  | total loss: \u001b[1m\u001b[32m1.71573\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2176 | loss: 1.71573 - acc: 0.6235 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2177  | total loss: \u001b[1m\u001b[32m1.59217\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2177 | loss: 1.59217 - acc: 0.6611 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2178  | total loss: \u001b[1m\u001b[32m1.76250\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2178 | loss: 1.76250 - acc: 0.6021 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2179  | total loss: \u001b[1m\u001b[32m1.63759\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2179 | loss: 1.63759 - acc: 0.6419 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2180  | total loss: \u001b[1m\u001b[32m1.52636\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2180 | loss: 1.52636 - acc: 0.6777 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2181  | total loss: \u001b[1m\u001b[32m1.42657\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2181 | loss: 1.42657 - acc: 0.7100 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2182  | total loss: \u001b[1m\u001b[32m1.61512\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2182 | loss: 1.61512 - acc: 0.6461 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2183  | total loss: \u001b[1m\u001b[32m1.50666\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2183 | loss: 1.50666 - acc: 0.6815 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2184  | total loss: \u001b[1m\u001b[32m1.40889\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2184 | loss: 1.40889 - acc: 0.7133 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2185  | total loss: \u001b[1m\u001b[32m1.32002\u001b[0m\u001b[0m | time: 0.025s\n",
      "| Adam | epoch: 2185 | loss: 1.32002 - acc: 0.7420 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2186  | total loss: \u001b[1m\u001b[32m1.51949\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2186 | loss: 1.51949 - acc: 0.6821 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2187  | total loss: \u001b[1m\u001b[32m1.41758\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2187 | loss: 1.41758 - acc: 0.7139 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2188  | total loss: \u001b[1m\u001b[32m1.64993\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2188 | loss: 1.64993 - acc: 0.6425 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2189  | total loss: \u001b[1m\u001b[32m1.53407\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2189 | loss: 1.53407 - acc: 0.6782 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2190  | total loss: \u001b[1m\u001b[32m1.42931\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2190 | loss: 1.42931 - acc: 0.7104 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2191  | total loss: \u001b[1m\u001b[32m1.33392\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2191 | loss: 1.33392 - acc: 0.7394 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2192  | total loss: \u001b[1m\u001b[32m1.50489\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2192 | loss: 1.50489 - acc: 0.6797 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2193  | total loss: \u001b[1m\u001b[32m1.39963\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2193 | loss: 1.39963 - acc: 0.7118 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2194  | total loss: \u001b[1m\u001b[32m1.30369\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2194 | loss: 1.30369 - acc: 0.7406 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2195  | total loss: \u001b[1m\u001b[32m1.21569\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2195 | loss: 1.21569 - acc: 0.7665 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2196  | total loss: \u001b[1m\u001b[32m1.13448\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2196 | loss: 1.13448 - acc: 0.7899 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2197  | total loss: \u001b[1m\u001b[32m1.05914\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2197 | loss: 1.05914 - acc: 0.8109 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2198  | total loss: \u001b[1m\u001b[32m1.36122\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2198 | loss: 1.36122 - acc: 0.7298 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2199  | total loss: \u001b[1m\u001b[32m1.25978\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2199 | loss: 1.25978 - acc: 0.7568 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2200  | total loss: \u001b[1m\u001b[32m1.16721\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2200 | loss: 1.16721 - acc: 0.7811 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2201  | total loss: \u001b[1m\u001b[32m1.08236\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2201 | loss: 1.08236 - acc: 0.8030 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2202  | total loss: \u001b[1m\u001b[32m1.37022\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2202 | loss: 1.37022 - acc: 0.7227 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2203  | total loss: \u001b[1m\u001b[32m1.26293\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2203 | loss: 1.26293 - acc: 0.7504 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2204  | total loss: \u001b[1m\u001b[32m1.47307\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2204 | loss: 1.47307 - acc: 0.6968 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2205  | total loss: \u001b[1m\u001b[32m1.35496\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2205 | loss: 1.35496 - acc: 0.7271 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2206  | total loss: \u001b[1m\u001b[32m1.24852\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2206 | loss: 1.24852 - acc: 0.7544 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2207  | total loss: \u001b[1m\u001b[32m1.15228\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2207 | loss: 1.15228 - acc: 0.7790 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2208  | total loss: \u001b[1m\u001b[32m1.45326\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2208 | loss: 1.45326 - acc: 0.7011 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2209  | total loss: \u001b[1m\u001b[32m1.33631\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2209 | loss: 1.33631 - acc: 0.7310 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2210  | total loss: \u001b[1m\u001b[32m1.59549\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2210 | loss: 1.59549 - acc: 0.6579 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2211  | total loss: \u001b[1m\u001b[32m1.46570\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2211 | loss: 1.46570 - acc: 0.6921 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2212  | total loss: \u001b[1m\u001b[32m1.67874\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2212 | loss: 1.67874 - acc: 0.6372 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2213  | total loss: \u001b[1m\u001b[32m1.54314\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2213 | loss: 1.54314 - acc: 0.6735 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2214  | total loss: \u001b[1m\u001b[32m1.42226\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2214 | loss: 1.42226 - acc: 0.7061 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2215  | total loss: \u001b[1m\u001b[32m1.31412\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2215 | loss: 1.31412 - acc: 0.7355 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2216  | total loss: \u001b[1m\u001b[32m1.54042\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2216 | loss: 1.54042 - acc: 0.6691 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2217  | total loss: \u001b[1m\u001b[32m1.42175\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2217 | loss: 1.42175 - acc: 0.7022 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2218  | total loss: \u001b[1m\u001b[32m1.63329\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2218 | loss: 1.63329 - acc: 0.6462 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2219  | total loss: \u001b[1m\u001b[32m1.50735\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2219 | loss: 1.50735 - acc: 0.6816 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2220  | total loss: \u001b[1m\u001b[32m1.39484\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2220 | loss: 1.39484 - acc: 0.7135 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2221  | total loss: \u001b[1m\u001b[32m1.29381\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2221 | loss: 1.29381 - acc: 0.7421 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2222  | total loss: \u001b[1m\u001b[32m1.49786\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2222 | loss: 1.49786 - acc: 0.6822 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2223  | total loss: \u001b[1m\u001b[32m1.38684\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2223 | loss: 1.38684 - acc: 0.7140 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2224  | total loss: \u001b[1m\u001b[32m1.59110\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2224 | loss: 1.59110 - acc: 0.6497 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2225  | total loss: \u001b[1m\u001b[32m1.47198\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2225 | loss: 1.47198 - acc: 0.6847 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2226  | total loss: \u001b[1m\u001b[32m1.36532\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2226 | loss: 1.36532 - acc: 0.7163 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2227  | total loss: \u001b[1m\u001b[32m1.26929\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2227 | loss: 1.26929 - acc: 0.7446 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2228  | total loss: \u001b[1m\u001b[32m1.46908\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2228 | loss: 1.46908 - acc: 0.6916 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2229  | total loss: \u001b[1m\u001b[32m1.36247\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2229 | loss: 1.36247 - acc: 0.7224 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2230  | total loss: \u001b[1m\u001b[32m1.54948\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2230 | loss: 1.54948 - acc: 0.6645 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2231  | total loss: \u001b[1m\u001b[32m1.43534\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2231 | loss: 1.43534 - acc: 0.6980 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2232  | total loss: \u001b[1m\u001b[32m1.65688\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2232 | loss: 1.65688 - acc: 0.6354 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2233  | total loss: \u001b[1m\u001b[32m1.53326\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2233 | loss: 1.53326 - acc: 0.6718 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2234  | total loss: \u001b[1m\u001b[32m1.74272\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2234 | loss: 1.74272 - acc: 0.6047 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2235  | total loss: \u001b[1m\u001b[32m1.61267\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2235 | loss: 1.61267 - acc: 0.6442 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2236  | total loss: \u001b[1m\u001b[32m1.49652\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2236 | loss: 1.49652 - acc: 0.6798 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2237  | total loss: \u001b[1m\u001b[32m1.39217\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2237 | loss: 1.39217 - acc: 0.7118 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2238  | total loss: \u001b[1m\u001b[32m1.57148\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2238 | loss: 1.57148 - acc: 0.6620 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2239  | total loss: \u001b[1m\u001b[32m1.45958\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2239 | loss: 1.45958 - acc: 0.6958 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2240  | total loss: \u001b[1m\u001b[32m1.35863\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2240 | loss: 1.35863 - acc: 0.7263 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2241  | total loss: \u001b[1m\u001b[32m1.26695\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2241 | loss: 1.26695 - acc: 0.7536 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2242  | total loss: \u001b[1m\u001b[32m1.50437\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2242 | loss: 1.50437 - acc: 0.6854 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2243  | total loss: \u001b[1m\u001b[32m1.39665\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2243 | loss: 1.39665 - acc: 0.7169 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2244  | total loss: \u001b[1m\u001b[32m1.64104\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2244 | loss: 1.64104 - acc: 0.6452 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2245  | total loss: \u001b[1m\u001b[32m1.51950\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2245 | loss: 1.51950 - acc: 0.6807 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2246  | total loss: \u001b[1m\u001b[32m1.68822\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2246 | loss: 1.68822 - acc: 0.6340 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2247  | total loss: \u001b[1m\u001b[32m1.56257\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2247 | loss: 1.56257 - acc: 0.6706 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2248  | total loss: \u001b[1m\u001b[32m1.76101\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2248 | loss: 1.76101 - acc: 0.6107 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2249  | total loss: \u001b[1m\u001b[32m1.62931\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2249 | loss: 1.62931 - acc: 0.6496 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2250  | total loss: \u001b[1m\u001b[32m1.82703\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2250 | loss: 1.82703 - acc: 0.5847 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2251  | total loss: \u001b[1m\u001b[32m1.69065\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2251 | loss: 1.69065 - acc: 0.6262 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2252  | total loss: \u001b[1m\u001b[32m1.86637\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2252 | loss: 1.86637 - acc: 0.5707 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2253  | total loss: \u001b[1m\u001b[32m1.72842\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2253 | loss: 1.72842 - acc: 0.6137 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2254  | total loss: \u001b[1m\u001b[32m1.90070\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2254 | loss: 1.90070 - acc: 0.5594 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2255  | total loss: \u001b[1m\u001b[32m1.76193\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2255 | loss: 1.76193 - acc: 0.6035 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2256  | total loss: \u001b[1m\u001b[32m1.89340\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2256 | loss: 1.89340 - acc: 0.5574 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2257  | total loss: \u001b[1m\u001b[32m1.75794\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2257 | loss: 1.75794 - acc: 0.6017 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2258  | total loss: \u001b[1m\u001b[32m1.89580\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2258 | loss: 1.89580 - acc: 0.5558 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2259  | total loss: \u001b[1m\u001b[32m1.76251\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2259 | loss: 1.76251 - acc: 0.6002 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2260  | total loss: \u001b[1m\u001b[32m1.92734\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2260 | loss: 1.92734 - acc: 0.5402 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2261  | total loss: \u001b[1m\u001b[32m1.79323\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2261 | loss: 1.79323 - acc: 0.5862 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2262  | total loss: \u001b[1m\u001b[32m1.67319\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2262 | loss: 1.67319 - acc: 0.6276 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2263  | total loss: \u001b[1m\u001b[32m1.56496\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2263 | loss: 1.56496 - acc: 0.6648 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2264  | total loss: \u001b[1m\u001b[32m1.74170\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2264 | loss: 1.74170 - acc: 0.6055 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2265  | total loss: \u001b[1m\u001b[32m1.62580\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2265 | loss: 1.62580 - acc: 0.6449 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2266  | total loss: \u001b[1m\u001b[32m1.83025\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2266 | loss: 1.83025 - acc: 0.5804 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2267  | total loss: \u001b[1m\u001b[32m1.70545\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2267 | loss: 1.70545 - acc: 0.6224 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2268  | total loss: \u001b[1m\u001b[32m1.90591\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2268 | loss: 1.90591 - acc: 0.5601 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2269  | total loss: \u001b[1m\u001b[32m1.77431\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2269 | loss: 1.77431 - acc: 0.6041 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2270  | total loss: \u001b[1m\u001b[32m1.94393\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2270 | loss: 1.94393 - acc: 0.5437 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2271  | total loss: \u001b[1m\u001b[32m1.80985\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2271 | loss: 1.80985 - acc: 0.5893 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2272  | total loss: \u001b[1m\u001b[32m1.96898\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2272 | loss: 1.96898 - acc: 0.5376 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2273  | total loss: \u001b[1m\u001b[32m1.83395\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2273 | loss: 1.83395 - acc: 0.5838 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2274  | total loss: \u001b[1m\u001b[32m1.95217\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2274 | loss: 1.95217 - acc: 0.5326 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2275  | total loss: \u001b[1m\u001b[32m1.82030\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2275 | loss: 1.82030 - acc: 0.5793 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2276  | total loss: \u001b[1m\u001b[32m1.95814\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2276 | loss: 1.95814 - acc: 0.5357 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2277  | total loss: \u001b[1m\u001b[32m1.82681\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2277 | loss: 1.82681 - acc: 0.5821 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2278  | total loss: \u001b[1m\u001b[32m1.95428\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2278 | loss: 1.95428 - acc: 0.5382 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2279  | total loss: \u001b[1m\u001b[32m1.82393\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2279 | loss: 1.82393 - acc: 0.5844 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2280  | total loss: \u001b[1m\u001b[32m1.98395\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2280 | loss: 1.98395 - acc: 0.5259 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2281  | total loss: \u001b[1m\u001b[32m1.85109\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2281 | loss: 1.85109 - acc: 0.5733 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2282  | total loss: \u001b[1m\u001b[32m2.00323\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2282 | loss: 2.00323 - acc: 0.5160 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2283  | total loss: \u001b[1m\u001b[32m1.86920\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2283 | loss: 1.86920 - acc: 0.5644 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2284  | total loss: \u001b[1m\u001b[32m1.74852\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2284 | loss: 1.74852 - acc: 0.6080 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2285  | total loss: \u001b[1m\u001b[32m1.63899\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2285 | loss: 1.63899 - acc: 0.6472 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2286  | total loss: \u001b[1m\u001b[32m1.81129\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2286 | loss: 1.81129 - acc: 0.5824 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2287  | total loss: \u001b[1m\u001b[32m1.69327\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2287 | loss: 1.69327 - acc: 0.6242 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2288  | total loss: \u001b[1m\u001b[32m1.58574\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2288 | loss: 1.58574 - acc: 0.6618 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2289  | total loss: \u001b[1m\u001b[32m1.48699\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2289 | loss: 1.48699 - acc: 0.6956 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2290  | total loss: \u001b[1m\u001b[32m1.67857\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2290 | loss: 1.67857 - acc: 0.6260 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2291  | total loss: \u001b[1m\u001b[32m1.56690\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2291 | loss: 1.56690 - acc: 0.6634 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2292  | total loss: \u001b[1m\u001b[32m1.72791\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2292 | loss: 1.72791 - acc: 0.6042 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2293  | total loss: \u001b[1m\u001b[32m1.60889\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2293 | loss: 1.60889 - acc: 0.6438 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2294  | total loss: \u001b[1m\u001b[32m1.79706\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2294 | loss: 1.79706 - acc: 0.5866 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2295  | total loss: \u001b[1m\u001b[32m1.66969\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2295 | loss: 1.66969 - acc: 0.6279 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2296  | total loss: \u001b[1m\u001b[32m1.81610\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2296 | loss: 1.81610 - acc: 0.5794 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2297  | total loss: \u001b[1m\u001b[32m1.68618\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2297 | loss: 1.68618 - acc: 0.6215 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2298  | total loss: \u001b[1m\u001b[32m1.84932\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2298 | loss: 1.84932 - acc: 0.5665 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2299  | total loss: \u001b[1m\u001b[32m1.71607\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2299 | loss: 1.71607 - acc: 0.6098 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2300  | total loss: \u001b[1m\u001b[32m1.91046\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2300 | loss: 1.91046 - acc: 0.5488 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2301  | total loss: \u001b[1m\u001b[32m1.77189\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2301 | loss: 1.77189 - acc: 0.5940 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2302  | total loss: \u001b[1m\u001b[32m1.95615\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2302 | loss: 1.95615 - acc: 0.5346 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2303  | total loss: \u001b[1m\u001b[32m1.81470\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2303 | loss: 1.81470 - acc: 0.5811 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2304  | total loss: \u001b[1m\u001b[32m1.98521\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2304 | loss: 1.98521 - acc: 0.5230 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2305  | total loss: \u001b[1m\u001b[32m1.84310\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2305 | loss: 1.84310 - acc: 0.5707 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2306  | total loss: \u001b[1m\u001b[32m1.98255\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2306 | loss: 1.98255 - acc: 0.5279 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2307  | total loss: \u001b[1m\u001b[32m1.84298\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2307 | loss: 1.84298 - acc: 0.5751 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2308  | total loss: \u001b[1m\u001b[32m1.97108\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2308 | loss: 1.97108 - acc: 0.5247 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2309  | total loss: \u001b[1m\u001b[32m1.83498\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2309 | loss: 1.83498 - acc: 0.5723 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2310  | total loss: \u001b[1m\u001b[32m2.00487\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2310 | loss: 2.00487 - acc: 0.5150 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2311  | total loss: \u001b[1m\u001b[32m1.86781\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2311 | loss: 1.86781 - acc: 0.5635 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2312  | total loss: \u001b[1m\u001b[32m2.03441\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2312 | loss: 2.03441 - acc: 0.5072 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2313  | total loss: \u001b[1m\u001b[32m1.89691\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2313 | loss: 1.89691 - acc: 0.5565 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2314  | total loss: \u001b[1m\u001b[32m2.01868\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2314 | loss: 2.01868 - acc: 0.5080 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2315  | total loss: \u001b[1m\u001b[32m1.88501\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2315 | loss: 1.88501 - acc: 0.5572 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2316  | total loss: \u001b[1m\u001b[32m1.97423\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2316 | loss: 1.97423 - acc: 0.5229 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2317  | total loss: \u001b[1m\u001b[32m1.84630\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2317 | loss: 1.84630 - acc: 0.5706 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2318  | total loss: \u001b[1m\u001b[32m1.99009\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2318 | loss: 1.99009 - acc: 0.5207 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2319  | total loss: \u001b[1m\u001b[32m1.86118\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2319 | loss: 1.86118 - acc: 0.5686 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2320  | total loss: \u001b[1m\u001b[32m1.99112\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2320 | loss: 1.99112 - acc: 0.5189 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2321  | total loss: \u001b[1m\u001b[32m1.86246\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2321 | loss: 1.86246 - acc: 0.5670 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2322  | total loss: \u001b[1m\u001b[32m1.74624\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2322 | loss: 1.74624 - acc: 0.6103 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2323  | total loss: \u001b[1m\u001b[32m1.64027\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2323 | loss: 1.64027 - acc: 0.6493 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2324  | total loss: \u001b[1m\u001b[32m1.79343\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2324 | loss: 1.79343 - acc: 0.5915 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2325  | total loss: \u001b[1m\u001b[32m1.67946\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2325 | loss: 1.67946 - acc: 0.6323 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2326  | total loss: \u001b[1m\u001b[32m1.83121\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2326 | loss: 1.83121 - acc: 0.5762 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2327  | total loss: \u001b[1m\u001b[32m1.71085\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2327 | loss: 1.71085 - acc: 0.6186 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2328  | total loss: \u001b[1m\u001b[32m1.89087\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2328 | loss: 1.89087 - acc: 0.5568 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2329  | total loss: \u001b[1m\u001b[32m1.76277\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2329 | loss: 1.76277 - acc: 0.6011 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2330  | total loss: \u001b[1m\u001b[32m1.89181\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2330 | loss: 1.89181 - acc: 0.5553 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2331  | total loss: \u001b[1m\u001b[32m1.76241\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2331 | loss: 1.76241 - acc: 0.5997 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2332  | total loss: \u001b[1m\u001b[32m1.91578\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2332 | loss: 1.91578 - acc: 0.5469 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2333  | total loss: \u001b[1m\u001b[32m1.78319\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2333 | loss: 1.78319 - acc: 0.5922 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2334  | total loss: \u001b[1m\u001b[32m1.95931\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2334 | loss: 1.95931 - acc: 0.5330 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2335  | total loss: \u001b[1m\u001b[32m1.82219\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2335 | loss: 1.82219 - acc: 0.5797 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2336  | total loss: \u001b[1m\u001b[32m1.69840\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2336 | loss: 1.69840 - acc: 0.6217 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2337  | total loss: \u001b[1m\u001b[32m1.58583\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2337 | loss: 1.58583 - acc: 0.6596 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2338  | total loss: \u001b[1m\u001b[32m1.74090\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2338 | loss: 1.74090 - acc: 0.6007 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2339  | total loss: \u001b[1m\u001b[32m1.62162\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2339 | loss: 1.62162 - acc: 0.6407 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2340  | total loss: \u001b[1m\u001b[32m1.51294\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2340 | loss: 1.51294 - acc: 0.6766 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2341  | total loss: \u001b[1m\u001b[32m1.41324\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2341 | loss: 1.41324 - acc: 0.7089 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2342  | total loss: \u001b[1m\u001b[32m1.63671\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2342 | loss: 1.63671 - acc: 0.6380 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2343  | total loss: \u001b[1m\u001b[32m1.52128\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2343 | loss: 1.52128 - acc: 0.6742 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2344  | total loss: \u001b[1m\u001b[32m1.73565\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2344 | loss: 1.73565 - acc: 0.6068 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2345  | total loss: \u001b[1m\u001b[32m1.60865\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2345 | loss: 1.60865 - acc: 0.6461 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2346  | total loss: \u001b[1m\u001b[32m1.80731\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2346 | loss: 1.80731 - acc: 0.5887 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2347  | total loss: \u001b[1m\u001b[32m1.67267\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2347 | loss: 1.67267 - acc: 0.6298 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2348  | total loss: \u001b[1m\u001b[32m1.55114\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2348 | loss: 1.55114 - acc: 0.6668 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2349  | total loss: \u001b[1m\u001b[32m1.44085\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2349 | loss: 1.44085 - acc: 0.7001 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2350  | total loss: \u001b[1m\u001b[32m1.65435\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2350 | loss: 1.65435 - acc: 0.6373 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2351  | total loss: \u001b[1m\u001b[32m1.53213\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2351 | loss: 1.53213 - acc: 0.6735 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2352  | total loss: \u001b[1m\u001b[32m1.69857\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2352 | loss: 1.69857 - acc: 0.6205 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2353  | total loss: \u001b[1m\u001b[32m1.57144\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2353 | loss: 1.57144 - acc: 0.6584 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2354  | total loss: \u001b[1m\u001b[32m1.45673\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2354 | loss: 1.45673 - acc: 0.6926 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2355  | total loss: \u001b[1m\u001b[32m1.35267\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2355 | loss: 1.35267 - acc: 0.7233 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2356  | total loss: \u001b[1m\u001b[32m1.60604\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2356 | loss: 1.60604 - acc: 0.6510 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2357  | total loss: \u001b[1m\u001b[32m1.48583\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2357 | loss: 1.48583 - acc: 0.6859 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2358  | total loss: \u001b[1m\u001b[32m1.37715\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2358 | loss: 1.37715 - acc: 0.7173 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2359  | total loss: \u001b[1m\u001b[32m1.27841\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2359 | loss: 1.27841 - acc: 0.7456 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2360  | total loss: \u001b[1m\u001b[32m1.50669\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2360 | loss: 1.50669 - acc: 0.6782 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2361  | total loss: \u001b[1m\u001b[32m1.39350\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2361 | loss: 1.39350 - acc: 0.7103 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2362  | total loss: \u001b[1m\u001b[32m1.63516\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2362 | loss: 1.63516 - acc: 0.6465 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2363  | total loss: \u001b[1m\u001b[32m1.50904\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2363 | loss: 1.50904 - acc: 0.6818 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2364  | total loss: \u001b[1m\u001b[32m1.74660\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2364 | loss: 1.74660 - acc: 0.6136 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2365  | total loss: \u001b[1m\u001b[32m1.61049\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2365 | loss: 1.61049 - acc: 0.6523 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2366  | total loss: \u001b[1m\u001b[32m1.84596\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2366 | loss: 1.84596 - acc: 0.5870 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2367  | total loss: \u001b[1m\u001b[32m1.70221\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2367 | loss: 1.70221 - acc: 0.6283 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2368  | total loss: \u001b[1m\u001b[32m1.88724\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2368 | loss: 1.88724 - acc: 0.5726 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2369  | total loss: \u001b[1m\u001b[32m1.74242\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2369 | loss: 1.74242 - acc: 0.6154 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2370  | total loss: \u001b[1m\u001b[32m1.92669\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2370 | loss: 1.92669 - acc: 0.5610 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2371  | total loss: \u001b[1m\u001b[32m1.78143\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2371 | loss: 1.78143 - acc: 0.6049 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2372  | total loss: \u001b[1m\u001b[32m1.91758\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2372 | loss: 1.91758 - acc: 0.5587 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2373  | total loss: \u001b[1m\u001b[32m1.77685\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2373 | loss: 1.77685 - acc: 0.6028 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2374  | total loss: \u001b[1m\u001b[32m1.96119\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2374 | loss: 1.96119 - acc: 0.5425 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2375  | total loss: \u001b[1m\u001b[32m1.81981\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2375 | loss: 1.81981 - acc: 0.5883 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2376  | total loss: \u001b[1m\u001b[32m1.98480\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2376 | loss: 1.98480 - acc: 0.5295 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2377  | total loss: \u001b[1m\u001b[32m1.84486\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2377 | loss: 1.84486 - acc: 0.5765 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2378  | total loss: \u001b[1m\u001b[32m1.72026\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2378 | loss: 1.72026 - acc: 0.6189 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2379  | total loss: \u001b[1m\u001b[32m1.60850\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2379 | loss: 1.60850 - acc: 0.6570 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2380  | total loss: \u001b[1m\u001b[32m1.73413\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2380 | loss: 1.73413 - acc: 0.6056 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2381  | total loss: \u001b[1m\u001b[32m1.62076\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2381 | loss: 1.62076 - acc: 0.6450 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2382  | total loss: \u001b[1m\u001b[32m1.80415\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2382 | loss: 1.80415 - acc: 0.5805 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2383  | total loss: \u001b[1m\u001b[32m1.68380\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2383 | loss: 1.68380 - acc: 0.6225 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2384  | total loss: \u001b[1m\u001b[32m1.57521\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2384 | loss: 1.57521 - acc: 0.6602 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2385  | total loss: \u001b[1m\u001b[32m1.47638\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2385 | loss: 1.47638 - acc: 0.6942 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2386  | total loss: \u001b[1m\u001b[32m1.67965\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2386 | loss: 1.67965 - acc: 0.6248 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2387  | total loss: \u001b[1m\u001b[32m1.56803\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2387 | loss: 1.56803 - acc: 0.6623 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2388  | total loss: \u001b[1m\u001b[32m1.74083\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2388 | loss: 1.74083 - acc: 0.6032 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2389  | total loss: \u001b[1m\u001b[32m1.62174\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2389 | loss: 1.62174 - acc: 0.6429 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2390  | total loss: \u001b[1m\u001b[32m1.79948\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2390 | loss: 1.79948 - acc: 0.5786 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2391  | total loss: \u001b[1m\u001b[32m1.67406\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2391 | loss: 1.67406 - acc: 0.6207 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2392  | total loss: \u001b[1m\u001b[32m1.56078\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2392 | loss: 1.56078 - acc: 0.6587 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2393  | total loss: \u001b[1m\u001b[32m1.45773\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2393 | loss: 1.45773 - acc: 0.6928 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2394  | total loss: \u001b[1m\u001b[32m1.63426\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2394 | loss: 1.63426 - acc: 0.6307 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2395  | total loss: \u001b[1m\u001b[32m1.52166\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2395 | loss: 1.52166 - acc: 0.6676 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2396  | total loss: \u001b[1m\u001b[32m1.41918\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2396 | loss: 1.41918 - acc: 0.7008 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2397  | total loss: \u001b[1m\u001b[32m1.32525\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2397 | loss: 1.32525 - acc: 0.7308 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2398  | total loss: \u001b[1m\u001b[32m1.56254\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2398 | loss: 1.56254 - acc: 0.6648 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2399  | total loss: \u001b[1m\u001b[32m1.45129\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2399 | loss: 1.45129 - acc: 0.6983 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2400  | total loss: \u001b[1m\u001b[32m1.34981\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2400 | loss: 1.34981 - acc: 0.7285 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2401  | total loss: \u001b[1m\u001b[32m1.25668\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2401 | loss: 1.25668 - acc: 0.7557 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2402  | total loss: \u001b[1m\u001b[32m1.49185\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2402 | loss: 1.49185 - acc: 0.6801 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2403  | total loss: \u001b[1m\u001b[32m1.38167\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2403 | loss: 1.38167 - acc: 0.7121 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2404  | total loss: \u001b[1m\u001b[32m1.56713\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2404 | loss: 1.56713 - acc: 0.6552 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2405  | total loss: \u001b[1m\u001b[32m1.44819\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2405 | loss: 1.44819 - acc: 0.6896 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2406  | total loss: \u001b[1m\u001b[32m1.34059\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2406 | loss: 1.34059 - acc: 0.7207 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2407  | total loss: \u001b[1m\u001b[32m1.24275\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2407 | loss: 1.24275 - acc: 0.7486 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2408  | total loss: \u001b[1m\u001b[32m1.50587\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2408 | loss: 1.50587 - acc: 0.6809 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2409  | total loss: \u001b[1m\u001b[32m1.39008\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2409 | loss: 1.39008 - acc: 0.7128 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2410  | total loss: \u001b[1m\u001b[32m1.62158\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2410 | loss: 1.62158 - acc: 0.6487 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2411  | total loss: \u001b[1m\u001b[32m1.49431\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2411 | loss: 1.49431 - acc: 0.6838 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2412  | total loss: \u001b[1m\u001b[32m1.73957\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2412 | loss: 1.73957 - acc: 0.6154 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2413  | total loss: \u001b[1m\u001b[32m1.60184\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2413 | loss: 1.60184 - acc: 0.6539 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2414  | total loss: \u001b[1m\u001b[32m1.75502\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2414 | loss: 1.75502 - acc: 0.6099 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2415  | total loss: \u001b[1m\u001b[32m1.61781\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2415 | loss: 1.61781 - acc: 0.6489 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2416  | total loss: \u001b[1m\u001b[32m1.79812\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2416 | loss: 1.79812 - acc: 0.5912 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2417  | total loss: \u001b[1m\u001b[32m1.65923\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2417 | loss: 1.65923 - acc: 0.6321 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2418  | total loss: \u001b[1m\u001b[32m1.88682\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2418 | loss: 1.88682 - acc: 0.5689 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2419  | total loss: \u001b[1m\u001b[32m1.74258\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2419 | loss: 1.74258 - acc: 0.6120 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2420  | total loss: \u001b[1m\u001b[32m1.89178\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2420 | loss: 1.89178 - acc: 0.5579 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2421  | total loss: \u001b[1m\u001b[32m1.75108\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2421 | loss: 1.75108 - acc: 0.6021 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2422  | total loss: \u001b[1m\u001b[32m1.90483\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2422 | loss: 1.90483 - acc: 0.5491 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2423  | total loss: \u001b[1m\u001b[32m1.76674\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2423 | loss: 1.76674 - acc: 0.5941 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2424  | total loss: \u001b[1m\u001b[32m1.92666\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2424 | loss: 1.92666 - acc: 0.5419 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2425  | total loss: \u001b[1m\u001b[32m1.79012\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2425 | loss: 1.79012 - acc: 0.5877 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2426  | total loss: \u001b[1m\u001b[32m1.95820\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2426 | loss: 1.95820 - acc: 0.5289 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2427  | total loss: \u001b[1m\u001b[32m1.82211\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2427 | loss: 1.82211 - acc: 0.5760 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2428  | total loss: \u001b[1m\u001b[32m1.70081\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2428 | loss: 1.70081 - acc: 0.6184 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2429  | total loss: \u001b[1m\u001b[32m1.59183\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2429 | loss: 1.59183 - acc: 0.6566 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2430  | total loss: \u001b[1m\u001b[32m1.49300\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2430 | loss: 1.49300 - acc: 0.6909 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2431  | total loss: \u001b[1m\u001b[32m1.40250\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2431 | loss: 1.40250 - acc: 0.7218 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2432  | total loss: \u001b[1m\u001b[32m1.31884\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2432 | loss: 1.31884 - acc: 0.7496 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2433  | total loss: \u001b[1m\u001b[32m1.24076\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2433 | loss: 1.24076 - acc: 0.7747 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2434  | total loss: \u001b[1m\u001b[32m1.48705\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2434 | loss: 1.48705 - acc: 0.6972 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2435  | total loss: \u001b[1m\u001b[32m1.38733\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2435 | loss: 1.38733 - acc: 0.7275 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2436  | total loss: \u001b[1m\u001b[32m1.53853\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2436 | loss: 1.53853 - acc: 0.6762 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2437  | total loss: \u001b[1m\u001b[32m1.43046\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2437 | loss: 1.43046 - acc: 0.7086 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2438  | total loss: \u001b[1m\u001b[32m1.63304\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2438 | loss: 1.63304 - acc: 0.6448 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2439  | total loss: \u001b[1m\u001b[32m1.51368\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2439 | loss: 1.51368 - acc: 0.6804 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2440  | total loss: \u001b[1m\u001b[32m1.73821\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2440 | loss: 1.73821 - acc: 0.6195 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2441  | total loss: \u001b[1m\u001b[32m1.60793\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2441 | loss: 1.60793 - acc: 0.6575 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2442  | total loss: \u001b[1m\u001b[32m1.82025\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2442 | loss: 1.82025 - acc: 0.5918 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2443  | total loss: \u001b[1m\u001b[32m1.68262\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2443 | loss: 1.68262 - acc: 0.6326 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2444  | total loss: \u001b[1m\u001b[32m1.87216\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2444 | loss: 1.87216 - acc: 0.5693 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2445  | total loss: \u001b[1m\u001b[32m1.73121\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2445 | loss: 1.73121 - acc: 0.6124 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2446  | total loss: \u001b[1m\u001b[32m1.60508\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2446 | loss: 1.60508 - acc: 0.6512 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2447  | total loss: \u001b[1m\u001b[32m1.49160\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2447 | loss: 1.49160 - acc: 0.6860 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2448  | total loss: \u001b[1m\u001b[32m1.68570\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2448 | loss: 1.68570 - acc: 0.6246 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2449  | total loss: \u001b[1m\u001b[32m1.56401\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2449 | loss: 1.56401 - acc: 0.6621 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2450  | total loss: \u001b[1m\u001b[32m1.77756\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2450 | loss: 1.77756 - acc: 0.5959 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2451  | total loss: \u001b[1m\u001b[32m1.64734\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2451 | loss: 1.64734 - acc: 0.6363 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2452  | total loss: \u001b[1m\u001b[32m1.82413\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2452 | loss: 1.82413 - acc: 0.5798 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2453  | total loss: \u001b[1m\u001b[32m1.69062\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2453 | loss: 1.69062 - acc: 0.6218 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2454  | total loss: \u001b[1m\u001b[32m1.84224\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2454 | loss: 1.84224 - acc: 0.5739 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2455  | total loss: \u001b[1m\u001b[32m1.70872\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2455 | loss: 1.70872 - acc: 0.6166 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2456  | total loss: \u001b[1m\u001b[32m1.58908\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2456 | loss: 1.58908 - acc: 0.6549 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2457  | total loss: \u001b[1m\u001b[32m1.48118\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2457 | loss: 1.48118 - acc: 0.6894 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2458  | total loss: \u001b[1m\u001b[32m1.62972\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2458 | loss: 1.62972 - acc: 0.6419 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2459  | total loss: \u001b[1m\u001b[32m1.51678\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2459 | loss: 1.51678 - acc: 0.6777 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2460  | total loss: \u001b[1m\u001b[32m1.66090\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2460 | loss: 1.66090 - acc: 0.6314 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2461  | total loss: \u001b[1m\u001b[32m1.54437\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2461 | loss: 1.54437 - acc: 0.6682 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2462  | total loss: \u001b[1m\u001b[32m1.43902\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2462 | loss: 1.43902 - acc: 0.7014 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2463  | total loss: \u001b[1m\u001b[32m1.34309\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2463 | loss: 1.34309 - acc: 0.7313 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2464  | total loss: \u001b[1m\u001b[32m1.25510\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2464 | loss: 1.25510 - acc: 0.7581 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2465  | total loss: \u001b[1m\u001b[32m1.17381\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2465 | loss: 1.17381 - acc: 0.7823 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2466  | total loss: \u001b[1m\u001b[32m1.39951\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2466 | loss: 1.39951 - acc: 0.7112 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2467  | total loss: \u001b[1m\u001b[32m1.30015\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2467 | loss: 1.30015 - acc: 0.7401 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2468  | total loss: \u001b[1m\u001b[32m1.55692\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2468 | loss: 1.55692 - acc: 0.6661 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2469  | total loss: \u001b[1m\u001b[32m1.43997\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2469 | loss: 1.43997 - acc: 0.6995 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2470  | total loss: \u001b[1m\u001b[32m1.33394\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2470 | loss: 1.33394 - acc: 0.7295 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2471  | total loss: \u001b[1m\u001b[32m1.23736\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2471 | loss: 1.23736 - acc: 0.7566 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2472  | total loss: \u001b[1m\u001b[32m1.14893\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2472 | loss: 1.14893 - acc: 0.7809 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2473  | total loss: \u001b[1m\u001b[32m1.06758\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2473 | loss: 1.06758 - acc: 0.8028 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2474  | total loss: \u001b[1m\u001b[32m1.31119\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2474 | loss: 1.31119 - acc: 0.7368 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2475  | total loss: \u001b[1m\u001b[32m1.21087\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2475 | loss: 1.21087 - acc: 0.7632 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2476  | total loss: \u001b[1m\u001b[32m1.11950\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2476 | loss: 1.11950 - acc: 0.7868 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2477  | total loss: \u001b[1m\u001b[32m1.03599\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2477 | loss: 1.03599 - acc: 0.8082 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2478  | total loss: \u001b[1m\u001b[32m1.30024\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2478 | loss: 1.30024 - acc: 0.7416 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2479  | total loss: \u001b[1m\u001b[32m1.19685\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2479 | loss: 1.19685 - acc: 0.7675 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2480  | total loss: \u001b[1m\u001b[32m1.45964\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2480 | loss: 1.45964 - acc: 0.6979 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2481  | total loss: \u001b[1m\u001b[32m1.34017\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2481 | loss: 1.34017 - acc: 0.7281 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2482  | total loss: \u001b[1m\u001b[32m1.58385\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2482 | loss: 1.58385 - acc: 0.6624 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2483  | total loss: \u001b[1m\u001b[32m1.45320\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2483 | loss: 1.45320 - acc: 0.6962 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2484  | total loss: \u001b[1m\u001b[32m1.72879\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2484 | loss: 1.72879 - acc: 0.6266 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2485  | total loss: \u001b[1m\u001b[32m1.58613\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2485 | loss: 1.58613 - acc: 0.6639 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2486  | total loss: \u001b[1m\u001b[32m1.45904\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2486 | loss: 1.45904 - acc: 0.6975 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2487  | total loss: \u001b[1m\u001b[32m1.34549\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2487 | loss: 1.34549 - acc: 0.7278 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2488  | total loss: \u001b[1m\u001b[32m1.24365\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2488 | loss: 1.24365 - acc: 0.7550 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2489  | total loss: \u001b[1m\u001b[32m1.15190\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2489 | loss: 1.15190 - acc: 0.7795 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2490  | total loss: \u001b[1m\u001b[32m1.06886\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2490 | loss: 1.06886 - acc: 0.8015 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2491  | total loss: \u001b[1m\u001b[32m0.99330\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2491 | loss: 0.99330 - acc: 0.8214 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2492  | total loss: \u001b[1m\u001b[32m0.92420\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2492 | loss: 0.92420 - acc: 0.8392 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2493  | total loss: \u001b[1m\u001b[32m0.86067\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2493 | loss: 0.86067 - acc: 0.8553 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2494  | total loss: \u001b[1m\u001b[32m1.16466\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2494 | loss: 1.16466 - acc: 0.7769 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2495  | total loss: \u001b[1m\u001b[32m1.07527\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2495 | loss: 1.07527 - acc: 0.7992 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2496  | total loss: \u001b[1m\u001b[32m1.35026\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2496 | loss: 1.35026 - acc: 0.7265 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2497  | total loss: \u001b[1m\u001b[32m1.24211\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2497 | loss: 1.24211 - acc: 0.7538 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2498  | total loss: \u001b[1m\u001b[32m1.48749\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2498 | loss: 1.48749 - acc: 0.6856 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2499  | total loss: \u001b[1m\u001b[32m1.36675\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2499 | loss: 1.36675 - acc: 0.7170 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2500  | total loss: \u001b[1m\u001b[32m1.25878\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2500 | loss: 1.25878 - acc: 0.7453 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2501  | total loss: \u001b[1m\u001b[32m1.16191\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2501 | loss: 1.16191 - acc: 0.7708 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2502  | total loss: \u001b[1m\u001b[32m1.07468\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2502 | loss: 1.07468 - acc: 0.7937 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2503  | total loss: \u001b[1m\u001b[32m0.99580\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2503 | loss: 0.99580 - acc: 0.8143 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2504  | total loss: \u001b[1m\u001b[32m0.92412\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2504 | loss: 0.92412 - acc: 0.8329 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2505  | total loss: \u001b[1m\u001b[32m0.85869\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2505 | loss: 0.85869 - acc: 0.8496 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2506  | total loss: \u001b[1m\u001b[32m1.19003\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2506 | loss: 1.19003 - acc: 0.7646 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2507  | total loss: \u001b[1m\u001b[32m1.09693\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2507 | loss: 1.09693 - acc: 0.7882 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2508  | total loss: \u001b[1m\u001b[32m1.40943\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2508 | loss: 1.40943 - acc: 0.7094 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2509  | total loss: \u001b[1m\u001b[32m1.29495\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2509 | loss: 1.29495 - acc: 0.7384 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2510  | total loss: \u001b[1m\u001b[32m1.53405\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2510 | loss: 1.53405 - acc: 0.6789 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2511  | total loss: \u001b[1m\u001b[32m1.40903\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2511 | loss: 1.40903 - acc: 0.7110 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2512  | total loss: \u001b[1m\u001b[32m1.67925\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2512 | loss: 1.67925 - acc: 0.6399 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2513  | total loss: \u001b[1m\u001b[32m1.54284\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2513 | loss: 1.54284 - acc: 0.6759 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2514  | total loss: \u001b[1m\u001b[32m1.72765\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2514 | loss: 1.72765 - acc: 0.6226 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2515  | total loss: \u001b[1m\u001b[32m1.59041\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2515 | loss: 1.59041 - acc: 0.6603 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2516  | total loss: \u001b[1m\u001b[32m1.46869\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2516 | loss: 1.46869 - acc: 0.6943 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2517  | total loss: \u001b[1m\u001b[32m1.36028\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2517 | loss: 1.36028 - acc: 0.7249 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2518  | total loss: \u001b[1m\u001b[32m1.59752\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2518 | loss: 1.59752 - acc: 0.6524 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2519  | total loss: \u001b[1m\u001b[32m1.47836\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2519 | loss: 1.47836 - acc: 0.6871 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2520  | total loss: \u001b[1m\u001b[32m1.37203\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2520 | loss: 1.37203 - acc: 0.7184 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2521  | total loss: \u001b[1m\u001b[32m1.27658\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2521 | loss: 1.27658 - acc: 0.7466 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2522  | total loss: \u001b[1m\u001b[32m1.53300\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2522 | loss: 1.53300 - acc: 0.6719 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2523  | total loss: \u001b[1m\u001b[32m1.42192\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2523 | loss: 1.42192 - acc: 0.7047 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2524  | total loss: \u001b[1m\u001b[32m1.62340\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2524 | loss: 1.62340 - acc: 0.6485 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2525  | total loss: \u001b[1m\u001b[32m1.50466\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2525 | loss: 1.50466 - acc: 0.6837 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2526  | total loss: \u001b[1m\u001b[32m1.69940\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2526 | loss: 1.69940 - acc: 0.6296 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2527  | total loss: \u001b[1m\u001b[32m1.57488\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2527 | loss: 1.57488 - acc: 0.6666 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2528  | total loss: \u001b[1m\u001b[32m1.77724\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2528 | loss: 1.77724 - acc: 0.6000 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2529  | total loss: \u001b[1m\u001b[32m1.64727\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2529 | loss: 1.64727 - acc: 0.6400 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2530  | total loss: \u001b[1m\u001b[32m1.74589\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2530 | loss: 1.74589 - acc: 0.6046 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2531  | total loss: \u001b[1m\u001b[32m1.62139\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2531 | loss: 1.62139 - acc: 0.6441 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2532  | total loss: \u001b[1m\u001b[32m1.50994\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2532 | loss: 1.50994 - acc: 0.6797 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2533  | total loss: \u001b[1m\u001b[32m1.40944\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2533 | loss: 1.40944 - acc: 0.7117 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2534  | total loss: \u001b[1m\u001b[32m1.61956\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2534 | loss: 1.61956 - acc: 0.6477 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2535  | total loss: \u001b[1m\u001b[32m1.50743\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2535 | loss: 1.50743 - acc: 0.6829 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2536  | total loss: \u001b[1m\u001b[32m1.40601\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2536 | loss: 1.40601 - acc: 0.7146 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2537  | total loss: \u001b[1m\u001b[32m1.31358\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2537 | loss: 1.31358 - acc: 0.7432 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2538  | total loss: \u001b[1m\u001b[32m1.22870\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2538 | loss: 1.22870 - acc: 0.7689 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2539  | total loss: \u001b[1m\u001b[32m1.15018\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2539 | loss: 1.15018 - acc: 0.7920 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2540  | total loss: \u001b[1m\u001b[32m1.42839\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2540 | loss: 1.42839 - acc: 0.7128 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2541  | total loss: \u001b[1m\u001b[32m1.32640\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2541 | loss: 1.32640 - acc: 0.7415 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2542  | total loss: \u001b[1m\u001b[32m1.23315\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2542 | loss: 1.23315 - acc: 0.7673 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2543  | total loss: \u001b[1m\u001b[32m1.14745\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2543 | loss: 1.14745 - acc: 0.7906 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2544  | total loss: \u001b[1m\u001b[32m1.42184\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2544 | loss: 1.42184 - acc: 0.7115 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2545  | total loss: \u001b[1m\u001b[32m1.31453\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2545 | loss: 1.31453 - acc: 0.7404 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2546  | total loss: \u001b[1m\u001b[32m1.57526\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2546 | loss: 1.57526 - acc: 0.6664 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2547  | total loss: \u001b[1m\u001b[32m1.45169\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2547 | loss: 1.45169 - acc: 0.6997 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2548  | total loss: \u001b[1m\u001b[32m1.63347\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2548 | loss: 1.63347 - acc: 0.6440 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2549  | total loss: \u001b[1m\u001b[32m1.50438\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2549 | loss: 1.50438 - acc: 0.6796 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2550  | total loss: \u001b[1m\u001b[32m1.75088\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2550 | loss: 1.75088 - acc: 0.6117 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2551  | total loss: \u001b[1m\u001b[32m1.61153\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2551 | loss: 1.61153 - acc: 0.6505 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2552  | total loss: \u001b[1m\u001b[32m1.85320\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2552 | loss: 1.85320 - acc: 0.5854 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2553  | total loss: \u001b[1m\u001b[32m1.70638\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2553 | loss: 1.70638 - acc: 0.6269 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2554  | total loss: \u001b[1m\u001b[32m1.84268\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2554 | loss: 1.84268 - acc: 0.5856 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2555  | total loss: \u001b[1m\u001b[32m1.70025\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2555 | loss: 1.70025 - acc: 0.6271 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2556  | total loss: \u001b[1m\u001b[32m1.90146\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2556 | loss: 1.90146 - acc: 0.5715 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2557  | total loss: \u001b[1m\u001b[32m1.75680\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2557 | loss: 1.75680 - acc: 0.6144 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2558  | total loss: \u001b[1m\u001b[32m1.62814\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2558 | loss: 1.62814 - acc: 0.6529 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2559  | total loss: \u001b[1m\u001b[32m1.51313\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2559 | loss: 1.51313 - acc: 0.6876 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2560  | total loss: \u001b[1m\u001b[32m1.40967\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2560 | loss: 1.40967 - acc: 0.7189 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2561  | total loss: \u001b[1m\u001b[32m1.31596\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2561 | loss: 1.31596 - acc: 0.7470 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2562  | total loss: \u001b[1m\u001b[32m1.52544\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2562 | loss: 1.52544 - acc: 0.6794 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2563  | total loss: \u001b[1m\u001b[32m1.41905\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2563 | loss: 1.41905 - acc: 0.7115 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2564  | total loss: \u001b[1m\u001b[32m1.62725\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2564 | loss: 1.62725 - acc: 0.6475 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2565  | total loss: \u001b[1m\u001b[32m1.51058\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2565 | loss: 1.51058 - acc: 0.6827 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2566  | total loss: \u001b[1m\u001b[32m1.70495\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2566 | loss: 1.70495 - acc: 0.6287 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2567  | total loss: \u001b[1m\u001b[32m1.58115\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2567 | loss: 1.58115 - acc: 0.6659 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2568  | total loss: \u001b[1m\u001b[32m1.77880\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2568 | loss: 1.77880 - acc: 0.6064 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2569  | total loss: \u001b[1m\u001b[32m1.64902\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2569 | loss: 1.64902 - acc: 0.6458 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2570  | total loss: \u001b[1m\u001b[32m1.79448\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2570 | loss: 1.79448 - acc: 0.5955 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2571  | total loss: \u001b[1m\u001b[32m1.66487\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2571 | loss: 1.66487 - acc: 0.6359 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2572  | total loss: \u001b[1m\u001b[32m1.85487\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2572 | loss: 1.85487 - acc: 0.5723 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2573  | total loss: \u001b[1m\u001b[32m1.72114\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2573 | loss: 1.72114 - acc: 0.6151 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2574  | total loss: \u001b[1m\u001b[32m1.60140\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2574 | loss: 1.60140 - acc: 0.6536 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2575  | total loss: \u001b[1m\u001b[32m1.49348\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2575 | loss: 1.49348 - acc: 0.6882 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2576  | total loss: \u001b[1m\u001b[32m1.67797\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2576 | loss: 1.67797 - acc: 0.6266 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2577  | total loss: \u001b[1m\u001b[32m1.56172\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2577 | loss: 1.56172 - acc: 0.6639 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2578  | total loss: \u001b[1m\u001b[32m1.45658\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2578 | loss: 1.45658 - acc: 0.6975 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2579  | total loss: \u001b[1m\u001b[32m1.36079\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2579 | loss: 1.36079 - acc: 0.7278 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2580  | total loss: \u001b[1m\u001b[32m1.27290\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2580 | loss: 1.27290 - acc: 0.7550 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2581  | total loss: \u001b[1m\u001b[32m1.19167\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2581 | loss: 1.19167 - acc: 0.7795 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2582  | total loss: \u001b[1m\u001b[32m1.39322\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2582 | loss: 1.39322 - acc: 0.7158 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2583  | total loss: \u001b[1m\u001b[32m1.29620\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2583 | loss: 1.29620 - acc: 0.7442 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2584  | total loss: \u001b[1m\u001b[32m1.20720\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2584 | loss: 1.20720 - acc: 0.7698 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2585  | total loss: \u001b[1m\u001b[32m1.12512\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2585 | loss: 1.12512 - acc: 0.7928 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2586  | total loss: \u001b[1m\u001b[32m1.37013\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2586 | loss: 1.37013 - acc: 0.7136 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2587  | total loss: \u001b[1m\u001b[32m1.26871\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2587 | loss: 1.26871 - acc: 0.7422 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2588  | total loss: \u001b[1m\u001b[32m1.49042\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2588 | loss: 1.49042 - acc: 0.6823 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2589  | total loss: \u001b[1m\u001b[32m1.37556\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2589 | loss: 1.37556 - acc: 0.7140 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2590  | total loss: \u001b[1m\u001b[32m1.64582\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2590 | loss: 1.64582 - acc: 0.6426 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2591  | total loss: \u001b[1m\u001b[32m1.51543\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2591 | loss: 1.51543 - acc: 0.6784 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2592  | total loss: \u001b[1m\u001b[32m1.72219\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2592 | loss: 1.72219 - acc: 0.6177 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2593  | total loss: \u001b[1m\u001b[32m1.58548\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2593 | loss: 1.58548 - acc: 0.6559 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2594  | total loss: \u001b[1m\u001b[32m1.46307\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2594 | loss: 1.46307 - acc: 0.6903 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2595  | total loss: \u001b[1m\u001b[32m1.35304\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2595 | loss: 1.35304 - acc: 0.7213 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2596  | total loss: \u001b[1m\u001b[32m1.25370\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2596 | loss: 1.25370 - acc: 0.7492 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2597  | total loss: \u001b[1m\u001b[32m1.16357\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2597 | loss: 1.16357 - acc: 0.7742 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2598  | total loss: \u001b[1m\u001b[32m1.08137\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2598 | loss: 1.08137 - acc: 0.7968 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2599  | total loss: \u001b[1m\u001b[32m1.00602\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2599 | loss: 1.00602 - acc: 0.8171 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2600  | total loss: \u001b[1m\u001b[32m1.27980\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2600 | loss: 1.27980 - acc: 0.7426 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2601  | total loss: \u001b[1m\u001b[32m1.18262\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2601 | loss: 1.18262 - acc: 0.7683 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2602  | total loss: \u001b[1m\u001b[32m1.09446\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2602 | loss: 1.09446 - acc: 0.7915 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2603  | total loss: \u001b[1m\u001b[32m1.01415\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2603 | loss: 1.01415 - acc: 0.8123 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2604  | total loss: \u001b[1m\u001b[32m1.26852\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2604 | loss: 1.26852 - acc: 0.7454 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2605  | total loss: \u001b[1m\u001b[32m1.16947\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2605 | loss: 1.16947 - acc: 0.7708 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2606  | total loss: \u001b[1m\u001b[32m1.41162\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2606 | loss: 1.41162 - acc: 0.7080 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2607  | total loss: \u001b[1m\u001b[32m1.29829\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2607 | loss: 1.29829 - acc: 0.7372 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2608  | total loss: \u001b[1m\u001b[32m1.53664\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2608 | loss: 1.53664 - acc: 0.6707 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2609  | total loss: \u001b[1m\u001b[32m1.41213\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2609 | loss: 1.41213 - acc: 0.7036 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2610  | total loss: \u001b[1m\u001b[32m1.65185\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2610 | loss: 1.65185 - acc: 0.6404 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2611  | total loss: \u001b[1m\u001b[32m1.51846\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2611 | loss: 1.51846 - acc: 0.6763 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2612  | total loss: \u001b[1m\u001b[32m1.70263\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2612 | loss: 1.70263 - acc: 0.6230 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2613  | total loss: \u001b[1m\u001b[32m1.56777\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2613 | loss: 1.56777 - acc: 0.6607 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2614  | total loss: \u001b[1m\u001b[32m1.44803\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2614 | loss: 1.44803 - acc: 0.6946 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2615  | total loss: \u001b[1m\u001b[32m1.34128\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2615 | loss: 1.34128 - acc: 0.7252 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2616  | total loss: \u001b[1m\u001b[32m1.56201\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2616 | loss: 1.56201 - acc: 0.6598 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2617  | total loss: \u001b[1m\u001b[32m1.44575\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2617 | loss: 1.44575 - acc: 0.6938 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2618  | total loss: \u001b[1m\u001b[32m1.63115\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2618 | loss: 1.63115 - acc: 0.6387 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2619  | total loss: \u001b[1m\u001b[32m1.51057\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2619 | loss: 1.51057 - acc: 0.6748 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2620  | total loss: \u001b[1m\u001b[32m1.74068\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2620 | loss: 1.74068 - acc: 0.6074 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2621  | total loss: \u001b[1m\u001b[32m1.61222\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2621 | loss: 1.61222 - acc: 0.6466 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2622  | total loss: \u001b[1m\u001b[32m1.79732\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2622 | loss: 1.79732 - acc: 0.5891 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2623  | total loss: \u001b[1m\u001b[32m1.66666\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2623 | loss: 1.66666 - acc: 0.6302 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2624  | total loss: \u001b[1m\u001b[32m1.84351\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2624 | loss: 1.84351 - acc: 0.5743 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2625  | total loss: \u001b[1m\u001b[32m1.71217\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2625 | loss: 1.71217 - acc: 0.6169 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2626  | total loss: \u001b[1m\u001b[32m1.85774\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2626 | loss: 1.85774 - acc: 0.5695 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2627  | total loss: \u001b[1m\u001b[32m1.72886\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2627 | loss: 1.72886 - acc: 0.6125 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2628  | total loss: \u001b[1m\u001b[32m1.61409\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2628 | loss: 1.61409 - acc: 0.6513 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2629  | total loss: \u001b[1m\u001b[32m1.51102\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2629 | loss: 1.51102 - acc: 0.6862 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2630  | total loss: \u001b[1m\u001b[32m1.71200\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2630 | loss: 1.71200 - acc: 0.6175 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2631  | total loss: \u001b[1m\u001b[32m1.59893\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2631 | loss: 1.59893 - acc: 0.6558 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2632  | total loss: \u001b[1m\u001b[32m1.74670\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2632 | loss: 1.74670 - acc: 0.6045 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2633  | total loss: \u001b[1m\u001b[32m1.63019\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2633 | loss: 1.63019 - acc: 0.6440 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2634  | total loss: \u001b[1m\u001b[32m1.83361\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2634 | loss: 1.83361 - acc: 0.5796 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2635  | total loss: \u001b[1m\u001b[32m1.70866\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2635 | loss: 1.70866 - acc: 0.6217 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2636  | total loss: \u001b[1m\u001b[32m1.59600\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2636 | loss: 1.59600 - acc: 0.6595 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2637  | total loss: \u001b[1m\u001b[32m1.49357\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2637 | loss: 1.49357 - acc: 0.6936 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2638  | total loss: \u001b[1m\u001b[32m1.66882\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2638 | loss: 1.66882 - acc: 0.6313 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2639  | total loss: \u001b[1m\u001b[32m1.55678\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2639 | loss: 1.55678 - acc: 0.6682 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2640  | total loss: \u001b[1m\u001b[32m1.71645\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2640 | loss: 1.71645 - acc: 0.6157 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2641  | total loss: \u001b[1m\u001b[32m1.59820\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2641 | loss: 1.59820 - acc: 0.6541 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2642  | total loss: \u001b[1m\u001b[32m1.78609\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2642 | loss: 1.78609 - acc: 0.5887 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2643  | total loss: \u001b[1m\u001b[32m1.66048\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2643 | loss: 1.66048 - acc: 0.6298 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2644  | total loss: \u001b[1m\u001b[32m1.85552\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2644 | loss: 1.85552 - acc: 0.5740 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2645  | total loss: \u001b[1m\u001b[32m1.72350\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2645 | loss: 1.72350 - acc: 0.6166 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2646  | total loss: \u001b[1m\u001b[32m1.91699\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2646 | loss: 1.91699 - acc: 0.5549 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2647  | total loss: \u001b[1m\u001b[32m1.77987\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2647 | loss: 1.77987 - acc: 0.5994 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2648  | total loss: \u001b[1m\u001b[32m1.92642\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2648 | loss: 1.92642 - acc: 0.5466 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2649  | total loss: \u001b[1m\u001b[32m1.78969\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2649 | loss: 1.78969 - acc: 0.5920 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2650  | total loss: \u001b[1m\u001b[32m1.93749\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2650 | loss: 1.93749 - acc: 0.5399 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2651  | total loss: \u001b[1m\u001b[32m1.80112\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2651 | loss: 1.80112 - acc: 0.5859 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2652  | total loss: \u001b[1m\u001b[32m1.96067\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2652 | loss: 1.96067 - acc: 0.5345 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2653  | total loss: \u001b[1m\u001b[32m1.82356\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2653 | loss: 1.82356 - acc: 0.5810 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2654  | total loss: \u001b[1m\u001b[32m1.97792\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2654 | loss: 1.97792 - acc: 0.5301 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2655  | total loss: \u001b[1m\u001b[32m1.84071\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2655 | loss: 1.84071 - acc: 0.5771 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2656  | total loss: \u001b[1m\u001b[32m1.98390\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2656 | loss: 1.98390 - acc: 0.5265 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2657  | total loss: \u001b[1m\u001b[32m1.84763\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2657 | loss: 1.84763 - acc: 0.5738 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2658  | total loss: \u001b[1m\u001b[32m1.99140\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2658 | loss: 1.99140 - acc: 0.5165 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2659  | total loss: \u001b[1m\u001b[32m1.85582\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2659 | loss: 1.85582 - acc: 0.5648 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2660  | total loss: \u001b[1m\u001b[32m1.98461\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2660 | loss: 1.98461 - acc: 0.5155 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2661  | total loss: \u001b[1m\u001b[32m1.85110\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2661 | loss: 1.85110 - acc: 0.5639 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2662  | total loss: \u001b[1m\u001b[32m2.02602\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2662 | loss: 2.02602 - acc: 0.5075 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2663  | total loss: \u001b[1m\u001b[32m1.88968\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2663 | loss: 1.88968 - acc: 0.5568 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2664  | total loss: \u001b[1m\u001b[32m1.98756\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2664 | loss: 1.98756 - acc: 0.5154 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2665  | total loss: \u001b[1m\u001b[32m1.85605\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2665 | loss: 1.85605 - acc: 0.5639 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2666  | total loss: \u001b[1m\u001b[32m1.98850\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2666 | loss: 1.98850 - acc: 0.5146 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2667  | total loss: \u001b[1m\u001b[32m1.85740\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2667 | loss: 1.85740 - acc: 0.5631 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2668  | total loss: \u001b[1m\u001b[32m2.00427\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2668 | loss: 2.00427 - acc: 0.5140 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2669  | total loss: \u001b[1m\u001b[32m1.87184\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2669 | loss: 1.87184 - acc: 0.5626 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2670  | total loss: \u001b[1m\u001b[32m1.99449\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2670 | loss: 1.99449 - acc: 0.5135 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2671  | total loss: \u001b[1m\u001b[32m1.86330\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2671 | loss: 1.86330 - acc: 0.5621 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2672  | total loss: \u001b[1m\u001b[32m1.99248\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2672 | loss: 1.99248 - acc: 0.5059 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2673  | total loss: \u001b[1m\u001b[32m1.86177\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2673 | loss: 1.86177 - acc: 0.5553 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2674  | total loss: \u001b[1m\u001b[32m1.74379\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2674 | loss: 1.74379 - acc: 0.5998 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2675  | total loss: \u001b[1m\u001b[32m1.63636\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2675 | loss: 1.63636 - acc: 0.6398 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2676  | total loss: \u001b[1m\u001b[32m1.81566\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2676 | loss: 1.81566 - acc: 0.5758 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2677  | total loss: \u001b[1m\u001b[32m1.69832\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2677 | loss: 1.69832 - acc: 0.6182 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2678  | total loss: \u001b[1m\u001b[32m1.82429\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2678 | loss: 1.82429 - acc: 0.5707 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2679  | total loss: \u001b[1m\u001b[32m1.70402\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2679 | loss: 1.70402 - acc: 0.6136 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2680  | total loss: \u001b[1m\u001b[32m1.59446\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2680 | loss: 1.59446 - acc: 0.6523 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2681  | total loss: \u001b[1m\u001b[32m1.49384\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 2681 | loss: 1.49384 - acc: 0.6870 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2682  | total loss: \u001b[1m\u001b[32m1.69499\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2682 | loss: 1.69499 - acc: 0.6255 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2683  | total loss: \u001b[1m\u001b[32m1.58049\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2683 | loss: 1.58049 - acc: 0.6629 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2684  | total loss: \u001b[1m\u001b[32m1.47560\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2684 | loss: 1.47560 - acc: 0.6966 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2685  | total loss: \u001b[1m\u001b[32m1.37888\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2685 | loss: 1.37888 - acc: 0.7270 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2686  | total loss: \u001b[1m\u001b[32m1.28913\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2686 | loss: 1.28913 - acc: 0.7543 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2687  | total loss: \u001b[1m\u001b[32m1.20537\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2687 | loss: 1.20537 - acc: 0.7789 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2688  | total loss: \u001b[1m\u001b[32m1.39401\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2688 | loss: 1.39401 - acc: 0.7153 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2689  | total loss: \u001b[1m\u001b[32m1.29475\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2689 | loss: 1.29475 - acc: 0.7437 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2690  | total loss: \u001b[1m\u001b[32m1.20328\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2690 | loss: 1.20328 - acc: 0.7694 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2691  | total loss: \u001b[1m\u001b[32m1.11864\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2691 | loss: 1.11864 - acc: 0.7924 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2692  | total loss: \u001b[1m\u001b[32m1.41863\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2692 | loss: 1.41863 - acc: 0.7132 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2693  | total loss: \u001b[1m\u001b[32m1.30904\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2693 | loss: 1.30904 - acc: 0.7419 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2694  | total loss: \u001b[1m\u001b[32m1.56477\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2694 | loss: 1.56477 - acc: 0.6748 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2695  | total loss: \u001b[1m\u001b[32m1.43926\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2695 | loss: 1.43926 - acc: 0.7073 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2696  | total loss: \u001b[1m\u001b[32m1.63244\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2696 | loss: 1.63244 - acc: 0.6509 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2697  | total loss: \u001b[1m\u001b[32m1.50037\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2697 | loss: 1.50037 - acc: 0.6858 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2698  | total loss: \u001b[1m\u001b[32m1.72583\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2698 | loss: 1.72583 - acc: 0.6244 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2699  | total loss: \u001b[1m\u001b[32m1.58593\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2699 | loss: 1.58593 - acc: 0.6619 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2700  | total loss: \u001b[1m\u001b[32m1.46085\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2700 | loss: 1.46085 - acc: 0.6957 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2701  | total loss: \u001b[1m\u001b[32m1.34863\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2701 | loss: 1.34863 - acc: 0.7262 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2702  | total loss: \u001b[1m\u001b[32m1.60119\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2702 | loss: 1.60119 - acc: 0.6535 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2703  | total loss: \u001b[1m\u001b[32m1.47604\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2703 | loss: 1.47604 - acc: 0.6882 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2704  | total loss: \u001b[1m\u001b[32m1.36405\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2704 | loss: 1.36405 - acc: 0.7194 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2705  | total loss: \u001b[1m\u001b[32m1.26339\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2705 | loss: 1.26339 - acc: 0.7474 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2706  | total loss: \u001b[1m\u001b[32m1.46644\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2706 | loss: 1.46644 - acc: 0.6870 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2707  | total loss: \u001b[1m\u001b[32m1.35584\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2707 | loss: 1.35584 - acc: 0.7183 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2708  | total loss: \u001b[1m\u001b[32m1.59775\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2708 | loss: 1.59775 - acc: 0.6536 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2709  | total loss: \u001b[1m\u001b[32m1.47533\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2709 | loss: 1.47533 - acc: 0.6882 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2710  | total loss: \u001b[1m\u001b[32m1.70014\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2710 | loss: 1.70014 - acc: 0.6266 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2711  | total loss: \u001b[1m\u001b[32m1.56960\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2711 | loss: 1.56960 - acc: 0.6639 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2712  | total loss: \u001b[1m\u001b[32m1.45297\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2712 | loss: 1.45297 - acc: 0.6975 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2713  | total loss: \u001b[1m\u001b[32m1.34823\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2713 | loss: 1.34823 - acc: 0.7278 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2714  | total loss: \u001b[1m\u001b[32m1.56009\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2714 | loss: 1.56009 - acc: 0.6621 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2715  | total loss: \u001b[1m\u001b[32m1.44498\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2715 | loss: 1.44498 - acc: 0.6959 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2716  | total loss: \u001b[1m\u001b[32m1.62272\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2716 | loss: 1.62272 - acc: 0.6406 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2717  | total loss: \u001b[1m\u001b[32m1.50253\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2717 | loss: 1.50253 - acc: 0.6765 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2718  | total loss: \u001b[1m\u001b[32m1.65111\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2718 | loss: 1.65111 - acc: 0.6303 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2719  | total loss: \u001b[1m\u001b[32m1.52973\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2719 | loss: 1.52973 - acc: 0.6673 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2720  | total loss: \u001b[1m\u001b[32m1.74165\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2720 | loss: 1.74165 - acc: 0.6006 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2721  | total loss: \u001b[1m\u001b[32m1.61339\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2721 | loss: 1.61339 - acc: 0.6405 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2722  | total loss: \u001b[1m\u001b[32m1.80439\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2722 | loss: 1.80439 - acc: 0.5836 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2723  | total loss: \u001b[1m\u001b[32m1.67265\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2723 | loss: 1.67265 - acc: 0.6252 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2724  | total loss: \u001b[1m\u001b[32m1.85310\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2724 | loss: 1.85310 - acc: 0.5699 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2725  | total loss: \u001b[1m\u001b[32m1.71935\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2725 | loss: 1.71935 - acc: 0.6129 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2726  | total loss: \u001b[1m\u001b[32m1.59989\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2726 | loss: 1.59989 - acc: 0.6516 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2727  | total loss: \u001b[1m\u001b[32m1.49242\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2727 | loss: 1.49242 - acc: 0.6864 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2728  | total loss: \u001b[1m\u001b[32m1.66088\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2728 | loss: 1.66088 - acc: 0.6321 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2729  | total loss: \u001b[1m\u001b[32m1.54689\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2729 | loss: 1.54689 - acc: 0.6689 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2730  | total loss: \u001b[1m\u001b[32m1.44380\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2730 | loss: 1.44380 - acc: 0.7020 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2731  | total loss: \u001b[1m\u001b[32m1.34983\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2731 | loss: 1.34983 - acc: 0.7318 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2732  | total loss: \u001b[1m\u001b[32m1.53046\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2732 | loss: 1.53046 - acc: 0.6729 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2733  | total loss: \u001b[1m\u001b[32m1.42527\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2733 | loss: 1.42527 - acc: 0.7056 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2734  | total loss: \u001b[1m\u001b[32m1.32926\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2734 | loss: 1.32926 - acc: 0.7350 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2735  | total loss: \u001b[1m\u001b[32m1.24102\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2735 | loss: 1.24102 - acc: 0.7615 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2736  | total loss: \u001b[1m\u001b[32m1.50836\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2736 | loss: 1.50836 - acc: 0.6854 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2737  | total loss: \u001b[1m\u001b[32m1.39917\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2737 | loss: 1.39917 - acc: 0.7168 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2738  | total loss: \u001b[1m\u001b[32m1.29964\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2738 | loss: 1.29964 - acc: 0.7452 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2739  | total loss: \u001b[1m\u001b[32m1.20841\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2739 | loss: 1.20841 - acc: 0.7706 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2740  | total loss: \u001b[1m\u001b[32m1.47513\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2740 | loss: 1.47513 - acc: 0.7007 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2741  | total loss: \u001b[1m\u001b[32m1.36382\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2741 | loss: 1.36382 - acc: 0.7306 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2742  | total loss: \u001b[1m\u001b[32m1.55753\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2742 | loss: 1.55753 - acc: 0.6719 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2743  | total loss: \u001b[1m\u001b[32m1.43709\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2743 | loss: 1.43709 - acc: 0.7047 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2744  | total loss: \u001b[1m\u001b[32m1.32833\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2744 | loss: 1.32833 - acc: 0.7342 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2745  | total loss: \u001b[1m\u001b[32m1.22972\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2745 | loss: 1.22972 - acc: 0.7608 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2746  | total loss: \u001b[1m\u001b[32m1.46716\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2746 | loss: 1.46716 - acc: 0.6990 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2747  | total loss: \u001b[1m\u001b[32m1.35364\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2747 | loss: 1.35364 - acc: 0.7291 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2748  | total loss: \u001b[1m\u001b[32m1.61320\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2748 | loss: 1.61320 - acc: 0.6562 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2749  | total loss: \u001b[1m\u001b[32m1.48554\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2749 | loss: 1.48554 - acc: 0.6906 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2750  | total loss: \u001b[1m\u001b[32m1.37099\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2750 | loss: 1.37099 - acc: 0.7215 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2751  | total loss: \u001b[1m\u001b[32m1.26778\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2751 | loss: 1.26778 - acc: 0.7494 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2752  | total loss: \u001b[1m\u001b[32m1.52594\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2752 | loss: 1.52594 - acc: 0.6816 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2753  | total loss: \u001b[1m\u001b[32m1.40729\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2753 | loss: 1.40729 - acc: 0.7134 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2754  | total loss: \u001b[1m\u001b[32m1.30061\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2754 | loss: 1.30061 - acc: 0.7421 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2755  | total loss: \u001b[1m\u001b[32m1.20427\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2755 | loss: 1.20427 - acc: 0.7679 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2756  | total loss: \u001b[1m\u001b[32m1.43443\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2756 | loss: 1.43443 - acc: 0.6982 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2757  | total loss: \u001b[1m\u001b[32m1.32434\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2757 | loss: 1.32434 - acc: 0.7284 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2758  | total loss: \u001b[1m\u001b[32m1.54500\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2758 | loss: 1.54500 - acc: 0.6698 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2759  | total loss: \u001b[1m\u001b[32m1.42462\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2759 | loss: 1.42462 - acc: 0.7029 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2760  | total loss: \u001b[1m\u001b[32m1.64458\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2760 | loss: 1.64458 - acc: 0.6397 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2761  | total loss: \u001b[1m\u001b[32m1.51602\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2761 | loss: 1.51602 - acc: 0.6757 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2762  | total loss: \u001b[1m\u001b[32m1.70071\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2762 | loss: 1.70071 - acc: 0.6225 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2763  | total loss: \u001b[1m\u001b[32m1.56907\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2763 | loss: 1.56907 - acc: 0.6602 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2764  | total loss: \u001b[1m\u001b[32m1.45167\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2764 | loss: 1.45167 - acc: 0.6942 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2765  | total loss: \u001b[1m\u001b[32m1.34643\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2765 | loss: 1.34643 - acc: 0.7248 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2766  | total loss: \u001b[1m\u001b[32m1.48566\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2766 | loss: 1.48566 - acc: 0.6880 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2767  | total loss: \u001b[1m\u001b[32m1.37722\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2767 | loss: 1.37722 - acc: 0.7192 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2768  | total loss: \u001b[1m\u001b[32m1.27942\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2768 | loss: 1.27942 - acc: 0.7473 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2769  | total loss: \u001b[1m\u001b[32m1.19067\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2769 | loss: 1.19067 - acc: 0.7726 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2770  | total loss: \u001b[1m\u001b[32m1.36127\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2770 | loss: 1.36127 - acc: 0.7167 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2771  | total loss: \u001b[1m\u001b[32m1.26294\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2771 | loss: 1.26294 - acc: 0.7451 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2772  | total loss: \u001b[1m\u001b[32m1.52711\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2772 | loss: 1.52711 - acc: 0.6706 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2773  | total loss: \u001b[1m\u001b[32m1.41207\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2773 | loss: 1.41207 - acc: 0.7035 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2774  | total loss: \u001b[1m\u001b[32m1.60207\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2774 | loss: 1.60207 - acc: 0.6474 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2775  | total loss: \u001b[1m\u001b[32m1.48066\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2775 | loss: 1.48066 - acc: 0.6827 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2776  | total loss: \u001b[1m\u001b[32m1.68675\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2776 | loss: 1.68675 - acc: 0.6216 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2777  | total loss: \u001b[1m\u001b[32m1.55889\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2777 | loss: 1.55889 - acc: 0.6594 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2778  | total loss: \u001b[1m\u001b[32m1.79874\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2778 | loss: 1.79874 - acc: 0.5935 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2779  | total loss: \u001b[1m\u001b[32m1.66253\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2779 | loss: 1.66253 - acc: 0.6341 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2780  | total loss: \u001b[1m\u001b[32m1.82472\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2780 | loss: 1.82472 - acc: 0.5850 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2781  | total loss: \u001b[1m\u001b[32m1.68910\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2781 | loss: 1.68910 - acc: 0.6265 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2782  | total loss: \u001b[1m\u001b[32m1.89084\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2782 | loss: 1.89084 - acc: 0.5638 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2783  | total loss: \u001b[1m\u001b[32m1.75208\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2783 | loss: 1.75208 - acc: 0.6075 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2784  | total loss: \u001b[1m\u001b[32m1.62862\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2784 | loss: 1.62862 - acc: 0.6467 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2785  | total loss: \u001b[1m\u001b[32m1.51808\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2785 | loss: 1.51808 - acc: 0.6820 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2786  | total loss: \u001b[1m\u001b[32m1.41836\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2786 | loss: 1.41836 - acc: 0.7138 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2787  | total loss: \u001b[1m\u001b[32m1.32767\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2787 | loss: 1.32767 - acc: 0.7425 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2788  | total loss: \u001b[1m\u001b[32m1.24449\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2788 | loss: 1.24449 - acc: 0.7682 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2789  | total loss: \u001b[1m\u001b[32m1.16757\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2789 | loss: 1.16757 - acc: 0.7914 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2790  | total loss: \u001b[1m\u001b[32m1.09587\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2790 | loss: 1.09587 - acc: 0.8122 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2791  | total loss: \u001b[1m\u001b[32m1.02858\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2791 | loss: 1.02858 - acc: 0.8310 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2792  | total loss: \u001b[1m\u001b[32m0.96508\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2792 | loss: 0.96508 - acc: 0.8479 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2793  | total loss: \u001b[1m\u001b[32m0.90488\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2793 | loss: 0.90488 - acc: 0.8631 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2794  | total loss: \u001b[1m\u001b[32m1.18136\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2794 | loss: 1.18136 - acc: 0.7840 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2795  | total loss: \u001b[1m\u001b[32m1.09491\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2795 | loss: 1.09491 - acc: 0.8056 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2796  | total loss: \u001b[1m\u001b[32m1.38620\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2796 | loss: 1.38620 - acc: 0.7250 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2797  | total loss: \u001b[1m\u001b[32m1.27712\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2797 | loss: 1.27712 - acc: 0.7525 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2798  | total loss: \u001b[1m\u001b[32m1.17825\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2798 | loss: 1.17825 - acc: 0.7773 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2799  | total loss: \u001b[1m\u001b[32m1.08832\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2799 | loss: 1.08832 - acc: 0.7995 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2800  | total loss: \u001b[1m\u001b[32m1.35685\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2800 | loss: 1.35685 - acc: 0.7267 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2801  | total loss: \u001b[1m\u001b[32m1.24781\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2801 | loss: 1.24781 - acc: 0.7540 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2802  | total loss: \u001b[1m\u001b[32m1.46525\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2802 | loss: 1.46525 - acc: 0.7001 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2803  | total loss: \u001b[1m\u001b[32m1.34546\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2803 | loss: 1.34546 - acc: 0.7301 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2804  | total loss: \u001b[1m\u001b[32m1.58235\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2804 | loss: 1.58235 - acc: 0.6713 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2805  | total loss: \u001b[1m\u001b[32m1.45199\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2805 | loss: 1.45199 - acc: 0.7042 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2806  | total loss: \u001b[1m\u001b[32m1.68019\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2806 | loss: 1.68019 - acc: 0.6409 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2807  | total loss: \u001b[1m\u001b[32m1.54227\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2807 | loss: 1.54227 - acc: 0.6768 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2808  | total loss: \u001b[1m\u001b[32m1.75351\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2808 | loss: 1.75351 - acc: 0.6092 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2809  | total loss: \u001b[1m\u001b[32m1.61157\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2809 | loss: 1.61157 - acc: 0.6482 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2810  | total loss: \u001b[1m\u001b[32m1.83231\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2810 | loss: 1.83231 - acc: 0.5906 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2811  | total loss: \u001b[1m\u001b[32m1.68674\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2811 | loss: 1.68674 - acc: 0.6315 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2812  | total loss: \u001b[1m\u001b[32m1.86543\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2812 | loss: 1.86543 - acc: 0.5755 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2813  | total loss: \u001b[1m\u001b[32m1.72144\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2813 | loss: 1.72144 - acc: 0.6179 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2814  | total loss: \u001b[1m\u001b[32m1.91897\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2814 | loss: 1.91897 - acc: 0.5562 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2815  | total loss: \u001b[1m\u001b[32m1.77493\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2815 | loss: 1.77493 - acc: 0.6005 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2816  | total loss: \u001b[1m\u001b[32m1.96089\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2816 | loss: 1.96089 - acc: 0.5405 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2817  | total loss: \u001b[1m\u001b[32m1.81822\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2817 | loss: 1.81822 - acc: 0.5864 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2818  | total loss: \u001b[1m\u001b[32m1.69210\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2818 | loss: 1.69210 - acc: 0.6278 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2819  | total loss: \u001b[1m\u001b[32m1.57986\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2819 | loss: 1.57986 - acc: 0.6650 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2820  | total loss: \u001b[1m\u001b[32m1.74052\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2820 | loss: 1.74052 - acc: 0.5985 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2821  | total loss: \u001b[1m\u001b[32m1.62512\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2821 | loss: 1.62512 - acc: 0.6387 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2822  | total loss: \u001b[1m\u001b[32m1.83974\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2822 | loss: 1.83974 - acc: 0.5748 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2823  | total loss: \u001b[1m\u001b[32m1.71629\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2823 | loss: 1.71629 - acc: 0.6173 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2824  | total loss: \u001b[1m\u001b[32m1.85784\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2824 | loss: 1.85784 - acc: 0.5699 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2825  | total loss: \u001b[1m\u001b[32m1.73431\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2825 | loss: 1.73431 - acc: 0.6129 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2826  | total loss: \u001b[1m\u001b[32m1.62339\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2826 | loss: 1.62339 - acc: 0.6516 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2827  | total loss: \u001b[1m\u001b[32m1.52288\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2827 | loss: 1.52288 - acc: 0.6864 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2828  | total loss: \u001b[1m\u001b[32m1.43095\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2828 | loss: 1.43095 - acc: 0.7178 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2829  | total loss: \u001b[1m\u001b[32m1.34603\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2829 | loss: 1.34603 - acc: 0.7460 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2830  | total loss: \u001b[1m\u001b[32m1.53719\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2830 | loss: 1.53719 - acc: 0.6786 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2831  | total loss: \u001b[1m\u001b[32m1.43753\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2831 | loss: 1.43753 - acc: 0.7107 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2832  | total loss: \u001b[1m\u001b[32m1.61472\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2832 | loss: 1.61472 - acc: 0.6539 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2833  | total loss: \u001b[1m\u001b[32m1.50440\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2833 | loss: 1.50440 - acc: 0.6885 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2834  | total loss: \u001b[1m\u001b[32m1.72441\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2834 | loss: 1.72441 - acc: 0.6197 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2835  | total loss: \u001b[1m\u001b[32m1.60139\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2835 | loss: 1.60139 - acc: 0.6577 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2836  | total loss: \u001b[1m\u001b[32m1.79525\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2836 | loss: 1.79525 - acc: 0.5919 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2837  | total loss: \u001b[1m\u001b[32m1.66457\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2837 | loss: 1.66457 - acc: 0.6327 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2838  | total loss: \u001b[1m\u001b[32m1.85559\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2838 | loss: 1.85559 - acc: 0.5695 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2839  | total loss: \u001b[1m\u001b[32m1.71932\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2839 | loss: 1.71932 - acc: 0.6125 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2840  | total loss: \u001b[1m\u001b[32m1.59677\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2840 | loss: 1.59677 - acc: 0.6513 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2841  | total loss: \u001b[1m\u001b[32m1.48590\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2841 | loss: 1.48590 - acc: 0.6861 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2842  | total loss: \u001b[1m\u001b[32m1.38495\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2842 | loss: 1.38495 - acc: 0.7175 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2843  | total loss: \u001b[1m\u001b[32m1.29242\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2843 | loss: 1.29242 - acc: 0.7458 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2844  | total loss: \u001b[1m\u001b[32m1.50832\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2844 | loss: 1.50832 - acc: 0.6783 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2845  | total loss: \u001b[1m\u001b[32m1.40051\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2845 | loss: 1.40051 - acc: 0.7105 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2846  | total loss: \u001b[1m\u001b[32m1.62159\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2846 | loss: 1.62159 - acc: 0.6395 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2847  | total loss: \u001b[1m\u001b[32m1.50110\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2847 | loss: 1.50110 - acc: 0.6755 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2848  | total loss: \u001b[1m\u001b[32m1.66114\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2848 | loss: 1.66114 - acc: 0.6222 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2849  | total loss: \u001b[1m\u001b[32m1.53656\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2849 | loss: 1.53656 - acc: 0.6600 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2850  | total loss: \u001b[1m\u001b[32m1.74208\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2850 | loss: 1.74208 - acc: 0.6012 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2851  | total loss: \u001b[1m\u001b[32m1.61015\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2851 | loss: 1.61015 - acc: 0.6410 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2852  | total loss: \u001b[1m\u001b[32m1.80344\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2852 | loss: 1.80344 - acc: 0.5841 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2853  | total loss: \u001b[1m\u001b[32m1.66692\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2853 | loss: 1.66692 - acc: 0.6257 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2854  | total loss: \u001b[1m\u001b[32m1.83298\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2854 | loss: 1.83298 - acc: 0.5702 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2855  | total loss: \u001b[1m\u001b[32m1.69563\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2855 | loss: 1.69563 - acc: 0.6132 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2856  | total loss: \u001b[1m\u001b[32m1.83714\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2856 | loss: 1.83714 - acc: 0.5590 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2857  | total loss: \u001b[1m\u001b[32m1.70190\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2857 | loss: 1.70190 - acc: 0.6031 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2858  | total loss: \u001b[1m\u001b[32m1.58113\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2858 | loss: 1.58113 - acc: 0.6428 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2859  | total loss: \u001b[1m\u001b[32m1.47259\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2859 | loss: 1.47259 - acc: 0.6785 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2860  | total loss: \u001b[1m\u001b[32m1.37433\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2860 | loss: 1.37433 - acc: 0.7107 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2861  | total loss: \u001b[1m\u001b[32m1.28470\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2861 | loss: 1.28470 - acc: 0.7396 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2862  | total loss: \u001b[1m\u001b[32m1.46727\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2862 | loss: 1.46727 - acc: 0.6799 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2863  | total loss: \u001b[1m\u001b[32m1.36604\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2863 | loss: 1.36604 - acc: 0.7119 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2864  | total loss: \u001b[1m\u001b[32m1.27378\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2864 | loss: 1.27378 - acc: 0.7408 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2865  | total loss: \u001b[1m\u001b[32m1.18912\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2865 | loss: 1.18912 - acc: 0.7667 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2866  | total loss: \u001b[1m\u001b[32m1.44377\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2866 | loss: 1.44377 - acc: 0.6900 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2867  | total loss: \u001b[1m\u001b[32m1.33943\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2867 | loss: 1.33943 - acc: 0.7210 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2868  | total loss: \u001b[1m\u001b[32m1.24438\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2868 | loss: 1.24438 - acc: 0.7489 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2869  | total loss: \u001b[1m\u001b[32m1.15733\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2869 | loss: 1.15733 - acc: 0.7740 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2870  | total loss: \u001b[1m\u001b[32m1.39688\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2870 | loss: 1.39688 - acc: 0.7038 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2871  | total loss: \u001b[1m\u001b[32m1.29223\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2871 | loss: 1.29223 - acc: 0.7334 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2872  | total loss: \u001b[1m\u001b[32m1.51197\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2872 | loss: 1.51197 - acc: 0.6672 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2873  | total loss: \u001b[1m\u001b[32m1.39510\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2873 | loss: 1.39510 - acc: 0.7005 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2874  | total loss: \u001b[1m\u001b[32m1.63461\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2874 | loss: 1.63461 - acc: 0.6304 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2875  | total loss: \u001b[1m\u001b[32m1.50612\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2875 | loss: 1.50612 - acc: 0.6674 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2876  | total loss: \u001b[1m\u001b[32m1.39084\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2876 | loss: 1.39084 - acc: 0.7006 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2877  | total loss: \u001b[1m\u001b[32m1.28697\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2877 | loss: 1.28697 - acc: 0.7306 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2878  | total loss: \u001b[1m\u001b[32m1.57673\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2878 | loss: 1.57673 - acc: 0.6575 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2879  | total loss: \u001b[1m\u001b[32m1.45449\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2879 | loss: 1.45449 - acc: 0.6918 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2880  | total loss: \u001b[1m\u001b[32m1.67479\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2880 | loss: 1.67479 - acc: 0.6297 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2881  | total loss: \u001b[1m\u001b[32m1.54434\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2881 | loss: 1.54434 - acc: 0.6668 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2882  | total loss: \u001b[1m\u001b[32m1.72786\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2882 | loss: 1.72786 - acc: 0.6144 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2883  | total loss: \u001b[1m\u001b[32m1.59446\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2883 | loss: 1.59446 - acc: 0.6529 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2884  | total loss: \u001b[1m\u001b[32m1.47536\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2884 | loss: 1.47536 - acc: 0.6876 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2885  | total loss: \u001b[1m\u001b[32m1.36851\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2885 | loss: 1.36851 - acc: 0.7189 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2886  | total loss: \u001b[1m\u001b[32m1.61556\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2886 | loss: 1.61556 - acc: 0.6541 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2887  | total loss: \u001b[1m\u001b[32m1.49532\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2887 | loss: 1.49532 - acc: 0.6887 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2888  | total loss: \u001b[1m\u001b[32m1.66648\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2888 | loss: 1.66648 - acc: 0.6341 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2889  | total loss: \u001b[1m\u001b[32m1.54248\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2889 | loss: 1.54248 - acc: 0.6707 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2890  | total loss: \u001b[1m\u001b[32m1.78352\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2890 | loss: 1.78352 - acc: 0.6036 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2891  | total loss: \u001b[1m\u001b[32m1.64983\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2891 | loss: 1.64983 - acc: 0.6433 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2892  | total loss: \u001b[1m\u001b[32m1.85581\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2892 | loss: 1.85581 - acc: 0.5790 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2893  | total loss: \u001b[1m\u001b[32m1.71778\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2893 | loss: 1.71778 - acc: 0.6211 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2894  | total loss: \u001b[1m\u001b[32m1.87563\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2894 | loss: 1.87563 - acc: 0.5661 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2895  | total loss: \u001b[1m\u001b[32m1.73882\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2895 | loss: 1.73882 - acc: 0.6095 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2896  | total loss: \u001b[1m\u001b[32m1.61682\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2896 | loss: 1.61682 - acc: 0.6485 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2897  | total loss: \u001b[1m\u001b[32m1.50732\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2897 | loss: 1.50732 - acc: 0.6837 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2898  | total loss: \u001b[1m\u001b[32m1.40831\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2898 | loss: 1.40831 - acc: 0.7153 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2899  | total loss: \u001b[1m\u001b[32m1.31807\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2899 | loss: 1.31807 - acc: 0.7438 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2900  | total loss: \u001b[1m\u001b[32m1.56252\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2900 | loss: 1.56252 - acc: 0.6694 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2901  | total loss: \u001b[1m\u001b[32m1.45489\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2901 | loss: 1.45489 - acc: 0.7025 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2902  | total loss: \u001b[1m\u001b[32m1.68259\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2902 | loss: 1.68259 - acc: 0.6394 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2903  | total loss: \u001b[1m\u001b[32m1.56226\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2903 | loss: 1.56226 - acc: 0.6754 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2904  | total loss: \u001b[1m\u001b[32m1.75035\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2904 | loss: 1.75035 - acc: 0.6222 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2905  | total loss: \u001b[1m\u001b[32m1.62333\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2905 | loss: 1.62333 - acc: 0.6600 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2906  | total loss: \u001b[1m\u001b[32m1.75119\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2906 | loss: 1.75119 - acc: 0.6154 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2907  | total loss: \u001b[1m\u001b[32m1.62466\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2907 | loss: 1.62466 - acc: 0.6538 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2908  | total loss: \u001b[1m\u001b[32m1.82625\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2908 | loss: 1.82625 - acc: 0.5885 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2909  | total loss: \u001b[1m\u001b[32m1.69340\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2909 | loss: 1.69340 - acc: 0.6296 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2910  | total loss: \u001b[1m\u001b[32m1.89903\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2910 | loss: 1.89903 - acc: 0.5667 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2911  | total loss: \u001b[1m\u001b[32m1.76089\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2911 | loss: 1.76089 - acc: 0.6100 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2912  | total loss: \u001b[1m\u001b[32m1.92259\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2912 | loss: 1.92259 - acc: 0.5561 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2913  | total loss: \u001b[1m\u001b[32m1.78439\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2913 | loss: 1.78439 - acc: 0.6005 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2914  | total loss: \u001b[1m\u001b[32m1.93784\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2914 | loss: 1.93784 - acc: 0.5476 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2915  | total loss: \u001b[1m\u001b[32m1.80046\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2915 | loss: 1.80046 - acc: 0.5928 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2916  | total loss: \u001b[1m\u001b[32m1.98407\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2916 | loss: 1.98407 - acc: 0.5336 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2917  | total loss: \u001b[1m\u001b[32m1.84464\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2917 | loss: 1.84464 - acc: 0.5802 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2918  | total loss: \u001b[1m\u001b[32m1.72002\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2918 | loss: 1.72002 - acc: 0.6222 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2919  | total loss: \u001b[1m\u001b[32m1.60785\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2919 | loss: 1.60785 - acc: 0.6600 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2920  | total loss: \u001b[1m\u001b[32m1.50607\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2920 | loss: 1.50607 - acc: 0.6940 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2921  | total loss: \u001b[1m\u001b[32m1.41292\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2921 | loss: 1.41292 - acc: 0.7246 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2922  | total loss: \u001b[1m\u001b[32m1.60046\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2922 | loss: 1.60046 - acc: 0.6593 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2923  | total loss: \u001b[1m\u001b[32m1.49470\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2923 | loss: 1.49470 - acc: 0.6933 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2924  | total loss: \u001b[1m\u001b[32m1.67726\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2924 | loss: 1.67726 - acc: 0.6311 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2925  | total loss: \u001b[1m\u001b[32m1.56168\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2925 | loss: 1.56168 - acc: 0.6680 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2926  | total loss: \u001b[1m\u001b[32m1.72550\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2926 | loss: 1.72550 - acc: 0.6084 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2927  | total loss: \u001b[1m\u001b[32m1.60397\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2927 | loss: 1.60397 - acc: 0.6475 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2928  | total loss: \u001b[1m\u001b[32m1.77453\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2928 | loss: 1.77453 - acc: 0.5899 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2929  | total loss: \u001b[1m\u001b[32m1.64782\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2929 | loss: 1.64782 - acc: 0.6309 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2930  | total loss: \u001b[1m\u001b[32m1.81946\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2930 | loss: 1.81946 - acc: 0.5750 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2931  | total loss: \u001b[1m\u001b[32m1.68857\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2931 | loss: 1.68857 - acc: 0.6175 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2932  | total loss: \u001b[1m\u001b[32m1.86943\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2932 | loss: 1.86943 - acc: 0.5557 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2933  | total loss: \u001b[1m\u001b[32m1.73440\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2933 | loss: 1.73440 - acc: 0.6002 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2934  | total loss: \u001b[1m\u001b[32m1.89768\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2934 | loss: 1.89768 - acc: 0.5473 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2935  | total loss: \u001b[1m\u001b[32m1.76105\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2935 | loss: 1.76105 - acc: 0.5926 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2936  | total loss: \u001b[1m\u001b[32m1.63831\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2936 | loss: 1.63831 - acc: 0.6333 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2937  | total loss: \u001b[1m\u001b[32m1.52735\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2937 | loss: 1.52735 - acc: 0.6700 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2938  | total loss: \u001b[1m\u001b[32m1.72382\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2938 | loss: 1.72382 - acc: 0.6101 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2939  | total loss: \u001b[1m\u001b[32m1.60302\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2939 | loss: 1.60302 - acc: 0.6491 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2940  | total loss: \u001b[1m\u001b[32m1.79182\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2940 | loss: 1.79182 - acc: 0.5913 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2941  | total loss: \u001b[1m\u001b[32m1.66380\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2941 | loss: 1.66380 - acc: 0.6322 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2942  | total loss: \u001b[1m\u001b[32m1.83176\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2942 | loss: 1.83176 - acc: 0.5761 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2943  | total loss: \u001b[1m\u001b[32m1.70003\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2943 | loss: 1.70003 - acc: 0.6185 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2944  | total loss: \u001b[1m\u001b[32m1.88920\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2944 | loss: 1.88920 - acc: 0.5567 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2945  | total loss: \u001b[1m\u001b[32m1.75262\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2945 | loss: 1.75262 - acc: 0.6010 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2946  | total loss: \u001b[1m\u001b[32m1.62989\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2946 | loss: 1.62989 - acc: 0.6409 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2947  | total loss: \u001b[1m\u001b[32m1.51892\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2947 | loss: 1.51892 - acc: 0.6768 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2948  | total loss: \u001b[1m\u001b[32m1.41789\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2948 | loss: 1.41789 - acc: 0.7091 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2949  | total loss: \u001b[1m\u001b[32m1.32527\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2949 | loss: 1.32527 - acc: 0.7382 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2950  | total loss: \u001b[1m\u001b[32m1.23977\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2950 | loss: 1.23977 - acc: 0.7644 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2951  | total loss: \u001b[1m\u001b[32m1.16034\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2951 | loss: 1.16034 - acc: 0.7880 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2952  | total loss: \u001b[1m\u001b[32m1.40347\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2952 | loss: 1.40347 - acc: 0.7092 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2953  | total loss: \u001b[1m\u001b[32m1.30360\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2953 | loss: 1.30360 - acc: 0.7382 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2954  | total loss: \u001b[1m\u001b[32m1.51934\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2954 | loss: 1.51934 - acc: 0.6716 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2955  | total loss: \u001b[1m\u001b[32m1.40560\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2955 | loss: 1.40560 - acc: 0.7044 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2956  | total loss: \u001b[1m\u001b[32m1.61158\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2956 | loss: 1.61158 - acc: 0.6411 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2957  | total loss: \u001b[1m\u001b[32m1.48766\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2957 | loss: 1.48766 - acc: 0.6770 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2958  | total loss: \u001b[1m\u001b[32m1.73926\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2958 | loss: 1.73926 - acc: 0.6093 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2959  | total loss: \u001b[1m\u001b[32m1.60287\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2959 | loss: 1.60287 - acc: 0.6484 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2960  | total loss: \u001b[1m\u001b[32m1.79896\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2960 | loss: 1.79896 - acc: 0.5907 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2961  | total loss: \u001b[1m\u001b[32m1.65786\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2961 | loss: 1.65786 - acc: 0.6316 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2962  | total loss: \u001b[1m\u001b[32m1.53138\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2962 | loss: 1.53138 - acc: 0.6684 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2963  | total loss: \u001b[1m\u001b[32m1.41753\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2963 | loss: 1.41753 - acc: 0.7016 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2964  | total loss: \u001b[1m\u001b[32m1.65251\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2964 | loss: 1.65251 - acc: 0.6386 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2965  | total loss: \u001b[1m\u001b[32m1.52662\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2965 | loss: 1.52662 - acc: 0.6747 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2966  | total loss: \u001b[1m\u001b[32m1.75884\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2966 | loss: 1.75884 - acc: 0.6073 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2967  | total loss: \u001b[1m\u001b[32m1.62356\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2967 | loss: 1.62356 - acc: 0.6465 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2968  | total loss: \u001b[1m\u001b[32m1.81759\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2968 | loss: 1.81759 - acc: 0.5819 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2969  | total loss: \u001b[1m\u001b[32m1.67862\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2969 | loss: 1.67862 - acc: 0.6237 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2970  | total loss: \u001b[1m\u001b[32m1.55448\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2970 | loss: 1.55448 - acc: 0.6613 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2971  | total loss: \u001b[1m\u001b[32m1.44303\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2971 | loss: 1.44303 - acc: 0.6952 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2972  | total loss: \u001b[1m\u001b[32m1.62071\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2972 | loss: 1.62071 - acc: 0.6328 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2973  | total loss: \u001b[1m\u001b[32m1.50294\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2973 | loss: 1.50294 - acc: 0.6695 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2974  | total loss: \u001b[1m\u001b[32m1.74278\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2974 | loss: 1.74278 - acc: 0.6026 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2975  | total loss: \u001b[1m\u001b[32m1.61396\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2975 | loss: 1.61396 - acc: 0.6423 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2976  | total loss: \u001b[1m\u001b[32m1.80035\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2976 | loss: 1.80035 - acc: 0.5852 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2977  | total loss: \u001b[1m\u001b[32m1.66769\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2977 | loss: 1.66769 - acc: 0.6267 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2978  | total loss: \u001b[1m\u001b[32m1.83243\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2978 | loss: 1.83243 - acc: 0.5783 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2979  | total loss: \u001b[1m\u001b[32m1.69880\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2979 | loss: 1.69880 - acc: 0.6205 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2980  | total loss: \u001b[1m\u001b[32m1.84925\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2980 | loss: 1.84925 - acc: 0.5656 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2981  | total loss: \u001b[1m\u001b[32m1.71628\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2981 | loss: 1.71628 - acc: 0.6090 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2982  | total loss: \u001b[1m\u001b[32m1.89981\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2982 | loss: 1.89981 - acc: 0.5553 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2983  | total loss: \u001b[1m\u001b[32m1.76431\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2983 | loss: 1.76431 - acc: 0.5997 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2984  | total loss: \u001b[1m\u001b[32m1.64326\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2984 | loss: 1.64326 - acc: 0.6398 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2985  | total loss: \u001b[1m\u001b[32m1.53440\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2985 | loss: 1.53440 - acc: 0.6758 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2986  | total loss: \u001b[1m\u001b[32m1.43578\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2986 | loss: 1.43578 - acc: 0.7082 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2987  | total loss: \u001b[1m\u001b[32m1.34572\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2987 | loss: 1.34572 - acc: 0.7374 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2988  | total loss: \u001b[1m\u001b[32m1.26283\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2988 | loss: 1.26283 - acc: 0.7636 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2989  | total loss: \u001b[1m\u001b[32m1.18592\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2989 | loss: 1.18592 - acc: 0.7873 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2990  | total loss: \u001b[1m\u001b[32m1.11405\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2990 | loss: 1.11405 - acc: 0.8086 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2991  | total loss: \u001b[1m\u001b[32m1.04647\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2991 | loss: 1.04647 - acc: 0.8277 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2992  | total loss: \u001b[1m\u001b[32m1.30272\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2992 | loss: 1.30272 - acc: 0.7521 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2993  | total loss: \u001b[1m\u001b[32m1.21153\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2993 | loss: 1.21153 - acc: 0.7769 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2994  | total loss: \u001b[1m\u001b[32m1.12753\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2994 | loss: 1.12753 - acc: 0.7992 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2995  | total loss: \u001b[1m\u001b[32m1.04981\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2995 | loss: 1.04981 - acc: 0.8193 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2996  | total loss: \u001b[1m\u001b[32m0.97763\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2996 | loss: 0.97763 - acc: 0.8373 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2997  | total loss: \u001b[1m\u001b[32m0.91038\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2997 | loss: 0.91038 - acc: 0.8536 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2998  | total loss: \u001b[1m\u001b[32m1.20993\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 2998 | loss: 1.20993 - acc: 0.7754 -- iter: 14/14\n",
      "--\n",
      "Training Step: 2999  | total loss: \u001b[1m\u001b[32m1.11618\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 2999 | loss: 1.11618 - acc: 0.7978 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3000  | total loss: \u001b[1m\u001b[32m1.38318\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3000 | loss: 1.38318 - acc: 0.7252 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3001  | total loss: \u001b[1m\u001b[32m1.27081\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3001 | loss: 1.27081 - acc: 0.7527 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3002  | total loss: \u001b[1m\u001b[32m1.49690\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3002 | loss: 1.49690 - acc: 0.6917 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3003  | total loss: \u001b[1m\u001b[32m1.37323\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3003 | loss: 1.37323 - acc: 0.7225 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3004  | total loss: \u001b[1m\u001b[32m1.63594\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3004 | loss: 1.63594 - acc: 0.6574 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3005  | total loss: \u001b[1m\u001b[32m1.49968\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3005 | loss: 1.49968 - acc: 0.6917 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3006  | total loss: \u001b[1m\u001b[32m1.73809\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3006 | loss: 1.73809 - acc: 0.6225 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3007  | total loss: \u001b[1m\u001b[32m1.59417\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3007 | loss: 1.59417 - acc: 0.6603 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3008  | total loss: \u001b[1m\u001b[32m1.83494\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3008 | loss: 1.83494 - acc: 0.5942 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3009  | total loss: \u001b[1m\u001b[32m1.68499\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3009 | loss: 1.68499 - acc: 0.6348 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3010  | total loss: \u001b[1m\u001b[32m1.85206\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3010 | loss: 1.85206 - acc: 0.5856 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3011  | total loss: \u001b[1m\u001b[32m1.70467\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3011 | loss: 1.70467 - acc: 0.6271 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3012  | total loss: \u001b[1m\u001b[32m1.57389\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3012 | loss: 1.57389 - acc: 0.6643 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3013  | total loss: \u001b[1m\u001b[32m1.45741\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3013 | loss: 1.45741 - acc: 0.6979 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3014  | total loss: \u001b[1m\u001b[32m1.66292\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3014 | loss: 1.66292 - acc: 0.6353 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3015  | total loss: \u001b[1m\u001b[32m1.53965\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3015 | loss: 1.53965 - acc: 0.6717 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3016  | total loss: \u001b[1m\u001b[32m1.73653\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3016 | loss: 1.73653 - acc: 0.6046 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3017  | total loss: \u001b[1m\u001b[32m1.60861\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3017 | loss: 1.60861 - acc: 0.6441 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3018  | total loss: \u001b[1m\u001b[32m1.74713\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3018 | loss: 1.74713 - acc: 0.6011 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3019  | total loss: \u001b[1m\u001b[32m1.62117\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3019 | loss: 1.62117 - acc: 0.6410 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3020  | total loss: \u001b[1m\u001b[32m1.76232\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3020 | loss: 1.76232 - acc: 0.5983 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3021  | total loss: \u001b[1m\u001b[32m1.63775\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3021 | loss: 1.63775 - acc: 0.6385 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3022  | total loss: \u001b[1m\u001b[32m1.85963\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3022 | loss: 1.85963 - acc: 0.5747 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3023  | total loss: \u001b[1m\u001b[32m1.72843\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3023 | loss: 1.72843 - acc: 0.6172 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3024  | total loss: \u001b[1m\u001b[32m1.86478\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3024 | loss: 1.86478 - acc: 0.5698 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3025  | total loss: \u001b[1m\u001b[32m1.73627\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3025 | loss: 1.73627 - acc: 0.6128 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3026  | total loss: \u001b[1m\u001b[32m1.90844\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3026 | loss: 1.90844 - acc: 0.5586 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3027  | total loss: \u001b[1m\u001b[32m1.77860\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3027 | loss: 1.77860 - acc: 0.6028 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3028  | total loss: \u001b[1m\u001b[32m1.66271\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3028 | loss: 1.66271 - acc: 0.6425 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3029  | total loss: \u001b[1m\u001b[32m1.55841\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3029 | loss: 1.55841 - acc: 0.6783 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3030  | total loss: \u001b[1m\u001b[32m1.75985\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3030 | loss: 1.75985 - acc: 0.6104 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3031  | total loss: \u001b[1m\u001b[32m1.64526\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3031 | loss: 1.64526 - acc: 0.6494 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3032  | total loss: \u001b[1m\u001b[32m1.82640\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3032 | loss: 1.82640 - acc: 0.5916 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3033  | total loss: \u001b[1m\u001b[32m1.70495\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3033 | loss: 1.70495 - acc: 0.6324 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3034  | total loss: \u001b[1m\u001b[32m1.84505\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3034 | loss: 1.84505 - acc: 0.5835 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3035  | total loss: \u001b[1m\u001b[32m1.72162\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3035 | loss: 1.72162 - acc: 0.6251 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3036  | total loss: \u001b[1m\u001b[32m1.90987\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3036 | loss: 1.90987 - acc: 0.5626 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3037  | total loss: \u001b[1m\u001b[32m1.78014\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3037 | loss: 1.78014 - acc: 0.6064 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3038  | total loss: \u001b[1m\u001b[32m1.91333\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3038 | loss: 1.91333 - acc: 0.5529 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3039  | total loss: \u001b[1m\u001b[32m1.78375\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3039 | loss: 1.78375 - acc: 0.5976 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3040  | total loss: \u001b[1m\u001b[32m1.66695\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3040 | loss: 1.66695 - acc: 0.6378 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3041  | total loss: \u001b[1m\u001b[32m1.56086\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3041 | loss: 1.56086 - acc: 0.6740 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3042  | total loss: \u001b[1m\u001b[32m1.70355\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3042 | loss: 1.70355 - acc: 0.6209 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3043  | total loss: \u001b[1m\u001b[32m1.59140\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3043 | loss: 1.59140 - acc: 0.6588 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3044  | total loss: \u001b[1m\u001b[32m1.76795\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3044 | loss: 1.76795 - acc: 0.6001 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3045  | total loss: \u001b[1m\u001b[32m1.64763\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3045 | loss: 1.64763 - acc: 0.6401 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3046  | total loss: \u001b[1m\u001b[32m1.53833\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3046 | loss: 1.53833 - acc: 0.6761 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3047  | total loss: \u001b[1m\u001b[32m1.43833\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3047 | loss: 1.43833 - acc: 0.7085 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3048  | total loss: \u001b[1m\u001b[32m1.63019\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3048 | loss: 1.63019 - acc: 0.6448 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3049  | total loss: \u001b[1m\u001b[32m1.51797\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3049 | loss: 1.51797 - acc: 0.6803 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3050  | total loss: \u001b[1m\u001b[32m1.41551\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3050 | loss: 1.41551 - acc: 0.7123 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3051  | total loss: \u001b[1m\u001b[32m1.32140\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3051 | loss: 1.32140 - acc: 0.7410 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3052  | total loss: \u001b[1m\u001b[32m1.57560\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3052 | loss: 1.57560 - acc: 0.6669 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3053  | total loss: \u001b[1m\u001b[32m1.46240\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3053 | loss: 1.46240 - acc: 0.7002 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3054  | total loss: \u001b[1m\u001b[32m1.67967\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3054 | loss: 1.67967 - acc: 0.6302 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3055  | total loss: \u001b[1m\u001b[32m1.55475\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3055 | loss: 1.55475 - acc: 0.6672 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3056  | total loss: \u001b[1m\u001b[32m1.44177\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3056 | loss: 1.44177 - acc: 0.7005 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3057  | total loss: \u001b[1m\u001b[32m1.33905\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3057 | loss: 1.33905 - acc: 0.7304 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3058  | total loss: \u001b[1m\u001b[32m1.54085\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3058 | loss: 1.54085 - acc: 0.6645 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3059  | total loss: \u001b[1m\u001b[32m1.42645\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3059 | loss: 1.42645 - acc: 0.6981 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3060  | total loss: \u001b[1m\u001b[32m1.62823\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3060 | loss: 1.62823 - acc: 0.6354 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3061  | total loss: \u001b[1m\u001b[32m1.50466\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3061 | loss: 1.50466 - acc: 0.6719 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3062  | total loss: \u001b[1m\u001b[32m1.72023\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3062 | loss: 1.72023 - acc: 0.6118 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3063  | total loss: \u001b[1m\u001b[32m1.58817\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3063 | loss: 1.58817 - acc: 0.6506 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3064  | total loss: \u001b[1m\u001b[32m1.80751\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3064 | loss: 1.80751 - acc: 0.5927 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3065  | total loss: \u001b[1m\u001b[32m1.66838\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3065 | loss: 1.66838 - acc: 0.6334 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3066  | total loss: \u001b[1m\u001b[32m1.86149\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3066 | loss: 1.86149 - acc: 0.5701 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3067  | total loss: \u001b[1m\u001b[32m1.71943\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3067 | loss: 1.71943 - acc: 0.6131 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3068  | total loss: \u001b[1m\u001b[32m1.59259\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3068 | loss: 1.59259 - acc: 0.6518 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3069  | total loss: \u001b[1m\u001b[32m1.47875\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3069 | loss: 1.47875 - acc: 0.6866 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3070  | total loss: \u001b[1m\u001b[32m1.37597\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3070 | loss: 1.37597 - acc: 0.7179 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3071  | total loss: \u001b[1m\u001b[32m1.28259\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3071 | loss: 1.28259 - acc: 0.7461 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3072  | total loss: \u001b[1m\u001b[32m1.51275\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3072 | loss: 1.51275 - acc: 0.6787 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3073  | total loss: \u001b[1m\u001b[32m1.40411\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3073 | loss: 1.40411 - acc: 0.7108 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3074  | total loss: \u001b[1m\u001b[32m1.64255\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3074 | loss: 1.64255 - acc: 0.6397 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3075  | total loss: \u001b[1m\u001b[32m1.52069\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3075 | loss: 1.52069 - acc: 0.6758 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3076  | total loss: \u001b[1m\u001b[32m1.41093\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3076 | loss: 1.41093 - acc: 0.7082 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3077  | total loss: \u001b[1m\u001b[32m1.31151\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3077 | loss: 1.31151 - acc: 0.7374 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3078  | total loss: \u001b[1m\u001b[32m1.51580\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3078 | loss: 1.51580 - acc: 0.6779 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3079  | total loss: \u001b[1m\u001b[32m1.40481\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3079 | loss: 1.40481 - acc: 0.7101 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3080  | total loss: \u001b[1m\u001b[32m1.63768\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3080 | loss: 1.63768 - acc: 0.6391 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3081  | total loss: \u001b[1m\u001b[32m1.51474\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3081 | loss: 1.51474 - acc: 0.6752 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3082  | total loss: \u001b[1m\u001b[32m1.40424\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3082 | loss: 1.40424 - acc: 0.7077 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3083  | total loss: \u001b[1m\u001b[32m1.30440\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3083 | loss: 1.30440 - acc: 0.7369 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3084  | total loss: \u001b[1m\u001b[32m1.21367\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3084 | loss: 1.21367 - acc: 0.7632 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3085  | total loss: \u001b[1m\u001b[32m1.13074\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3085 | loss: 1.13074 - acc: 0.7869 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3086  | total loss: \u001b[1m\u001b[32m1.05449\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3086 | loss: 1.05449 - acc: 0.8082 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3087  | total loss: \u001b[1m\u001b[32m0.98400\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3087 | loss: 0.98400 - acc: 0.8274 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3088  | total loss: \u001b[1m\u001b[32m0.91850\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3088 | loss: 0.91850 - acc: 0.8446 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3089  | total loss: \u001b[1m\u001b[32m0.85739\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3089 | loss: 0.85739 - acc: 0.8602 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3090  | total loss: \u001b[1m\u001b[32m0.80018\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3090 | loss: 0.80018 - acc: 0.8742 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3091  | total loss: \u001b[1m\u001b[32m0.74648\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3091 | loss: 0.74648 - acc: 0.8867 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3092  | total loss: \u001b[1m\u001b[32m1.02364\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3092 | loss: 1.02364 - acc: 0.8124 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3093  | total loss: \u001b[1m\u001b[32m0.94442\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3093 | loss: 0.94442 - acc: 0.8311 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3094  | total loss: \u001b[1m\u001b[32m1.31054\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3094 | loss: 1.31054 - acc: 0.7480 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3095  | total loss: \u001b[1m\u001b[32m1.20149\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3095 | loss: 1.20149 - acc: 0.7732 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3096  | total loss: \u001b[1m\u001b[32m1.52562\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3096 | loss: 1.52562 - acc: 0.6959 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3097  | total loss: \u001b[1m\u001b[32m1.39550\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3097 | loss: 1.39550 - acc: 0.7263 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3098  | total loss: \u001b[1m\u001b[32m1.64011\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3098 | loss: 1.64011 - acc: 0.6608 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3099  | total loss: \u001b[1m\u001b[32m1.50026\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3099 | loss: 1.50026 - acc: 0.6947 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3100  | total loss: \u001b[1m\u001b[32m1.74587\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3100 | loss: 1.74587 - acc: 0.6324 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3101  | total loss: \u001b[1m\u001b[32m1.59824\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3101 | loss: 1.59824 - acc: 0.6692 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3102  | total loss: \u001b[1m\u001b[32m1.82185\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3102 | loss: 1.82185 - acc: 0.6094 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3103  | total loss: \u001b[1m\u001b[32m1.67040\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3103 | loss: 1.67040 - acc: 0.6484 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3104  | total loss: \u001b[1m\u001b[32m1.92163\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3104 | loss: 1.92163 - acc: 0.5836 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3105  | total loss: \u001b[1m\u001b[32m1.76500\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3105 | loss: 1.76500 - acc: 0.6252 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3106  | total loss: \u001b[1m\u001b[32m1.62635\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3106 | loss: 1.62635 - acc: 0.6627 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3107  | total loss: \u001b[1m\u001b[32m1.50323\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3107 | loss: 1.50323 - acc: 0.6964 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3108  | total loss: \u001b[1m\u001b[32m1.73810\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3108 | loss: 1.73810 - acc: 0.6268 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3109  | total loss: \u001b[1m\u001b[32m1.60689\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3109 | loss: 1.60689 - acc: 0.6641 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3110  | total loss: \u001b[1m\u001b[32m1.81738\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3110 | loss: 1.81738 - acc: 0.6049 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3111  | total loss: \u001b[1m\u001b[32m1.68195\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3111 | loss: 1.68195 - acc: 0.6444 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3112  | total loss: \u001b[1m\u001b[32m1.56160\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3112 | loss: 1.56160 - acc: 0.6799 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3113  | total loss: \u001b[1m\u001b[32m1.45402\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3113 | loss: 1.45402 - acc: 0.7119 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3114  | total loss: \u001b[1m\u001b[32m1.35718\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3114 | loss: 1.35718 - acc: 0.7407 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3115  | total loss: \u001b[1m\u001b[32m1.26934\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3115 | loss: 1.26934 - acc: 0.7667 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3116  | total loss: \u001b[1m\u001b[32m1.18902\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3116 | loss: 1.18902 - acc: 0.7900 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3117  | total loss: \u001b[1m\u001b[32m1.11499\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3117 | loss: 1.11499 - acc: 0.8110 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3118  | total loss: \u001b[1m\u001b[32m1.32632\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3118 | loss: 1.32632 - acc: 0.7442 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3119  | total loss: \u001b[1m\u001b[32m1.23555\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3119 | loss: 1.23555 - acc: 0.7698 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3120  | total loss: \u001b[1m\u001b[32m1.48774\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3120 | loss: 1.48774 - acc: 0.6928 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3121  | total loss: \u001b[1m\u001b[32m1.37947\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3121 | loss: 1.37947 - acc: 0.7235 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3122  | total loss: \u001b[1m\u001b[32m1.59672\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3122 | loss: 1.59672 - acc: 0.6512 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3123  | total loss: \u001b[1m\u001b[32m1.47763\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3123 | loss: 1.47763 - acc: 0.6860 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3124  | total loss: \u001b[1m\u001b[32m1.64380\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3124 | loss: 1.64380 - acc: 0.6389 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3125  | total loss: \u001b[1m\u001b[32m1.52088\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3125 | loss: 1.52088 - acc: 0.6750 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3126  | total loss: \u001b[1m\u001b[32m1.75390\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3126 | loss: 1.75390 - acc: 0.6075 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3127  | total loss: \u001b[1m\u001b[32m1.62159\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3127 | loss: 1.62159 - acc: 0.6467 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3128  | total loss: \u001b[1m\u001b[32m1.50319\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3128 | loss: 1.50319 - acc: 0.6821 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3129  | total loss: \u001b[1m\u001b[32m1.39666\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3129 | loss: 1.39666 - acc: 0.7139 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3130  | total loss: \u001b[1m\u001b[32m1.61697\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3130 | loss: 1.61697 - acc: 0.6425 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3131  | total loss: \u001b[1m\u001b[32m1.49914\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3131 | loss: 1.49914 - acc: 0.6782 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3132  | total loss: \u001b[1m\u001b[32m1.70075\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3132 | loss: 1.70075 - acc: 0.6175 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3133  | total loss: \u001b[1m\u001b[32m1.57551\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3133 | loss: 1.57551 - acc: 0.6558 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3134  | total loss: \u001b[1m\u001b[32m1.79003\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3134 | loss: 1.79003 - acc: 0.5902 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3135  | total loss: \u001b[1m\u001b[32m1.65751\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3135 | loss: 1.65751 - acc: 0.6312 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3136  | total loss: \u001b[1m\u001b[32m1.82560\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3136 | loss: 1.82560 - acc: 0.5752 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3137  | total loss: \u001b[1m\u001b[32m1.69159\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3137 | loss: 1.69159 - acc: 0.6177 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3138  | total loss: \u001b[1m\u001b[32m1.84338\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3138 | loss: 1.84338 - acc: 0.5631 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3139  | total loss: \u001b[1m\u001b[32m1.70992\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3139 | loss: 1.70992 - acc: 0.6068 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3140  | total loss: \u001b[1m\u001b[32m1.86035\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3140 | loss: 1.86035 - acc: 0.5532 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3141  | total loss: \u001b[1m\u001b[32m1.72766\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3141 | loss: 1.72766 - acc: 0.5979 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3142  | total loss: \u001b[1m\u001b[32m1.88330\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3142 | loss: 1.88330 - acc: 0.5453 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3143  | total loss: \u001b[1m\u001b[32m1.75075\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3143 | loss: 1.75075 - acc: 0.5907 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3144  | total loss: \u001b[1m\u001b[32m1.91550\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3144 | loss: 1.91550 - acc: 0.5388 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3145  | total loss: \u001b[1m\u001b[32m1.78214\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3145 | loss: 1.78214 - acc: 0.5849 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3146  | total loss: \u001b[1m\u001b[32m1.92709\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3146 | loss: 1.92709 - acc: 0.5407 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3147  | total loss: \u001b[1m\u001b[32m1.79478\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3147 | loss: 1.79478 - acc: 0.5866 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3148  | total loss: \u001b[1m\u001b[32m1.94641\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3148 | loss: 1.94641 - acc: 0.5351 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3149  | total loss: \u001b[1m\u001b[32m1.81385\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3149 | loss: 1.81385 - acc: 0.5816 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3150  | total loss: \u001b[1m\u001b[32m1.69477\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3150 | loss: 1.69477 - acc: 0.6234 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3151  | total loss: \u001b[1m\u001b[32m1.58696\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3151 | loss: 1.58696 - acc: 0.6611 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3152  | total loss: \u001b[1m\u001b[32m1.79889\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3152 | loss: 1.79889 - acc: 0.5950 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3153  | total loss: \u001b[1m\u001b[32m1.67914\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3153 | loss: 1.67914 - acc: 0.6355 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3154  | total loss: \u001b[1m\u001b[32m1.84717\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3154 | loss: 1.84717 - acc: 0.5791 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3155  | total loss: \u001b[1m\u001b[32m1.72172\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3155 | loss: 1.72172 - acc: 0.6212 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3156  | total loss: \u001b[1m\u001b[32m1.60808\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3156 | loss: 1.60808 - acc: 0.6591 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3157  | total loss: \u001b[1m\u001b[32m1.50439\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3157 | loss: 1.50439 - acc: 0.6932 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3158  | total loss: \u001b[1m\u001b[32m1.69633\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3158 | loss: 1.69633 - acc: 0.6310 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3159  | total loss: \u001b[1m\u001b[32m1.58095\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3159 | loss: 1.58095 - acc: 0.6679 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3160  | total loss: \u001b[1m\u001b[32m1.80580\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3160 | loss: 1.80580 - acc: 0.6011 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3161  | total loss: \u001b[1m\u001b[32m1.67786\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3161 | loss: 1.67786 - acc: 0.6410 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3162  | total loss: \u001b[1m\u001b[32m1.87253\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3162 | loss: 1.87253 - acc: 0.5769 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3163  | total loss: \u001b[1m\u001b[32m1.73746\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3163 | loss: 1.73746 - acc: 0.6192 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3164  | total loss: \u001b[1m\u001b[32m1.89556\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3164 | loss: 1.89556 - acc: 0.5716 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3165  | total loss: \u001b[1m\u001b[32m1.75841\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3165 | loss: 1.75841 - acc: 0.6144 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3166  | total loss: \u001b[1m\u001b[32m1.63485\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3166 | loss: 1.63485 - acc: 0.6530 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3167  | total loss: \u001b[1m\u001b[32m1.52287\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3167 | loss: 1.52287 - acc: 0.6877 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3168  | total loss: \u001b[1m\u001b[32m1.70836\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3168 | loss: 1.70836 - acc: 0.6260 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3169  | total loss: \u001b[1m\u001b[32m1.58738\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3169 | loss: 1.58738 - acc: 0.6634 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3170  | total loss: \u001b[1m\u001b[32m1.47759\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3170 | loss: 1.47759 - acc: 0.6971 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3171  | total loss: \u001b[1m\u001b[32m1.37737\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3171 | loss: 1.37737 - acc: 0.7274 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3172  | total loss: \u001b[1m\u001b[32m1.57735\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3172 | loss: 1.57735 - acc: 0.6618 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3173  | total loss: \u001b[1m\u001b[32m1.46467\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3173 | loss: 1.46467 - acc: 0.6956 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3174  | total loss: \u001b[1m\u001b[32m1.65286\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3174 | loss: 1.65286 - acc: 0.6332 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3175  | total loss: \u001b[1m\u001b[32m1.53138\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3175 | loss: 1.53138 - acc: 0.6699 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3176  | total loss: \u001b[1m\u001b[32m1.42142\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3176 | loss: 1.42142 - acc: 0.7029 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3177  | total loss: \u001b[1m\u001b[32m1.32135\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3177 | loss: 1.32135 - acc: 0.7326 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3178  | total loss: \u001b[1m\u001b[32m1.22981\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3178 | loss: 1.22981 - acc: 0.7593 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3179  | total loss: \u001b[1m\u001b[32m1.14562\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3179 | loss: 1.14562 - acc: 0.7834 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3180  | total loss: \u001b[1m\u001b[32m1.06781\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3180 | loss: 1.06781 - acc: 0.8051 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3181  | total loss: \u001b[1m\u001b[32m0.99556\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3181 | loss: 0.99556 - acc: 0.8246 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3182  | total loss: \u001b[1m\u001b[32m1.22646\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3182 | loss: 1.22646 - acc: 0.7635 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3183  | total loss: \u001b[1m\u001b[32m1.13468\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3183 | loss: 1.13468 - acc: 0.7872 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3184  | total loss: \u001b[1m\u001b[32m1.41486\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3184 | loss: 1.41486 - acc: 0.7156 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3185  | total loss: \u001b[1m\u001b[32m1.30242\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3185 | loss: 1.30242 - acc: 0.7440 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3186  | total loss: \u001b[1m\u001b[32m1.20065\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3186 | loss: 1.20065 - acc: 0.7696 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3187  | total loss: \u001b[1m\u001b[32m1.10825\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3187 | loss: 1.10825 - acc: 0.7927 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3188  | total loss: \u001b[1m\u001b[32m1.40301\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3188 | loss: 1.40301 - acc: 0.7205 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3189  | total loss: \u001b[1m\u001b[32m1.28945\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3189 | loss: 1.28945 - acc: 0.7485 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3190  | total loss: \u001b[1m\u001b[32m1.18705\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3190 | loss: 1.18705 - acc: 0.7736 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3191  | total loss: \u001b[1m\u001b[32m1.09444\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3191 | loss: 1.09444 - acc: 0.7963 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3192  | total loss: \u001b[1m\u001b[32m1.38250\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3192 | loss: 1.38250 - acc: 0.7238 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3193  | total loss: \u001b[1m\u001b[32m1.27003\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3193 | loss: 1.27003 - acc: 0.7514 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3194  | total loss: \u001b[1m\u001b[32m1.49957\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3194 | loss: 1.49957 - acc: 0.6906 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3195  | total loss: \u001b[1m\u001b[32m1.37632\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3195 | loss: 1.37632 - acc: 0.7215 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3196  | total loss: \u001b[1m\u001b[32m1.60112\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3196 | loss: 1.60112 - acc: 0.6636 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3197  | total loss: \u001b[1m\u001b[32m1.46958\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3197 | loss: 1.46958 - acc: 0.6973 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3198  | total loss: \u001b[1m\u001b[32m1.70990\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3198 | loss: 1.70990 - acc: 0.6275 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3199  | total loss: \u001b[1m\u001b[32m1.57038\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3199 | loss: 1.57038 - acc: 0.6648 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3200  | total loss: \u001b[1m\u001b[32m1.44629\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3200 | loss: 1.44629 - acc: 0.6983 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3201  | total loss: \u001b[1m\u001b[32m1.33556\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3201 | loss: 1.33556 - acc: 0.7285 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3202  | total loss: \u001b[1m\u001b[32m1.56485\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3202 | loss: 1.56485 - acc: 0.6628 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3203  | total loss: \u001b[1m\u001b[32m1.44414\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3203 | loss: 1.44414 - acc: 0.6965 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3204  | total loss: \u001b[1m\u001b[32m1.66328\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3204 | loss: 1.66328 - acc: 0.6268 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3205  | total loss: \u001b[1m\u001b[32m1.53540\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3205 | loss: 1.53540 - acc: 0.6642 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3206  | total loss: \u001b[1m\u001b[32m1.75669\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3206 | loss: 1.75669 - acc: 0.5977 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3207  | total loss: \u001b[1m\u001b[32m1.62285\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3207 | loss: 1.62285 - acc: 0.6380 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3208  | total loss: \u001b[1m\u001b[32m1.79976\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3208 | loss: 1.79976 - acc: 0.5813 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3209  | total loss: \u001b[1m\u001b[32m1.66535\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3209 | loss: 1.66535 - acc: 0.6232 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3210  | total loss: \u001b[1m\u001b[32m1.54587\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3210 | loss: 1.54587 - acc: 0.6609 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3211  | total loss: \u001b[1m\u001b[32m1.43908\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3211 | loss: 1.43908 - acc: 0.6948 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3212  | total loss: \u001b[1m\u001b[32m1.65681\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3212 | loss: 1.65681 - acc: 0.6253 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3213  | total loss: \u001b[1m\u001b[32m1.54006\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3213 | loss: 1.54006 - acc: 0.6628 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3214  | total loss: \u001b[1m\u001b[32m1.66843\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3214 | loss: 1.66843 - acc: 0.6179 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3215  | total loss: \u001b[1m\u001b[32m1.55189\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3215 | loss: 1.55189 - acc: 0.6561 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3216  | total loss: \u001b[1m\u001b[32m1.44727\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3216 | loss: 1.44727 - acc: 0.6905 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3217  | total loss: \u001b[1m\u001b[32m1.35267\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3217 | loss: 1.35267 - acc: 0.7215 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3218  | total loss: \u001b[1m\u001b[32m1.26646\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3218 | loss: 1.26646 - acc: 0.7493 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3219  | total loss: \u001b[1m\u001b[32m1.18728\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3219 | loss: 1.18728 - acc: 0.7744 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3220  | total loss: \u001b[1m\u001b[32m1.11397\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3220 | loss: 1.11397 - acc: 0.7969 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3221  | total loss: \u001b[1m\u001b[32m1.04562\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3221 | loss: 1.04562 - acc: 0.8173 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3222  | total loss: \u001b[1m\u001b[32m1.32810\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3222 | loss: 1.32810 - acc: 0.7355 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3223  | total loss: \u001b[1m\u001b[32m1.23452\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3223 | loss: 1.23452 - acc: 0.7620 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3224  | total loss: \u001b[1m\u001b[32m1.14875\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3224 | loss: 1.14875 - acc: 0.7858 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3225  | total loss: \u001b[1m\u001b[32m1.06976\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3225 | loss: 1.06976 - acc: 0.8072 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3226  | total loss: \u001b[1m\u001b[32m1.27210\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3226 | loss: 1.27210 - acc: 0.7479 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3227  | total loss: \u001b[1m\u001b[32m1.17768\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3227 | loss: 1.17768 - acc: 0.7731 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3228  | total loss: \u001b[1m\u001b[32m1.44879\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3228 | loss: 1.44879 - acc: 0.7029 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3229  | total loss: \u001b[1m\u001b[32m1.33519\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3229 | loss: 1.33519 - acc: 0.7327 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3230  | total loss: \u001b[1m\u001b[32m1.61587\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3230 | loss: 1.61587 - acc: 0.6594 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3231  | total loss: \u001b[1m\u001b[32m1.48572\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3231 | loss: 1.48572 - acc: 0.6935 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3232  | total loss: \u001b[1m\u001b[32m1.36885\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3232 | loss: 1.36885 - acc: 0.7241 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3233  | total loss: \u001b[1m\u001b[32m1.26355\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3233 | loss: 1.26355 - acc: 0.7517 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3234  | total loss: \u001b[1m\u001b[32m1.16833\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3234 | loss: 1.16833 - acc: 0.7765 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3235  | total loss: \u001b[1m\u001b[32m1.08188\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3235 | loss: 1.08188 - acc: 0.7989 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3236  | total loss: \u001b[1m\u001b[32m1.00309\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3236 | loss: 1.00309 - acc: 0.8190 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3237  | total loss: \u001b[1m\u001b[32m0.93097\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3237 | loss: 0.93097 - acc: 0.8371 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3238  | total loss: \u001b[1m\u001b[32m1.22830\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3238 | loss: 1.22830 - acc: 0.7534 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3239  | total loss: \u001b[1m\u001b[32m1.13206\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3239 | loss: 1.13206 - acc: 0.7780 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3240  | total loss: \u001b[1m\u001b[32m1.37499\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3240 | loss: 1.37499 - acc: 0.7145 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3241  | total loss: \u001b[1m\u001b[32m1.26399\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3241 | loss: 1.26399 - acc: 0.7431 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3242  | total loss: \u001b[1m\u001b[32m1.48223\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3242 | loss: 1.48223 - acc: 0.6902 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3243  | total loss: \u001b[1m\u001b[32m1.36143\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3243 | loss: 1.36143 - acc: 0.7212 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3244  | total loss: \u001b[1m\u001b[32m1.25320\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3244 | loss: 1.25320 - acc: 0.7491 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3245  | total loss: \u001b[1m\u001b[32m1.15593\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3245 | loss: 1.15593 - acc: 0.7741 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3246  | total loss: \u001b[1m\u001b[32m1.39832\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3246 | loss: 1.39832 - acc: 0.7110 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3247  | total loss: \u001b[1m\u001b[32m1.28709\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3247 | loss: 1.28709 - acc: 0.7399 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3248  | total loss: \u001b[1m\u001b[32m1.18733\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3248 | loss: 1.18733 - acc: 0.7659 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3249  | total loss: \u001b[1m\u001b[32m1.09754\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3249 | loss: 1.09754 - acc: 0.7893 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3250  | total loss: \u001b[1m\u001b[32m1.33769\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3250 | loss: 1.33769 - acc: 0.7247 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3251  | total loss: \u001b[1m\u001b[32m1.23312\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3251 | loss: 1.23312 - acc: 0.7522 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3252  | total loss: \u001b[1m\u001b[32m1.54314\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3252 | loss: 1.54314 - acc: 0.6770 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3253  | total loss: \u001b[1m\u001b[32m1.41955\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3253 | loss: 1.41955 - acc: 0.7093 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3254  | total loss: \u001b[1m\u001b[32m1.30918\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3254 | loss: 1.30918 - acc: 0.7384 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3255  | total loss: \u001b[1m\u001b[32m1.21025\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3255 | loss: 1.21025 - acc: 0.7645 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3256  | total loss: \u001b[1m\u001b[32m1.12118\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3256 | loss: 1.12118 - acc: 0.7881 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3257  | total loss: \u001b[1m\u001b[32m1.04062\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3257 | loss: 1.04062 - acc: 0.8093 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3258  | total loss: \u001b[1m\u001b[32m1.32682\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3258 | loss: 1.32682 - acc: 0.7355 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3259  | total loss: \u001b[1m\u001b[32m1.22535\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3259 | loss: 1.22535 - acc: 0.7619 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3260  | total loss: \u001b[1m\u001b[32m1.49993\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3260 | loss: 1.49993 - acc: 0.6857 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3261  | total loss: \u001b[1m\u001b[32m1.38219\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3261 | loss: 1.38219 - acc: 0.7172 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3262  | total loss: \u001b[1m\u001b[32m1.61496\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3262 | loss: 1.61496 - acc: 0.6526 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3263  | total loss: \u001b[1m\u001b[32m1.48786\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3263 | loss: 1.48786 - acc: 0.6873 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3264  | total loss: \u001b[1m\u001b[32m1.71597\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3264 | loss: 1.71597 - acc: 0.6186 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3265  | total loss: \u001b[1m\u001b[32m1.58194\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3265 | loss: 1.58194 - acc: 0.6567 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3266  | total loss: \u001b[1m\u001b[32m1.82272\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3266 | loss: 1.82272 - acc: 0.5911 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3267  | total loss: \u001b[1m\u001b[32m1.68212\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3267 | loss: 1.68212 - acc: 0.6320 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3268  | total loss: \u001b[1m\u001b[32m1.76233\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3268 | loss: 1.76233 - acc: 0.6045 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3269  | total loss: \u001b[1m\u001b[32m1.63174\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3269 | loss: 1.63174 - acc: 0.6440 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3270  | total loss: \u001b[1m\u001b[32m1.80688\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3270 | loss: 1.80688 - acc: 0.5939 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3271  | total loss: \u001b[1m\u001b[32m1.67534\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3271 | loss: 1.67534 - acc: 0.6345 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3272  | total loss: \u001b[1m\u001b[32m1.55827\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3272 | loss: 1.55827 - acc: 0.6711 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3273  | total loss: \u001b[1m\u001b[32m1.45343\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3273 | loss: 1.45343 - acc: 0.7040 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3274  | total loss: \u001b[1m\u001b[32m1.64505\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3274 | loss: 1.64505 - acc: 0.6407 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3275  | total loss: \u001b[1m\u001b[32m1.53213\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3275 | loss: 1.53213 - acc: 0.6766 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3276  | total loss: \u001b[1m\u001b[32m1.43055\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3276 | loss: 1.43055 - acc: 0.7090 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3277  | total loss: \u001b[1m\u001b[32m1.33847\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3277 | loss: 1.33847 - acc: 0.7381 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3278  | total loss: \u001b[1m\u001b[32m1.25435\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3278 | loss: 1.25435 - acc: 0.7643 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3279  | total loss: \u001b[1m\u001b[32m1.17687\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3279 | loss: 1.17687 - acc: 0.7878 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3280  | total loss: \u001b[1m\u001b[32m1.10495\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3280 | loss: 1.10495 - acc: 0.8091 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3281  | total loss: \u001b[1m\u001b[32m1.03772\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3281 | loss: 1.03772 - acc: 0.8282 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3282  | total loss: \u001b[1m\u001b[32m1.31147\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3282 | loss: 1.31147 - acc: 0.7453 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3283  | total loss: \u001b[1m\u001b[32m1.21955\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3283 | loss: 1.21955 - acc: 0.7708 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3284  | total loss: \u001b[1m\u001b[32m1.13517\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3284 | loss: 1.13517 - acc: 0.7937 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3285  | total loss: \u001b[1m\u001b[32m1.05735\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3285 | loss: 1.05735 - acc: 0.8144 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3286  | total loss: \u001b[1m\u001b[32m1.36561\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3286 | loss: 1.36561 - acc: 0.7329 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3287  | total loss: \u001b[1m\u001b[32m1.26192\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3287 | loss: 1.26192 - acc: 0.7596 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3288  | total loss: \u001b[1m\u001b[32m1.49179\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3288 | loss: 1.49179 - acc: 0.6979 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3289  | total loss: \u001b[1m\u001b[32m1.37434\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3289 | loss: 1.37434 - acc: 0.7282 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3290  | total loss: \u001b[1m\u001b[32m1.26818\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3290 | loss: 1.26818 - acc: 0.7553 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3291  | total loss: \u001b[1m\u001b[32m1.17189\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3291 | loss: 1.17189 - acc: 0.7798 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3292  | total loss: \u001b[1m\u001b[32m1.44957\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3292 | loss: 1.44957 - acc: 0.7018 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3293  | total loss: \u001b[1m\u001b[32m1.33424\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3293 | loss: 1.33424 - acc: 0.7316 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3294  | total loss: \u001b[1m\u001b[32m1.58240\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3294 | loss: 1.58240 - acc: 0.6656 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3295  | total loss: \u001b[1m\u001b[32m1.45431\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3295 | loss: 1.45431 - acc: 0.6991 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3296  | total loss: \u001b[1m\u001b[32m1.72787\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3296 | loss: 1.72787 - acc: 0.6292 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3297  | total loss: \u001b[1m\u001b[32m1.58705\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3297 | loss: 1.58705 - acc: 0.6662 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3298  | total loss: \u001b[1m\u001b[32m1.82674\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3298 | loss: 1.82674 - acc: 0.5996 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3299  | total loss: \u001b[1m\u001b[32m1.67905\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3299 | loss: 1.67905 - acc: 0.6397 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3300  | total loss: \u001b[1m\u001b[32m1.87528\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3300 | loss: 1.87528 - acc: 0.5757 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3301  | total loss: \u001b[1m\u001b[32m1.72668\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3301 | loss: 1.72668 - acc: 0.6181 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3302  | total loss: \u001b[1m\u001b[32m1.93242\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3302 | loss: 1.93242 - acc: 0.5563 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3303  | total loss: \u001b[1m\u001b[32m1.78271\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3303 | loss: 1.78271 - acc: 0.6007 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3304  | total loss: \u001b[1m\u001b[32m1.65001\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3304 | loss: 1.65001 - acc: 0.6406 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3305  | total loss: \u001b[1m\u001b[32m1.53185\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3305 | loss: 1.53185 - acc: 0.6765 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3306  | total loss: \u001b[1m\u001b[32m1.73181\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3306 | loss: 1.73181 - acc: 0.6089 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3307  | total loss: \u001b[1m\u001b[32m1.60758\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3307 | loss: 1.60758 - acc: 0.6480 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3308  | total loss: \u001b[1m\u001b[32m1.74768\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3308 | loss: 1.74768 - acc: 0.6046 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3309  | total loss: \u001b[1m\u001b[32m1.62408\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3309 | loss: 1.62408 - acc: 0.6442 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3310  | total loss: \u001b[1m\u001b[32m1.77702\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3310 | loss: 1.77702 - acc: 0.5940 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3311  | total loss: \u001b[1m\u001b[32m1.65253\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3311 | loss: 1.65253 - acc: 0.6346 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3312  | total loss: \u001b[1m\u001b[32m1.54106\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3312 | loss: 1.54106 - acc: 0.6712 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3313  | total loss: \u001b[1m\u001b[32m1.44050\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3313 | loss: 1.44050 - acc: 0.7041 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3314  | total loss: \u001b[1m\u001b[32m1.58802\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3314 | loss: 1.58802 - acc: 0.6551 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3315  | total loss: \u001b[1m\u001b[32m1.48174\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3315 | loss: 1.48174 - acc: 0.6896 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3316  | total loss: \u001b[1m\u001b[32m1.62689\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3316 | loss: 1.62689 - acc: 0.6349 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3317  | total loss: \u001b[1m\u001b[32m1.51597\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3317 | loss: 1.51597 - acc: 0.6714 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3318  | total loss: \u001b[1m\u001b[32m1.41548\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3318 | loss: 1.41548 - acc: 0.7043 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3319  | total loss: \u001b[1m\u001b[32m1.32379\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3319 | loss: 1.32379 - acc: 0.7338 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3320  | total loss: \u001b[1m\u001b[32m1.54856\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3320 | loss: 1.54856 - acc: 0.6605 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3321  | total loss: \u001b[1m\u001b[32m1.44127\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3321 | loss: 1.44127 - acc: 0.6944 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3322  | total loss: \u001b[1m\u001b[32m1.34362\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3322 | loss: 1.34362 - acc: 0.7250 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3323  | total loss: \u001b[1m\u001b[32m1.25417\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3323 | loss: 1.25417 - acc: 0.7525 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3324  | total loss: \u001b[1m\u001b[32m1.51984\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3324 | loss: 1.51984 - acc: 0.6772 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3325  | total loss: \u001b[1m\u001b[32m1.41022\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3325 | loss: 1.41022 - acc: 0.7095 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3326  | total loss: \u001b[1m\u001b[32m1.31048\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3326 | loss: 1.31048 - acc: 0.7386 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3327  | total loss: \u001b[1m\u001b[32m1.21927\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3327 | loss: 1.21927 - acc: 0.7647 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3328  | total loss: \u001b[1m\u001b[32m1.13541\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3328 | loss: 1.13541 - acc: 0.7882 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3329  | total loss: \u001b[1m\u001b[32m1.05793\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3329 | loss: 1.05793 - acc: 0.8094 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3330  | total loss: \u001b[1m\u001b[32m1.28800\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3330 | loss: 1.28800 - acc: 0.7427 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3331  | total loss: \u001b[1m\u001b[32m1.19202\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3331 | loss: 1.19202 - acc: 0.7685 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3332  | total loss: \u001b[1m\u001b[32m1.44833\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3332 | loss: 1.44833 - acc: 0.6988 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3333  | total loss: \u001b[1m\u001b[32m1.33478\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3333 | loss: 1.33478 - acc: 0.7289 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3334  | total loss: \u001b[1m\u001b[32m1.56193\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3334 | loss: 1.56193 - acc: 0.6703 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3335  | total loss: \u001b[1m\u001b[32m1.43688\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3335 | loss: 1.43688 - acc: 0.7033 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3336  | total loss: \u001b[1m\u001b[32m1.68184\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3336 | loss: 1.68184 - acc: 0.6401 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3337  | total loss: \u001b[1m\u001b[32m1.54585\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3337 | loss: 1.54585 - acc: 0.6761 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3338  | total loss: \u001b[1m\u001b[32m1.42405\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3338 | loss: 1.42405 - acc: 0.7085 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3339  | total loss: \u001b[1m\u001b[32m1.31460\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3339 | loss: 1.31460 - acc: 0.7376 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3340  | total loss: \u001b[1m\u001b[32m1.55147\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3340 | loss: 1.55147 - acc: 0.6710 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3341  | total loss: \u001b[1m\u001b[32m1.42977\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3341 | loss: 1.42977 - acc: 0.7039 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3342  | total loss: \u001b[1m\u001b[32m1.66640\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3342 | loss: 1.66640 - acc: 0.6407 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3343  | total loss: \u001b[1m\u001b[32m1.53469\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3343 | loss: 1.53469 - acc: 0.6766 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3344  | total loss: \u001b[1m\u001b[32m1.41686\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3344 | loss: 1.41686 - acc: 0.7089 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3345  | total loss: \u001b[1m\u001b[32m1.31103\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3345 | loss: 1.31103 - acc: 0.7380 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3346  | total loss: \u001b[1m\u001b[32m1.21555\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3346 | loss: 1.21555 - acc: 0.7642 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3347  | total loss: \u001b[1m\u001b[32m1.12899\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3347 | loss: 1.12899 - acc: 0.7878 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3348  | total loss: \u001b[1m\u001b[32m1.05011\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3348 | loss: 1.05011 - acc: 0.8090 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3349  | total loss: \u001b[1m\u001b[32m0.97786\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3349 | loss: 0.97786 - acc: 0.8281 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3350  | total loss: \u001b[1m\u001b[32m1.25205\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3350 | loss: 1.25205 - acc: 0.7525 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3351  | total loss: \u001b[1m\u001b[32m1.15772\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3351 | loss: 1.15772 - acc: 0.7772 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3352  | total loss: \u001b[1m\u001b[32m1.45492\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3352 | loss: 1.45492 - acc: 0.6995 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3353  | total loss: \u001b[1m\u001b[32m1.34012\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3353 | loss: 1.34012 - acc: 0.7295 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3354  | total loss: \u001b[1m\u001b[32m1.23694\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3354 | loss: 1.23694 - acc: 0.7566 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3355  | total loss: \u001b[1m\u001b[32m1.14386\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3355 | loss: 1.14386 - acc: 0.7809 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3356  | total loss: \u001b[1m\u001b[32m1.41091\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3356 | loss: 1.41091 - acc: 0.7100 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3357  | total loss: \u001b[1m\u001b[32m1.30037\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3357 | loss: 1.30037 - acc: 0.7390 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3358  | total loss: \u001b[1m\u001b[32m1.20100\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3358 | loss: 1.20100 - acc: 0.7651 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3359  | total loss: \u001b[1m\u001b[32m1.11131\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3359 | loss: 1.11131 - acc: 0.7886 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3360  | total loss: \u001b[1m\u001b[32m1.35068\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3360 | loss: 1.35068 - acc: 0.7240 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3361  | total loss: \u001b[1m\u001b[32m1.24592\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3361 | loss: 1.24592 - acc: 0.7516 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3362  | total loss: \u001b[1m\u001b[32m1.50049\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3362 | loss: 1.50049 - acc: 0.6764 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3363  | total loss: \u001b[1m\u001b[32m1.38211\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3363 | loss: 1.38211 - acc: 0.7088 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3364  | total loss: \u001b[1m\u001b[32m1.27637\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3364 | loss: 1.27637 - acc: 0.7379 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3365  | total loss: \u001b[1m\u001b[32m1.18155\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3365 | loss: 1.18155 - acc: 0.7641 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3366  | total loss: \u001b[1m\u001b[32m1.40561\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3366 | loss: 1.40561 - acc: 0.7020 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3367  | total loss: \u001b[1m\u001b[32m1.29861\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3367 | loss: 1.29861 - acc: 0.7318 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3368  | total loss: \u001b[1m\u001b[32m1.57727\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3368 | loss: 1.57727 - acc: 0.6586 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3369  | total loss: \u001b[1m\u001b[32m1.45484\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3369 | loss: 1.45484 - acc: 0.6928 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3370  | total loss: \u001b[1m\u001b[32m1.34548\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3370 | loss: 1.34548 - acc: 0.7235 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3371  | total loss: \u001b[1m\u001b[32m1.24736\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3371 | loss: 1.24736 - acc: 0.7511 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3372  | total loss: \u001b[1m\u001b[32m1.50071\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3372 | loss: 1.50071 - acc: 0.6760 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3373  | total loss: \u001b[1m\u001b[32m1.38788\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3373 | loss: 1.38788 - acc: 0.7084 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3374  | total loss: \u001b[1m\u001b[32m1.28678\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3374 | loss: 1.28678 - acc: 0.7376 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3375  | total loss: \u001b[1m\u001b[32m1.19569\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3375 | loss: 1.19569 - acc: 0.7638 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3376  | total loss: \u001b[1m\u001b[32m1.44157\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3376 | loss: 1.44157 - acc: 0.6874 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3377  | total loss: \u001b[1m\u001b[32m1.33507\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3377 | loss: 1.33507 - acc: 0.7187 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3378  | total loss: \u001b[1m\u001b[32m1.54106\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3378 | loss: 1.54106 - acc: 0.6611 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3379  | total loss: \u001b[1m\u001b[32m1.42571\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3379 | loss: 1.42571 - acc: 0.6950 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3380  | total loss: \u001b[1m\u001b[32m1.64100\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3380 | loss: 1.64100 - acc: 0.6326 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3381  | total loss: \u001b[1m\u001b[32m1.51754\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3381 | loss: 1.51754 - acc: 0.6694 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3382  | total loss: \u001b[1m\u001b[32m1.73329\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3382 | loss: 1.73329 - acc: 0.6096 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3383  | total loss: \u001b[1m\u001b[32m1.60323\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3383 | loss: 1.60323 - acc: 0.6486 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3384  | total loss: \u001b[1m\u001b[32m1.81253\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3384 | loss: 1.81253 - acc: 0.5838 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3385  | total loss: \u001b[1m\u001b[32m1.67770\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3385 | loss: 1.67770 - acc: 0.6254 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3386  | total loss: \u001b[1m\u001b[32m1.86759\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3386 | loss: 1.86759 - acc: 0.5628 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3387  | total loss: \u001b[1m\u001b[32m1.73082\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3387 | loss: 1.73082 - acc: 0.6066 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3388  | total loss: \u001b[1m\u001b[32m1.91607\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3388 | loss: 1.91607 - acc: 0.5530 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3389  | total loss: \u001b[1m\u001b[32m1.77804\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3389 | loss: 1.77804 - acc: 0.5977 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3390  | total loss: \u001b[1m\u001b[32m1.65503\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3390 | loss: 1.65503 - acc: 0.6380 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3391  | total loss: \u001b[1m\u001b[32m1.54461\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3391 | loss: 1.54461 - acc: 0.6742 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3392  | total loss: \u001b[1m\u001b[32m1.72975\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3392 | loss: 1.72975 - acc: 0.6139 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3393  | total loss: \u001b[1m\u001b[32m1.61174\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3393 | loss: 1.61174 - acc: 0.6525 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3394  | total loss: \u001b[1m\u001b[32m1.80148\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3394 | loss: 1.80148 - acc: 0.5944 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3395  | total loss: \u001b[1m\u001b[32m1.67656\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3395 | loss: 1.67656 - acc: 0.6350 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3396  | total loss: \u001b[1m\u001b[32m1.88707\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3396 | loss: 1.88707 - acc: 0.5715 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3397  | total loss: \u001b[1m\u001b[32m1.75426\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3397 | loss: 1.75426 - acc: 0.6143 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3398  | total loss: \u001b[1m\u001b[32m1.63475\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3398 | loss: 1.63475 - acc: 0.6529 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3399  | total loss: \u001b[1m\u001b[32m1.52646\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3399 | loss: 1.52646 - acc: 0.6876 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3400  | total loss: \u001b[1m\u001b[32m1.67032\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3400 | loss: 1.67032 - acc: 0.6331 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3401  | total loss: \u001b[1m\u001b[32m1.55664\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3401 | loss: 1.55664 - acc: 0.6698 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3402  | total loss: \u001b[1m\u001b[32m1.69100\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3402 | loss: 1.69100 - acc: 0.6243 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3403  | total loss: \u001b[1m\u001b[32m1.57386\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3403 | loss: 1.57386 - acc: 0.6618 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3404  | total loss: \u001b[1m\u001b[32m1.75189\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3404 | loss: 1.75189 - acc: 0.6028 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3405  | total loss: \u001b[1m\u001b[32m1.62798\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3405 | loss: 1.62798 - acc: 0.6425 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3406  | total loss: \u001b[1m\u001b[32m1.51601\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3406 | loss: 1.51601 - acc: 0.6783 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3407  | total loss: \u001b[1m\u001b[32m1.41420\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3407 | loss: 1.41420 - acc: 0.7104 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3408  | total loss: \u001b[1m\u001b[32m1.63779\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3408 | loss: 1.63779 - acc: 0.6394 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3409  | total loss: \u001b[1m\u001b[32m1.52184\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3409 | loss: 1.52184 - acc: 0.6755 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3410  | total loss: \u001b[1m\u001b[32m1.74168\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3410 | loss: 1.74168 - acc: 0.6079 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3411  | total loss: \u001b[1m\u001b[32m1.61462\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3411 | loss: 1.61462 - acc: 0.6471 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3412  | total loss: \u001b[1m\u001b[32m1.78504\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3412 | loss: 1.78504 - acc: 0.5895 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3413  | total loss: \u001b[1m\u001b[32m1.65382\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3413 | loss: 1.65382 - acc: 0.6306 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3414  | total loss: \u001b[1m\u001b[32m1.83222\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3414 | loss: 1.83222 - acc: 0.5747 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3415  | total loss: \u001b[1m\u001b[32m1.69709\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3415 | loss: 1.69709 - acc: 0.6172 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3416  | total loss: \u001b[1m\u001b[32m1.57570\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3416 | loss: 1.57570 - acc: 0.6555 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3417  | total loss: \u001b[1m\u001b[32m1.46605\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3417 | loss: 1.46605 - acc: 0.6899 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3418  | total loss: \u001b[1m\u001b[32m1.64543\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3418 | loss: 1.64543 - acc: 0.6352 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3419  | total loss: \u001b[1m\u001b[32m1.52786\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3419 | loss: 1.52786 - acc: 0.6717 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3420  | total loss: \u001b[1m\u001b[32m1.70801\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3420 | loss: 1.70801 - acc: 0.6117 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3421  | total loss: \u001b[1m\u001b[32m1.58404\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3421 | loss: 1.58404 - acc: 0.6505 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3422  | total loss: \u001b[1m\u001b[32m1.47227\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3422 | loss: 1.47227 - acc: 0.6855 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3423  | total loss: \u001b[1m\u001b[32m1.37091\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3423 | loss: 1.37091 - acc: 0.7169 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3424  | total loss: \u001b[1m\u001b[32m1.62136\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3424 | loss: 1.62136 - acc: 0.6452 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3425  | total loss: \u001b[1m\u001b[32m1.50381\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3425 | loss: 1.50381 - acc: 0.6807 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3426  | total loss: \u001b[1m\u001b[32m1.70067\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3426 | loss: 1.70067 - acc: 0.6198 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3427  | total loss: \u001b[1m\u001b[32m1.57504\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3427 | loss: 1.57504 - acc: 0.6578 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3428  | total loss: \u001b[1m\u001b[32m1.80027\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3428 | loss: 1.80027 - acc: 0.5920 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3429  | total loss: \u001b[1m\u001b[32m1.66556\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3429 | loss: 1.66556 - acc: 0.6328 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3430  | total loss: \u001b[1m\u001b[32m1.79999\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3430 | loss: 1.79999 - acc: 0.5838 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3431  | total loss: \u001b[1m\u001b[32m1.66692\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3431 | loss: 1.66692 - acc: 0.6254 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3432  | total loss: \u001b[1m\u001b[32m1.84383\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3432 | loss: 1.84383 - acc: 0.5700 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3433  | total loss: \u001b[1m\u001b[32m1.70830\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3433 | loss: 1.70830 - acc: 0.6130 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3434  | total loss: \u001b[1m\u001b[32m1.85516\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3434 | loss: 1.85516 - acc: 0.5660 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3435  | total loss: \u001b[1m\u001b[32m1.72062\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3435 | loss: 1.72062 - acc: 0.6094 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3436  | total loss: \u001b[1m\u001b[32m1.88786\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3436 | loss: 1.88786 - acc: 0.5556 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3437  | total loss: \u001b[1m\u001b[32m1.75228\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3437 | loss: 1.75228 - acc: 0.6001 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3438  | total loss: \u001b[1m\u001b[32m1.93277\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3438 | loss: 1.93277 - acc: 0.5400 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3439  | total loss: \u001b[1m\u001b[32m1.79512\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3439 | loss: 1.79512 - acc: 0.5860 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3440  | total loss: \u001b[1m\u001b[32m1.92823\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3440 | loss: 1.92823 - acc: 0.5417 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3441  | total loss: \u001b[1m\u001b[32m1.79339\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3441 | loss: 1.79339 - acc: 0.5876 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3442  | total loss: \u001b[1m\u001b[32m1.67267\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3442 | loss: 1.67267 - acc: 0.6288 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3443  | total loss: \u001b[1m\u001b[32m1.56382\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3443 | loss: 1.56382 - acc: 0.6659 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3444  | total loss: \u001b[1m\u001b[32m1.74198\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3444 | loss: 1.74198 - acc: 0.6065 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3445  | total loss: \u001b[1m\u001b[32m1.62527\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3445 | loss: 1.62527 - acc: 0.6458 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3446  | total loss: \u001b[1m\u001b[32m1.82596\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3446 | loss: 1.82596 - acc: 0.5812 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3447  | total loss: \u001b[1m\u001b[32m1.70058\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3447 | loss: 1.70058 - acc: 0.6231 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3448  | total loss: \u001b[1m\u001b[32m1.86513\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3448 | loss: 1.86513 - acc: 0.5679 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3449  | total loss: \u001b[1m\u001b[32m1.73612\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3449 | loss: 1.73612 - acc: 0.6112 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3450  | total loss: \u001b[1m\u001b[32m1.89728\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3450 | loss: 1.89728 - acc: 0.5500 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3451  | total loss: \u001b[1m\u001b[32m1.76564\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3451 | loss: 1.76564 - acc: 0.5950 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3452  | total loss: \u001b[1m\u001b[32m1.84826\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3452 | loss: 1.84826 - acc: 0.5641 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3453  | total loss: \u001b[1m\u001b[32m1.72200\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3453 | loss: 1.72200 - acc: 0.6077 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3454  | total loss: \u001b[1m\u001b[32m1.88881\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3454 | loss: 1.88881 - acc: 0.5541 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3455  | total loss: \u001b[1m\u001b[32m1.75884\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3455 | loss: 1.75884 - acc: 0.5987 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3456  | total loss: \u001b[1m\u001b[32m1.90591\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3456 | loss: 1.90591 - acc: 0.5459 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3457  | total loss: \u001b[1m\u001b[32m1.77476\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3457 | loss: 1.77476 - acc: 0.5913 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3458  | total loss: \u001b[1m\u001b[32m1.93132\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3458 | loss: 1.93132 - acc: 0.5394 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3459  | total loss: \u001b[1m\u001b[32m1.79836\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3459 | loss: 1.79836 - acc: 0.5854 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3460  | total loss: \u001b[1m\u001b[32m1.94070\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3460 | loss: 1.94070 - acc: 0.5340 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3461  | total loss: \u001b[1m\u001b[32m1.80762\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3461 | loss: 1.80762 - acc: 0.5806 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3462  | total loss: \u001b[1m\u001b[32m1.91776\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3462 | loss: 1.91776 - acc: 0.5440 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3463  | total loss: \u001b[1m\u001b[32m1.78764\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3463 | loss: 1.78764 - acc: 0.5896 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3464  | total loss: \u001b[1m\u001b[32m1.91680\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3464 | loss: 1.91680 - acc: 0.5449 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3465  | total loss: \u001b[1m\u001b[32m1.78732\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3465 | loss: 1.78732 - acc: 0.5904 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3466  | total loss: \u001b[1m\u001b[32m1.96096\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3466 | loss: 1.96096 - acc: 0.5314 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3467  | total loss: \u001b[1m\u001b[32m1.82764\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3467 | loss: 1.82764 - acc: 0.5782 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3468  | total loss: \u001b[1m\u001b[32m1.94770\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3468 | loss: 1.94770 - acc: 0.5347 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3469  | total loss: \u001b[1m\u001b[32m1.81626\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3469 | loss: 1.81626 - acc: 0.5812 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3470  | total loss: \u001b[1m\u001b[32m1.93456\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3470 | loss: 1.93456 - acc: 0.5374 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3471  | total loss: \u001b[1m\u001b[32m1.80471\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3471 | loss: 1.80471 - acc: 0.5837 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3472  | total loss: \u001b[1m\u001b[32m1.98159\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3472 | loss: 1.98159 - acc: 0.5253 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3473  | total loss: \u001b[1m\u001b[32m1.84741\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3473 | loss: 1.84741 - acc: 0.5728 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3474  | total loss: \u001b[1m\u001b[32m1.99683\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3474 | loss: 1.99683 - acc: 0.5155 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3475  | total loss: \u001b[1m\u001b[32m1.86180\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3475 | loss: 1.86180 - acc: 0.5639 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3476  | total loss: \u001b[1m\u001b[32m1.93756\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3476 | loss: 1.93756 - acc: 0.5290 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3477  | total loss: \u001b[1m\u001b[32m1.80876\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3477 | loss: 1.80876 - acc: 0.5761 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3478  | total loss: \u001b[1m\u001b[32m1.92909\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3478 | loss: 1.92909 - acc: 0.5328 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3479  | total loss: \u001b[1m\u001b[32m1.80094\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3479 | loss: 1.80094 - acc: 0.5795 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3480  | total loss: \u001b[1m\u001b[32m1.68505\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3480 | loss: 1.68505 - acc: 0.6215 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3481  | total loss: \u001b[1m\u001b[32m1.57944\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3481 | loss: 1.57944 - acc: 0.6594 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3482  | total loss: \u001b[1m\u001b[32m1.77389\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3482 | loss: 1.77389 - acc: 0.5934 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3483  | total loss: \u001b[1m\u001b[32m1.65684\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3483 | loss: 1.65684 - acc: 0.6341 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3484  | total loss: \u001b[1m\u001b[32m1.79125\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3484 | loss: 1.79125 - acc: 0.5778 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3485  | total loss: \u001b[1m\u001b[32m1.67078\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3485 | loss: 1.67078 - acc: 0.6200 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3486  | total loss: \u001b[1m\u001b[32m1.82419\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3486 | loss: 1.82419 - acc: 0.5723 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3487  | total loss: \u001b[1m\u001b[32m1.69911\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3487 | loss: 1.69911 - acc: 0.6151 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3488  | total loss: \u001b[1m\u001b[32m1.58561\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3488 | loss: 1.58561 - acc: 0.6536 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3489  | total loss: \u001b[1m\u001b[32m1.48194\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3489 | loss: 1.48194 - acc: 0.6882 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3490  | total loss: \u001b[1m\u001b[32m1.68711\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3490 | loss: 1.68711 - acc: 0.6194 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3491  | total loss: \u001b[1m\u001b[32m1.57060\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3491 | loss: 1.57060 - acc: 0.6575 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3492  | total loss: \u001b[1m\u001b[32m1.46452\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3492 | loss: 1.46452 - acc: 0.6917 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3493  | total loss: \u001b[1m\u001b[32m1.36734\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3493 | loss: 1.36734 - acc: 0.7225 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3494  | total loss: \u001b[1m\u001b[32m1.55851\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3494 | loss: 1.55851 - acc: 0.6503 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3495  | total loss: \u001b[1m\u001b[32m1.44900\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3495 | loss: 1.44900 - acc: 0.6853 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3496  | total loss: \u001b[1m\u001b[32m1.34913\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3496 | loss: 1.34913 - acc: 0.7167 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3497  | total loss: \u001b[1m\u001b[32m1.25755\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3497 | loss: 1.25755 - acc: 0.7451 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3498  | total loss: \u001b[1m\u001b[32m1.46053\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3498 | loss: 1.46053 - acc: 0.6920 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3499  | total loss: \u001b[1m\u001b[32m1.35480\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3499 | loss: 1.35480 - acc: 0.7228 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3500  | total loss: \u001b[1m\u001b[32m1.58225\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3500 | loss: 1.58225 - acc: 0.6577 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3501  | total loss: \u001b[1m\u001b[32m1.46277\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3501 | loss: 1.46277 - acc: 0.6919 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3502  | total loss: \u001b[1m\u001b[32m1.69369\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3502 | loss: 1.69369 - acc: 0.6298 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3503  | total loss: \u001b[1m\u001b[32m1.56292\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3503 | loss: 1.56292 - acc: 0.6669 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3504  | total loss: \u001b[1m\u001b[32m1.73338\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3504 | loss: 1.73338 - acc: 0.6073 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3505  | total loss: \u001b[1m\u001b[32m1.59963\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3505 | loss: 1.59963 - acc: 0.6466 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3506  | total loss: \u001b[1m\u001b[32m1.80947\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3506 | loss: 1.80947 - acc: 0.5891 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3507  | total loss: \u001b[1m\u001b[32m1.67004\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3507 | loss: 1.67004 - acc: 0.6302 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3508  | total loss: \u001b[1m\u001b[32m1.54543\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3508 | loss: 1.54543 - acc: 0.6671 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3509  | total loss: \u001b[1m\u001b[32m1.43355\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3509 | loss: 1.43355 - acc: 0.7004 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3510  | total loss: \u001b[1m\u001b[32m1.33257\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3510 | loss: 1.33257 - acc: 0.7304 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3511  | total loss: \u001b[1m\u001b[32m1.24087\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3511 | loss: 1.24087 - acc: 0.7573 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3512  | total loss: \u001b[1m\u001b[32m1.15711\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3512 | loss: 1.15711 - acc: 0.7816 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3513  | total loss: \u001b[1m\u001b[32m1.08011\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3513 | loss: 1.08011 - acc: 0.8035 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3514  | total loss: \u001b[1m\u001b[32m1.32589\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3514 | loss: 1.32589 - acc: 0.7231 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3515  | total loss: \u001b[1m\u001b[32m1.22939\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3515 | loss: 1.22939 - acc: 0.7508 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3516  | total loss: \u001b[1m\u001b[32m1.51082\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3516 | loss: 1.51082 - acc: 0.6757 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3517  | total loss: \u001b[1m\u001b[32m1.39490\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3517 | loss: 1.39490 - acc: 0.7081 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3518  | total loss: \u001b[1m\u001b[32m1.65757\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3518 | loss: 1.65757 - acc: 0.6445 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3519  | total loss: \u001b[1m\u001b[32m1.52744\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3519 | loss: 1.52744 - acc: 0.6800 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3520  | total loss: \u001b[1m\u001b[32m1.41060\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3520 | loss: 1.41060 - acc: 0.7120 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3521  | total loss: \u001b[1m\u001b[32m1.30525\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3521 | loss: 1.30525 - acc: 0.7408 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3522  | total loss: \u001b[1m\u001b[32m1.49951\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3522 | loss: 1.49951 - acc: 0.6810 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3523  | total loss: \u001b[1m\u001b[32m1.38507\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3523 | loss: 1.38507 - acc: 0.7129 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3524  | total loss: \u001b[1m\u001b[32m1.28203\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3524 | loss: 1.28203 - acc: 0.7416 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3525  | total loss: \u001b[1m\u001b[32m1.18883\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3525 | loss: 1.18883 - acc: 0.7675 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3526  | total loss: \u001b[1m\u001b[32m1.40700\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3526 | loss: 1.40700 - acc: 0.7050 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3527  | total loss: \u001b[1m\u001b[32m1.30066\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3527 | loss: 1.30066 - acc: 0.7345 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3528  | total loss: \u001b[1m\u001b[32m1.20474\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3528 | loss: 1.20474 - acc: 0.7611 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3529  | total loss: \u001b[1m\u001b[32m1.11781\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3529 | loss: 1.11781 - acc: 0.7849 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3530  | total loss: \u001b[1m\u001b[32m1.36108\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3530 | loss: 1.36108 - acc: 0.7207 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3531  | total loss: \u001b[1m\u001b[32m1.25767\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3531 | loss: 1.25767 - acc: 0.7487 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3532  | total loss: \u001b[1m\u001b[32m1.47070\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3532 | loss: 1.47070 - acc: 0.6881 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3533  | total loss: \u001b[1m\u001b[32m1.35664\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3533 | loss: 1.35664 - acc: 0.7193 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3534  | total loss: \u001b[1m\u001b[32m1.25417\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3534 | loss: 1.25417 - acc: 0.7473 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3535  | total loss: \u001b[1m\u001b[32m1.16171\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3535 | loss: 1.16171 - acc: 0.7726 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3536  | total loss: \u001b[1m\u001b[32m1.07790\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3536 | loss: 1.07790 - acc: 0.7954 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3537  | total loss: \u001b[1m\u001b[32m1.00153\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3537 | loss: 1.00153 - acc: 0.8158 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3538  | total loss: \u001b[1m\u001b[32m1.28319\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3538 | loss: 1.28319 - acc: 0.7414 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3539  | total loss: \u001b[1m\u001b[32m1.18505\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3539 | loss: 1.18505 - acc: 0.7672 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3540  | total loss: \u001b[1m\u001b[32m1.46330\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3540 | loss: 1.46330 - acc: 0.6977 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3541  | total loss: \u001b[1m\u001b[32m1.34738\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3541 | loss: 1.34738 - acc: 0.7279 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3542  | total loss: \u001b[1m\u001b[32m1.58560\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3542 | loss: 1.58560 - acc: 0.6622 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3543  | total loss: \u001b[1m\u001b[32m1.45884\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3543 | loss: 1.45884 - acc: 0.6960 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3544  | total loss: \u001b[1m\u001b[32m1.69848\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3544 | loss: 1.69848 - acc: 0.6264 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3545  | total loss: \u001b[1m\u001b[32m1.56297\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3545 | loss: 1.56297 - acc: 0.6638 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3546  | total loss: \u001b[1m\u001b[32m1.44226\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3546 | loss: 1.44226 - acc: 0.6974 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3547  | total loss: \u001b[1m\u001b[32m1.33433\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3547 | loss: 1.33433 - acc: 0.7277 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3548  | total loss: \u001b[1m\u001b[32m1.23737\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3548 | loss: 1.23737 - acc: 0.7549 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3549  | total loss: \u001b[1m\u001b[32m1.14981\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3549 | loss: 1.14981 - acc: 0.7794 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3550  | total loss: \u001b[1m\u001b[32m1.43661\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3550 | loss: 1.43661 - acc: 0.7015 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3551  | total loss: \u001b[1m\u001b[32m1.32889\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3551 | loss: 1.32889 - acc: 0.7313 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3552  | total loss: \u001b[1m\u001b[32m1.59002\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3552 | loss: 1.59002 - acc: 0.6582 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3553  | total loss: \u001b[1m\u001b[32m1.46814\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3553 | loss: 1.46814 - acc: 0.6924 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3554  | total loss: \u001b[1m\u001b[32m1.72067\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3554 | loss: 1.72067 - acc: 0.6231 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3555  | total loss: \u001b[1m\u001b[32m1.58812\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3555 | loss: 1.58812 - acc: 0.6608 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3556  | total loss: \u001b[1m\u001b[32m1.74371\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3556 | loss: 1.74371 - acc: 0.6090 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3557  | total loss: \u001b[1m\u001b[32m1.61194\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3557 | loss: 1.61194 - acc: 0.6481 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3558  | total loss: \u001b[1m\u001b[32m1.78857\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3558 | loss: 1.78857 - acc: 0.5905 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3559  | total loss: \u001b[1m\u001b[32m1.65570\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3559 | loss: 1.65570 - acc: 0.6314 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3560  | total loss: \u001b[1m\u001b[32m1.53748\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3560 | loss: 1.53748 - acc: 0.6683 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3561  | total loss: \u001b[1m\u001b[32m1.43167\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3561 | loss: 1.43167 - acc: 0.7014 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3562  | total loss: \u001b[1m\u001b[32m1.62510\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3562 | loss: 1.62510 - acc: 0.6384 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3563  | total loss: \u001b[1m\u001b[32m1.51131\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3563 | loss: 1.51131 - acc: 0.6746 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3564  | total loss: \u001b[1m\u001b[32m1.70853\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3564 | loss: 1.70853 - acc: 0.6214 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3565  | total loss: \u001b[1m\u001b[32m1.58764\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3565 | loss: 1.58764 - acc: 0.6593 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3566  | total loss: \u001b[1m\u001b[32m1.76817\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3566 | loss: 1.76817 - acc: 0.6005 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3567  | total loss: \u001b[1m\u001b[32m1.64290\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3567 | loss: 1.64290 - acc: 0.6404 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3568  | total loss: \u001b[1m\u001b[32m1.82673\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3568 | loss: 1.82673 - acc: 0.5835 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3569  | total loss: \u001b[1m\u001b[32m1.69735\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3569 | loss: 1.69735 - acc: 0.6252 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3570  | total loss: \u001b[1m\u001b[32m1.87031\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3570 | loss: 1.87031 - acc: 0.5698 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3571  | total loss: \u001b[1m\u001b[32m1.73827\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3571 | loss: 1.73827 - acc: 0.6128 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3572  | total loss: \u001b[1m\u001b[32m1.91765\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3572 | loss: 1.91765 - acc: 0.5587 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3573  | total loss: \u001b[1m\u001b[32m1.78258\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3573 | loss: 1.78258 - acc: 0.6028 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3574  | total loss: \u001b[1m\u001b[32m1.66145\u001b[0m\u001b[0m | time: 0.037s\n",
      "| Adam | epoch: 3574 | loss: 1.66145 - acc: 0.6425 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3575  | total loss: \u001b[1m\u001b[32m1.55205\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3575 | loss: 1.55205 - acc: 0.6783 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3576  | total loss: \u001b[1m\u001b[32m1.74450\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3576 | loss: 1.74450 - acc: 0.6105 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3577  | total loss: \u001b[1m\u001b[32m1.62573\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3577 | loss: 1.62573 - acc: 0.6494 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3578  | total loss: \u001b[1m\u001b[32m1.74622\u001b[0m\u001b[0m | time: 0.020s\n",
      "| Adam | epoch: 3578 | loss: 1.74622 - acc: 0.6130 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3579  | total loss: \u001b[1m\u001b[32m1.62639\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3579 | loss: 1.62639 - acc: 0.6517 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3580  | total loss: \u001b[1m\u001b[32m1.80574\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3580 | loss: 1.80574 - acc: 0.5937 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3581  | total loss: \u001b[1m\u001b[32m1.67932\u001b[0m\u001b[0m | time: 0.013s\n",
      "| Adam | epoch: 3581 | loss: 1.67932 - acc: 0.6343 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3582  | total loss: \u001b[1m\u001b[32m1.81702\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3582 | loss: 1.81702 - acc: 0.5852 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3583  | total loss: \u001b[1m\u001b[32m1.68926\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3583 | loss: 1.68926 - acc: 0.6267 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3584  | total loss: \u001b[1m\u001b[32m1.84521\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3584 | loss: 1.84521 - acc: 0.5711 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3585  | total loss: \u001b[1m\u001b[32m1.71478\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3585 | loss: 1.71478 - acc: 0.6140 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3586  | total loss: \u001b[1m\u001b[32m1.86938\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3586 | loss: 1.86938 - acc: 0.5598 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3587  | total loss: \u001b[1m\u001b[32m1.73701\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3587 | loss: 1.73701 - acc: 0.6038 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3588  | total loss: \u001b[1m\u001b[32m1.90720\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3588 | loss: 1.90720 - acc: 0.5506 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3589  | total loss: \u001b[1m\u001b[32m1.77178\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3589 | loss: 1.77178 - acc: 0.5955 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3590  | total loss: \u001b[1m\u001b[32m1.94488\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3590 | loss: 1.94488 - acc: 0.5360 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3591  | total loss: \u001b[1m\u001b[32m1.80687\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3591 | loss: 1.80687 - acc: 0.5824 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3592  | total loss: \u001b[1m\u001b[32m1.98137\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3592 | loss: 1.98137 - acc: 0.5241 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3593  | total loss: \u001b[1m\u001b[32m1.84128\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3593 | loss: 1.84128 - acc: 0.5717 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3594  | total loss: \u001b[1m\u001b[32m1.97398\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3594 | loss: 1.97398 - acc: 0.5217 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3595  | total loss: \u001b[1m\u001b[32m1.83618\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3595 | loss: 1.83618 - acc: 0.5695 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3596  | total loss: \u001b[1m\u001b[32m1.99010\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3596 | loss: 1.99010 - acc: 0.5126 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3597  | total loss: \u001b[1m\u001b[32m1.85225\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3597 | loss: 1.85225 - acc: 0.5613 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3598  | total loss: \u001b[1m\u001b[32m1.97045\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3598 | loss: 1.97045 - acc: 0.5195 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3599  | total loss: \u001b[1m\u001b[32m1.83595\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3599 | loss: 1.83595 - acc: 0.5675 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3600  | total loss: \u001b[1m\u001b[32m1.96484\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3600 | loss: 1.96484 - acc: 0.5179 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3601  | total loss: \u001b[1m\u001b[32m1.83204\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3601 | loss: 1.83204 - acc: 0.5661 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3602  | total loss: \u001b[1m\u001b[32m1.99325\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3602 | loss: 1.99325 - acc: 0.5095 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3603  | total loss: \u001b[1m\u001b[32m1.85893\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3603 | loss: 1.85893 - acc: 0.5586 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3604  | total loss: \u001b[1m\u001b[32m1.99680\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3604 | loss: 1.99680 - acc: 0.5098 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3605  | total loss: \u001b[1m\u001b[32m1.86358\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3605 | loss: 1.86358 - acc: 0.5589 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3606  | total loss: \u001b[1m\u001b[32m1.97219\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3606 | loss: 1.97219 - acc: 0.5173 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3607  | total loss: \u001b[1m\u001b[32m1.84263\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3607 | loss: 1.84263 - acc: 0.5655 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3608  | total loss: \u001b[1m\u001b[32m1.72602\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3608 | loss: 1.72602 - acc: 0.6090 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3609  | total loss: \u001b[1m\u001b[32m1.62022\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3609 | loss: 1.62022 - acc: 0.6481 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3610  | total loss: \u001b[1m\u001b[32m1.75062\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3610 | loss: 1.75062 - acc: 0.5976 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3611  | total loss: \u001b[1m\u001b[32m1.63997\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3611 | loss: 1.63997 - acc: 0.6378 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3612  | total loss: \u001b[1m\u001b[32m1.53888\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3612 | loss: 1.53888 - acc: 0.6740 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3613  | total loss: \u001b[1m\u001b[32m1.44575\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3613 | loss: 1.44575 - acc: 0.7066 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3614  | total loss: \u001b[1m\u001b[32m1.35928\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3614 | loss: 1.35928 - acc: 0.7360 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3615  | total loss: \u001b[1m\u001b[32m1.27837\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3615 | loss: 1.27837 - acc: 0.7624 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3616  | total loss: \u001b[1m\u001b[32m1.46209\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3616 | loss: 1.46209 - acc: 0.7004 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3617  | total loss: \u001b[1m\u001b[32m1.36540\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3617 | loss: 1.36540 - acc: 0.7304 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3618  | total loss: \u001b[1m\u001b[32m1.56800\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3618 | loss: 1.56800 - acc: 0.6645 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3619  | total loss: \u001b[1m\u001b[32m1.45705\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3619 | loss: 1.45705 - acc: 0.6980 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3620  | total loss: \u001b[1m\u001b[32m1.35558\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3620 | loss: 1.35558 - acc: 0.7282 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3621  | total loss: \u001b[1m\u001b[32m1.26229\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3621 | loss: 1.26229 - acc: 0.7554 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3622  | total loss: \u001b[1m\u001b[32m1.17609\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3622 | loss: 1.17609 - acc: 0.7799 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3623  | total loss: \u001b[1m\u001b[32m1.09610\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3623 | loss: 1.09610 - acc: 0.8019 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3624  | total loss: \u001b[1m\u001b[32m1.30655\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3624 | loss: 1.30655 - acc: 0.7431 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3625  | total loss: \u001b[1m\u001b[32m1.20955\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3625 | loss: 1.20955 - acc: 0.7688 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3626  | total loss: \u001b[1m\u001b[32m1.12064\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3626 | loss: 1.12064 - acc: 0.7919 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3627  | total loss: \u001b[1m\u001b[32m1.03885\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3627 | loss: 1.03885 - acc: 0.8127 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3628  | total loss: \u001b[1m\u001b[32m1.34356\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3628 | loss: 1.34356 - acc: 0.7315 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3629  | total loss: \u001b[1m\u001b[32m1.23706\u001b[0m\u001b[0m | time: 0.014s\n",
      "| Adam | epoch: 3629 | loss: 1.23706 - acc: 0.7583 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3630  | total loss: \u001b[1m\u001b[32m1.50253\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3630 | loss: 1.50253 - acc: 0.6896 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3631  | total loss: \u001b[1m\u001b[32m1.37962\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3631 | loss: 1.37962 - acc: 0.7207 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3632  | total loss: \u001b[1m\u001b[32m1.26897\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3632 | loss: 1.26897 - acc: 0.7486 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3633  | total loss: \u001b[1m\u001b[32m1.16906\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3633 | loss: 1.16906 - acc: 0.7737 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3634  | total loss: \u001b[1m\u001b[32m1.07857\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3634 | loss: 1.07857 - acc: 0.7964 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3635  | total loss: \u001b[1m\u001b[32m0.99635\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3635 | loss: 0.99635 - acc: 0.8167 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3636  | total loss: \u001b[1m\u001b[32m1.26778\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3636 | loss: 1.26778 - acc: 0.7493 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3637  | total loss: \u001b[1m\u001b[32m1.16566\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3637 | loss: 1.16566 - acc: 0.7744 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3638  | total loss: \u001b[1m\u001b[32m1.46260\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3638 | loss: 1.46260 - acc: 0.7041 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3639  | total loss: \u001b[1m\u001b[32m1.34145\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3639 | loss: 1.34145 - acc: 0.7337 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3640  | total loss: \u001b[1m\u001b[32m1.60766\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3640 | loss: 1.60766 - acc: 0.6675 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3641  | total loss: \u001b[1m\u001b[32m1.47370\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3641 | loss: 1.47370 - acc: 0.7007 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3642  | total loss: \u001b[1m\u001b[32m1.68679\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3642 | loss: 1.68679 - acc: 0.6378 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3643  | total loss: \u001b[1m\u001b[32m1.54759\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3643 | loss: 1.54759 - acc: 0.6740 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3644  | total loss: \u001b[1m\u001b[32m1.42360\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3644 | loss: 1.42360 - acc: 0.7066 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3645  | total loss: \u001b[1m\u001b[32m1.31282\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3645 | loss: 1.31282 - acc: 0.7360 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3646  | total loss: \u001b[1m\u001b[32m1.52344\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3646 | loss: 1.52344 - acc: 0.6766 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3647  | total loss: \u001b[1m\u001b[32m1.40428\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3647 | loss: 1.40428 - acc: 0.7090 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3648  | total loss: \u001b[1m\u001b[32m1.64889\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3648 | loss: 1.64889 - acc: 0.6381 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3649  | total loss: \u001b[1m\u001b[32m1.51969\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3649 | loss: 1.51969 - acc: 0.6743 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3650  | total loss: \u001b[1m\u001b[32m1.75208\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3650 | loss: 1.75208 - acc: 0.6068 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3651  | total loss: \u001b[1m\u001b[32m1.61606\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3651 | loss: 1.61606 - acc: 0.6462 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3652  | total loss: \u001b[1m\u001b[32m1.83830\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3652 | loss: 1.83830 - acc: 0.5815 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3653  | total loss: \u001b[1m\u001b[32m1.69786\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3653 | loss: 1.69786 - acc: 0.6234 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3654  | total loss: \u001b[1m\u001b[32m1.87212\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3654 | loss: 1.87212 - acc: 0.5682 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3655  | total loss: \u001b[1m\u001b[32m1.73282\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3655 | loss: 1.73282 - acc: 0.6114 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3656  | total loss: \u001b[1m\u001b[32m1.88526\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3656 | loss: 1.88526 - acc: 0.5574 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3657  | total loss: \u001b[1m\u001b[32m1.74909\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3657 | loss: 1.74909 - acc: 0.6016 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3658  | total loss: \u001b[1m\u001b[32m1.62820\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3658 | loss: 1.62820 - acc: 0.6415 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3659  | total loss: \u001b[1m\u001b[32m1.52017\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3659 | loss: 1.52017 - acc: 0.6773 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3660  | total loss: \u001b[1m\u001b[32m1.72194\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3660 | loss: 1.72194 - acc: 0.6096 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3661  | total loss: \u001b[1m\u001b[32m1.60551\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3661 | loss: 1.60551 - acc: 0.6486 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3662  | total loss: \u001b[1m\u001b[32m1.78659\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3662 | loss: 1.78659 - acc: 0.5909 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3663  | total loss: \u001b[1m\u001b[32m1.66510\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3663 | loss: 1.66510 - acc: 0.6318 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3664  | total loss: \u001b[1m\u001b[32m1.55610\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3664 | loss: 1.55610 - acc: 0.6686 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3665  | total loss: \u001b[1m\u001b[32m1.45751\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3665 | loss: 1.45751 - acc: 0.7018 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3666  | total loss: \u001b[1m\u001b[32m1.36758\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3666 | loss: 1.36758 - acc: 0.7316 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3667  | total loss: \u001b[1m\u001b[32m1.28482\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3667 | loss: 1.28482 - acc: 0.7584 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3668  | total loss: \u001b[1m\u001b[32m1.49506\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3668 | loss: 1.49506 - acc: 0.6826 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3669  | total loss: \u001b[1m\u001b[32m1.39622\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3669 | loss: 1.39622 - acc: 0.7143 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3670  | total loss: \u001b[1m\u001b[32m1.30568\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3670 | loss: 1.30568 - acc: 0.7429 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3671  | total loss: \u001b[1m\u001b[32m1.22217\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3671 | loss: 1.22217 - acc: 0.7686 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3672  | total loss: \u001b[1m\u001b[32m1.48217\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3672 | loss: 1.48217 - acc: 0.6918 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3673  | total loss: \u001b[1m\u001b[32m1.37769\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3673 | loss: 1.37769 - acc: 0.7226 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3674  | total loss: \u001b[1m\u001b[32m1.59465\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3674 | loss: 1.59465 - acc: 0.6575 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3675  | total loss: \u001b[1m\u001b[32m1.47730\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3675 | loss: 1.47730 - acc: 0.6917 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3676  | total loss: \u001b[1m\u001b[32m1.73236\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3676 | loss: 1.73236 - acc: 0.6225 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3677  | total loss: \u001b[1m\u001b[32m1.60096\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3677 | loss: 1.60096 - acc: 0.6603 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3678  | total loss: \u001b[1m\u001b[32m1.82418\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3678 | loss: 1.82418 - acc: 0.5943 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3679  | total loss: \u001b[1m\u001b[32m1.68452\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3679 | loss: 1.68452 - acc: 0.6348 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3680  | total loss: \u001b[1m\u001b[32m1.55922\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3680 | loss: 1.55922 - acc: 0.6714 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3681  | total loss: \u001b[1m\u001b[32m1.44628\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3681 | loss: 1.44628 - acc: 0.7042 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3682  | total loss: \u001b[1m\u001b[32m1.63012\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3682 | loss: 1.63012 - acc: 0.6409 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3683  | total loss: \u001b[1m\u001b[32m1.50976\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3683 | loss: 1.50976 - acc: 0.6768 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3684  | total loss: \u001b[1m\u001b[32m1.40123\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3684 | loss: 1.40123 - acc: 0.7092 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3685  | total loss: \u001b[1m\u001b[32m1.30285\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3685 | loss: 1.30285 - acc: 0.7382 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3686  | total loss: \u001b[1m\u001b[32m1.51717\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3686 | loss: 1.51717 - acc: 0.6716 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3687  | total loss: \u001b[1m\u001b[32m1.40593\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3687 | loss: 1.40593 - acc: 0.7044 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3688  | total loss: \u001b[1m\u001b[32m1.61462\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3688 | loss: 1.61462 - acc: 0.6411 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3689  | total loss: \u001b[1m\u001b[32m1.49345\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3689 | loss: 1.49345 - acc: 0.6770 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3690  | total loss: \u001b[1m\u001b[32m1.65289\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3690 | loss: 1.65289 - acc: 0.6236 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3691  | total loss: \u001b[1m\u001b[32m1.52854\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3691 | loss: 1.52854 - acc: 0.6612 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3692  | total loss: \u001b[1m\u001b[32m1.73061\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3692 | loss: 1.73061 - acc: 0.6022 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3693  | total loss: \u001b[1m\u001b[32m1.59985\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3693 | loss: 1.59985 - acc: 0.6420 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3694  | total loss: \u001b[1m\u001b[32m1.79923\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3694 | loss: 1.79923 - acc: 0.5778 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3695  | total loss: \u001b[1m\u001b[32m1.66369\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3695 | loss: 1.66369 - acc: 0.6200 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3696  | total loss: \u001b[1m\u001b[32m1.85323\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3696 | loss: 1.85323 - acc: 0.5652 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3697  | total loss: \u001b[1m\u001b[32m1.71493\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3697 | loss: 1.71493 - acc: 0.6087 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3698  | total loss: \u001b[1m\u001b[32m1.86277\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3698 | loss: 1.86277 - acc: 0.5549 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3699  | total loss: \u001b[1m\u001b[32m1.72645\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3699 | loss: 1.72645 - acc: 0.5994 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3700  | total loss: \u001b[1m\u001b[32m1.88390\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3700 | loss: 1.88390 - acc: 0.5538 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3701  | total loss: \u001b[1m\u001b[32m1.74847\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3701 | loss: 1.74847 - acc: 0.5984 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3702  | total loss: \u001b[1m\u001b[32m1.91433\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3702 | loss: 1.91433 - acc: 0.5457 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3703  | total loss: \u001b[1m\u001b[32m1.77893\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3703 | loss: 1.77893 - acc: 0.5911 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3704  | total loss: \u001b[1m\u001b[32m1.92093\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3704 | loss: 1.92093 - acc: 0.5463 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3705  | total loss: \u001b[1m\u001b[32m1.78770\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3705 | loss: 1.78770 - acc: 0.5917 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3706  | total loss: \u001b[1m\u001b[32m1.94494\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3706 | loss: 1.94494 - acc: 0.5325 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3707  | total loss: \u001b[1m\u001b[32m1.81173\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3707 | loss: 1.81173 - acc: 0.5793 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3708  | total loss: \u001b[1m\u001b[32m1.96775\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3708 | loss: 1.96775 - acc: 0.5213 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3709  | total loss: \u001b[1m\u001b[32m1.83453\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3709 | loss: 1.83453 - acc: 0.5692 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3710  | total loss: \u001b[1m\u001b[32m1.71524\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3710 | loss: 1.71524 - acc: 0.6123 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3711  | total loss: \u001b[1m\u001b[32m1.60760\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3711 | loss: 1.60760 - acc: 0.6511 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3712  | total loss: \u001b[1m\u001b[32m1.50963\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3712 | loss: 1.50963 - acc: 0.6859 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3713  | total loss: \u001b[1m\u001b[32m1.41967\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3713 | loss: 1.41967 - acc: 0.7174 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3714  | total loss: \u001b[1m\u001b[32m1.62611\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3714 | loss: 1.62611 - acc: 0.6456 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3715  | total loss: \u001b[1m\u001b[32m1.52110\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3715 | loss: 1.52110 - acc: 0.6811 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3716  | total loss: \u001b[1m\u001b[32m1.42494\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3716 | loss: 1.42494 - acc: 0.7129 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3717  | total loss: \u001b[1m\u001b[32m1.33622\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3717 | loss: 1.33622 - acc: 0.7417 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3718  | total loss: \u001b[1m\u001b[32m1.25379\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3718 | loss: 1.25379 - acc: 0.7675 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3719  | total loss: \u001b[1m\u001b[32m1.17669\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3719 | loss: 1.17669 - acc: 0.7907 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3720  | total loss: \u001b[1m\u001b[32m1.35305\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3720 | loss: 1.35305 - acc: 0.7331 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3721  | total loss: \u001b[1m\u001b[32m1.26086\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3721 | loss: 1.26086 - acc: 0.7598 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3722  | total loss: \u001b[1m\u001b[32m1.50966\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3722 | loss: 1.50966 - acc: 0.6838 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3723  | total loss: \u001b[1m\u001b[32m1.39856\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3723 | loss: 1.39856 - acc: 0.7154 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3724  | total loss: \u001b[1m\u001b[32m1.57552\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3724 | loss: 1.57552 - acc: 0.6582 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3725  | total loss: \u001b[1m\u001b[32m1.45621\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3725 | loss: 1.45621 - acc: 0.6924 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3726  | total loss: \u001b[1m\u001b[32m1.62089\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3726 | loss: 1.62089 - acc: 0.6445 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3727  | total loss: \u001b[1m\u001b[32m1.49645\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3727 | loss: 1.49645 - acc: 0.6801 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3728  | total loss: \u001b[1m\u001b[32m1.72540\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3728 | loss: 1.72540 - acc: 0.6121 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3729  | total loss: \u001b[1m\u001b[32m1.59093\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3729 | loss: 1.59093 - acc: 0.6509 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3730  | total loss: \u001b[1m\u001b[32m1.79375\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3730 | loss: 1.79375 - acc: 0.6001 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3731  | total loss: \u001b[1m\u001b[32m1.65381\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3731 | loss: 1.65381 - acc: 0.6401 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3732  | total loss: \u001b[1m\u001b[32m1.85261\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3732 | loss: 1.85261 - acc: 0.5832 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3733  | total loss: \u001b[1m\u001b[32m1.70887\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3733 | loss: 1.70887 - acc: 0.6249 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3734  | total loss: \u001b[1m\u001b[32m1.89382\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3734 | loss: 1.89382 - acc: 0.5624 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3735  | total loss: \u001b[1m\u001b[32m1.74859\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3735 | loss: 1.74859 - acc: 0.6062 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3736  | total loss: \u001b[1m\u001b[32m1.61894\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3736 | loss: 1.61894 - acc: 0.6455 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3737  | total loss: \u001b[1m\u001b[32m1.50264\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3737 | loss: 1.50264 - acc: 0.6810 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3738  | total loss: \u001b[1m\u001b[32m1.66888\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3738 | loss: 1.66888 - acc: 0.6272 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3739  | total loss: \u001b[1m\u001b[32m1.54799\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3739 | loss: 1.54799 - acc: 0.6645 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3740  | total loss: \u001b[1m\u001b[32m1.73887\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3740 | loss: 1.73887 - acc: 0.6052 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3741  | total loss: \u001b[1m\u001b[32m1.61193\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3741 | loss: 1.61193 - acc: 0.6446 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3742  | total loss: \u001b[1m\u001b[32m1.79797\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3742 | loss: 1.79797 - acc: 0.5802 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3743  | total loss: \u001b[1m\u001b[32m1.66670\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3743 | loss: 1.66670 - acc: 0.6222 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3744  | total loss: \u001b[1m\u001b[32m1.79640\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3744 | loss: 1.79640 - acc: 0.5814 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3745  | total loss: \u001b[1m\u001b[32m1.66705\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3745 | loss: 1.66705 - acc: 0.6232 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3746  | total loss: \u001b[1m\u001b[32m1.84880\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3746 | loss: 1.84880 - acc: 0.5681 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3747  | total loss: \u001b[1m\u001b[32m1.71600\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3747 | loss: 1.71600 - acc: 0.6112 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3748  | total loss: \u001b[1m\u001b[32m1.91053\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3748 | loss: 1.91053 - acc: 0.5501 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3749  | total loss: \u001b[1m\u001b[32m1.77370\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3749 | loss: 1.77370 - acc: 0.5951 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3750  | total loss: \u001b[1m\u001b[32m1.94865\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3750 | loss: 1.94865 - acc: 0.5356 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3751  | total loss: \u001b[1m\u001b[32m1.81054\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3751 | loss: 1.81054 - acc: 0.5820 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3752  | total loss: \u001b[1m\u001b[32m1.97206\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3752 | loss: 1.97206 - acc: 0.5238 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3753  | total loss: \u001b[1m\u001b[32m1.83423\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3753 | loss: 1.83423 - acc: 0.5715 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3754  | total loss: \u001b[1m\u001b[32m1.97880\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3754 | loss: 1.97880 - acc: 0.5214 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3755  | total loss: \u001b[1m\u001b[32m1.84274\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3755 | loss: 1.84274 - acc: 0.5693 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3756  | total loss: \u001b[1m\u001b[32m2.01419\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3756 | loss: 2.01419 - acc: 0.5124 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3757  | total loss: \u001b[1m\u001b[32m1.87701\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3757 | loss: 1.87701 - acc: 0.5611 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3758  | total loss: \u001b[1m\u001b[32m2.01128\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3758 | loss: 2.01128 - acc: 0.5122 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3759  | total loss: \u001b[1m\u001b[32m1.87668\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3759 | loss: 1.87668 - acc: 0.5609 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3760  | total loss: \u001b[1m\u001b[32m2.02485\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3760 | loss: 2.02485 - acc: 0.5049 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3761  | total loss: \u001b[1m\u001b[32m1.89095\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3761 | loss: 1.89095 - acc: 0.5544 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3762  | total loss: \u001b[1m\u001b[32m1.77093\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3762 | loss: 1.77093 - acc: 0.5989 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3763  | total loss: \u001b[1m\u001b[32m1.66249\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3763 | loss: 1.66249 - acc: 0.6390 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3764  | total loss: \u001b[1m\u001b[32m1.56366\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3764 | loss: 1.56366 - acc: 0.6751 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3765  | total loss: \u001b[1m\u001b[32m1.47278\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3765 | loss: 1.47278 - acc: 0.7076 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3766  | total loss: \u001b[1m\u001b[32m1.38845\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3766 | loss: 1.38845 - acc: 0.7369 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3767  | total loss: \u001b[1m\u001b[32m1.30952\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3767 | loss: 1.30952 - acc: 0.7632 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3768  | total loss: \u001b[1m\u001b[32m1.50947\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3768 | loss: 1.50947 - acc: 0.6940 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3769  | total loss: \u001b[1m\u001b[32m1.41290\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3769 | loss: 1.41290 - acc: 0.7246 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3770  | total loss: \u001b[1m\u001b[32m1.60031\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3770 | loss: 1.60031 - acc: 0.6593 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3771  | total loss: \u001b[1m\u001b[32m1.49070\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3771 | loss: 1.49070 - acc: 0.6934 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3772  | total loss: \u001b[1m\u001b[32m1.39020\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3772 | loss: 1.39020 - acc: 0.7240 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3773  | total loss: \u001b[1m\u001b[32m1.29755\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3773 | loss: 1.29755 - acc: 0.7516 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3774  | total loss: \u001b[1m\u001b[32m1.51629\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3774 | loss: 1.51629 - acc: 0.6836 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3775  | total loss: \u001b[1m\u001b[32m1.40726\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3775 | loss: 1.40726 - acc: 0.7152 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3776  | total loss: \u001b[1m\u001b[32m1.60859\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3776 | loss: 1.60859 - acc: 0.6509 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3777  | total loss: \u001b[1m\u001b[32m1.48817\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3777 | loss: 1.48817 - acc: 0.6858 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3778  | total loss: \u001b[1m\u001b[32m1.37887\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3778 | loss: 1.37887 - acc: 0.7172 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3779  | total loss: \u001b[1m\u001b[32m1.27924\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3779 | loss: 1.27924 - acc: 0.7455 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3780  | total loss: \u001b[1m\u001b[32m1.54761\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3780 | loss: 1.54761 - acc: 0.6709 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3781  | total loss: \u001b[1m\u001b[32m1.42928\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3781 | loss: 1.42928 - acc: 0.7038 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3782  | total loss: \u001b[1m\u001b[32m1.32210\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3782 | loss: 1.32210 - acc: 0.7335 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3783  | total loss: \u001b[1m\u001b[32m1.22464\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3783 | loss: 1.22464 - acc: 0.7601 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3784  | total loss: \u001b[1m\u001b[32m1.13566\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3784 | loss: 1.13566 - acc: 0.7841 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3785  | total loss: \u001b[1m\u001b[32m1.05411\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3785 | loss: 1.05411 - acc: 0.8057 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3786  | total loss: \u001b[1m\u001b[32m1.28738\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3786 | loss: 1.28738 - acc: 0.7465 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3787  | total loss: \u001b[1m\u001b[32m1.18838\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3787 | loss: 1.18838 - acc: 0.7719 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3788  | total loss: \u001b[1m\u001b[32m1.09841\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3788 | loss: 1.09841 - acc: 0.7947 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3789  | total loss: \u001b[1m\u001b[32m1.01637\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3789 | loss: 1.01637 - acc: 0.8152 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3790  | total loss: \u001b[1m\u001b[32m0.94132\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3790 | loss: 0.94132 - acc: 0.8337 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3791  | total loss: \u001b[1m\u001b[32m0.87246\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3791 | loss: 0.87246 - acc: 0.8503 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3792  | total loss: \u001b[1m\u001b[32m0.80908\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3792 | loss: 0.80908 - acc: 0.8653 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3793  | total loss: \u001b[1m\u001b[32m0.75060\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3793 | loss: 0.75060 - acc: 0.8788 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3794  | total loss: \u001b[1m\u001b[32m1.09637\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3794 | loss: 1.09637 - acc: 0.7980 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3795  | total loss: \u001b[1m\u001b[32m1.00722\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3795 | loss: 1.00722 - acc: 0.8182 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3796  | total loss: \u001b[1m\u001b[32m1.32042\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3796 | loss: 1.32042 - acc: 0.7436 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3797  | total loss: \u001b[1m\u001b[32m1.20847\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3797 | loss: 1.20847 - acc: 0.7692 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3798  | total loss: \u001b[1m\u001b[32m1.54969\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3798 | loss: 1.54969 - acc: 0.6923 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3799  | total loss: \u001b[1m\u001b[32m1.41570\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3799 | loss: 1.41570 - acc: 0.7231 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3800  | total loss: \u001b[1m\u001b[32m1.60885\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3800 | loss: 1.60885 - acc: 0.6722 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3801  | total loss: \u001b[1m\u001b[32m1.47083\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3801 | loss: 1.47083 - acc: 0.7050 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3802  | total loss: \u001b[1m\u001b[32m1.74556\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3802 | loss: 1.74556 - acc: 0.6345 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3803  | total loss: \u001b[1m\u001b[32m1.59672\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3803 | loss: 1.59672 - acc: 0.6710 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3804  | total loss: \u001b[1m\u001b[32m1.78354\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3804 | loss: 1.78354 - acc: 0.6182 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3805  | total loss: \u001b[1m\u001b[32m1.63475\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3805 | loss: 1.63475 - acc: 0.6564 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3806  | total loss: \u001b[1m\u001b[32m1.81354\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3806 | loss: 1.81354 - acc: 0.6050 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3807  | total loss: \u001b[1m\u001b[32m1.66618\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3807 | loss: 1.66618 - acc: 0.6445 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3808  | total loss: \u001b[1m\u001b[32m1.88645\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3808 | loss: 1.88645 - acc: 0.5801 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3809  | total loss: \u001b[1m\u001b[32m1.73681\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3809 | loss: 1.73681 - acc: 0.6221 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3810  | total loss: \u001b[1m\u001b[32m1.88781\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3810 | loss: 1.88781 - acc: 0.5741 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3811  | total loss: \u001b[1m\u001b[32m1.74344\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3811 | loss: 1.74344 - acc: 0.6167 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3812  | total loss: \u001b[1m\u001b[32m1.91175\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3812 | loss: 1.91175 - acc: 0.5622 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3813  | total loss: \u001b[1m\u001b[32m1.77025\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3813 | loss: 1.77025 - acc: 0.6060 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3814  | total loss: \u001b[1m\u001b[32m1.64501\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3814 | loss: 1.64501 - acc: 0.6454 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3815  | total loss: \u001b[1m\u001b[32m1.53352\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3815 | loss: 1.53352 - acc: 0.6808 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3816  | total loss: \u001b[1m\u001b[32m1.68259\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3816 | loss: 1.68259 - acc: 0.6270 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3817  | total loss: \u001b[1m\u001b[32m1.56899\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3817 | loss: 1.56899 - acc: 0.6643 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3818  | total loss: \u001b[1m\u001b[32m1.70503\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3818 | loss: 1.70503 - acc: 0.6193 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3819  | total loss: \u001b[1m\u001b[32m1.59055\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3819 | loss: 1.59055 - acc: 0.6574 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3820  | total loss: \u001b[1m\u001b[32m1.75120\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3820 | loss: 1.75120 - acc: 0.5988 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3821  | total loss: \u001b[1m\u001b[32m1.63325\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3821 | loss: 1.63325 - acc: 0.6389 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3822  | total loss: \u001b[1m\u001b[32m1.81863\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3822 | loss: 1.81863 - acc: 0.5750 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3823  | total loss: \u001b[1m\u001b[32m1.69516\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3823 | loss: 1.69516 - acc: 0.6175 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3824  | total loss: \u001b[1m\u001b[32m1.89137\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3824 | loss: 1.89137 - acc: 0.5558 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3825  | total loss: \u001b[1m\u001b[32m1.76194\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3825 | loss: 1.76194 - acc: 0.6002 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3826  | total loss: \u001b[1m\u001b[32m1.64570\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3826 | loss: 1.64570 - acc: 0.6402 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3827  | total loss: \u001b[1m\u001b[32m1.54053\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3827 | loss: 1.54053 - acc: 0.6762 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3828  | total loss: \u001b[1m\u001b[32m1.68863\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3828 | loss: 1.68863 - acc: 0.6157 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3829  | total loss: \u001b[1m\u001b[32m1.57769\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3829 | loss: 1.57769 - acc: 0.6541 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3830  | total loss: \u001b[1m\u001b[32m1.47690\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3830 | loss: 1.47690 - acc: 0.6887 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3831  | total loss: \u001b[1m\u001b[32m1.38464\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3831 | loss: 1.38464 - acc: 0.7198 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3832  | total loss: \u001b[1m\u001b[32m1.29956\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3832 | loss: 1.29956 - acc: 0.7479 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3833  | total loss: \u001b[1m\u001b[32m1.22055\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3833 | loss: 1.22055 - acc: 0.7731 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3834  | total loss: \u001b[1m\u001b[32m1.43280\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 3834 | loss: 1.43280 - acc: 0.7100 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3835  | total loss: \u001b[1m\u001b[32m1.33615\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3835 | loss: 1.33615 - acc: 0.7390 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3836  | total loss: \u001b[1m\u001b[32m1.52167\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3836 | loss: 1.52167 - acc: 0.6794 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3837  | total loss: \u001b[1m\u001b[32m1.41332\u001b[0m\u001b[0m | time: 0.032s\n",
      "| Adam | epoch: 3837 | loss: 1.41332 - acc: 0.7115 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3838  | total loss: \u001b[1m\u001b[32m1.62718\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3838 | loss: 1.62718 - acc: 0.6475 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3839  | total loss: \u001b[1m\u001b[32m1.50692\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3839 | loss: 1.50692 - acc: 0.6827 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3840  | total loss: \u001b[1m\u001b[32m1.39812\u001b[0m\u001b[0m | time: 0.013s\n",
      "| Adam | epoch: 3840 | loss: 1.39812 - acc: 0.7145 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3841  | total loss: \u001b[1m\u001b[32m1.29923\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3841 | loss: 1.29923 - acc: 0.7430 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3842  | total loss: \u001b[1m\u001b[32m1.54918\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3842 | loss: 1.54918 - acc: 0.6687 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3843  | total loss: \u001b[1m\u001b[32m1.43368\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3843 | loss: 1.43368 - acc: 0.7018 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3844  | total loss: \u001b[1m\u001b[32m1.65889\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3844 | loss: 1.65889 - acc: 0.6317 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3845  | total loss: \u001b[1m\u001b[32m1.53225\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3845 | loss: 1.53225 - acc: 0.6685 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3846  | total loss: \u001b[1m\u001b[32m1.72787\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3846 | loss: 1.72787 - acc: 0.6088 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3847  | total loss: \u001b[1m\u001b[32m1.59511\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3847 | loss: 1.59511 - acc: 0.6479 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3848  | total loss: \u001b[1m\u001b[32m1.80811\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3848 | loss: 1.80811 - acc: 0.5903 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3849  | total loss: \u001b[1m\u001b[32m1.66876\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3849 | loss: 1.66876 - acc: 0.6312 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3850  | total loss: \u001b[1m\u001b[32m1.54392\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3850 | loss: 1.54392 - acc: 0.6681 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3851  | total loss: \u001b[1m\u001b[32m1.43158\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3851 | loss: 1.43158 - acc: 0.7013 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3852  | total loss: \u001b[1m\u001b[32m1.67765\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3852 | loss: 1.67765 - acc: 0.6312 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3853  | total loss: \u001b[1m\u001b[32m1.55215\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3853 | loss: 1.55215 - acc: 0.6681 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3854  | total loss: \u001b[1m\u001b[32m1.74999\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3854 | loss: 1.74999 - acc: 0.6084 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3855  | total loss: \u001b[1m\u001b[32m1.61851\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3855 | loss: 1.61851 - acc: 0.6475 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3856  | total loss: \u001b[1m\u001b[32m1.50069\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3856 | loss: 1.50069 - acc: 0.6828 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3857  | total loss: \u001b[1m\u001b[32m1.39460\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3857 | loss: 1.39460 - acc: 0.7145 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3858  | total loss: \u001b[1m\u001b[32m1.29853\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3858 | loss: 1.29853 - acc: 0.7431 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3859  | total loss: \u001b[1m\u001b[32m1.21104\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3859 | loss: 1.21104 - acc: 0.7688 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3860  | total loss: \u001b[1m\u001b[32m1.46425\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3860 | loss: 1.46425 - acc: 0.6919 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3861  | total loss: \u001b[1m\u001b[32m1.35854\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3861 | loss: 1.35854 - acc: 0.7227 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3862  | total loss: \u001b[1m\u001b[32m1.60646\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3862 | loss: 1.60646 - acc: 0.6504 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3863  | total loss: \u001b[1m\u001b[32m1.48628\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3863 | loss: 1.48628 - acc: 0.6854 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3864  | total loss: \u001b[1m\u001b[32m1.69591\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3864 | loss: 1.69591 - acc: 0.6168 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3865  | total loss: \u001b[1m\u001b[32m1.56764\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3865 | loss: 1.56764 - acc: 0.6552 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3866  | total loss: \u001b[1m\u001b[32m1.76569\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3866 | loss: 1.76569 - acc: 0.5968 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3867  | total loss: \u001b[1m\u001b[32m1.63215\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3867 | loss: 1.63215 - acc: 0.6371 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3868  | total loss: \u001b[1m\u001b[32m1.51267\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3868 | loss: 1.51267 - acc: 0.6734 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3869  | total loss: \u001b[1m\u001b[32m1.40526\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3869 | loss: 1.40526 - acc: 0.7061 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3870  | total loss: \u001b[1m\u001b[32m1.30815\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3870 | loss: 1.30815 - acc: 0.7355 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3871  | total loss: \u001b[1m\u001b[32m1.21984\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3871 | loss: 1.21984 - acc: 0.7619 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3872  | total loss: \u001b[1m\u001b[32m1.45189\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3872 | loss: 1.45189 - acc: 0.6929 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3873  | total loss: \u001b[1m\u001b[32m1.34767\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3873 | loss: 1.34767 - acc: 0.7236 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3874  | total loss: \u001b[1m\u001b[32m1.25320\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3874 | loss: 1.25320 - acc: 0.7512 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3875  | total loss: \u001b[1m\u001b[32m1.16711\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3875 | loss: 1.16711 - acc: 0.7761 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3876  | total loss: \u001b[1m\u001b[32m1.08820\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3876 | loss: 1.08820 - acc: 0.7985 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3877  | total loss: \u001b[1m\u001b[32m1.01550\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3877 | loss: 1.01550 - acc: 0.8186 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3878  | total loss: \u001b[1m\u001b[32m0.94818\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3878 | loss: 0.94818 - acc: 0.8368 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3879  | total loss: \u001b[1m\u001b[32m0.88556\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3879 | loss: 0.88556 - acc: 0.8531 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3880  | total loss: \u001b[1m\u001b[32m1.19061\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3880 | loss: 1.19061 - acc: 0.7749 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3881  | total loss: \u001b[1m\u001b[32m1.10077\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3881 | loss: 1.10077 - acc: 0.7974 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3882  | total loss: \u001b[1m\u001b[32m1.35351\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3882 | loss: 1.35351 - acc: 0.7248 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3883  | total loss: \u001b[1m\u001b[32m1.24618\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3883 | loss: 1.24618 - acc: 0.7524 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3884  | total loss: \u001b[1m\u001b[32m1.14919\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3884 | loss: 1.14919 - acc: 0.7771 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3885  | total loss: \u001b[1m\u001b[32m1.06126\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3885 | loss: 1.06126 - acc: 0.7994 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3886  | total loss: \u001b[1m\u001b[32m0.98128\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3886 | loss: 0.98128 - acc: 0.8195 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3887  | total loss: \u001b[1m\u001b[32m0.90829\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3887 | loss: 0.90829 - acc: 0.8375 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3888  | total loss: \u001b[1m\u001b[32m1.22970\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3888 | loss: 1.22970 - acc: 0.7538 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3889  | total loss: \u001b[1m\u001b[32m1.13065\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3889 | loss: 1.13065 - acc: 0.7784 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3890  | total loss: \u001b[1m\u001b[32m1.04119\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3890 | loss: 1.04119 - acc: 0.8006 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3891  | total loss: \u001b[1m\u001b[32m0.96018\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3891 | loss: 0.96018 - acc: 0.8205 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3892  | total loss: \u001b[1m\u001b[32m1.24090\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3892 | loss: 1.24090 - acc: 0.7456 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3893  | total loss: \u001b[1m\u001b[32m1.13946\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3893 | loss: 1.13946 - acc: 0.7710 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3894  | total loss: \u001b[1m\u001b[32m1.04813\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3894 | loss: 1.04813 - acc: 0.7939 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3895  | total loss: \u001b[1m\u001b[32m0.96570\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3895 | loss: 0.96570 - acc: 0.8145 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3896  | total loss: \u001b[1m\u001b[32m1.29684\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3896 | loss: 1.29684 - acc: 0.7331 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3897  | total loss: \u001b[1m\u001b[32m1.18963\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3897 | loss: 1.18963 - acc: 0.7598 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3898  | total loss: \u001b[1m\u001b[32m1.46151\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3898 | loss: 1.46151 - acc: 0.6981 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3899  | total loss: \u001b[1m\u001b[32m1.33911\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3899 | loss: 1.33911 - acc: 0.7283 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3900  | total loss: \u001b[1m\u001b[32m1.22967\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3900 | loss: 1.22967 - acc: 0.7554 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3901  | total loss: \u001b[1m\u001b[32m1.13158\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3901 | loss: 1.13158 - acc: 0.7799 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3902  | total loss: \u001b[1m\u001b[32m1.40105\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3902 | loss: 1.40105 - acc: 0.7162 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3903  | total loss: \u001b[1m\u001b[32m1.28691\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3903 | loss: 1.28691 - acc: 0.7446 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3904  | total loss: \u001b[1m\u001b[32m1.18482\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3904 | loss: 1.18482 - acc: 0.7701 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3905  | total loss: \u001b[1m\u001b[32m1.09321\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3905 | loss: 1.09321 - acc: 0.7931 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3906  | total loss: \u001b[1m\u001b[32m1.36127\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3906 | loss: 1.36127 - acc: 0.7281 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3907  | total loss: \u001b[1m\u001b[32m1.25284\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3907 | loss: 1.25284 - acc: 0.7553 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3908  | total loss: \u001b[1m\u001b[32m1.54084\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3908 | loss: 1.54084 - acc: 0.6797 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3909  | total loss: \u001b[1m\u001b[32m1.41646\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3909 | loss: 1.41646 - acc: 0.7118 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3910  | total loss: \u001b[1m\u001b[32m1.67432\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3910 | loss: 1.67432 - acc: 0.6406 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3911  | total loss: \u001b[1m\u001b[32m1.53974\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3911 | loss: 1.53974 - acc: 0.6765 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3912  | total loss: \u001b[1m\u001b[32m1.42014\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3912 | loss: 1.42014 - acc: 0.7089 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3913  | total loss: \u001b[1m\u001b[32m1.31349\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3913 | loss: 1.31349 - acc: 0.7380 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3914  | total loss: \u001b[1m\u001b[32m1.53198\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3914 | loss: 1.53198 - acc: 0.6713 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3915  | total loss: \u001b[1m\u001b[32m1.41597\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3915 | loss: 1.41597 - acc: 0.7042 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3916  | total loss: \u001b[1m\u001b[32m1.63420\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3916 | loss: 1.63420 - acc: 0.6481 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3917  | total loss: \u001b[1m\u001b[32m1.51038\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3917 | loss: 1.51038 - acc: 0.6833 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3918  | total loss: \u001b[1m\u001b[32m1.68222\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3918 | loss: 1.68222 - acc: 0.6292 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3919  | total loss: \u001b[1m\u001b[32m1.55634\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3919 | loss: 1.55634 - acc: 0.6663 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3920  | total loss: \u001b[1m\u001b[32m1.74334\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3920 | loss: 1.74334 - acc: 0.6068 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3921  | total loss: \u001b[1m\u001b[32m1.61422\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3921 | loss: 1.61422 - acc: 0.6461 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3922  | total loss: \u001b[1m\u001b[32m1.49909\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3922 | loss: 1.49909 - acc: 0.6815 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3923  | total loss: \u001b[1m\u001b[32m1.39583\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3923 | loss: 1.39583 - acc: 0.7134 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3924  | total loss: \u001b[1m\u001b[32m1.30259\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3924 | loss: 1.30259 - acc: 0.7420 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3925  | total loss: \u001b[1m\u001b[32m1.21781\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3925 | loss: 1.21781 - acc: 0.7678 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3926  | total loss: \u001b[1m\u001b[32m1.14014\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3926 | loss: 1.14014 - acc: 0.7910 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3927  | total loss: \u001b[1m\u001b[32m1.06849\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3927 | loss: 1.06849 - acc: 0.8119 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3928  | total loss: \u001b[1m\u001b[32m1.00195\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3928 | loss: 1.00195 - acc: 0.8307 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3929  | total loss: \u001b[1m\u001b[32m0.93979\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3929 | loss: 0.93979 - acc: 0.8477 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3930  | total loss: \u001b[1m\u001b[32m0.88143\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3930 | loss: 0.88143 - acc: 0.8629 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3931  | total loss: \u001b[1m\u001b[32m0.82642\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3931 | loss: 0.82642 - acc: 0.8766 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3932  | total loss: \u001b[1m\u001b[32m1.10541\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3932 | loss: 1.10541 - acc: 0.8032 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3933  | total loss: \u001b[1m\u001b[32m1.02422\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3933 | loss: 1.02422 - acc: 0.8229 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3934  | total loss: \u001b[1m\u001b[32m1.31026\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3934 | loss: 1.31026 - acc: 0.7478 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3935  | total loss: \u001b[1m\u001b[32m1.20683\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3935 | loss: 1.20683 - acc: 0.7730 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3936  | total loss: \u001b[1m\u001b[32m1.50894\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3936 | loss: 1.50894 - acc: 0.7028 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3937  | total loss: \u001b[1m\u001b[32m1.38552\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3937 | loss: 1.38552 - acc: 0.7325 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3938  | total loss: \u001b[1m\u001b[32m1.56604\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3938 | loss: 1.56604 - acc: 0.6807 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3939  | total loss: \u001b[1m\u001b[32m1.43788\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3939 | loss: 1.43788 - acc: 0.7127 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3940  | total loss: \u001b[1m\u001b[32m1.61772\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3940 | loss: 1.61772 - acc: 0.6628 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3941  | total loss: \u001b[1m\u001b[32m1.48613\u001b[0m\u001b[0m | time: 0.028s\n",
      "| Adam | epoch: 3941 | loss: 1.48613 - acc: 0.6965 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3942  | total loss: \u001b[1m\u001b[32m1.68693\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3942 | loss: 1.68693 - acc: 0.6340 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3943  | total loss: \u001b[1m\u001b[32m1.55088\u001b[0m\u001b[0m | time: 0.015s\n",
      "| Adam | epoch: 3943 | loss: 1.55088 - acc: 0.6706 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3944  | total loss: \u001b[1m\u001b[32m1.77538\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3944 | loss: 1.77538 - acc: 0.6107 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3945  | total loss: \u001b[1m\u001b[32m1.63365\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3945 | loss: 1.63365 - acc: 0.6496 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3946  | total loss: \u001b[1m\u001b[32m1.87083\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3946 | loss: 1.87083 - acc: 0.5847 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3947  | total loss: \u001b[1m\u001b[32m1.72345\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3947 | loss: 1.72345 - acc: 0.6262 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3948  | total loss: \u001b[1m\u001b[32m1.93113\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3948 | loss: 1.93113 - acc: 0.5636 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3949  | total loss: \u001b[1m\u001b[32m1.78231\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3949 | loss: 1.78231 - acc: 0.6072 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3950  | total loss: \u001b[1m\u001b[32m1.65044\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3950 | loss: 1.65044 - acc: 0.6465 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3951  | total loss: \u001b[1m\u001b[32m1.53308\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3951 | loss: 1.53308 - acc: 0.6819 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3952  | total loss: \u001b[1m\u001b[32m1.73298\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3952 | loss: 1.73298 - acc: 0.6137 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3953  | total loss: \u001b[1m\u001b[32m1.60963\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3953 | loss: 1.60963 - acc: 0.6523 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3954  | total loss: \u001b[1m\u001b[32m1.49951\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3954 | loss: 1.49951 - acc: 0.6871 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3955  | total loss: \u001b[1m\u001b[32m1.40056\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3955 | loss: 1.40056 - acc: 0.7184 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3956  | total loss: \u001b[1m\u001b[32m1.62731\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3956 | loss: 1.62731 - acc: 0.6465 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3957  | total loss: \u001b[1m\u001b[32m1.51560\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3957 | loss: 1.51560 - acc: 0.6819 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3958  | total loss: \u001b[1m\u001b[32m1.64557\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3958 | loss: 1.64557 - acc: 0.6351 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3959  | total loss: \u001b[1m\u001b[32m1.53240\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3959 | loss: 1.53240 - acc: 0.6716 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3960  | total loss: \u001b[1m\u001b[32m1.43039\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3960 | loss: 1.43039 - acc: 0.7044 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3961  | total loss: \u001b[1m\u001b[32m1.33779\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3961 | loss: 1.33779 - acc: 0.7340 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3962  | total loss: \u001b[1m\u001b[32m1.56385\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3962 | loss: 1.56385 - acc: 0.6677 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3963  | total loss: \u001b[1m\u001b[32m1.45626\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3963 | loss: 1.45626 - acc: 0.7010 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3964  | total loss: \u001b[1m\u001b[32m1.63581\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3964 | loss: 1.63581 - acc: 0.6452 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3965  | total loss: \u001b[1m\u001b[32m1.52017\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3965 | loss: 1.52017 - acc: 0.6806 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3966  | total loss: \u001b[1m\u001b[32m1.69101\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3966 | loss: 1.69101 - acc: 0.6269 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3967  | total loss: \u001b[1m\u001b[32m1.56957\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3967 | loss: 1.56957 - acc: 0.6642 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3968  | total loss: \u001b[1m\u001b[32m1.77609\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3968 | loss: 1.77609 - acc: 0.6049 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3969  | total loss: \u001b[1m\u001b[32m1.64654\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3969 | loss: 1.64654 - acc: 0.6444 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3970  | total loss: \u001b[1m\u001b[32m1.79012\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3970 | loss: 1.79012 - acc: 0.5871 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3971  | total loss: \u001b[1m\u001b[32m1.66019\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3971 | loss: 1.66019 - acc: 0.6284 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3972  | total loss: \u001b[1m\u001b[32m1.54352\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3972 | loss: 1.54352 - acc: 0.6656 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3973  | total loss: \u001b[1m\u001b[32m1.43814\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3973 | loss: 1.43814 - acc: 0.6990 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3974  | total loss: \u001b[1m\u001b[32m1.34233\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3974 | loss: 1.34233 - acc: 0.7291 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3975  | total loss: \u001b[1m\u001b[32m1.25466\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3975 | loss: 1.25466 - acc: 0.7562 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3976  | total loss: \u001b[1m\u001b[32m1.17389\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3976 | loss: 1.17389 - acc: 0.7806 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3977  | total loss: \u001b[1m\u001b[32m1.09902\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3977 | loss: 1.09902 - acc: 0.8025 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3978  | total loss: \u001b[1m\u001b[32m1.02920\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3978 | loss: 1.02920 - acc: 0.8223 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3979  | total loss: \u001b[1m\u001b[32m0.96379\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3979 | loss: 0.96379 - acc: 0.8400 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3980  | total loss: \u001b[1m\u001b[32m0.90224\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3980 | loss: 0.90224 - acc: 0.8560 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3981  | total loss: \u001b[1m\u001b[32m0.84417\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3981 | loss: 0.84417 - acc: 0.8704 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3982  | total loss: \u001b[1m\u001b[32m1.10595\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3982 | loss: 1.10595 - acc: 0.7977 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3983  | total loss: \u001b[1m\u001b[32m1.02336\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3983 | loss: 1.02336 - acc: 0.8179 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3984  | total loss: \u001b[1m\u001b[32m1.28336\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3984 | loss: 1.28336 - acc: 0.7504 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3985  | total loss: \u001b[1m\u001b[32m1.18086\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3985 | loss: 1.18086 - acc: 0.7754 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3986  | total loss: \u001b[1m\u001b[32m1.08785\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3986 | loss: 1.08785 - acc: 0.7978 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3987  | total loss: \u001b[1m\u001b[32m1.00323\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3987 | loss: 1.00323 - acc: 0.8180 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3988  | total loss: \u001b[1m\u001b[32m1.28431\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3988 | loss: 1.28431 - acc: 0.7505 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3989  | total loss: \u001b[1m\u001b[32m1.17896\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3989 | loss: 1.17896 - acc: 0.7755 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3990  | total loss: \u001b[1m\u001b[32m1.46428\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3990 | loss: 1.46428 - acc: 0.7051 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3991  | total loss: \u001b[1m\u001b[32m1.34121\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3991 | loss: 1.34121 - acc: 0.7346 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3992  | total loss: \u001b[1m\u001b[32m1.23075\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3992 | loss: 1.23075 - acc: 0.7611 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3993  | total loss: \u001b[1m\u001b[32m1.13137\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3993 | loss: 1.13137 - acc: 0.7850 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3994  | total loss: \u001b[1m\u001b[32m1.46661\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3994 | loss: 1.46661 - acc: 0.7065 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3995  | total loss: \u001b[1m\u001b[32m1.34414\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3995 | loss: 1.34414 - acc: 0.7358 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3996  | total loss: \u001b[1m\u001b[32m1.56335\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 3996 | loss: 1.56335 - acc: 0.6765 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3997  | total loss: \u001b[1m\u001b[32m1.43267\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3997 | loss: 1.43267 - acc: 0.7089 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3998  | total loss: \u001b[1m\u001b[32m1.70466\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3998 | loss: 1.70466 - acc: 0.6380 -- iter: 14/14\n",
      "--\n",
      "Training Step: 3999  | total loss: \u001b[1m\u001b[32m1.56225\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 3999 | loss: 1.56225 - acc: 0.6742 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4000  | total loss: \u001b[1m\u001b[32m1.78915\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4000 | loss: 1.78915 - acc: 0.6139 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4001  | total loss: \u001b[1m\u001b[32m1.64163\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4001 | loss: 1.64163 - acc: 0.6525 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4002  | total loss: \u001b[1m\u001b[32m1.87912\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4002 | loss: 1.87912 - acc: 0.5873 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4003  | total loss: \u001b[1m\u001b[32m1.72683\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4003 | loss: 1.72683 - acc: 0.6286 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4004  | total loss: \u001b[1m\u001b[32m1.88220\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4004 | loss: 1.88220 - acc: 0.5800 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4005  | total loss: \u001b[1m\u001b[32m1.73445\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4005 | loss: 1.73445 - acc: 0.6220 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4006  | total loss: \u001b[1m\u001b[32m1.90283\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4006 | loss: 1.90283 - acc: 0.5669 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4007  | total loss: \u001b[1m\u001b[32m1.75806\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4007 | loss: 1.75806 - acc: 0.6102 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4008  | total loss: \u001b[1m\u001b[32m1.94662\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4008 | loss: 1.94662 - acc: 0.5492 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4009  | total loss: \u001b[1m\u001b[32m1.80273\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4009 | loss: 1.80273 - acc: 0.5943 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4010  | total loss: \u001b[1m\u001b[32m1.99007\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4010 | loss: 1.99007 - acc: 0.5349 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4011  | total loss: \u001b[1m\u001b[32m1.84725\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4011 | loss: 1.84725 - acc: 0.5814 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4012  | total loss: \u001b[1m\u001b[32m1.96075\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4012 | loss: 1.96075 - acc: 0.5447 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4013  | total loss: \u001b[1m\u001b[32m1.82589\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4013 | loss: 1.82589 - acc: 0.5902 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4014  | total loss: \u001b[1m\u001b[32m1.97557\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4014 | loss: 1.97557 - acc: 0.5383 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4015  | total loss: \u001b[1m\u001b[32m1.84342\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4015 | loss: 1.84342 - acc: 0.5845 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4016  | total loss: \u001b[1m\u001b[32m1.97833\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4016 | loss: 1.97833 - acc: 0.5403 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4017  | total loss: \u001b[1m\u001b[32m1.84915\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4017 | loss: 1.84915 - acc: 0.5863 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4018  | total loss: \u001b[1m\u001b[32m1.95923\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4018 | loss: 1.95923 - acc: 0.5420 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4019  | total loss: \u001b[1m\u001b[32m1.83425\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4019 | loss: 1.83425 - acc: 0.5878 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4020  | total loss: \u001b[1m\u001b[32m1.93645\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4020 | loss: 1.93645 - acc: 0.5504 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4021  | total loss: \u001b[1m\u001b[32m1.81511\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4021 | loss: 1.81511 - acc: 0.5954 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4022  | total loss: \u001b[1m\u001b[32m1.92833\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4022 | loss: 1.92833 - acc: 0.5573 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4023  | total loss: \u001b[1m\u001b[32m1.80828\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4023 | loss: 1.80828 - acc: 0.6015 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4024  | total loss: \u001b[1m\u001b[32m1.93548\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4024 | loss: 1.93548 - acc: 0.5485 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4025  | total loss: \u001b[1m\u001b[32m1.81472\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4025 | loss: 1.81472 - acc: 0.5937 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4026  | total loss: \u001b[1m\u001b[32m1.93552\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4026 | loss: 1.93552 - acc: 0.5414 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4027  | total loss: \u001b[1m\u001b[32m1.81443\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4027 | loss: 1.81443 - acc: 0.5873 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4028  | total loss: \u001b[1m\u001b[32m1.94486\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4028 | loss: 1.94486 - acc: 0.5286 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4029  | total loss: \u001b[1m\u001b[32m1.82238\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4029 | loss: 1.82238 - acc: 0.5757 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4030  | total loss: \u001b[1m\u001b[32m1.71149\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4030 | loss: 1.71149 - acc: 0.6181 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4031  | total loss: \u001b[1m\u001b[32m1.61025\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4031 | loss: 1.61025 - acc: 0.6563 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4032  | total loss: \u001b[1m\u001b[32m1.78014\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4032 | loss: 1.78014 - acc: 0.5907 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4033  | total loss: \u001b[1m\u001b[32m1.66885\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4033 | loss: 1.66885 - acc: 0.6316 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4034  | total loss: \u001b[1m\u001b[32m1.81808\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4034 | loss: 1.81808 - acc: 0.5756 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4035  | total loss: \u001b[1m\u001b[32m1.70042\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4035 | loss: 1.70042 - acc: 0.6180 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4036  | total loss: \u001b[1m\u001b[32m1.89137\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4036 | loss: 1.89137 - acc: 0.5562 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4037  | total loss: \u001b[1m\u001b[32m1.76455\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4037 | loss: 1.76455 - acc: 0.6006 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4038  | total loss: \u001b[1m\u001b[32m1.89141\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4038 | loss: 1.89141 - acc: 0.5477 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4039  | total loss: \u001b[1m\u001b[32m1.76339\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4039 | loss: 1.76339 - acc: 0.5929 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4040  | total loss: \u001b[1m\u001b[32m1.90604\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4040 | loss: 1.90604 - acc: 0.5479 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4041  | total loss: \u001b[1m\u001b[32m1.77563\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4041 | loss: 1.77563 - acc: 0.5931 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4042  | total loss: \u001b[1m\u001b[32m1.93004\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4042 | loss: 1.93004 - acc: 0.5410 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4043  | total loss: \u001b[1m\u001b[32m1.79673\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4043 | loss: 1.79673 - acc: 0.5869 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4044  | total loss: \u001b[1m\u001b[32m1.95850\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 4044 | loss: 1.95850 - acc: 0.5282 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4045  | total loss: \u001b[1m\u001b[32m1.82242\u001b[0m\u001b[0m | time: 0.018s\n",
      "| Adam | epoch: 4045 | loss: 1.82242 - acc: 0.5754 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4046  | total loss: \u001b[1m\u001b[32m1.96313\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4046 | loss: 1.96313 - acc: 0.5250 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4047  | total loss: \u001b[1m\u001b[32m1.82704\u001b[0m\u001b[0m | time: 0.024s\n",
      "| Adam | epoch: 4047 | loss: 1.82704 - acc: 0.5725 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4048  | total loss: \u001b[1m\u001b[32m1.95883\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4048 | loss: 1.95883 - acc: 0.5224 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4049  | total loss: \u001b[1m\u001b[32m1.82365\u001b[0m\u001b[0m | time: 0.015s\n",
      "| Adam | epoch: 4049 | loss: 1.82365 - acc: 0.5701 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4050  | total loss: \u001b[1m\u001b[32m1.95031\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4050 | loss: 1.95031 - acc: 0.5203 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4051  | total loss: \u001b[1m\u001b[32m1.81636\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4051 | loss: 1.81636 - acc: 0.5682 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4052  | total loss: \u001b[1m\u001b[32m1.98930\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4052 | loss: 1.98930 - acc: 0.5114 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4053  | total loss: \u001b[1m\u001b[32m1.85190\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4053 | loss: 1.85190 - acc: 0.5603 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4054  | total loss: \u001b[1m\u001b[32m1.98921\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4054 | loss: 1.98921 - acc: 0.5114 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4055  | total loss: \u001b[1m\u001b[32m1.85243\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4055 | loss: 1.85243 - acc: 0.5602 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4056  | total loss: \u001b[1m\u001b[32m1.72925\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4056 | loss: 1.72925 - acc: 0.6042 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4057  | total loss: \u001b[1m\u001b[32m1.61760\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4057 | loss: 1.61760 - acc: 0.6438 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4058  | total loss: \u001b[1m\u001b[32m1.73692\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4058 | loss: 1.73692 - acc: 0.6008 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4059  | total loss: \u001b[1m\u001b[32m1.62234\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4059 | loss: 1.62234 - acc: 0.6408 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4060  | total loss: \u001b[1m\u001b[32m1.75758\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4060 | loss: 1.75758 - acc: 0.5910 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4061  | total loss: \u001b[1m\u001b[32m1.63916\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4061 | loss: 1.63916 - acc: 0.6319 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4062  | total loss: \u001b[1m\u001b[32m1.80025\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4062 | loss: 1.80025 - acc: 0.5830 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4063  | total loss: \u001b[1m\u001b[32m1.67630\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4063 | loss: 1.67630 - acc: 0.6247 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4064  | total loss: \u001b[1m\u001b[32m1.80215\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4064 | loss: 1.80215 - acc: 0.5836 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4065  | total loss: \u001b[1m\u001b[32m1.67708\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4065 | loss: 1.67708 - acc: 0.6253 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4066  | total loss: \u001b[1m\u001b[32m1.85556\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4066 | loss: 1.85556 - acc: 0.5627 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4067  | total loss: \u001b[1m\u001b[32m1.72481\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4067 | loss: 1.72481 - acc: 0.6065 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4068  | total loss: \u001b[1m\u001b[32m1.88363\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4068 | loss: 1.88363 - acc: 0.5458 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4069  | total loss: \u001b[1m\u001b[32m1.75038\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4069 | loss: 1.75038 - acc: 0.5912 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4070  | total loss: \u001b[1m\u001b[32m1.63034\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4070 | loss: 1.63034 - acc: 0.6321 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4071  | total loss: \u001b[1m\u001b[32m1.52154\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4071 | loss: 1.52154 - acc: 0.6689 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4072  | total loss: \u001b[1m\u001b[32m1.42229\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4072 | loss: 1.42229 - acc: 0.7020 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4073  | total loss: \u001b[1m\u001b[32m1.33116\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4073 | loss: 1.33116 - acc: 0.7318 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4074  | total loss: \u001b[1m\u001b[32m1.56539\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4074 | loss: 1.56539 - acc: 0.6586 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4075  | total loss: \u001b[1m\u001b[32m1.45683\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4075 | loss: 1.45683 - acc: 0.6928 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4076  | total loss: \u001b[1m\u001b[32m1.70007\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4076 | loss: 1.70007 - acc: 0.6235 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4077  | total loss: \u001b[1m\u001b[32m1.57655\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4077 | loss: 1.57655 - acc: 0.6611 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4078  | total loss: \u001b[1m\u001b[32m1.79050\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4078 | loss: 1.79050 - acc: 0.6022 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4079  | total loss: \u001b[1m\u001b[32m1.65780\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4079 | loss: 1.65780 - acc: 0.6420 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4080  | total loss: \u001b[1m\u001b[32m1.86432\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4080 | loss: 1.86432 - acc: 0.5778 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4081  | total loss: \u001b[1m\u001b[32m1.72521\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4081 | loss: 1.72521 - acc: 0.6200 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4082  | total loss: \u001b[1m\u001b[32m1.92342\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4082 | loss: 1.92342 - acc: 0.5580 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4083  | total loss: \u001b[1m\u001b[32m1.78023\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4083 | loss: 1.78023 - acc: 0.6022 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4084  | total loss: \u001b[1m\u001b[32m1.92318\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4084 | loss: 1.92318 - acc: 0.5563 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4085  | total loss: \u001b[1m\u001b[32m1.78208\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4085 | loss: 1.78208 - acc: 0.6006 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4086  | total loss: \u001b[1m\u001b[32m1.65571\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4086 | loss: 1.65571 - acc: 0.6406 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4087  | total loss: \u001b[1m\u001b[32m1.54190\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4087 | loss: 1.54190 - acc: 0.6765 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4088  | total loss: \u001b[1m\u001b[32m1.75102\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4088 | loss: 1.75102 - acc: 0.6089 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4089  | total loss: \u001b[1m\u001b[32m1.62732\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4089 | loss: 1.62732 - acc: 0.6480 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4090  | total loss: \u001b[1m\u001b[32m1.78407\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4090 | loss: 1.78407 - acc: 0.5975 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4091  | total loss: \u001b[1m\u001b[32m1.65723\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4091 | loss: 1.65723 - acc: 0.6377 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4092  | total loss: \u001b[1m\u001b[32m1.82037\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4092 | loss: 1.82037 - acc: 0.5811 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4093  | total loss: \u001b[1m\u001b[32m1.69040\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4093 | loss: 1.69040 - acc: 0.6230 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4094  | total loss: \u001b[1m\u001b[32m1.57343\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4094 | loss: 1.57343 - acc: 0.6607 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4095  | total loss: \u001b[1m\u001b[32m1.46752\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4095 | loss: 1.46752 - acc: 0.6946 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4096  | total loss: \u001b[1m\u001b[32m1.67188\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4096 | loss: 1.67188 - acc: 0.6323 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4097  | total loss: \u001b[1m\u001b[32m1.55484\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4097 | loss: 1.55484 - acc: 0.6691 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4098  | total loss: \u001b[1m\u001b[32m1.77125\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4098 | loss: 1.77125 - acc: 0.6022 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4099  | total loss: \u001b[1m\u001b[32m1.64398\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4099 | loss: 1.64398 - acc: 0.6419 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4100  | total loss: \u001b[1m\u001b[32m1.79861\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4100 | loss: 1.79861 - acc: 0.5920 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4101  | total loss: \u001b[1m\u001b[32m1.66893\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4101 | loss: 1.66893 - acc: 0.6328 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4102  | total loss: \u001b[1m\u001b[32m1.83272\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4102 | loss: 1.83272 - acc: 0.5767 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4103  | total loss: \u001b[1m\u001b[32m1.70030\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4103 | loss: 1.70030 - acc: 0.6190 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4104  | total loss: \u001b[1m\u001b[32m1.58120\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4104 | loss: 1.58120 - acc: 0.6571 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4105  | total loss: \u001b[1m\u001b[32m1.47347\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4105 | loss: 1.47347 - acc: 0.6914 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4106  | total loss: \u001b[1m\u001b[32m1.64991\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4106 | loss: 1.64991 - acc: 0.6294 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4107  | total loss: \u001b[1m\u001b[32m1.53407\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4107 | loss: 1.53407 - acc: 0.6665 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4108  | total loss: \u001b[1m\u001b[32m1.71931\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4108 | loss: 1.71931 - acc: 0.6070 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4109  | total loss: \u001b[1m\u001b[32m1.59609\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4109 | loss: 1.59609 - acc: 0.6463 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4110  | total loss: \u001b[1m\u001b[32m1.48487\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4110 | loss: 1.48487 - acc: 0.6816 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4111  | total loss: \u001b[1m\u001b[32m1.38391\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4111 | loss: 1.38391 - acc: 0.7135 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4112  | total loss: \u001b[1m\u001b[32m1.58641\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4112 | loss: 1.58641 - acc: 0.6493 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4113  | total loss: \u001b[1m\u001b[32m1.47368\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4113 | loss: 1.47368 - acc: 0.6843 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4114  | total loss: \u001b[1m\u001b[32m1.67304\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4114 | loss: 1.67304 - acc: 0.6231 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4115  | total loss: \u001b[1m\u001b[32m1.55103\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4115 | loss: 1.55103 - acc: 0.6607 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4116  | total loss: \u001b[1m\u001b[32m1.74671\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4116 | loss: 1.74671 - acc: 0.6018 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4117  | total loss: \u001b[1m\u001b[32m1.61755\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4117 | loss: 1.61755 - acc: 0.6416 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4118  | total loss: \u001b[1m\u001b[32m1.80280\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4118 | loss: 1.80280 - acc: 0.5846 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4119  | total loss: \u001b[1m\u001b[32m1.66896\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4119 | loss: 1.66896 - acc: 0.6262 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4120  | total loss: \u001b[1m\u001b[32m1.54880\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4120 | loss: 1.54880 - acc: 0.6635 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4121  | total loss: \u001b[1m\u001b[32m1.44035\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4121 | loss: 1.44035 - acc: 0.6972 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4122  | total loss: \u001b[1m\u001b[32m1.63092\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4122 | loss: 1.63092 - acc: 0.6346 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4123  | total loss: \u001b[1m\u001b[32m1.51353\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4123 | loss: 1.51353 - acc: 0.6711 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4124  | total loss: \u001b[1m\u001b[32m1.66976\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4124 | loss: 1.66976 - acc: 0.6183 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4125  | total loss: \u001b[1m\u001b[32m1.54845\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4125 | loss: 1.54845 - acc: 0.6565 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4126  | total loss: \u001b[1m\u001b[32m1.73965\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4126 | loss: 1.73965 - acc: 0.5980 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4127  | total loss: \u001b[1m\u001b[32m1.61199\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4127 | loss: 1.61199 - acc: 0.6382 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4128  | total loss: \u001b[1m\u001b[32m1.79698\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4128 | loss: 1.79698 - acc: 0.5815 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4129  | total loss: \u001b[1m\u001b[32m1.66492\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4129 | loss: 1.66492 - acc: 0.6234 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4130  | total loss: \u001b[1m\u001b[32m1.54651\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4130 | loss: 1.54651 - acc: 0.6610 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4131  | total loss: \u001b[1m\u001b[32m1.43976\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4131 | loss: 1.43976 - acc: 0.6949 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4132  | total loss: \u001b[1m\u001b[32m1.63987\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4132 | loss: 1.63987 - acc: 0.6326 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4133  | total loss: \u001b[1m\u001b[32m1.52326\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4133 | loss: 1.52326 - acc: 0.6693 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4134  | total loss: \u001b[1m\u001b[32m1.68601\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4134 | loss: 1.68601 - acc: 0.6167 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4135  | total loss: \u001b[1m\u001b[32m1.56490\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4135 | loss: 1.56490 - acc: 0.6550 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4136  | total loss: \u001b[1m\u001b[32m1.74477\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4136 | loss: 1.74477 - acc: 0.6038 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4137  | total loss: \u001b[1m\u001b[32m1.61841\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4137 | loss: 1.61841 - acc: 0.6434 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4138  | total loss: \u001b[1m\u001b[32m1.78895\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4138 | loss: 1.78895 - acc: 0.5862 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4139  | total loss: \u001b[1m\u001b[32m1.65932\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4139 | loss: 1.65932 - acc: 0.6276 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4140  | total loss: \u001b[1m\u001b[32m1.80836\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4140 | loss: 1.80836 - acc: 0.5720 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4141  | total loss: \u001b[1m\u001b[32m1.67833\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4141 | loss: 1.67833 - acc: 0.6148 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4142  | total loss: \u001b[1m\u001b[32m1.84316\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4142 | loss: 1.84316 - acc: 0.5676 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4143  | total loss: \u001b[1m\u001b[32m1.71133\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4143 | loss: 1.71133 - acc: 0.6108 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4144  | total loss: \u001b[1m\u001b[32m1.85442\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4144 | loss: 1.85442 - acc: 0.5640 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4145  | total loss: \u001b[1m\u001b[32m1.72307\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4145 | loss: 1.72307 - acc: 0.6076 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4146  | total loss: \u001b[1m\u001b[32m1.60522\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4146 | loss: 1.60522 - acc: 0.6469 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4147  | total loss: \u001b[1m\u001b[32m1.49878\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4147 | loss: 1.49878 - acc: 0.6822 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4148  | total loss: \u001b[1m\u001b[32m1.63649\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4148 | loss: 1.63649 - acc: 0.6354 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4149  | total loss: \u001b[1m\u001b[32m1.52567\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4149 | loss: 1.52567 - acc: 0.6718 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4150  | total loss: \u001b[1m\u001b[32m1.68357\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4150 | loss: 1.68357 - acc: 0.6189 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4151  | total loss: \u001b[1m\u001b[32m1.56729\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4151 | loss: 1.56729 - acc: 0.6571 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4152  | total loss: \u001b[1m\u001b[32m1.46207\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4152 | loss: 1.46207 - acc: 0.6913 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4153  | total loss: \u001b[1m\u001b[32m1.36621\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4153 | loss: 1.36621 - acc: 0.7222 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4154  | total loss: \u001b[1m\u001b[32m1.58778\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4154 | loss: 1.58778 - acc: 0.6500 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4155  | total loss: \u001b[1m\u001b[32m1.47720\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4155 | loss: 1.47720 - acc: 0.6850 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4156  | total loss: \u001b[1m\u001b[32m1.70412\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4156 | loss: 1.70412 - acc: 0.6165 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4157  | total loss: \u001b[1m\u001b[32m1.58107\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4157 | loss: 1.58107 - acc: 0.6548 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4158  | total loss: \u001b[1m\u001b[32m1.46990\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4158 | loss: 1.46990 - acc: 0.6894 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4159  | total loss: \u001b[1m\u001b[32m1.36887\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4159 | loss: 1.36887 - acc: 0.7204 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4160  | total loss: \u001b[1m\u001b[32m1.62066\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4160 | loss: 1.62066 - acc: 0.6484 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4161  | total loss: \u001b[1m\u001b[32m1.50294\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4161 | loss: 1.50294 - acc: 0.6835 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4162  | total loss: \u001b[1m\u001b[32m1.71102\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4162 | loss: 1.71102 - acc: 0.6152 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4163  | total loss: \u001b[1m\u001b[32m1.58392\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4163 | loss: 1.58392 - acc: 0.6537 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4164  | total loss: \u001b[1m\u001b[32m1.74822\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4164 | loss: 1.74822 - acc: 0.6026 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4165  | total loss: \u001b[1m\u001b[32m1.61778\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4165 | loss: 1.61778 - acc: 0.6423 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4166  | total loss: \u001b[1m\u001b[32m1.50038\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4166 | loss: 1.50038 - acc: 0.6781 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4167  | total loss: \u001b[1m\u001b[32m1.39417\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4167 | loss: 1.39417 - acc: 0.7103 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4168  | total loss: \u001b[1m\u001b[32m1.59208\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4168 | loss: 1.59208 - acc: 0.6535 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4169  | total loss: \u001b[1m\u001b[32m1.47563\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4169 | loss: 1.47563 - acc: 0.6882 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4170  | total loss: \u001b[1m\u001b[32m1.71665\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4170 | loss: 1.71665 - acc: 0.6194 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4171  | total loss: \u001b[1m\u001b[32m1.58776\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4171 | loss: 1.58776 - acc: 0.6574 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4172  | total loss: \u001b[1m\u001b[32m1.77023\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4172 | loss: 1.77023 - acc: 0.5988 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4173  | total loss: \u001b[1m\u001b[32m1.63692\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4173 | loss: 1.63692 - acc: 0.6389 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4174  | total loss: \u001b[1m\u001b[32m1.78490\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4174 | loss: 1.78490 - acc: 0.5893 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4175  | total loss: \u001b[1m\u001b[32m1.65154\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4175 | loss: 1.65154 - acc: 0.6304 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4176  | total loss: \u001b[1m\u001b[32m1.82227\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4176 | loss: 1.82227 - acc: 0.5745 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4177  | total loss: \u001b[1m\u001b[32m1.68681\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4177 | loss: 1.68681 - acc: 0.6171 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4178  | total loss: \u001b[1m\u001b[32m1.86413\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4178 | loss: 1.86413 - acc: 0.5554 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4179  | total loss: \u001b[1m\u001b[32m1.72648\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4179 | loss: 1.72648 - acc: 0.5998 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4180  | total loss: \u001b[1m\u001b[32m1.90357\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4180 | loss: 1.90357 - acc: 0.5398 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4181  | total loss: \u001b[1m\u001b[32m1.76433\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4181 | loss: 1.76433 - acc: 0.5859 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4182  | total loss: \u001b[1m\u001b[32m1.63988\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4182 | loss: 1.63988 - acc: 0.6273 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4183  | total loss: \u001b[1m\u001b[32m1.52800\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4183 | loss: 1.52800 - acc: 0.6645 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4184  | total loss: \u001b[1m\u001b[32m1.70627\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4184 | loss: 1.70627 - acc: 0.6052 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4185  | total loss: \u001b[1m\u001b[32m1.58771\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4185 | loss: 1.58771 - acc: 0.6447 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4186  | total loss: \u001b[1m\u001b[32m1.77199\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4186 | loss: 1.77199 - acc: 0.5802 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4187  | total loss: \u001b[1m\u001b[32m1.64742\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4187 | loss: 1.64742 - acc: 0.6222 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4188  | total loss: \u001b[1m\u001b[32m1.53534\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4188 | loss: 1.53534 - acc: 0.6600 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4189  | total loss: \u001b[1m\u001b[32m1.43382\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4189 | loss: 1.43382 - acc: 0.6940 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4190  | total loss: \u001b[1m\u001b[32m1.34121\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4190 | loss: 1.34121 - acc: 0.7246 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4191  | total loss: \u001b[1m\u001b[32m1.25612\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4191 | loss: 1.25612 - acc: 0.7521 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4192  | total loss: \u001b[1m\u001b[32m1.47700\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4192 | loss: 1.47700 - acc: 0.6841 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4193  | total loss: \u001b[1m\u001b[32m1.37522\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4193 | loss: 1.37522 - acc: 0.7157 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4194  | total loss: \u001b[1m\u001b[32m1.56753\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4194 | loss: 1.56753 - acc: 0.6584 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4195  | total loss: \u001b[1m\u001b[32m1.45488\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4195 | loss: 1.45488 - acc: 0.6925 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4196  | total loss: \u001b[1m\u001b[32m1.62802\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4196 | loss: 1.62802 - acc: 0.6376 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4197  | total loss: \u001b[1m\u001b[32m1.50852\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4197 | loss: 1.50852 - acc: 0.6738 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4198  | total loss: \u001b[1m\u001b[32m1.72255\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4198 | loss: 1.72255 - acc: 0.6064 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4199  | total loss: \u001b[1m\u001b[32m1.59385\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4199 | loss: 1.59385 - acc: 0.6458 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4200  | total loss: \u001b[1m\u001b[32m1.77026\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4200 | loss: 1.77026 - acc: 0.5884 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4201  | total loss: \u001b[1m\u001b[32m1.63793\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4201 | loss: 1.63793 - acc: 0.6295 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4202  | total loss: \u001b[1m\u001b[32m1.84654\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4202 | loss: 1.84654 - acc: 0.5666 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4203  | total loss: \u001b[1m\u001b[32m1.70847\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4203 | loss: 1.70847 - acc: 0.6099 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4204  | total loss: \u001b[1m\u001b[32m1.58497\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4204 | loss: 1.58497 - acc: 0.6489 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4205  | total loss: \u001b[1m\u001b[32m1.47391\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4205 | loss: 1.47391 - acc: 0.6840 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4206  | total loss: \u001b[1m\u001b[32m1.37343\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4206 | loss: 1.37343 - acc: 0.7156 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4207  | total loss: \u001b[1m\u001b[32m1.28191\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4207 | loss: 1.28191 - acc: 0.7441 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4208  | total loss: \u001b[1m\u001b[32m1.19800\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4208 | loss: 1.19800 - acc: 0.7697 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4209  | total loss: \u001b[1m\u001b[32m1.12054\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4209 | loss: 1.12054 - acc: 0.7927 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4210  | total loss: \u001b[1m\u001b[32m1.41497\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4210 | loss: 1.41497 - acc: 0.7134 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4211  | total loss: \u001b[1m\u001b[32m1.31270\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4211 | loss: 1.31270 - acc: 0.7421 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4212  | total loss: \u001b[1m\u001b[32m1.58045\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4212 | loss: 1.58045 - acc: 0.6679 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4213  | total loss: \u001b[1m\u001b[32m1.46035\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4213 | loss: 1.46035 - acc: 0.7011 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4214  | total loss: \u001b[1m\u001b[32m1.35179\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4214 | loss: 1.35179 - acc: 0.7310 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4215  | total loss: \u001b[1m\u001b[32m1.25324\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4215 | loss: 1.25324 - acc: 0.7579 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4216  | total loss: \u001b[1m\u001b[32m1.16335\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4216 | loss: 1.16335 - acc: 0.7821 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4217  | total loss: \u001b[1m\u001b[32m1.08101\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4217 | loss: 1.08101 - acc: 0.8039 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4218  | total loss: \u001b[1m\u001b[32m1.36539\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4218 | loss: 1.36539 - acc: 0.7235 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4219  | total loss: \u001b[1m\u001b[32m1.26069\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4219 | loss: 1.26069 - acc: 0.7511 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4220  | total loss: \u001b[1m\u001b[32m1.47100\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4220 | loss: 1.47100 - acc: 0.6975 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4221  | total loss: \u001b[1m\u001b[32m1.35509\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4221 | loss: 1.35509 - acc: 0.7277 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4222  | total loss: \u001b[1m\u001b[32m1.60994\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4222 | loss: 1.60994 - acc: 0.6549 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4223  | total loss: \u001b[1m\u001b[32m1.48081\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4223 | loss: 1.48081 - acc: 0.6894 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4224  | total loss: \u001b[1m\u001b[32m1.65289\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4224 | loss: 1.65289 - acc: 0.6419 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4225  | total loss: \u001b[1m\u001b[32m1.52113\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4225 | loss: 1.52113 - acc: 0.6777 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4226  | total loss: \u001b[1m\u001b[32m1.75776\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4226 | loss: 1.75776 - acc: 0.6171 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4227  | total loss: \u001b[1m\u001b[32m1.61792\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4227 | loss: 1.61792 - acc: 0.6554 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4228  | total loss: \u001b[1m\u001b[32m1.81656\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4228 | loss: 1.81656 - acc: 0.5970 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4229  | total loss: \u001b[1m\u001b[32m1.67408\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4229 | loss: 1.67408 - acc: 0.6373 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4230  | total loss: \u001b[1m\u001b[32m1.54731\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4230 | loss: 1.54731 - acc: 0.6736 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4231  | total loss: \u001b[1m\u001b[32m1.43407\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4231 | loss: 1.43407 - acc: 0.7062 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4232  | total loss: \u001b[1m\u001b[32m1.66430\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4232 | loss: 1.66430 - acc: 0.6356 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4233  | total loss: \u001b[1m\u001b[32m1.54093\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4233 | loss: 1.54093 - acc: 0.6720 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4234  | total loss: \u001b[1m\u001b[32m1.76578\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4234 | loss: 1.76578 - acc: 0.6048 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4235  | total loss: \u001b[1m\u001b[32m1.63452\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4235 | loss: 1.63452 - acc: 0.6443 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4236  | total loss: \u001b[1m\u001b[32m1.83591\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4236 | loss: 1.83591 - acc: 0.5799 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4237  | total loss: \u001b[1m\u001b[32m1.70045\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4237 | loss: 1.70045 - acc: 0.6219 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4238  | total loss: \u001b[1m\u001b[32m1.87205\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4238 | loss: 1.87205 - acc: 0.5669 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4239  | total loss: \u001b[1m\u001b[32m1.73607\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4239 | loss: 1.73607 - acc: 0.6102 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4240  | total loss: \u001b[1m\u001b[32m1.90478\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4240 | loss: 1.90478 - acc: 0.5492 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4241  | total loss: \u001b[1m\u001b[32m1.76861\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4241 | loss: 1.76861 - acc: 0.5942 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4242  | total loss: \u001b[1m\u001b[32m1.64713\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4242 | loss: 1.64713 - acc: 0.6348 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4243  | total loss: \u001b[1m\u001b[32m1.53804\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4243 | loss: 1.53804 - acc: 0.6713 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4244  | total loss: \u001b[1m\u001b[32m1.43936\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4244 | loss: 1.43936 - acc: 0.7042 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4245  | total loss: \u001b[1m\u001b[32m1.34939\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4245 | loss: 1.34939 - acc: 0.7338 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4246  | total loss: \u001b[1m\u001b[32m1.26669\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4246 | loss: 1.26669 - acc: 0.7604 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4247  | total loss: \u001b[1m\u001b[32m1.19009\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4247 | loss: 1.19009 - acc: 0.7844 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4248  | total loss: \u001b[1m\u001b[32m1.11860\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4248 | loss: 1.11860 - acc: 0.8059 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4249  | total loss: \u001b[1m\u001b[32m1.05146\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4249 | loss: 1.05146 - acc: 0.8253 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4250  | total loss: \u001b[1m\u001b[32m1.26518\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4250 | loss: 1.26518 - acc: 0.7571 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4251  | total loss: \u001b[1m\u001b[32m1.17868\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4251 | loss: 1.17868 - acc: 0.7814 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4252  | total loss: \u001b[1m\u001b[32m1.42744\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4252 | loss: 1.42744 - acc: 0.7032 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4253  | total loss: \u001b[1m\u001b[32m1.32203\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4253 | loss: 1.32203 - acc: 0.7329 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4254  | total loss: \u001b[1m\u001b[32m1.22610\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4254 | loss: 1.22610 - acc: 0.7596 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4255  | total loss: \u001b[1m\u001b[32m1.13842\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4255 | loss: 1.13842 - acc: 0.7837 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4256  | total loss: \u001b[1m\u001b[32m1.05793\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4256 | loss: 1.05793 - acc: 0.8053 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4257  | total loss: \u001b[1m\u001b[32m0.98374\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4257 | loss: 0.98374 - acc: 0.8248 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4258  | total loss: \u001b[1m\u001b[32m0.91511\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4258 | loss: 0.91511 - acc: 0.8423 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4259  | total loss: \u001b[1m\u001b[32m0.85141\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4259 | loss: 0.85141 - acc: 0.8581 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4260  | total loss: \u001b[1m\u001b[32m1.15686\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4260 | loss: 1.15686 - acc: 0.7723 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4261  | total loss: \u001b[1m\u001b[32m1.06627\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4261 | loss: 1.06627 - acc: 0.7950 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4262  | total loss: \u001b[1m\u001b[32m1.32486\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4262 | loss: 1.32486 - acc: 0.7298 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4263  | total loss: \u001b[1m\u001b[32m1.21643\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4263 | loss: 1.21643 - acc: 0.7568 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4264  | total loss: \u001b[1m\u001b[32m1.52558\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4264 | loss: 1.52558 - acc: 0.6811 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4265  | total loss: \u001b[1m\u001b[32m1.39740\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4265 | loss: 1.39740 - acc: 0.7130 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4266  | total loss: \u001b[1m\u001b[32m1.63672\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4266 | loss: 1.63672 - acc: 0.6489 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4267  | total loss: \u001b[1m\u001b[32m1.49898\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4267 | loss: 1.49898 - acc: 0.6840 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4268  | total loss: \u001b[1m\u001b[32m1.77685\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4268 | loss: 1.77685 - acc: 0.6156 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4269  | total loss: \u001b[1m\u001b[32m1.62778\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4269 | loss: 1.62778 - acc: 0.6540 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4270  | total loss: \u001b[1m\u001b[32m1.85344\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4270 | loss: 1.85344 - acc: 0.5886 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4271  | total loss: \u001b[1m\u001b[32m1.70037\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4271 | loss: 1.70037 - acc: 0.6298 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4272  | total loss: \u001b[1m\u001b[32m1.91439\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4272 | loss: 1.91439 - acc: 0.5668 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4273  | total loss: \u001b[1m\u001b[32m1.75965\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4273 | loss: 1.75965 - acc: 0.6101 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4274  | total loss: \u001b[1m\u001b[32m1.90217\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4274 | loss: 1.90217 - acc: 0.5634 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4275  | total loss: \u001b[1m\u001b[32m1.75337\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4275 | loss: 1.75337 - acc: 0.6070 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4276  | total loss: \u001b[1m\u001b[32m1.62144\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4276 | loss: 1.62144 - acc: 0.6463 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4277  | total loss: \u001b[1m\u001b[32m1.50397\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4277 | loss: 1.50397 - acc: 0.6817 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4278  | total loss: \u001b[1m\u001b[32m1.39882\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4278 | loss: 1.39882 - acc: 0.7135 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4279  | total loss: \u001b[1m\u001b[32m1.30413\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4279 | loss: 1.30413 - acc: 0.7422 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4280  | total loss: \u001b[1m\u001b[32m1.53678\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4280 | loss: 1.53678 - acc: 0.6751 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4281  | total loss: \u001b[1m\u001b[32m1.42797\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4281 | loss: 1.42797 - acc: 0.7076 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4282  | total loss: \u001b[1m\u001b[32m1.32977\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4282 | loss: 1.32977 - acc: 0.7368 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4283  | total loss: \u001b[1m\u001b[32m1.24060\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4283 | loss: 1.24060 - acc: 0.7632 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4284  | total loss: \u001b[1m\u001b[32m1.46369\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4284 | loss: 1.46369 - acc: 0.6940 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4285  | total loss: \u001b[1m\u001b[32m1.35985\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4285 | loss: 1.35985 - acc: 0.7246 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4286  | total loss: \u001b[1m\u001b[32m1.55980\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4286 | loss: 1.55980 - acc: 0.6664 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4287  | total loss: \u001b[1m\u001b[32m1.44615\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4287 | loss: 1.44615 - acc: 0.6998 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4288  | total loss: \u001b[1m\u001b[32m1.68828\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4288 | loss: 1.68828 - acc: 0.6298 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4289  | total loss: \u001b[1m\u001b[32m1.56245\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4289 | loss: 1.56245 - acc: 0.6668 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4290  | total loss: \u001b[1m\u001b[32m1.76204\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4290 | loss: 1.76204 - acc: 0.6073 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4291  | total loss: \u001b[1m\u001b[32m1.63039\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4291 | loss: 1.63039 - acc: 0.6465 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4292  | total loss: \u001b[1m\u001b[32m1.83654\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4292 | loss: 1.83654 - acc: 0.5890 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4293  | total loss: \u001b[1m\u001b[32m1.69952\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4293 | loss: 1.69952 - acc: 0.6301 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4294  | total loss: \u001b[1m\u001b[32m1.84548\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4294 | loss: 1.84548 - acc: 0.5885 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4295  | total loss: \u001b[1m\u001b[32m1.70965\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4295 | loss: 1.70965 - acc: 0.6297 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4296  | total loss: \u001b[1m\u001b[32m1.87541\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4296 | loss: 1.87541 - acc: 0.5739 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4297  | total loss: \u001b[1m\u001b[32m1.73862\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4297 | loss: 1.73862 - acc: 0.6165 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4298  | total loss: \u001b[1m\u001b[32m1.91718\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4298 | loss: 1.91718 - acc: 0.5548 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4299  | total loss: \u001b[1m\u001b[32m1.77851\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4299 | loss: 1.77851 - acc: 0.5993 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4300  | total loss: \u001b[1m\u001b[32m1.91423\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4300 | loss: 1.91423 - acc: 0.5466 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4301  | total loss: \u001b[1m\u001b[32m1.77831\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4301 | loss: 1.77831 - acc: 0.5919 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4302  | total loss: \u001b[1m\u001b[32m1.90252\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4302 | loss: 1.90252 - acc: 0.5470 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4303  | total loss: \u001b[1m\u001b[32m1.77003\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4303 | loss: 1.77003 - acc: 0.5923 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4304  | total loss: \u001b[1m\u001b[32m1.92001\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4304 | loss: 1.92001 - acc: 0.5402 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4305  | total loss: \u001b[1m\u001b[32m1.78788\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4305 | loss: 1.78788 - acc: 0.5862 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4306  | total loss: \u001b[1m\u001b[32m1.92440\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4306 | loss: 1.92440 - acc: 0.5347 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4307  | total loss: \u001b[1m\u001b[32m1.79382\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4307 | loss: 1.79382 - acc: 0.5812 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4308  | total loss: \u001b[1m\u001b[32m1.91056\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4308 | loss: 1.91056 - acc: 0.5374 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4309  | total loss: \u001b[1m\u001b[32m1.78299\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4309 | loss: 1.78299 - acc: 0.5837 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4310  | total loss: \u001b[1m\u001b[32m1.90895\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4310 | loss: 1.90895 - acc: 0.5396 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4311  | total loss: \u001b[1m\u001b[32m1.78271\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4311 | loss: 1.78271 - acc: 0.5856 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4312  | total loss: \u001b[1m\u001b[32m1.91062\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4312 | loss: 1.91062 - acc: 0.5271 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4313  | total loss: \u001b[1m\u001b[32m1.78513\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4313 | loss: 1.78513 - acc: 0.5744 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4314  | total loss: \u001b[1m\u001b[32m1.94684\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4314 | loss: 1.94684 - acc: 0.5241 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4315  | total loss: \u001b[1m\u001b[32m1.81864\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4315 | loss: 1.81864 - acc: 0.5717 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4316  | total loss: \u001b[1m\u001b[32m1.70330\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4316 | loss: 1.70330 - acc: 0.6145 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4317  | total loss: \u001b[1m\u001b[32m1.59871\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4317 | loss: 1.59871 - acc: 0.6530 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4318  | total loss: \u001b[1m\u001b[32m1.71061\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4318 | loss: 1.71061 - acc: 0.6092 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4319  | total loss: \u001b[1m\u001b[32m1.60307\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4319 | loss: 1.60307 - acc: 0.6482 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4320  | total loss: \u001b[1m\u001b[32m1.79592\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4320 | loss: 1.79592 - acc: 0.5834 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4321  | total loss: \u001b[1m\u001b[32m1.67806\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4321 | loss: 1.67806 - acc: 0.6251 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4322  | total loss: \u001b[1m\u001b[32m1.57092\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4322 | loss: 1.57092 - acc: 0.6626 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4323  | total loss: \u001b[1m\u001b[32m1.47279\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4323 | loss: 1.47279 - acc: 0.6963 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4324  | total loss: \u001b[1m\u001b[32m1.68715\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4324 | loss: 1.68715 - acc: 0.6267 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4325  | total loss: \u001b[1m\u001b[32m1.57427\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4325 | loss: 1.57427 - acc: 0.6640 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4326  | total loss: \u001b[1m\u001b[32m1.47117\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4326 | loss: 1.47117 - acc: 0.6976 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4327  | total loss: \u001b[1m\u001b[32m1.37640\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4327 | loss: 1.37640 - acc: 0.7279 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4328  | total loss: \u001b[1m\u001b[32m1.28872\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4328 | loss: 1.28872 - acc: 0.7551 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4329  | total loss: \u001b[1m\u001b[32m1.20713\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4329 | loss: 1.20713 - acc: 0.7796 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4330  | total loss: \u001b[1m\u001b[32m1.13080\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4330 | loss: 1.13080 - acc: 0.8016 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4331  | total loss: \u001b[1m\u001b[32m1.05909\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4331 | loss: 1.05909 - acc: 0.8214 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4332  | total loss: \u001b[1m\u001b[32m0.99148\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4332 | loss: 0.99148 - acc: 0.8393 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4333  | total loss: \u001b[1m\u001b[32m0.92758\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4333 | loss: 0.92758 - acc: 0.8554 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4334  | total loss: \u001b[1m\u001b[32m1.22807\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4334 | loss: 1.22807 - acc: 0.7698 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4335  | total loss: \u001b[1m\u001b[32m1.13594\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4335 | loss: 1.13594 - acc: 0.7928 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4336  | total loss: \u001b[1m\u001b[32m1.41911\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4336 | loss: 1.41911 - acc: 0.7136 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4337  | total loss: \u001b[1m\u001b[32m1.30566\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4337 | loss: 1.30566 - acc: 0.7422 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4338  | total loss: \u001b[1m\u001b[32m1.20282\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4338 | loss: 1.20282 - acc: 0.7680 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4339  | total loss: \u001b[1m\u001b[32m1.10934\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4339 | loss: 1.10934 - acc: 0.7912 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4340  | total loss: \u001b[1m\u001b[32m1.39585\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4340 | loss: 1.39585 - acc: 0.7121 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4341  | total loss: \u001b[1m\u001b[32m1.28193\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4341 | loss: 1.28193 - acc: 0.7409 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4342  | total loss: \u001b[1m\u001b[32m1.56241\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4342 | loss: 1.56241 - acc: 0.6739 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4343  | total loss: \u001b[1m\u001b[32m1.43216\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4343 | loss: 1.43216 - acc: 0.7065 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4344  | total loss: \u001b[1m\u001b[32m1.31527\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4344 | loss: 1.31527 - acc: 0.7359 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4345  | total loss: \u001b[1m\u001b[32m1.21012\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4345 | loss: 1.21012 - acc: 0.7623 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4346  | total loss: \u001b[1m\u001b[32m1.11528\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4346 | loss: 1.11528 - acc: 0.7861 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4347  | total loss: \u001b[1m\u001b[32m1.02948\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4347 | loss: 1.02948 - acc: 0.8075 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4348  | total loss: \u001b[1m\u001b[32m0.95161\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4348 | loss: 0.95161 - acc: 0.8267 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4349  | total loss: \u001b[1m\u001b[32m0.88072\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4349 | loss: 0.88072 - acc: 0.8440 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4350  | total loss: \u001b[1m\u001b[32m1.18353\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4350 | loss: 1.18353 - acc: 0.7668 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4351  | total loss: \u001b[1m\u001b[32m1.08849\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4351 | loss: 1.08849 - acc: 0.7901 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4352  | total loss: \u001b[1m\u001b[32m1.42145\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4352 | loss: 1.42145 - acc: 0.7111 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4353  | total loss: \u001b[1m\u001b[32m1.30312\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4353 | loss: 1.30312 - acc: 0.7400 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4354  | total loss: \u001b[1m\u001b[32m1.56034\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4354 | loss: 1.56034 - acc: 0.6731 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4355  | total loss: \u001b[1m\u001b[32m1.42982\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4355 | loss: 1.42982 - acc: 0.7058 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4356  | total loss: \u001b[1m\u001b[32m1.66623\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4356 | loss: 1.66623 - acc: 0.6424 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4357  | total loss: \u001b[1m\u001b[32m1.52773\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4357 | loss: 1.52773 - acc: 0.6781 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4358  | total loss: \u001b[1m\u001b[32m1.40436\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4358 | loss: 1.40436 - acc: 0.7103 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4359  | total loss: \u001b[1m\u001b[32m1.29419\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4359 | loss: 1.29419 - acc: 0.7393 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4360  | total loss: \u001b[1m\u001b[32m1.51498\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4360 | loss: 1.51498 - acc: 0.6796 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4361  | total loss: \u001b[1m\u001b[32m1.39543\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4361 | loss: 1.39543 - acc: 0.7117 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4362  | total loss: \u001b[1m\u001b[32m1.66606\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4362 | loss: 1.66606 - acc: 0.6405 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4363  | total loss: \u001b[1m\u001b[32m1.53395\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4363 | loss: 1.53395 - acc: 0.6765 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4364  | total loss: \u001b[1m\u001b[32m1.41629\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4364 | loss: 1.41629 - acc: 0.7088 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4365  | total loss: \u001b[1m\u001b[32m1.31109\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4365 | loss: 1.31109 - acc: 0.7379 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4366  | total loss: \u001b[1m\u001b[32m1.57194\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4366 | loss: 1.57194 - acc: 0.6641 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4367  | total loss: \u001b[1m\u001b[32m1.45260\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4367 | loss: 1.45260 - acc: 0.6977 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4368  | total loss: \u001b[1m\u001b[32m1.70523\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4368 | loss: 1.70523 - acc: 0.6280 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4369  | total loss: \u001b[1m\u001b[32m1.57487\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4369 | loss: 1.57487 - acc: 0.6652 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4370  | total loss: \u001b[1m\u001b[32m1.72409\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4370 | loss: 1.72409 - acc: 0.6129 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4371  | total loss: \u001b[1m\u001b[32m1.59458\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4371 | loss: 1.59458 - acc: 0.6516 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4372  | total loss: \u001b[1m\u001b[32m1.76696\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4372 | loss: 1.76696 - acc: 0.6008 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4373  | total loss: \u001b[1m\u001b[32m1.63611\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4373 | loss: 1.63611 - acc: 0.6407 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4374  | total loss: \u001b[1m\u001b[32m1.86799\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4374 | loss: 1.86799 - acc: 0.5766 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4375  | total loss: \u001b[1m\u001b[32m1.73030\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4375 | loss: 1.73030 - acc: 0.6190 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4376  | total loss: \u001b[1m\u001b[32m1.60769\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4376 | loss: 1.60769 - acc: 0.6571 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4377  | total loss: \u001b[1m\u001b[32m1.49790\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4377 | loss: 1.49790 - acc: 0.6914 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4378  | total loss: \u001b[1m\u001b[32m1.70524\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4378 | loss: 1.70524 - acc: 0.6222 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4379  | total loss: \u001b[1m\u001b[32m1.58664\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4379 | loss: 1.58664 - acc: 0.6600 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4380  | total loss: \u001b[1m\u001b[32m1.78738\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4380 | loss: 1.78738 - acc: 0.5940 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4381  | total loss: \u001b[1m\u001b[32m1.66214\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4381 | loss: 1.66214 - acc: 0.6346 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4382  | total loss: \u001b[1m\u001b[32m1.84764\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4382 | loss: 1.84764 - acc: 0.5711 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4383  | total loss: \u001b[1m\u001b[32m1.71829\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4383 | loss: 1.71829 - acc: 0.6140 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4384  | total loss: \u001b[1m\u001b[32m1.89634\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4384 | loss: 1.89634 - acc: 0.5526 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4385  | total loss: \u001b[1m\u001b[32m1.76430\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4385 | loss: 1.76430 - acc: 0.5974 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4386  | total loss: \u001b[1m\u001b[32m1.91499\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4386 | loss: 1.91499 - acc: 0.5448 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4387  | total loss: \u001b[1m\u001b[32m1.78319\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4387 | loss: 1.78319 - acc: 0.5903 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4388  | total loss: \u001b[1m\u001b[32m1.94698\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4388 | loss: 1.94698 - acc: 0.5313 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4389  | total loss: \u001b[1m\u001b[32m1.81394\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4389 | loss: 1.81394 - acc: 0.5781 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4390  | total loss: \u001b[1m\u001b[32m1.93655\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4390 | loss: 1.93655 - acc: 0.5346 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4391  | total loss: \u001b[1m\u001b[32m1.80622\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4391 | loss: 1.80622 - acc: 0.5811 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4392  | total loss: \u001b[1m\u001b[32m1.92897\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4392 | loss: 1.92897 - acc: 0.5373 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4393  | total loss: \u001b[1m\u001b[32m1.80053\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4393 | loss: 1.80053 - acc: 0.5836 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4394  | total loss: \u001b[1m\u001b[32m1.96882\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4394 | loss: 1.96882 - acc: 0.5252 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4395  | total loss: \u001b[1m\u001b[32m1.83732\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4395 | loss: 1.83732 - acc: 0.5727 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4396  | total loss: \u001b[1m\u001b[32m1.98430\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4396 | loss: 1.98430 - acc: 0.5154 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4397  | total loss: \u001b[1m\u001b[32m1.85222\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4397 | loss: 1.85222 - acc: 0.5639 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4398  | total loss: \u001b[1m\u001b[32m1.97959\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4398 | loss: 1.97959 - acc: 0.5075 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4399  | total loss: \u001b[1m\u001b[32m1.84899\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4399 | loss: 1.84899 - acc: 0.5568 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4400  | total loss: \u001b[1m\u001b[32m1.92967\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4400 | loss: 1.92967 - acc: 0.5225 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4401  | total loss: \u001b[1m\u001b[32m1.80465\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4401 | loss: 1.80465 - acc: 0.5703 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4402  | total loss: \u001b[1m\u001b[32m1.69180\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4402 | loss: 1.69180 - acc: 0.6132 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4403  | total loss: \u001b[1m\u001b[32m1.58912\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4403 | loss: 1.58912 - acc: 0.6519 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4404  | total loss: \u001b[1m\u001b[32m1.74807\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4404 | loss: 1.74807 - acc: 0.6010 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4405  | total loss: \u001b[1m\u001b[32m1.63710\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4405 | loss: 1.63710 - acc: 0.6409 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4406  | total loss: \u001b[1m\u001b[32m1.80834\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4406 | loss: 1.80834 - acc: 0.5840 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4407  | total loss: \u001b[1m\u001b[32m1.68928\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4407 | loss: 1.68928 - acc: 0.6256 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4408  | total loss: \u001b[1m\u001b[32m1.81180\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4408 | loss: 1.81180 - acc: 0.5773 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4409  | total loss: \u001b[1m\u001b[32m1.69093\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4409 | loss: 1.69093 - acc: 0.6196 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4410  | total loss: \u001b[1m\u001b[32m1.58121\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4410 | loss: 1.58121 - acc: 0.6576 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4411  | total loss: \u001b[1m\u001b[32m1.48093\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4411 | loss: 1.48093 - acc: 0.6918 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4412  | total loss: \u001b[1m\u001b[32m1.65139\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4412 | loss: 1.65139 - acc: 0.6298 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4413  | total loss: \u001b[1m\u001b[32m1.54110\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4413 | loss: 1.54110 - acc: 0.6668 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4414  | total loss: \u001b[1m\u001b[32m1.74427\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4414 | loss: 1.74427 - acc: 0.6001 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4415  | total loss: \u001b[1m\u001b[32m1.62282\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4415 | loss: 1.62282 - acc: 0.6401 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4416  | total loss: \u001b[1m\u001b[32m1.85448\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4416 | loss: 1.85448 - acc: 0.5761 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4417  | total loss: \u001b[1m\u001b[32m1.72128\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4417 | loss: 1.72128 - acc: 0.6185 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4418  | total loss: \u001b[1m\u001b[32m1.88211\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4418 | loss: 1.88211 - acc: 0.5638 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4419  | total loss: \u001b[1m\u001b[32m1.74622\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4419 | loss: 1.74622 - acc: 0.6074 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4420  | total loss: \u001b[1m\u001b[32m1.92924\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4420 | loss: 1.92924 - acc: 0.5467 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4421  | total loss: \u001b[1m\u001b[32m1.78926\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4421 | loss: 1.78926 - acc: 0.5920 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4422  | total loss: \u001b[1m\u001b[32m1.96438\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4422 | loss: 1.96438 - acc: 0.5328 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4423  | total loss: \u001b[1m\u001b[32m1.82210\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4423 | loss: 1.82210 - acc: 0.5795 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4424  | total loss: \u001b[1m\u001b[32m1.94706\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4424 | loss: 1.94706 - acc: 0.5359 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4425  | total loss: \u001b[1m\u001b[32m1.80799\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4425 | loss: 1.80799 - acc: 0.5823 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4426  | total loss: \u001b[1m\u001b[32m1.97537\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4426 | loss: 1.97537 - acc: 0.5240 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4427  | total loss: \u001b[1m\u001b[32m1.83505\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4427 | loss: 1.83505 - acc: 0.5716 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4428  | total loss: \u001b[1m\u001b[32m1.95313\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4428 | loss: 1.95313 - acc: 0.5288 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4429  | total loss: \u001b[1m\u001b[32m1.81659\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4429 | loss: 1.81659 - acc: 0.5759 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4430  | total loss: \u001b[1m\u001b[32m1.96834\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4430 | loss: 1.96834 - acc: 0.5183 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4431  | total loss: \u001b[1m\u001b[32m1.83179\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4431 | loss: 1.83179 - acc: 0.5665 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4432  | total loss: \u001b[1m\u001b[32m1.95736\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4432 | loss: 1.95736 - acc: 0.5241 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4433  | total loss: \u001b[1m\u001b[32m1.82345\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4433 | loss: 1.82345 - acc: 0.5717 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4434  | total loss: \u001b[1m\u001b[32m1.93091\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4434 | loss: 1.93091 - acc: 0.5360 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4435  | total loss: \u001b[1m\u001b[32m1.80080\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4435 | loss: 1.80080 - acc: 0.5824 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4436  | total loss: \u001b[1m\u001b[32m1.95421\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4436 | loss: 1.95421 - acc: 0.5313 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4437  | total loss: \u001b[1m\u001b[32m1.82265\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4437 | loss: 1.82265 - acc: 0.5781 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4438  | total loss: \u001b[1m\u001b[32m1.98695\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4438 | loss: 1.98695 - acc: 0.5203 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4439  | total loss: \u001b[1m\u001b[32m1.85312\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4439 | loss: 1.85312 - acc: 0.5683 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4440  | total loss: \u001b[1m\u001b[32m1.96166\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4440 | loss: 1.96166 - acc: 0.5257 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4441  | total loss: \u001b[1m\u001b[32m1.83122\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4441 | loss: 1.83122 - acc: 0.5732 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4442  | total loss: \u001b[1m\u001b[32m1.95828\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4442 | loss: 1.95828 - acc: 0.5301 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4443  | total loss: \u001b[1m\u001b[32m1.82874\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4443 | loss: 1.82874 - acc: 0.5771 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4444  | total loss: \u001b[1m\u001b[32m1.94949\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4444 | loss: 1.94949 - acc: 0.5337 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4445  | total loss: \u001b[1m\u001b[32m1.82112\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4445 | loss: 1.82112 - acc: 0.5803 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4446  | total loss: \u001b[1m\u001b[32m1.70520\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4446 | loss: 1.70520 - acc: 0.6223 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4447  | total loss: \u001b[1m\u001b[32m1.59975\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4447 | loss: 1.59975 - acc: 0.6601 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4448  | total loss: \u001b[1m\u001b[32m1.75613\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4448 | loss: 1.75613 - acc: 0.6012 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4449  | total loss: \u001b[1m\u001b[32m1.64290\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4449 | loss: 1.64290 - acc: 0.6411 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4450  | total loss: \u001b[1m\u001b[32m1.80015\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4450 | loss: 1.80015 - acc: 0.5841 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4451  | total loss: \u001b[1m\u001b[32m1.68039\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4451 | loss: 1.68039 - acc: 0.6257 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4452  | total loss: \u001b[1m\u001b[32m1.57137\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4452 | loss: 1.57137 - acc: 0.6631 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4453  | total loss: \u001b[1m\u001b[32m1.47143\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4453 | loss: 1.47143 - acc: 0.6968 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4454  | total loss: \u001b[1m\u001b[32m1.67235\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4454 | loss: 1.67235 - acc: 0.6271 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4455  | total loss: \u001b[1m\u001b[32m1.55890\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4455 | loss: 1.55890 - acc: 0.6644 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4456  | total loss: \u001b[1m\u001b[32m1.74376\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4456 | loss: 1.74376 - acc: 0.5980 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4457  | total loss: \u001b[1m\u001b[32m1.62096\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4457 | loss: 1.62096 - acc: 0.6382 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4458  | total loss: \u001b[1m\u001b[32m1.50932\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4458 | loss: 1.50932 - acc: 0.6744 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4459  | total loss: \u001b[1m\u001b[32m1.40726\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4459 | loss: 1.40726 - acc: 0.7069 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4460  | total loss: \u001b[1m\u001b[32m1.31346\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4460 | loss: 1.31346 - acc: 0.7362 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4461  | total loss: \u001b[1m\u001b[32m1.22676\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4461 | loss: 1.22676 - acc: 0.7626 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4462  | total loss: \u001b[1m\u001b[32m1.14626\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4462 | loss: 1.14626 - acc: 0.7864 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4463  | total loss: \u001b[1m\u001b[32m1.07116\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4463 | loss: 1.07116 - acc: 0.8077 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4464  | total loss: \u001b[1m\u001b[32m1.31565\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4464 | loss: 1.31565 - acc: 0.7341 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4465  | total loss: \u001b[1m\u001b[32m1.21943\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4465 | loss: 1.21943 - acc: 0.7607 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4466  | total loss: \u001b[1m\u001b[32m1.44986\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4466 | loss: 1.44986 - acc: 0.6918 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4467  | total loss: \u001b[1m\u001b[32m1.33792\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4467 | loss: 1.33792 - acc: 0.7226 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4468  | total loss: \u001b[1m\u001b[32m1.55392\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4468 | loss: 1.55392 - acc: 0.6575 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4469  | total loss: \u001b[1m\u001b[32m1.43078\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4469 | loss: 1.43078 - acc: 0.6917 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4470  | total loss: \u001b[1m\u001b[32m1.62402\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 4470 | loss: 1.62402 - acc: 0.6368 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4471  | total loss: \u001b[1m\u001b[32m1.49429\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4471 | loss: 1.49429 - acc: 0.6731 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4472  | total loss: \u001b[1m\u001b[32m1.73839\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4472 | loss: 1.73839 - acc: 0.6058 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4473  | total loss: \u001b[1m\u001b[32m1.59874\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4473 | loss: 1.59874 - acc: 0.6452 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4474  | total loss: \u001b[1m\u001b[32m1.80469\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4474 | loss: 1.80469 - acc: 0.5879 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4475  | total loss: \u001b[1m\u001b[32m1.66075\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4475 | loss: 1.66075 - acc: 0.6291 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4476  | total loss: \u001b[1m\u001b[32m1.53221\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4476 | loss: 1.53221 - acc: 0.6662 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4477  | total loss: \u001b[1m\u001b[32m1.41701\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4477 | loss: 1.41701 - acc: 0.6996 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4478  | total loss: \u001b[1m\u001b[32m1.64771\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4478 | loss: 1.64771 - acc: 0.6367 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4479  | total loss: \u001b[1m\u001b[32m1.52198\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4479 | loss: 1.52198 - acc: 0.6731 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4480  | total loss: \u001b[1m\u001b[32m1.72908\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4480 | loss: 1.72908 - acc: 0.6058 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4481  | total loss: \u001b[1m\u001b[32m1.59711\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4481 | loss: 1.59711 - acc: 0.6452 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4482  | total loss: \u001b[1m\u001b[32m1.77259\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4482 | loss: 1.77259 - acc: 0.5950 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4483  | total loss: \u001b[1m\u001b[32m1.63863\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4483 | loss: 1.63863 - acc: 0.6355 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4484  | total loss: \u001b[1m\u001b[32m1.85411\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4484 | loss: 1.85411 - acc: 0.5719 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4485  | total loss: \u001b[1m\u001b[32m1.71471\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4485 | loss: 1.71471 - acc: 0.6147 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4486  | total loss: \u001b[1m\u001b[32m1.90285\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4486 | loss: 1.90285 - acc: 0.5604 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4487  | total loss: \u001b[1m\u001b[32m1.76168\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4487 | loss: 1.76168 - acc: 0.6044 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4488  | total loss: \u001b[1m\u001b[32m1.93173\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4488 | loss: 1.93173 - acc: 0.5511 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4489  | total loss: \u001b[1m\u001b[32m1.79095\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4489 | loss: 1.79095 - acc: 0.5960 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4490  | total loss: \u001b[1m\u001b[32m1.66548\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4490 | loss: 1.66548 - acc: 0.6364 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4491  | total loss: \u001b[1m\u001b[32m1.55302\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4491 | loss: 1.55302 - acc: 0.6727 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4492  | total loss: \u001b[1m\u001b[32m1.74769\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4492 | loss: 1.74769 - acc: 0.6055 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4493  | total loss: \u001b[1m\u001b[32m1.62744\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4493 | loss: 1.62744 - acc: 0.6449 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4494  | total loss: \u001b[1m\u001b[32m1.51917\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4494 | loss: 1.51917 - acc: 0.6804 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4495  | total loss: \u001b[1m\u001b[32m1.42103\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4495 | loss: 1.42103 - acc: 0.7124 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4496  | total loss: \u001b[1m\u001b[32m1.58634\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4496 | loss: 1.58634 - acc: 0.6626 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4497  | total loss: \u001b[1m\u001b[32m1.47968\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4497 | loss: 1.47968 - acc: 0.6963 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4498  | total loss: \u001b[1m\u001b[32m1.64856\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4498 | loss: 1.64856 - acc: 0.6410 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4499  | total loss: \u001b[1m\u001b[32m1.53446\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4499 | loss: 1.53446 - acc: 0.6769 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4500  | total loss: \u001b[1m\u001b[32m1.43103\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4500 | loss: 1.43103 - acc: 0.7092 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4501  | total loss: \u001b[1m\u001b[32m1.33669\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4501 | loss: 1.33669 - acc: 0.7383 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4502  | total loss: \u001b[1m\u001b[32m1.25011\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4502 | loss: 1.25011 - acc: 0.7644 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4503  | total loss: \u001b[1m\u001b[32m1.17015\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4503 | loss: 1.17015 - acc: 0.7880 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4504  | total loss: \u001b[1m\u001b[32m1.09588\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4504 | loss: 1.09588 - acc: 0.8092 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4505  | total loss: \u001b[1m\u001b[32m1.02654\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4505 | loss: 1.02654 - acc: 0.8283 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4506  | total loss: \u001b[1m\u001b[32m1.28078\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4506 | loss: 1.28078 - acc: 0.7526 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4507  | total loss: \u001b[1m\u001b[32m1.18892\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4507 | loss: 1.18892 - acc: 0.7773 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4508  | total loss: \u001b[1m\u001b[32m1.46061\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4508 | loss: 1.46061 - acc: 0.6996 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4509  | total loss: \u001b[1m\u001b[32m1.34868\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4509 | loss: 1.34868 - acc: 0.7296 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4510  | total loss: \u001b[1m\u001b[32m1.59347\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4510 | loss: 1.59347 - acc: 0.6567 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4511  | total loss: \u001b[1m\u001b[32m1.46778\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4511 | loss: 1.46778 - acc: 0.6910 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4512  | total loss: \u001b[1m\u001b[32m1.65550\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4512 | loss: 1.65550 - acc: 0.6362 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4513  | total loss: \u001b[1m\u001b[32m1.52430\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4513 | loss: 1.52430 - acc: 0.6726 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4514  | total loss: \u001b[1m\u001b[32m1.40657\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4514 | loss: 1.40657 - acc: 0.7053 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4515  | total loss: \u001b[1m\u001b[32m1.30053\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4515 | loss: 1.30053 - acc: 0.7348 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4516  | total loss: \u001b[1m\u001b[32m1.53259\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4516 | loss: 1.53259 - acc: 0.6684 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4517  | total loss: \u001b[1m\u001b[32m1.41396\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4517 | loss: 1.41396 - acc: 0.7016 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4518  | total loss: \u001b[1m\u001b[32m1.66513\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4518 | loss: 1.66513 - acc: 0.6314 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4519  | total loss: \u001b[1m\u001b[32m1.53437\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4519 | loss: 1.53437 - acc: 0.6683 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4520  | total loss: \u001b[1m\u001b[32m1.78892\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4520 | loss: 1.78892 - acc: 0.6015 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4521  | total loss: \u001b[1m\u001b[32m1.64799\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4521 | loss: 1.64799 - acc: 0.6413 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4522  | total loss: \u001b[1m\u001b[32m1.83777\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4522 | loss: 1.83777 - acc: 0.5843 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4523  | total loss: \u001b[1m\u001b[32m1.69494\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4523 | loss: 1.69494 - acc: 0.6259 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4524  | total loss: \u001b[1m\u001b[32m1.87974\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4524 | loss: 1.87974 - acc: 0.5705 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4525  | total loss: \u001b[1m\u001b[32m1.73610\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4525 | loss: 1.73610 - acc: 0.6134 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4526  | total loss: \u001b[1m\u001b[32m1.90827\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4526 | loss: 1.90827 - acc: 0.5592 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4527  | total loss: \u001b[1m\u001b[32m1.76540\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4527 | loss: 1.76540 - acc: 0.6033 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4528  | total loss: \u001b[1m\u001b[32m1.87240\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4528 | loss: 1.87240 - acc: 0.5644 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4529  | total loss: \u001b[1m\u001b[32m1.73652\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4529 | loss: 1.73652 - acc: 0.6079 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4530  | total loss: \u001b[1m\u001b[32m1.94323\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 4530 | loss: 1.94323 - acc: 0.5472 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4531  | total loss: \u001b[1m\u001b[32m1.80354\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4531 | loss: 1.80354 - acc: 0.5924 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4532  | total loss: \u001b[1m\u001b[32m1.95965\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4532 | loss: 1.95965 - acc: 0.5403 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4533  | total loss: \u001b[1m\u001b[32m1.82157\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4533 | loss: 1.82157 - acc: 0.5863 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4534  | total loss: \u001b[1m\u001b[32m1.97038\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4534 | loss: 1.97038 - acc: 0.5348 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4535  | total loss: \u001b[1m\u001b[32m1.83409\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4535 | loss: 1.83409 - acc: 0.5813 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4536  | total loss: \u001b[1m\u001b[32m1.96040\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4536 | loss: 1.96040 - acc: 0.5375 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4537  | total loss: \u001b[1m\u001b[32m1.82744\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4537 | loss: 1.82744 - acc: 0.5837 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4538  | total loss: \u001b[1m\u001b[32m1.98646\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4538 | loss: 1.98646 - acc: 0.5254 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4539  | total loss: \u001b[1m\u001b[32m1.85281\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4539 | loss: 1.85281 - acc: 0.5728 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4540  | total loss: \u001b[1m\u001b[32m1.96155\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4540 | loss: 1.96155 - acc: 0.5370 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4541  | total loss: \u001b[1m\u001b[32m1.83178\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4541 | loss: 1.83178 - acc: 0.5833 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4542  | total loss: \u001b[1m\u001b[32m1.71505\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4542 | loss: 1.71505 - acc: 0.6249 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4543  | total loss: \u001b[1m\u001b[32m1.60928\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4543 | loss: 1.60928 - acc: 0.6625 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4544  | total loss: \u001b[1m\u001b[32m1.51268\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4544 | loss: 1.51268 - acc: 0.6962 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4545  | total loss: \u001b[1m\u001b[32m1.42375\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4545 | loss: 1.42375 - acc: 0.7266 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4546  | total loss: \u001b[1m\u001b[32m1.34123\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4546 | loss: 1.34123 - acc: 0.7539 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4547  | total loss: \u001b[1m\u001b[32m1.26409\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4547 | loss: 1.26409 - acc: 0.7785 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4548  | total loss: \u001b[1m\u001b[32m1.19149\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4548 | loss: 1.19149 - acc: 0.8007 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4549  | total loss: \u001b[1m\u001b[32m1.12276\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4549 | loss: 1.12276 - acc: 0.8206 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4550  | total loss: \u001b[1m\u001b[32m1.36738\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4550 | loss: 1.36738 - acc: 0.7386 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4551  | total loss: \u001b[1m\u001b[32m1.27541\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4551 | loss: 1.27541 - acc: 0.7647 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4552  | total loss: \u001b[1m\u001b[32m1.19026\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4552 | loss: 1.19026 - acc: 0.7882 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4553  | total loss: \u001b[1m\u001b[32m1.11110\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4553 | loss: 1.11110 - acc: 0.8094 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4554  | total loss: \u001b[1m\u001b[32m1.03725\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4554 | loss: 1.03725 - acc: 0.8285 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4555  | total loss: \u001b[1m\u001b[32m0.96814\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4555 | loss: 0.96814 - acc: 0.8456 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4556  | total loss: \u001b[1m\u001b[32m1.25799\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4556 | loss: 1.25799 - acc: 0.7611 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4557  | total loss: \u001b[1m\u001b[32m1.16287\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4557 | loss: 1.16287 - acc: 0.7850 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4558  | total loss: \u001b[1m\u001b[32m1.44637\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4558 | loss: 1.44637 - acc: 0.7136 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4559  | total loss: \u001b[1m\u001b[32m1.33058\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4559 | loss: 1.33058 - acc: 0.7422 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4560  | total loss: \u001b[1m\u001b[32m1.56194\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4560 | loss: 1.56194 - acc: 0.6752 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4561  | total loss: \u001b[1m\u001b[32m1.43428\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4561 | loss: 1.43428 - acc: 0.7076 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4562  | total loss: \u001b[1m\u001b[32m1.67846\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4562 | loss: 1.67846 - acc: 0.6369 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4563  | total loss: \u001b[1m\u001b[32m1.54004\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4563 | loss: 1.54004 - acc: 0.6732 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4564  | total loss: \u001b[1m\u001b[32m1.41602\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4564 | loss: 1.41602 - acc: 0.7059 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4565  | total loss: \u001b[1m\u001b[32m1.30460\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4565 | loss: 1.30460 - acc: 0.7353 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4566  | total loss: \u001b[1m\u001b[32m1.20420\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4566 | loss: 1.20420 - acc: 0.7618 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4567  | total loss: \u001b[1m\u001b[32m1.11342\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4567 | loss: 1.11342 - acc: 0.7856 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4568  | total loss: \u001b[1m\u001b[32m1.39434\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4568 | loss: 1.39434 - acc: 0.7142 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4569  | total loss: \u001b[1m\u001b[32m1.28417\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4569 | loss: 1.28417 - acc: 0.7427 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4570  | total loss: \u001b[1m\u001b[32m1.53856\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4570 | loss: 1.53856 - acc: 0.6685 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4571  | total loss: \u001b[1m\u001b[32m1.41482\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4571 | loss: 1.41482 - acc: 0.7016 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4572  | total loss: \u001b[1m\u001b[32m1.69041\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4572 | loss: 1.69041 - acc: 0.6315 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4573  | total loss: \u001b[1m\u001b[32m1.55352\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4573 | loss: 1.55352 - acc: 0.6683 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4574  | total loss: \u001b[1m\u001b[32m1.81168\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4574 | loss: 1.81168 - acc: 0.6015 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4575  | total loss: \u001b[1m\u001b[32m1.66572\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4575 | loss: 1.66572 - acc: 0.6413 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4576  | total loss: \u001b[1m\u001b[32m1.82301\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4576 | loss: 1.82301 - acc: 0.5915 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4577  | total loss: \u001b[1m\u001b[32m1.67948\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4577 | loss: 1.67948 - acc: 0.6323 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4578  | total loss: \u001b[1m\u001b[32m1.87012\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4578 | loss: 1.87012 - acc: 0.5762 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4579  | total loss: \u001b[1m\u001b[32m1.72567\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4579 | loss: 1.72567 - acc: 0.6186 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4580  | total loss: \u001b[1m\u001b[32m1.90734\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4580 | loss: 1.90734 - acc: 0.5568 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4581  | total loss: \u001b[1m\u001b[32m1.76331\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4581 | loss: 1.76331 - acc: 0.6011 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4582  | total loss: \u001b[1m\u001b[32m1.96456\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4582 | loss: 1.96456 - acc: 0.5410 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4583  | total loss: \u001b[1m\u001b[32m1.81924\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4583 | loss: 1.81924 - acc: 0.5869 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4584  | total loss: \u001b[1m\u001b[32m1.95799\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4584 | loss: 1.95799 - acc: 0.5425 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4585  | total loss: \u001b[1m\u001b[32m1.81752\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4585 | loss: 1.81752 - acc: 0.5882 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4586  | total loss: \u001b[1m\u001b[32m1.97612\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4586 | loss: 1.97612 - acc: 0.5365 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4587  | total loss: \u001b[1m\u001b[32m1.83751\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4587 | loss: 1.83751 - acc: 0.5829 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4588  | total loss: \u001b[1m\u001b[32m1.96635\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4588 | loss: 1.96635 - acc: 0.5389 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4589  | total loss: \u001b[1m\u001b[32m1.83186\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4589 | loss: 1.83186 - acc: 0.5850 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4590  | total loss: \u001b[1m\u001b[32m1.98428\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4590 | loss: 1.98428 - acc: 0.5265 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4591  | total loss: \u001b[1m\u001b[32m1.85078\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4591 | loss: 1.85078 - acc: 0.5739 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4592  | total loss: \u001b[1m\u001b[32m1.73151\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4592 | loss: 1.73151 - acc: 0.6165 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4593  | total loss: \u001b[1m\u001b[32m1.62420\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4593 | loss: 1.62420 - acc: 0.6548 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4594  | total loss: \u001b[1m\u001b[32m1.78560\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4594 | loss: 1.78560 - acc: 0.5893 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4595  | total loss: \u001b[1m\u001b[32m1.67232\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4595 | loss: 1.67232 - acc: 0.6304 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4596  | total loss: \u001b[1m\u001b[32m1.83665\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4596 | loss: 1.83665 - acc: 0.5674 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4597  | total loss: \u001b[1m\u001b[32m1.71808\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4597 | loss: 1.71808 - acc: 0.6106 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4598  | total loss: \u001b[1m\u001b[32m1.90917\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4598 | loss: 1.90917 - acc: 0.5496 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4599  | total loss: \u001b[1m\u001b[32m1.78356\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4599 | loss: 1.78356 - acc: 0.5946 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4600  | total loss: \u001b[1m\u001b[32m1.86208\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4600 | loss: 1.86208 - acc: 0.5637 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4601  | total loss: \u001b[1m\u001b[32m1.74122\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4601 | loss: 1.74122 - acc: 0.6073 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4602  | total loss: \u001b[1m\u001b[32m1.86813\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4602 | loss: 1.86813 - acc: 0.5538 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4603  | total loss: \u001b[1m\u001b[32m1.74637\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4603 | loss: 1.74637 - acc: 0.5984 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4604  | total loss: \u001b[1m\u001b[32m1.88233\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4604 | loss: 1.88233 - acc: 0.5528 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4605  | total loss: \u001b[1m\u001b[32m1.75884\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4605 | loss: 1.75884 - acc: 0.5975 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4606  | total loss: \u001b[1m\u001b[32m1.64716\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4606 | loss: 1.64716 - acc: 0.6378 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4607  | total loss: \u001b[1m\u001b[32m1.54540\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4607 | loss: 1.54540 - acc: 0.6740 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4608  | total loss: \u001b[1m\u001b[32m1.69874\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4608 | loss: 1.69874 - acc: 0.6209 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4609  | total loss: \u001b[1m\u001b[32m1.58904\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4609 | loss: 1.58904 - acc: 0.6588 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4610  | total loss: \u001b[1m\u001b[32m1.78315\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4610 | loss: 1.78315 - acc: 0.5929 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4611  | total loss: \u001b[1m\u001b[32m1.66306\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4611 | loss: 1.66306 - acc: 0.6336 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4612  | total loss: \u001b[1m\u001b[32m1.84783\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4612 | loss: 1.84783 - acc: 0.5703 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4613  | total loss: \u001b[1m\u001b[32m1.72025\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4613 | loss: 1.72025 - acc: 0.6132 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4614  | total loss: \u001b[1m\u001b[32m1.88446\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4614 | loss: 1.88446 - acc: 0.5591 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4615  | total loss: \u001b[1m\u001b[32m1.75288\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4615 | loss: 1.75288 - acc: 0.6032 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4616  | total loss: \u001b[1m\u001b[32m1.90913\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4616 | loss: 1.90913 - acc: 0.5428 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4617  | total loss: \u001b[1m\u001b[32m1.77535\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4617 | loss: 1.77535 - acc: 0.5886 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4618  | total loss: \u001b[1m\u001b[32m1.93993\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4618 | loss: 1.93993 - acc: 0.5368 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4619  | total loss: \u001b[1m\u001b[32m1.80384\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4619 | loss: 1.80384 - acc: 0.5832 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4620  | total loss: \u001b[1m\u001b[32m1.96590\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4620 | loss: 1.96590 - acc: 0.5320 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4621  | total loss: \u001b[1m\u001b[32m1.82828\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4621 | loss: 1.82828 - acc: 0.5788 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4622  | total loss: \u001b[1m\u001b[32m1.97373\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4622 | loss: 1.97373 - acc: 0.5281 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4623  | total loss: \u001b[1m\u001b[32m1.83651\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4623 | loss: 1.83651 - acc: 0.5752 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4624  | total loss: \u001b[1m\u001b[32m1.71317\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4624 | loss: 1.71317 - acc: 0.6177 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4625  | total loss: \u001b[1m\u001b[32m1.60159\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4625 | loss: 1.60159 - acc: 0.6559 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4626  | total loss: \u001b[1m\u001b[32m1.72558\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4626 | loss: 1.72558 - acc: 0.6118 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4627  | total loss: \u001b[1m\u001b[32m1.61109\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4627 | loss: 1.61109 - acc: 0.6506 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4628  | total loss: \u001b[1m\u001b[32m1.79354\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4628 | loss: 1.79354 - acc: 0.5927 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4629  | total loss: \u001b[1m\u001b[32m1.67095\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4629 | loss: 1.67095 - acc: 0.6334 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4630  | total loss: \u001b[1m\u001b[32m1.85866\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4630 | loss: 1.85866 - acc: 0.5701 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4631  | total loss: \u001b[1m\u001b[32m1.72877\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4631 | loss: 1.72877 - acc: 0.6131 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4632  | total loss: \u001b[1m\u001b[32m1.87305\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4632 | loss: 1.87305 - acc: 0.5660 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4633  | total loss: \u001b[1m\u001b[32m1.74126\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4633 | loss: 1.74126 - acc: 0.6094 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4634  | total loss: \u001b[1m\u001b[32m1.62210\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4634 | loss: 1.62210 - acc: 0.6485 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4635  | total loss: \u001b[1m\u001b[32m1.51372\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4635 | loss: 1.51372 - acc: 0.6836 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4636  | total loss: \u001b[1m\u001b[32m1.63680\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4636 | loss: 1.63680 - acc: 0.6367 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4637  | total loss: \u001b[1m\u001b[32m1.52444\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4637 | loss: 1.52444 - acc: 0.6730 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4638  | total loss: \u001b[1m\u001b[32m1.71262\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4638 | loss: 1.71262 - acc: 0.6129 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4639  | total loss: \u001b[1m\u001b[32m1.59093\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4639 | loss: 1.59093 - acc: 0.6516 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4640  | total loss: \u001b[1m\u001b[32m1.48051\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4640 | loss: 1.48051 - acc: 0.6864 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4641  | total loss: \u001b[1m\u001b[32m1.37979\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4641 | loss: 1.37979 - acc: 0.7178 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4642  | total loss: \u001b[1m\u001b[32m1.58807\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4642 | loss: 1.58807 - acc: 0.6532 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4643  | total loss: \u001b[1m\u001b[32m1.47414\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4643 | loss: 1.47414 - acc: 0.6878 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4644  | total loss: \u001b[1m\u001b[32m1.65408\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4644 | loss: 1.65408 - acc: 0.6333 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4645  | total loss: \u001b[1m\u001b[32m1.53212\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4645 | loss: 1.53212 - acc: 0.6700 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4646  | total loss: \u001b[1m\u001b[32m1.72526\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4646 | loss: 1.72526 - acc: 0.6101 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4647  | total loss: \u001b[1m\u001b[32m1.59568\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4647 | loss: 1.59568 - acc: 0.6491 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4648  | total loss: \u001b[1m\u001b[32m1.73916\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4648 | loss: 1.73916 - acc: 0.5985 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4649  | total loss: \u001b[1m\u001b[32m1.60840\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4649 | loss: 1.60840 - acc: 0.6387 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4650  | total loss: \u001b[1m\u001b[32m1.81944\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4650 | loss: 1.81944 - acc: 0.5748 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4651  | total loss: \u001b[1m\u001b[32m1.68154\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4651 | loss: 1.68154 - acc: 0.6173 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4652  | total loss: \u001b[1m\u001b[32m1.86499\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4652 | loss: 1.86499 - acc: 0.5627 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4653  | total loss: \u001b[1m\u001b[32m1.72408\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4653 | loss: 1.72408 - acc: 0.6065 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4654  | total loss: \u001b[1m\u001b[32m1.83459\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4654 | loss: 1.83459 - acc: 0.5744 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4655  | total loss: \u001b[1m\u001b[32m1.69845\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4655 | loss: 1.69845 - acc: 0.6169 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4656  | total loss: \u001b[1m\u001b[32m1.88304\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4656 | loss: 1.88304 - acc: 0.5624 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4657  | total loss: \u001b[1m\u001b[32m1.74387\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4657 | loss: 1.74387 - acc: 0.6061 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4658  | total loss: \u001b[1m\u001b[32m1.61927\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4658 | loss: 1.61927 - acc: 0.6455 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4659  | total loss: \u001b[1m\u001b[32m1.50716\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4659 | loss: 1.50716 - acc: 0.6810 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4660  | total loss: \u001b[1m\u001b[32m1.68089\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4660 | loss: 1.68089 - acc: 0.6200 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4661  | total loss: \u001b[1m\u001b[32m1.56238\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4661 | loss: 1.56238 - acc: 0.6580 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4662  | total loss: \u001b[1m\u001b[32m1.75892\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4662 | loss: 1.75892 - acc: 0.5922 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4663  | total loss: \u001b[1m\u001b[32m1.63300\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4663 | loss: 1.63300 - acc: 0.6330 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4664  | total loss: \u001b[1m\u001b[32m1.79567\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4664 | loss: 1.79567 - acc: 0.5840 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4665  | total loss: \u001b[1m\u001b[32m1.66699\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4665 | loss: 1.66699 - acc: 0.6256 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4666  | total loss: \u001b[1m\u001b[32m1.80742\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4666 | loss: 1.80742 - acc: 0.5773 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4667  | total loss: \u001b[1m\u001b[32m1.67870\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4667 | loss: 1.67870 - acc: 0.6196 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4668  | total loss: \u001b[1m\u001b[32m1.86478\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4668 | loss: 1.86478 - acc: 0.5576 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4669  | total loss: \u001b[1m\u001b[32m1.73164\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4669 | loss: 1.73164 - acc: 0.6019 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4670  | total loss: \u001b[1m\u001b[32m1.84627\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4670 | loss: 1.84627 - acc: 0.5631 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4671  | total loss: \u001b[1m\u001b[32m1.71635\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4671 | loss: 1.71635 - acc: 0.6068 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4672  | total loss: \u001b[1m\u001b[32m1.86289\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4672 | loss: 1.86289 - acc: 0.5533 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4673  | total loss: \u001b[1m\u001b[32m1.73266\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4673 | loss: 1.73266 - acc: 0.5979 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4674  | total loss: \u001b[1m\u001b[32m1.86334\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4674 | loss: 1.86334 - acc: 0.5524 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4675  | total loss: \u001b[1m\u001b[32m1.73439\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4675 | loss: 1.73439 - acc: 0.5972 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4676  | total loss: \u001b[1m\u001b[32m1.90125\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4676 | loss: 1.90125 - acc: 0.5446 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4677  | total loss: \u001b[1m\u001b[32m1.76981\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4677 | loss: 1.76981 - acc: 0.5901 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4678  | total loss: \u001b[1m\u001b[32m1.65179\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4678 | loss: 1.65179 - acc: 0.6311 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4679  | total loss: \u001b[1m\u001b[32m1.54514\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4679 | loss: 1.54514 - acc: 0.6680 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4680  | total loss: \u001b[1m\u001b[32m1.75227\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4680 | loss: 1.75227 - acc: 0.6012 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4681  | total loss: \u001b[1m\u001b[32m1.63441\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4681 | loss: 1.63441 - acc: 0.6411 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4682  | total loss: \u001b[1m\u001b[32m1.81111\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4682 | loss: 1.81111 - acc: 0.5841 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4683  | total loss: \u001b[1m\u001b[32m1.68676\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4683 | loss: 1.68676 - acc: 0.6257 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4684  | total loss: \u001b[1m\u001b[32m1.78083\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4684 | loss: 1.78083 - acc: 0.5917 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4685  | total loss: \u001b[1m\u001b[32m1.65896\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4685 | loss: 1.65896 - acc: 0.6325 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4686  | total loss: \u001b[1m\u001b[32m1.74971\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4686 | loss: 1.74971 - acc: 0.5979 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4687  | total loss: \u001b[1m\u001b[32m1.63009\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4687 | loss: 1.63009 - acc: 0.6381 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4688  | total loss: \u001b[1m\u001b[32m1.82190\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4688 | loss: 1.82190 - acc: 0.5743 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4689  | total loss: \u001b[1m\u001b[32m1.69451\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4689 | loss: 1.69451 - acc: 0.6168 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4690  | total loss: \u001b[1m\u001b[32m1.88453\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4690 | loss: 1.88453 - acc: 0.5552 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4691  | total loss: \u001b[1m\u001b[32m1.75103\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4691 | loss: 1.75103 - acc: 0.5996 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4692  | total loss: \u001b[1m\u001b[32m1.89192\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4692 | loss: 1.89192 - acc: 0.5468 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4693  | total loss: \u001b[1m\u001b[32m1.75829\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4693 | loss: 1.75829 - acc: 0.5921 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4694  | total loss: \u001b[1m\u001b[32m1.90196\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4694 | loss: 1.90196 - acc: 0.5401 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4695  | total loss: \u001b[1m\u001b[32m1.76823\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4695 | loss: 1.76823 - acc: 0.5861 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4696  | total loss: \u001b[1m\u001b[32m1.91702\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4696 | loss: 1.91702 - acc: 0.5346 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4697  | total loss: \u001b[1m\u001b[32m1.78297\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4697 | loss: 1.78297 - acc: 0.5811 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4698  | total loss: \u001b[1m\u001b[32m1.66259\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4698 | loss: 1.66259 - acc: 0.6230 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4699  | total loss: \u001b[1m\u001b[32m1.55381\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4699 | loss: 1.55381 - acc: 0.6607 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4700  | total loss: \u001b[1m\u001b[32m1.70990\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4700 | loss: 1.70990 - acc: 0.6089 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4701  | total loss: \u001b[1m\u001b[32m1.59509\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4701 | loss: 1.59509 - acc: 0.6480 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4702  | total loss: \u001b[1m\u001b[32m1.49089\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4702 | loss: 1.49089 - acc: 0.6832 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4703  | total loss: \u001b[1m\u001b[32m1.39569\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4703 | loss: 1.39569 - acc: 0.7149 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4704  | total loss: \u001b[1m\u001b[32m1.62508\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4704 | loss: 1.62508 - acc: 0.6434 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4705  | total loss: \u001b[1m\u001b[32m1.51394\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4705 | loss: 1.51394 - acc: 0.6791 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4706  | total loss: \u001b[1m\u001b[32m1.71094\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4706 | loss: 1.71094 - acc: 0.6112 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4707  | total loss: \u001b[1m\u001b[32m1.58991\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4707 | loss: 1.58991 - acc: 0.6501 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4708  | total loss: \u001b[1m\u001b[32m1.80622\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4708 | loss: 1.80622 - acc: 0.5850 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4709  | total loss: \u001b[1m\u001b[32m1.67521\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4709 | loss: 1.67521 - acc: 0.6265 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4710  | total loss: \u001b[1m\u001b[32m1.84178\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4710 | loss: 1.84178 - acc: 0.5710 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4711  | total loss: \u001b[1m\u001b[32m1.70745\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4711 | loss: 1.70745 - acc: 0.6139 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4712  | total loss: \u001b[1m\u001b[32m1.87503\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4712 | loss: 1.87503 - acc: 0.5597 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4713  | total loss: \u001b[1m\u001b[32m1.73810\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4713 | loss: 1.73810 - acc: 0.6037 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4714  | total loss: \u001b[1m\u001b[32m1.94317\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4714 | loss: 1.94317 - acc: 0.5433 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4715  | total loss: \u001b[1m\u001b[32m1.80066\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4715 | loss: 1.80066 - acc: 0.5890 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4716  | total loss: \u001b[1m\u001b[32m1.67282\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4716 | loss: 1.67282 - acc: 0.6301 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4717  | total loss: \u001b[1m\u001b[32m1.55753\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4717 | loss: 1.55753 - acc: 0.6671 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4718  | total loss: \u001b[1m\u001b[32m1.76091\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4718 | loss: 1.76091 - acc: 0.6004 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4719  | total loss: \u001b[1m\u001b[32m1.63621\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4719 | loss: 1.63621 - acc: 0.6403 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4720  | total loss: \u001b[1m\u001b[32m1.80792\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4720 | loss: 1.80792 - acc: 0.5835 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4721  | total loss: \u001b[1m\u001b[32m1.67864\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4721 | loss: 1.67864 - acc: 0.6251 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4722  | total loss: \u001b[1m\u001b[32m1.85940\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4722 | loss: 1.85940 - acc: 0.5626 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4723  | total loss: \u001b[1m\u001b[32m1.72577\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4723 | loss: 1.72577 - acc: 0.6063 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4724  | total loss: \u001b[1m\u001b[32m1.84756\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4724 | loss: 1.84756 - acc: 0.5671 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4725  | total loss: \u001b[1m\u001b[32m1.71619\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4725 | loss: 1.71619 - acc: 0.6104 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4726  | total loss: \u001b[1m\u001b[32m1.89880\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4726 | loss: 1.89880 - acc: 0.5565 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4727  | total loss: \u001b[1m\u001b[32m1.76343\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4727 | loss: 1.76343 - acc: 0.6009 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4728  | total loss: \u001b[1m\u001b[32m1.91680\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4728 | loss: 1.91680 - acc: 0.5479 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4729  | total loss: \u001b[1m\u001b[32m1.78093\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4729 | loss: 1.78093 - acc: 0.5931 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4730  | total loss: \u001b[1m\u001b[32m1.92678\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4730 | loss: 1.92678 - acc: 0.5410 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4731  | total loss: \u001b[1m\u001b[32m1.79131\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4731 | loss: 1.79131 - acc: 0.5869 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4732  | total loss: \u001b[1m\u001b[32m1.93735\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4732 | loss: 1.93735 - acc: 0.5353 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4733  | total loss: \u001b[1m\u001b[32m1.80224\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4733 | loss: 1.80224 - acc: 0.5818 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4734  | total loss: \u001b[1m\u001b[32m1.68096\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4734 | loss: 1.68096 - acc: 0.6236 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4735  | total loss: \u001b[1m\u001b[32m1.57139\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4735 | loss: 1.57139 - acc: 0.6612 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4736  | total loss: \u001b[1m\u001b[32m1.72611\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4736 | loss: 1.72611 - acc: 0.6094 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4737  | total loss: \u001b[1m\u001b[32m1.61070\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4737 | loss: 1.61070 - acc: 0.6485 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4738  | total loss: \u001b[1m\u001b[32m1.50593\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4738 | loss: 1.50593 - acc: 0.6836 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4739  | total loss: \u001b[1m\u001b[32m1.41016\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4739 | loss: 1.41016 - acc: 0.7153 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4740  | total loss: \u001b[1m\u001b[32m1.55653\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4740 | loss: 1.55653 - acc: 0.6652 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4741  | total loss: \u001b[1m\u001b[32m1.45261\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4741 | loss: 1.45261 - acc: 0.6986 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4742  | total loss: \u001b[1m\u001b[32m1.67195\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4742 | loss: 1.67195 - acc: 0.6288 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4743  | total loss: \u001b[1m\u001b[32m1.55445\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4743 | loss: 1.55445 - acc: 0.6659 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4744  | total loss: \u001b[1m\u001b[32m1.78129\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4744 | loss: 1.78129 - acc: 0.5993 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4745  | total loss: \u001b[1m\u001b[32m1.65211\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4745 | loss: 1.65211 - acc: 0.6394 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4746  | total loss: \u001b[1m\u001b[32m1.83227\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4746 | loss: 1.83227 - acc: 0.5754 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4747  | total loss: \u001b[1m\u001b[32m1.69825\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4747 | loss: 1.69825 - acc: 0.6179 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4748  | total loss: \u001b[1m\u001b[32m1.87076\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4748 | loss: 1.87076 - acc: 0.5633 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4749  | total loss: \u001b[1m\u001b[32m1.73378\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4749 | loss: 1.73378 - acc: 0.6069 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4750  | total loss: \u001b[1m\u001b[32m1.61070\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4750 | loss: 1.61070 - acc: 0.6462 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4751  | total loss: \u001b[1m\u001b[32m1.49955\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4751 | loss: 1.49955 - acc: 0.6816 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4752  | total loss: \u001b[1m\u001b[32m1.39857\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4752 | loss: 1.39857 - acc: 0.7134 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4753  | total loss: \u001b[1m\u001b[32m1.30631\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4753 | loss: 1.30631 - acc: 0.7421 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4754  | total loss: \u001b[1m\u001b[32m1.53193\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4754 | loss: 1.53193 - acc: 0.6679 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4755  | total loss: \u001b[1m\u001b[32m1.42396\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4755 | loss: 1.42396 - acc: 0.7011 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4756  | total loss: \u001b[1m\u001b[32m1.57080\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4756 | loss: 1.57080 - acc: 0.6524 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4757  | total loss: \u001b[1m\u001b[32m1.45758\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4757 | loss: 1.45758 - acc: 0.6872 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4758  | total loss: \u001b[1m\u001b[32m1.35489\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4758 | loss: 1.35489 - acc: 0.7185 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4759  | total loss: \u001b[1m\u001b[32m1.26129\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4759 | loss: 1.26129 - acc: 0.7466 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4760  | total loss: \u001b[1m\u001b[32m1.47407\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4760 | loss: 1.47407 - acc: 0.6862 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4761  | total loss: \u001b[1m\u001b[32m1.36640\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4761 | loss: 1.36640 - acc: 0.7176 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4762  | total loss: \u001b[1m\u001b[32m1.59347\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4762 | loss: 1.59347 - acc: 0.6459 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4763  | total loss: \u001b[1m\u001b[32m1.47296\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4763 | loss: 1.47296 - acc: 0.6813 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4764  | total loss: \u001b[1m\u001b[32m1.36414\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4764 | loss: 1.36414 - acc: 0.7131 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4765  | total loss: \u001b[1m\u001b[32m1.26546\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4765 | loss: 1.26546 - acc: 0.7418 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4766  | total loss: \u001b[1m\u001b[32m1.51504\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4766 | loss: 1.51504 - acc: 0.6676 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4767  | total loss: \u001b[1m\u001b[32m1.40015\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4767 | loss: 1.40015 - acc: 0.7009 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4768  | total loss: \u001b[1m\u001b[32m1.66339\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4768 | loss: 1.66339 - acc: 0.6308 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4769  | total loss: \u001b[1m\u001b[32m1.53376\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4769 | loss: 1.53376 - acc: 0.6677 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4770  | total loss: \u001b[1m\u001b[32m1.73987\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4770 | loss: 1.73987 - acc: 0.6081 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4771  | total loss: \u001b[1m\u001b[32m1.60363\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4771 | loss: 1.60363 - acc: 0.6473 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4772  | total loss: \u001b[1m\u001b[32m1.79884\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4772 | loss: 1.79884 - acc: 0.5897 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4773  | total loss: \u001b[1m\u001b[32m1.65847\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4773 | loss: 1.65847 - acc: 0.6307 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4774  | total loss: \u001b[1m\u001b[32m1.88193\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4774 | loss: 1.88193 - acc: 0.5677 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4775  | total loss: \u001b[1m\u001b[32m1.73579\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4775 | loss: 1.73579 - acc: 0.6109 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4776  | total loss: \u001b[1m\u001b[32m1.89989\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4776 | loss: 1.89989 - acc: 0.5569 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4777  | total loss: \u001b[1m\u001b[32m1.75505\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4777 | loss: 1.75505 - acc: 0.6012 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4778  | total loss: \u001b[1m\u001b[32m1.62597\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4778 | loss: 1.62597 - acc: 0.6411 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4779  | total loss: \u001b[1m\u001b[32m1.51041\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4779 | loss: 1.51041 - acc: 0.6770 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4780  | total loss: \u001b[1m\u001b[32m1.66598\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4780 | loss: 1.66598 - acc: 0.6236 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4781  | total loss: \u001b[1m\u001b[32m1.54719\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4781 | loss: 1.54719 - acc: 0.6612 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4782  | total loss: \u001b[1m\u001b[32m1.71367\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4782 | loss: 1.71367 - acc: 0.6094 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4783  | total loss: \u001b[1m\u001b[32m1.59107\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4783 | loss: 1.59107 - acc: 0.6485 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4784  | total loss: \u001b[1m\u001b[32m1.76123\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4784 | loss: 1.76123 - acc: 0.5908 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4785  | total loss: \u001b[1m\u001b[32m1.63503\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4785 | loss: 1.63503 - acc: 0.6317 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4786  | total loss: \u001b[1m\u001b[32m1.81980\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4786 | loss: 1.81980 - acc: 0.5757 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4787  | total loss: \u001b[1m\u001b[32m1.68931\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4787 | loss: 1.68931 - acc: 0.6181 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4788  | total loss: \u001b[1m\u001b[32m1.85983\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4788 | loss: 1.85983 - acc: 0.5634 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4789  | total loss: \u001b[1m\u001b[32m1.72712\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4789 | loss: 1.72712 - acc: 0.6071 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4790  | total loss: \u001b[1m\u001b[32m1.89568\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4790 | loss: 1.89568 - acc: 0.5535 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4791  | total loss: \u001b[1m\u001b[32m1.76111\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4791 | loss: 1.76111 - acc: 0.5982 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4792  | total loss: \u001b[1m\u001b[32m1.92472\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4792 | loss: 1.92472 - acc: 0.5383 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4793  | total loss: \u001b[1m\u001b[32m1.78902\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4793 | loss: 1.78902 - acc: 0.5845 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4794  | total loss: \u001b[1m\u001b[32m1.90784\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4794 | loss: 1.90784 - acc: 0.5403 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4795  | total loss: \u001b[1m\u001b[32m1.77562\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4795 | loss: 1.77562 - acc: 0.5863 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4796  | total loss: \u001b[1m\u001b[32m1.65708\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4796 | loss: 1.65708 - acc: 0.6277 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4797  | total loss: \u001b[1m\u001b[32m1.55012\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4797 | loss: 1.55012 - acc: 0.6649 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4798  | total loss: \u001b[1m\u001b[32m1.69055\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4798 | loss: 1.69055 - acc: 0.6127 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4799  | total loss: \u001b[1m\u001b[32m1.57903\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4799 | loss: 1.57903 - acc: 0.6514 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4800  | total loss: \u001b[1m\u001b[32m1.76781\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4800 | loss: 1.76781 - acc: 0.5934 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4801  | total loss: \u001b[1m\u001b[32m1.64770\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4801 | loss: 1.64770 - acc: 0.6341 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4802  | total loss: \u001b[1m\u001b[32m1.82859\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4802 | loss: 1.82859 - acc: 0.5707 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4803  | total loss: \u001b[1m\u001b[32m1.70213\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4803 | loss: 1.70213 - acc: 0.6136 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4804  | total loss: \u001b[1m\u001b[32m1.86201\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4804 | loss: 1.86201 - acc: 0.5594 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4805  | total loss: \u001b[1m\u001b[32m1.73244\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4805 | loss: 1.73244 - acc: 0.6035 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4806  | total loss: \u001b[1m\u001b[32m1.90154\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4806 | loss: 1.90154 - acc: 0.5503 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4807  | total loss: \u001b[1m\u001b[32m1.76865\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4807 | loss: 1.76865 - acc: 0.5952 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4808  | total loss: \u001b[1m\u001b[32m1.90995\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4808 | loss: 1.90995 - acc: 0.5428 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4809  | total loss: \u001b[1m\u001b[32m1.77712\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4809 | loss: 1.77712 - acc: 0.5886 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4810  | total loss: \u001b[1m\u001b[32m1.92362\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4810 | loss: 1.92362 - acc: 0.5368 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4811  | total loss: \u001b[1m\u001b[32m1.79032\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4811 | loss: 1.79032 - acc: 0.5832 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4812  | total loss: \u001b[1m\u001b[32m1.96412\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4812 | loss: 1.96412 - acc: 0.5248 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4813  | total loss: \u001b[1m\u001b[32m1.82777\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4813 | loss: 1.82777 - acc: 0.5724 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4814  | total loss: \u001b[1m\u001b[32m2.00511\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4814 | loss: 2.00511 - acc: 0.5151 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4815  | total loss: \u001b[1m\u001b[32m1.86602\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4815 | loss: 1.86602 - acc: 0.5636 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4816  | total loss: \u001b[1m\u001b[32m2.02420\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4816 | loss: 2.02420 - acc: 0.5073 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4817  | total loss: \u001b[1m\u001b[32m1.88479\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4817 | loss: 1.88479 - acc: 0.5565 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4818  | total loss: \u001b[1m\u001b[32m2.03092\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4818 | loss: 2.03092 - acc: 0.5009 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4819  | total loss: \u001b[1m\u001b[32m1.89238\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4819 | loss: 1.89238 - acc: 0.5508 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4820  | total loss: \u001b[1m\u001b[32m2.02596\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4820 | loss: 2.02596 - acc: 0.4957 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4821  | total loss: \u001b[1m\u001b[32m1.88938\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4821 | loss: 1.88938 - acc: 0.5461 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4822  | total loss: \u001b[1m\u001b[32m1.99757\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4822 | loss: 1.99757 - acc: 0.4987 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4823  | total loss: \u001b[1m\u001b[32m1.86510\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4823 | loss: 1.86510 - acc: 0.5488 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4824  | total loss: \u001b[1m\u001b[32m2.01133\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4824 | loss: 2.01133 - acc: 0.5011 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4825  | total loss: \u001b[1m\u001b[32m1.87857\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4825 | loss: 1.87857 - acc: 0.5510 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4826  | total loss: \u001b[1m\u001b[32m2.00185\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4826 | loss: 2.00185 - acc: 0.5101 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4827  | total loss: \u001b[1m\u001b[32m1.87095\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4827 | loss: 1.87095 - acc: 0.5591 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4828  | total loss: \u001b[1m\u001b[32m2.00672\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4828 | loss: 2.00672 - acc: 0.5032 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4829  | total loss: \u001b[1m\u001b[32m1.87609\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4829 | loss: 1.87609 - acc: 0.5529 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4830  | total loss: \u001b[1m\u001b[32m1.94433\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4830 | loss: 1.94433 - acc: 0.5262 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4831  | total loss: \u001b[1m\u001b[32m1.82023\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4831 | loss: 1.82023 - acc: 0.5736 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4832  | total loss: \u001b[1m\u001b[32m1.70807\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4832 | loss: 1.70807 - acc: 0.6162 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4833  | total loss: \u001b[1m\u001b[32m1.60592\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4833 | loss: 1.60592 - acc: 0.6546 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4834  | total loss: \u001b[1m\u001b[32m1.74431\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4834 | loss: 1.74431 - acc: 0.5963 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4835  | total loss: \u001b[1m\u001b[32m1.63591\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4835 | loss: 1.63591 - acc: 0.6366 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4836  | total loss: \u001b[1m\u001b[32m1.79408\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 4836 | loss: 1.79408 - acc: 0.5801 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4837  | total loss: \u001b[1m\u001b[32m1.67871\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4837 | loss: 1.67871 - acc: 0.6221 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4838  | total loss: \u001b[1m\u001b[32m1.85803\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4838 | loss: 1.85803 - acc: 0.5599 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4839  | total loss: \u001b[1m\u001b[32m1.73491\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4839 | loss: 1.73491 - acc: 0.6039 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4840  | total loss: \u001b[1m\u001b[32m1.62326\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4840 | loss: 1.62326 - acc: 0.6435 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4841  | total loss: \u001b[1m\u001b[32m1.52130\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4841 | loss: 1.52130 - acc: 0.6792 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4842  | total loss: \u001b[1m\u001b[32m1.73684\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4842 | loss: 1.73684 - acc: 0.6112 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4843  | total loss: \u001b[1m\u001b[32m1.62065\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4843 | loss: 1.62065 - acc: 0.6501 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4844  | total loss: \u001b[1m\u001b[32m1.51464\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4844 | loss: 1.51464 - acc: 0.6851 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4845  | total loss: \u001b[1m\u001b[32m1.41731\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4845 | loss: 1.41731 - acc: 0.7166 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4846  | total loss: \u001b[1m\u001b[32m1.59360\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4846 | loss: 1.59360 - acc: 0.6521 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4847  | total loss: \u001b[1m\u001b[32m1.48485\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4847 | loss: 1.48485 - acc: 0.6869 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4848  | total loss: \u001b[1m\u001b[32m1.61284\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4848 | loss: 1.61284 - acc: 0.6396 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4849  | total loss: \u001b[1m\u001b[32m1.49968\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4849 | loss: 1.49968 - acc: 0.6757 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4850  | total loss: \u001b[1m\u001b[32m1.67794\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4850 | loss: 1.67794 - acc: 0.6152 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4851  | total loss: \u001b[1m\u001b[32m1.55670\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4851 | loss: 1.55670 - acc: 0.6537 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4852  | total loss: \u001b[1m\u001b[32m1.75493\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4852 | loss: 1.75493 - acc: 0.5955 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4853  | total loss: \u001b[1m\u001b[32m1.62542\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4853 | loss: 1.62542 - acc: 0.6359 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4854  | total loss: \u001b[1m\u001b[32m1.80464\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4854 | loss: 1.80464 - acc: 0.5795 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4855  | total loss: \u001b[1m\u001b[32m1.67040\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4855 | loss: 1.67040 - acc: 0.6215 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4856  | total loss: \u001b[1m\u001b[32m1.84923\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4856 | loss: 1.84923 - acc: 0.5665 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4857  | total loss: \u001b[1m\u001b[32m1.71138\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4857 | loss: 1.71138 - acc: 0.6099 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4858  | total loss: \u001b[1m\u001b[32m1.89702\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4858 | loss: 1.89702 - acc: 0.5560 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4859  | total loss: \u001b[1m\u001b[32m1.75569\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4859 | loss: 1.75569 - acc: 0.6004 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4860  | total loss: \u001b[1m\u001b[32m1.91150\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4860 | loss: 1.91150 - acc: 0.5475 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4861  | total loss: \u001b[1m\u001b[32m1.77025\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4861 | loss: 1.77025 - acc: 0.5928 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4862  | total loss: \u001b[1m\u001b[32m1.89902\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4862 | loss: 1.89902 - acc: 0.5478 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4863  | total loss: \u001b[1m\u001b[32m1.76050\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4863 | loss: 1.76050 - acc: 0.5930 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4864  | total loss: \u001b[1m\u001b[32m1.93698\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4864 | loss: 1.93698 - acc: 0.5337 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4865  | total loss: \u001b[1m\u001b[32m1.79636\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4865 | loss: 1.79636 - acc: 0.5803 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4866  | total loss: \u001b[1m\u001b[32m1.67041\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4866 | loss: 1.67041 - acc: 0.6223 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4867  | total loss: \u001b[1m\u001b[32m1.55698\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4867 | loss: 1.55698 - acc: 0.6601 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4868  | total loss: \u001b[1m\u001b[32m1.71432\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4868 | loss: 1.71432 - acc: 0.6012 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4869  | total loss: \u001b[1m\u001b[32m1.59605\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4869 | loss: 1.59605 - acc: 0.6411 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4870  | total loss: \u001b[1m\u001b[32m1.48924\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4870 | loss: 1.48924 - acc: 0.6770 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4871  | total loss: \u001b[1m\u001b[32m1.39214\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4871 | loss: 1.39214 - acc: 0.7093 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4872  | total loss: \u001b[1m\u001b[32m1.30328\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4872 | loss: 1.30328 - acc: 0.7384 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4873  | total loss: \u001b[1m\u001b[32m1.22141\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4873 | loss: 1.22141 - acc: 0.7645 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4874  | total loss: \u001b[1m\u001b[32m1.44026\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4874 | loss: 1.44026 - acc: 0.6952 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4875  | total loss: \u001b[1m\u001b[32m1.34146\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4875 | loss: 1.34146 - acc: 0.7257 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4876  | total loss: \u001b[1m\u001b[32m1.25115\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4876 | loss: 1.25115 - acc: 0.7531 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4877  | total loss: \u001b[1m\u001b[32m1.16813\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4877 | loss: 1.16813 - acc: 0.7778 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4878  | total loss: \u001b[1m\u001b[32m1.39148\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4878 | loss: 1.39148 - acc: 0.7072 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4879  | total loss: \u001b[1m\u001b[32m1.29151\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4879 | loss: 1.29151 - acc: 0.7365 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4880  | total loss: \u001b[1m\u001b[32m1.46524\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4880 | loss: 1.46524 - acc: 0.6842 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4881  | total loss: \u001b[1m\u001b[32m1.35631\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4881 | loss: 1.35631 - acc: 0.7158 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4882  | total loss: \u001b[1m\u001b[32m1.54914\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4882 | loss: 1.54914 - acc: 0.6585 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4883  | total loss: \u001b[1m\u001b[32m1.43128\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4883 | loss: 1.43128 - acc: 0.6927 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4884  | total loss: \u001b[1m\u001b[32m1.61778\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4884 | loss: 1.61778 - acc: 0.6377 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4885  | total loss: \u001b[1m\u001b[32m1.49339\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4885 | loss: 1.49339 - acc: 0.6739 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4886  | total loss: \u001b[1m\u001b[32m1.38156\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4886 | loss: 1.38156 - acc: 0.7065 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4887  | total loss: \u001b[1m\u001b[32m1.28059\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4887 | loss: 1.28059 - acc: 0.7359 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4888  | total loss: \u001b[1m\u001b[32m1.18902\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4888 | loss: 1.18902 - acc: 0.7623 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4889  | total loss: \u001b[1m\u001b[32m1.10557\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4889 | loss: 1.10557 - acc: 0.7861 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4890  | total loss: \u001b[1m\u001b[32m1.34409\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 4890 | loss: 1.34409 - acc: 0.7146 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4891  | total loss: \u001b[1m\u001b[32m1.24356\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4891 | loss: 1.24356 - acc: 0.7431 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4892  | total loss: \u001b[1m\u001b[32m1.51243\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4892 | loss: 1.51243 - acc: 0.6688 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4893  | total loss: \u001b[1m\u001b[32m1.39491\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4893 | loss: 1.39491 - acc: 0.7019 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4894  | total loss: \u001b[1m\u001b[32m1.55834\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4894 | loss: 1.55834 - acc: 0.6532 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4895  | total loss: \u001b[1m\u001b[32m1.43707\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4895 | loss: 1.43707 - acc: 0.6879 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4896  | total loss: \u001b[1m\u001b[32m1.66949\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4896 | loss: 1.66949 - acc: 0.6262 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4897  | total loss: \u001b[1m\u001b[32m1.53872\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4897 | loss: 1.53872 - acc: 0.6636 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4898  | total loss: \u001b[1m\u001b[32m1.71945\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4898 | loss: 1.71945 - acc: 0.6115 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4899  | total loss: \u001b[1m\u001b[32m1.58592\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4899 | loss: 1.58592 - acc: 0.6504 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4900  | total loss: \u001b[1m\u001b[32m1.72241\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4900 | loss: 1.72241 - acc: 0.6068 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4901  | total loss: \u001b[1m\u001b[32m1.59103\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4901 | loss: 1.59103 - acc: 0.6461 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4902  | total loss: \u001b[1m\u001b[32m1.47371\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4902 | loss: 1.47371 - acc: 0.6815 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4903  | total loss: \u001b[1m\u001b[32m1.36846\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4903 | loss: 1.36846 - acc: 0.7133 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4904  | total loss: \u001b[1m\u001b[32m1.27351\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4904 | loss: 1.27351 - acc: 0.7420 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4905  | total loss: \u001b[1m\u001b[32m1.18735\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4905 | loss: 1.18735 - acc: 0.7678 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4906  | total loss: \u001b[1m\u001b[32m1.46804\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4906 | loss: 1.46804 - acc: 0.6910 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4907  | total loss: \u001b[1m\u001b[32m1.36131\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4907 | loss: 1.36131 - acc: 0.7219 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4908  | total loss: \u001b[1m\u001b[32m1.59080\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4908 | loss: 1.59080 - acc: 0.6569 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4909  | total loss: \u001b[1m\u001b[32m1.47177\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4909 | loss: 1.47177 - acc: 0.6912 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4910  | total loss: \u001b[1m\u001b[32m1.67564\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4910 | loss: 1.67564 - acc: 0.6292 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4911  | total loss: \u001b[1m\u001b[32m1.54893\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4911 | loss: 1.54893 - acc: 0.6663 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4912  | total loss: \u001b[1m\u001b[32m1.75162\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4912 | loss: 1.75162 - acc: 0.5997 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4913  | total loss: \u001b[1m\u001b[32m1.61898\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4913 | loss: 1.61898 - acc: 0.6397 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4914  | total loss: \u001b[1m\u001b[32m1.80363\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4914 | loss: 1.80363 - acc: 0.5829 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4915  | total loss: \u001b[1m\u001b[32m1.66820\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4915 | loss: 1.66820 - acc: 0.6246 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4916  | total loss: \u001b[1m\u001b[32m1.54733\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4916 | loss: 1.54733 - acc: 0.6621 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4917  | total loss: \u001b[1m\u001b[32m1.43893\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4917 | loss: 1.43893 - acc: 0.6959 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4918  | total loss: \u001b[1m\u001b[32m1.66464\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4918 | loss: 1.66464 - acc: 0.6263 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4919  | total loss: \u001b[1m\u001b[32m1.54517\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4919 | loss: 1.54517 - acc: 0.6637 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4920  | total loss: \u001b[1m\u001b[32m1.74506\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4920 | loss: 1.74506 - acc: 0.5973 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4921  | total loss: \u001b[1m\u001b[32m1.61906\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4921 | loss: 1.61906 - acc: 0.6376 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4922  | total loss: \u001b[1m\u001b[32m1.82922\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4922 | loss: 1.82922 - acc: 0.5738 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4923  | total loss: \u001b[1m\u001b[32m1.69686\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4923 | loss: 1.69686 - acc: 0.6164 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4924  | total loss: \u001b[1m\u001b[32m1.57843\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4924 | loss: 1.57843 - acc: 0.6548 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4925  | total loss: \u001b[1m\u001b[32m1.47182\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4925 | loss: 1.47182 - acc: 0.6893 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4926  | total loss: \u001b[1m\u001b[32m1.69444\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4926 | loss: 1.69444 - acc: 0.6204 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4927  | total loss: \u001b[1m\u001b[32m1.57597\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4927 | loss: 1.57597 - acc: 0.6583 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4928  | total loss: \u001b[1m\u001b[32m1.46906\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4928 | loss: 1.46906 - acc: 0.6925 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4929  | total loss: \u001b[1m\u001b[32m1.37196\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4929 | loss: 1.37196 - acc: 0.7233 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4930  | total loss: \u001b[1m\u001b[32m1.59724\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4930 | loss: 1.59724 - acc: 0.6509 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4931  | total loss: \u001b[1m\u001b[32m1.48562\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4931 | loss: 1.48562 - acc: 0.6858 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4932  | total loss: \u001b[1m\u001b[32m1.63604\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4932 | loss: 1.63604 - acc: 0.6315 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4933  | total loss: \u001b[1m\u001b[32m1.51980\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4933 | loss: 1.51980 - acc: 0.6684 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4934  | total loss: \u001b[1m\u001b[32m1.70627\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4934 | loss: 1.70627 - acc: 0.6230 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4935  | total loss: \u001b[1m\u001b[32m1.58273\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4935 | loss: 1.58273 - acc: 0.6607 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4936  | total loss: \u001b[1m\u001b[32m1.74888\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4936 | loss: 1.74888 - acc: 0.6018 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4937  | total loss: \u001b[1m\u001b[32m1.62125\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4937 | loss: 1.62125 - acc: 0.6416 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4938  | total loss: \u001b[1m\u001b[32m1.50632\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4938 | loss: 1.50632 - acc: 0.6774 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4939  | total loss: \u001b[1m\u001b[32m1.40227\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4939 | loss: 1.40227 - acc: 0.7097 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4940  | total loss: \u001b[1m\u001b[32m1.30752\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4940 | loss: 1.30752 - acc: 0.7387 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4941  | total loss: \u001b[1m\u001b[32m1.22074\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4941 | loss: 1.22074 - acc: 0.7648 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4942  | total loss: \u001b[1m\u001b[32m1.47124\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4942 | loss: 1.47124 - acc: 0.6884 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4943  | total loss: \u001b[1m\u001b[32m1.36561\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4943 | loss: 1.36561 - acc: 0.7195 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4944  | total loss: \u001b[1m\u001b[32m1.56808\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4944 | loss: 1.56808 - acc: 0.6619 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4945  | total loss: \u001b[1m\u001b[32m1.45163\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4945 | loss: 1.45163 - acc: 0.6957 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4946  | total loss: \u001b[1m\u001b[32m1.65694\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4946 | loss: 1.65694 - acc: 0.6332 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4947  | total loss: \u001b[1m\u001b[32m1.53152\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4947 | loss: 1.53152 - acc: 0.6699 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4948  | total loss: \u001b[1m\u001b[32m1.74808\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4948 | loss: 1.74808 - acc: 0.6029 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4949  | total loss: \u001b[1m\u001b[32m1.61457\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4949 | loss: 1.61457 - acc: 0.6426 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4950  | total loss: \u001b[1m\u001b[32m1.82929\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4950 | loss: 1.82929 - acc: 0.5784 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4951  | total loss: \u001b[1m\u001b[32m1.68976\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4951 | loss: 1.68976 - acc: 0.6205 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4952  | total loss: \u001b[1m\u001b[32m1.56510\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4952 | loss: 1.56510 - acc: 0.6585 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4953  | total loss: \u001b[1m\u001b[32m1.45323\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4953 | loss: 1.45323 - acc: 0.6926 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4954  | total loss: \u001b[1m\u001b[32m1.66117\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4954 | loss: 1.66117 - acc: 0.6305 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4955  | total loss: \u001b[1m\u001b[32m1.54015\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4955 | loss: 1.54015 - acc: 0.6675 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4956  | total loss: \u001b[1m\u001b[32m1.43134\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4956 | loss: 1.43134 - acc: 0.7007 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4957  | total loss: \u001b[1m\u001b[32m1.33295\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4957 | loss: 1.33295 - acc: 0.7306 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4958  | total loss: \u001b[1m\u001b[32m1.60120\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4958 | loss: 1.60120 - acc: 0.6576 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4959  | total loss: \u001b[1m\u001b[32m1.48513\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4959 | loss: 1.48513 - acc: 0.6918 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4960  | total loss: \u001b[1m\u001b[32m1.69074\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4960 | loss: 1.69074 - acc: 0.6298 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4961  | total loss: \u001b[1m\u001b[32m1.56604\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4961 | loss: 1.56604 - acc: 0.6668 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4962  | total loss: \u001b[1m\u001b[32m1.76227\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4962 | loss: 1.76227 - acc: 0.6001 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4963  | total loss: \u001b[1m\u001b[32m1.63154\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4963 | loss: 1.63154 - acc: 0.6401 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4964  | total loss: \u001b[1m\u001b[32m1.80718\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4964 | loss: 1.80718 - acc: 0.5832 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4965  | total loss: \u001b[1m\u001b[32m1.67382\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4965 | loss: 1.67382 - acc: 0.6249 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4966  | total loss: \u001b[1m\u001b[32m1.55453\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4966 | loss: 1.55453 - acc: 0.6624 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4967  | total loss: \u001b[1m\u001b[32m1.44723\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4967 | loss: 1.44723 - acc: 0.6962 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4968  | total loss: \u001b[1m\u001b[32m1.35010\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4968 | loss: 1.35010 - acc: 0.7266 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4969  | total loss: \u001b[1m\u001b[32m1.26160\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4969 | loss: 1.26160 - acc: 0.7539 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4970  | total loss: \u001b[1m\u001b[32m1.51195\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4970 | loss: 1.51195 - acc: 0.6785 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4971  | total loss: \u001b[1m\u001b[32m1.40530\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4971 | loss: 1.40530 - acc: 0.7107 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4972  | total loss: \u001b[1m\u001b[32m1.59969\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4972 | loss: 1.59969 - acc: 0.6467 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4973  | total loss: \u001b[1m\u001b[32m1.48340\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4973 | loss: 1.48340 - acc: 0.6821 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4974  | total loss: \u001b[1m\u001b[32m1.73278\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4974 | loss: 1.73278 - acc: 0.6139 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4975  | total loss: \u001b[1m\u001b[32m1.60326\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4975 | loss: 1.60326 - acc: 0.6525 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4976  | total loss: \u001b[1m\u001b[32m1.80423\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4976 | loss: 1.80423 - acc: 0.5872 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4977  | total loss: \u001b[1m\u001b[32m1.66847\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4977 | loss: 1.66847 - acc: 0.6285 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4978  | total loss: \u001b[1m\u001b[32m1.86593\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4978 | loss: 1.86593 - acc: 0.5728 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4979  | total loss: \u001b[1m\u001b[32m1.72547\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4979 | loss: 1.72547 - acc: 0.6155 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4980  | total loss: \u001b[1m\u001b[32m1.59956\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4980 | loss: 1.59956 - acc: 0.6540 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4981  | total loss: \u001b[1m\u001b[32m1.48616\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4981 | loss: 1.48616 - acc: 0.6886 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4982  | total loss: \u001b[1m\u001b[32m1.68720\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4982 | loss: 1.68720 - acc: 0.6197 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4983  | total loss: \u001b[1m\u001b[32m1.56474\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4983 | loss: 1.56474 - acc: 0.6577 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4984  | total loss: \u001b[1m\u001b[32m1.45429\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4984 | loss: 1.45429 - acc: 0.6920 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4985  | total loss: \u001b[1m\u001b[32m1.35413\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4985 | loss: 1.35413 - acc: 0.7228 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4986  | total loss: \u001b[1m\u001b[32m1.56663\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4986 | loss: 1.56663 - acc: 0.6576 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4987  | total loss: \u001b[1m\u001b[32m1.45388\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4987 | loss: 1.45388 - acc: 0.6919 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4988  | total loss: \u001b[1m\u001b[32m1.35175\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4988 | loss: 1.35175 - acc: 0.7227 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4989  | total loss: \u001b[1m\u001b[32m1.25876\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4989 | loss: 1.25876 - acc: 0.7504 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4990  | total loss: \u001b[1m\u001b[32m1.51899\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4990 | loss: 1.51899 - acc: 0.6754 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4991  | total loss: \u001b[1m\u001b[32m1.40761\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4991 | loss: 1.40761 - acc: 0.7078 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4992  | total loss: \u001b[1m\u001b[32m1.62125\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4992 | loss: 1.62125 - acc: 0.6513 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4993  | total loss: \u001b[1m\u001b[32m1.49921\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4993 | loss: 1.49921 - acc: 0.6862 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4994  | total loss: \u001b[1m\u001b[32m1.38912\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4994 | loss: 1.38912 - acc: 0.7176 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4995  | total loss: \u001b[1m\u001b[32m1.28937\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4995 | loss: 1.28937 - acc: 0.7458 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4996  | total loss: \u001b[1m\u001b[32m1.51267\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4996 | loss: 1.51267 - acc: 0.6855 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4997  | total loss: \u001b[1m\u001b[32m1.39941\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4997 | loss: 1.39941 - acc: 0.7170 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4998  | total loss: \u001b[1m\u001b[32m1.59073\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4998 | loss: 1.59073 - acc: 0.6596 -- iter: 14/14\n",
      "--\n",
      "Training Step: 4999  | total loss: \u001b[1m\u001b[32m1.46943\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4999 | loss: 1.46943 - acc: 0.6936 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5000  | total loss: \u001b[1m\u001b[32m1.67602\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5000 | loss: 1.67602 - acc: 0.6314 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5001  | total loss: \u001b[1m\u001b[32m1.54670\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5001 | loss: 1.54670 - acc: 0.6683 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5002  | total loss: \u001b[1m\u001b[32m1.71733\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5002 | loss: 1.71733 - acc: 0.6157 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5003  | total loss: \u001b[1m\u001b[32m1.58502\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5003 | loss: 1.58502 - acc: 0.6541 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5004  | total loss: \u001b[1m\u001b[32m1.79175\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5004 | loss: 1.79175 - acc: 0.5887 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5005  | total loss: \u001b[1m\u001b[32m1.65377\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5005 | loss: 1.65377 - acc: 0.6299 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5006  | total loss: \u001b[1m\u001b[32m1.82257\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5006 | loss: 1.82257 - acc: 0.5740 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5007  | total loss: \u001b[1m\u001b[32m1.68384\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5007 | loss: 1.68384 - acc: 0.6166 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5008  | total loss: \u001b[1m\u001b[32m1.85869\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5008 | loss: 1.85869 - acc: 0.5621 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5009  | total loss: \u001b[1m\u001b[32m1.71892\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5009 | loss: 1.71892 - acc: 0.6059 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5010  | total loss: \u001b[1m\u001b[32m1.87858\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5010 | loss: 1.87858 - acc: 0.5524 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5011  | total loss: \u001b[1m\u001b[32m1.73954\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5011 | loss: 1.73954 - acc: 0.5972 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5012  | total loss: \u001b[1m\u001b[32m1.61544\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5012 | loss: 1.61544 - acc: 0.6375 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5013  | total loss: \u001b[1m\u001b[32m1.50409\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5013 | loss: 1.50409 - acc: 0.6737 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5014  | total loss: \u001b[1m\u001b[32m1.63819\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5014 | loss: 1.63819 - acc: 0.6278 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5015  | total loss: \u001b[1m\u001b[32m1.52471\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5015 | loss: 1.52471 - acc: 0.6650 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5016  | total loss: \u001b[1m\u001b[32m1.42238\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5016 | loss: 1.42238 - acc: 0.6985 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5017  | total loss: \u001b[1m\u001b[32m1.32953\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5017 | loss: 1.32953 - acc: 0.7287 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5018  | total loss: \u001b[1m\u001b[32m1.24470\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5018 | loss: 1.24470 - acc: 0.7558 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5019  | total loss: \u001b[1m\u001b[32m1.16668\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5019 | loss: 1.16668 - acc: 0.7802 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5020  | total loss: \u001b[1m\u001b[32m1.43115\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5020 | loss: 1.43115 - acc: 0.7022 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5021  | total loss: \u001b[1m\u001b[32m1.33168\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5021 | loss: 1.33168 - acc: 0.7320 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5022  | total loss: \u001b[1m\u001b[32m1.54372\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5022 | loss: 1.54372 - acc: 0.6659 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5023  | total loss: \u001b[1m\u001b[32m1.43165\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5023 | loss: 1.43165 - acc: 0.6993 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5024  | total loss: \u001b[1m\u001b[32m1.33017\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5024 | loss: 1.33017 - acc: 0.7294 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5025  | total loss: \u001b[1m\u001b[32m1.23785\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 5025 | loss: 1.23785 - acc: 0.7565 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5026  | total loss: \u001b[1m\u001b[32m1.15342\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5026 | loss: 1.15342 - acc: 0.7808 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5027  | total loss: \u001b[1m\u001b[32m1.07582\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5027 | loss: 1.07582 - acc: 0.8027 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5028  | total loss: \u001b[1m\u001b[32m1.00415\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5028 | loss: 1.00415 - acc: 0.8225 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5029  | total loss: \u001b[1m\u001b[32m0.93768\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5029 | loss: 0.93768 - acc: 0.8402 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5030  | total loss: \u001b[1m\u001b[32m1.20131\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5030 | loss: 1.20131 - acc: 0.7633 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5031  | total loss: \u001b[1m\u001b[32m1.11212\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5031 | loss: 1.11212 - acc: 0.7870 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5032  | total loss: \u001b[1m\u001b[32m1.39882\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5032 | loss: 1.39882 - acc: 0.7083 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5033  | total loss: \u001b[1m\u001b[32m1.28870\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5033 | loss: 1.28870 - acc: 0.7375 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5034  | total loss: \u001b[1m\u001b[32m1.55301\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5034 | loss: 1.55301 - acc: 0.6637 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5035  | total loss: \u001b[1m\u001b[32m1.42772\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5035 | loss: 1.42772 - acc: 0.6973 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5036  | total loss: \u001b[1m\u001b[32m1.65837\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5036 | loss: 1.65837 - acc: 0.6348 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5037  | total loss: \u001b[1m\u001b[32m1.52383\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5037 | loss: 1.52383 - acc: 0.6713 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5038  | total loss: \u001b[1m\u001b[32m1.77618\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5038 | loss: 1.77618 - acc: 0.6042 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5039  | total loss: \u001b[1m\u001b[32m1.63206\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5039 | loss: 1.63206 - acc: 0.6437 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5040  | total loss: \u001b[1m\u001b[32m1.50343\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5040 | loss: 1.50343 - acc: 0.6794 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5041  | total loss: \u001b[1m\u001b[32m1.38827\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5041 | loss: 1.38827 - acc: 0.7114 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5042  | total loss: \u001b[1m\u001b[32m1.58926\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5042 | loss: 1.58926 - acc: 0.6546 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5043  | total loss: \u001b[1m\u001b[32m1.46668\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5043 | loss: 1.46668 - acc: 0.6891 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5044  | total loss: \u001b[1m\u001b[32m1.62596\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5044 | loss: 1.62596 - acc: 0.6416 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5045  | total loss: \u001b[1m\u001b[32m1.50136\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5045 | loss: 1.50136 - acc: 0.6775 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5046  | total loss: \u001b[1m\u001b[32m1.71268\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5046 | loss: 1.71268 - acc: 0.6169 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5047  | total loss: \u001b[1m\u001b[32m1.58157\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5047 | loss: 1.58157 - acc: 0.6552 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5048  | total loss: \u001b[1m\u001b[32m1.46454\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5048 | loss: 1.46454 - acc: 0.6897 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5049  | total loss: \u001b[1m\u001b[32m1.35964\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5049 | loss: 1.35964 - acc: 0.7207 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5050  | total loss: \u001b[1m\u001b[32m1.59822\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5050 | loss: 1.59822 - acc: 0.6486 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5051  | total loss: \u001b[1m\u001b[32m1.48072\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5051 | loss: 1.48072 - acc: 0.6838 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5052  | total loss: \u001b[1m\u001b[32m1.68464\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5052 | loss: 1.68464 - acc: 0.6225 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5053  | total loss: \u001b[1m\u001b[32m1.56001\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5053 | loss: 1.56001 - acc: 0.6603 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5054  | total loss: \u001b[1m\u001b[32m1.77356\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5054 | loss: 1.77356 - acc: 0.5942 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5055  | total loss: \u001b[1m\u001b[32m1.64227\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5055 | loss: 1.64227 - acc: 0.6348 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5056  | total loss: \u001b[1m\u001b[32m1.81527\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5056 | loss: 1.81527 - acc: 0.5785 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5057  | total loss: \u001b[1m\u001b[32m1.68242\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5057 | loss: 1.68242 - acc: 0.6206 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5058  | total loss: \u001b[1m\u001b[32m1.81116\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5058 | loss: 1.81116 - acc: 0.5729 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5059  | total loss: \u001b[1m\u001b[32m1.68122\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5059 | loss: 1.68122 - acc: 0.6156 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5060  | total loss: \u001b[1m\u001b[32m1.85563\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5060 | loss: 1.85563 - acc: 0.5612 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5061  | total loss: \u001b[1m\u001b[32m1.72368\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5061 | loss: 1.72368 - acc: 0.6050 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5062  | total loss: \u001b[1m\u001b[32m1.60578\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5062 | loss: 1.60578 - acc: 0.6445 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5063  | total loss: \u001b[1m\u001b[32m1.49977\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5063 | loss: 1.49977 - acc: 0.6801 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5064  | total loss: \u001b[1m\u001b[32m1.69382\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 5064 | loss: 1.69382 - acc: 0.6192 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5065  | total loss: \u001b[1m\u001b[32m1.57876\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 5065 | loss: 1.57876 - acc: 0.6573 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5066  | total loss: \u001b[1m\u001b[32m1.74926\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5066 | loss: 1.74926 - acc: 0.5987 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5067  | total loss: \u001b[1m\u001b[32m1.62885\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5067 | loss: 1.62885 - acc: 0.6388 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5068  | total loss: \u001b[1m\u001b[32m1.52031\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5068 | loss: 1.52031 - acc: 0.6750 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5069  | total loss: \u001b[1m\u001b[32m1.42181\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5069 | loss: 1.42181 - acc: 0.7075 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5070  | total loss: \u001b[1m\u001b[32m1.63829\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5070 | loss: 1.63829 - acc: 0.6367 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5071  | total loss: \u001b[1m\u001b[32m1.52645\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5071 | loss: 1.52645 - acc: 0.6730 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5072  | total loss: \u001b[1m\u001b[32m1.69120\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5072 | loss: 1.69120 - acc: 0.6200 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5073  | total loss: \u001b[1m\u001b[32m1.57340\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5073 | loss: 1.57340 - acc: 0.6580 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5074  | total loss: \u001b[1m\u001b[32m1.76015\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5074 | loss: 1.76015 - acc: 0.5994 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5075  | total loss: \u001b[1m\u001b[32m1.63541\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5075 | loss: 1.63541 - acc: 0.6394 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5076  | total loss: \u001b[1m\u001b[32m1.82826\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5076 | loss: 1.82826 - acc: 0.5755 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5077  | total loss: \u001b[1m\u001b[32m1.69733\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5077 | loss: 1.69733 - acc: 0.6179 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5078  | total loss: \u001b[1m\u001b[32m1.81587\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5078 | loss: 1.81587 - acc: 0.5776 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5079  | total loss: \u001b[1m\u001b[32m1.68693\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5079 | loss: 1.68693 - acc: 0.6198 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5080  | total loss: \u001b[1m\u001b[32m1.57087\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5080 | loss: 1.57087 - acc: 0.6578 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5081  | total loss: \u001b[1m\u001b[32m1.46579\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5081 | loss: 1.46579 - acc: 0.6920 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5082  | total loss: \u001b[1m\u001b[32m1.62586\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5082 | loss: 1.62586 - acc: 0.6371 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5083  | total loss: \u001b[1m\u001b[32m1.51378\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5083 | loss: 1.51378 - acc: 0.6734 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5084  | total loss: \u001b[1m\u001b[32m1.69649\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5084 | loss: 1.69649 - acc: 0.6132 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5085  | total loss: \u001b[1m\u001b[32m1.57656\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5085 | loss: 1.57656 - acc: 0.6519 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5086  | total loss: \u001b[1m\u001b[32m1.77636\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5086 | loss: 1.77636 - acc: 0.5867 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5087  | total loss: \u001b[1m\u001b[32m1.64843\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5087 | loss: 1.64843 - acc: 0.6280 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5088  | total loss: \u001b[1m\u001b[32m1.78807\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5088 | loss: 1.78807 - acc: 0.5867 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5089  | total loss: \u001b[1m\u001b[32m1.65935\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5089 | loss: 1.65935 - acc: 0.6280 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5090  | total loss: \u001b[1m\u001b[32m1.81047\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5090 | loss: 1.81047 - acc: 0.5723 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5091  | total loss: \u001b[1m\u001b[32m1.68021\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5091 | loss: 1.68021 - acc: 0.6151 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5092  | total loss: \u001b[1m\u001b[32m1.88774\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5092 | loss: 1.88774 - acc: 0.5536 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5093  | total loss: \u001b[1m\u001b[32m1.75089\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5093 | loss: 1.75089 - acc: 0.5982 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5094  | total loss: \u001b[1m\u001b[32m1.87596\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5094 | loss: 1.87596 - acc: 0.5527 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5095  | total loss: \u001b[1m\u001b[32m1.74158\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5095 | loss: 1.74158 - acc: 0.5974 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5096  | total loss: \u001b[1m\u001b[32m1.89751\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5096 | loss: 1.89751 - acc: 0.5377 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5097  | total loss: \u001b[1m\u001b[32m1.76240\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5097 | loss: 1.76240 - acc: 0.5839 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5098  | total loss: \u001b[1m\u001b[32m1.91698\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5098 | loss: 1.91698 - acc: 0.5255 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5099  | total loss: \u001b[1m\u001b[32m1.78159\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5099 | loss: 1.78159 - acc: 0.5730 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5100  | total loss: \u001b[1m\u001b[32m1.92394\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5100 | loss: 1.92394 - acc: 0.5228 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5101  | total loss: \u001b[1m\u001b[32m1.78970\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5101 | loss: 1.78970 - acc: 0.5705 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5102  | total loss: \u001b[1m\u001b[32m1.96952\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5102 | loss: 1.96952 - acc: 0.5135 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5103  | total loss: \u001b[1m\u001b[32m1.83268\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5103 | loss: 1.83268 - acc: 0.5621 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5104  | total loss: \u001b[1m\u001b[32m1.71010\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5104 | loss: 1.71010 - acc: 0.6059 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5105  | total loss: \u001b[1m\u001b[32m1.59957\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5105 | loss: 1.59957 - acc: 0.6453 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5106  | total loss: \u001b[1m\u001b[32m1.78525\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5106 | loss: 1.78525 - acc: 0.5808 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5107  | total loss: \u001b[1m\u001b[32m1.66640\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5107 | loss: 1.66640 - acc: 0.6227 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5108  | total loss: \u001b[1m\u001b[32m1.55880\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5108 | loss: 1.55880 - acc: 0.6604 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5109  | total loss: \u001b[1m\u001b[32m1.46072\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5109 | loss: 1.46072 - acc: 0.6944 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5110  | total loss: \u001b[1m\u001b[32m1.65770\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5110 | loss: 1.65770 - acc: 0.6250 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5111  | total loss: \u001b[1m\u001b[32m1.54736\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5111 | loss: 1.54736 - acc: 0.6625 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5112  | total loss: \u001b[1m\u001b[32m1.75125\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5112 | loss: 1.75125 - acc: 0.5962 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5113  | total loss: \u001b[1m\u001b[32m1.63033\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 5113 | loss: 1.63033 - acc: 0.6366 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5114  | total loss: \u001b[1m\u001b[32m1.75808\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5114 | loss: 1.75808 - acc: 0.5944 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5115  | total loss: \u001b[1m\u001b[32m1.63581\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5115 | loss: 1.63581 - acc: 0.6349 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5116  | total loss: \u001b[1m\u001b[32m1.82160\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 5116 | loss: 1.82160 - acc: 0.5786 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5117  | total loss: \u001b[1m\u001b[32m1.69266\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5117 | loss: 1.69266 - acc: 0.6207 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5118  | total loss: \u001b[1m\u001b[32m1.79057\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5118 | loss: 1.79057 - acc: 0.5801 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5119  | total loss: \u001b[1m\u001b[32m1.66464\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5119 | loss: 1.66464 - acc: 0.6221 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5120  | total loss: \u001b[1m\u001b[32m1.82961\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5120 | loss: 1.82961 - acc: 0.5670 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5121  | total loss: \u001b[1m\u001b[32m1.69988\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5121 | loss: 1.69988 - acc: 0.6103 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5122  | total loss: \u001b[1m\u001b[32m1.89025\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5122 | loss: 1.89025 - acc: 0.5493 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5123  | total loss: \u001b[1m\u001b[32m1.75502\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5123 | loss: 1.75502 - acc: 0.5943 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5124  | total loss: \u001b[1m\u001b[32m1.93052\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5124 | loss: 1.93052 - acc: 0.5349 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5125  | total loss: \u001b[1m\u001b[32m1.79240\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5125 | loss: 1.79240 - acc: 0.5814 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5126  | total loss: \u001b[1m\u001b[32m1.96269\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5126 | loss: 1.96269 - acc: 0.5233 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5127  | total loss: \u001b[1m\u001b[32m1.82294\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5127 | loss: 1.82294 - acc: 0.5710 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5128  | total loss: \u001b[1m\u001b[32m1.99922\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5128 | loss: 1.99922 - acc: 0.5139 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5129  | total loss: \u001b[1m\u001b[32m1.85767\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5129 | loss: 1.85767 - acc: 0.5625 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5130  | total loss: \u001b[1m\u001b[32m2.00337\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5130 | loss: 2.00337 - acc: 0.5134 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5131  | total loss: \u001b[1m\u001b[32m1.86329\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5131 | loss: 1.86329 - acc: 0.5620 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5132  | total loss: \u001b[1m\u001b[32m2.02533\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5132 | loss: 2.02533 - acc: 0.5058 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5133  | total loss: \u001b[1m\u001b[32m1.88480\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5133 | loss: 1.88480 - acc: 0.5552 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5134  | total loss: \u001b[1m\u001b[32m2.02866\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5134 | loss: 2.02866 - acc: 0.4997 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5135  | total loss: \u001b[1m\u001b[32m1.88944\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5135 | loss: 1.88944 - acc: 0.5497 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5136  | total loss: \u001b[1m\u001b[32m2.00203\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5136 | loss: 2.00203 - acc: 0.5162 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5137  | total loss: \u001b[1m\u001b[32m1.86672\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5137 | loss: 1.86672 - acc: 0.5646 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5138  | total loss: \u001b[1m\u001b[32m1.97646\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5138 | loss: 1.97646 - acc: 0.5296 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5139  | total loss: \u001b[1m\u001b[32m1.84430\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5139 | loss: 1.84430 - acc: 0.5766 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5140  | total loss: \u001b[1m\u001b[32m1.97053\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5140 | loss: 1.97053 - acc: 0.5261 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5141  | total loss: \u001b[1m\u001b[32m1.83921\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 5141 | loss: 1.83921 - acc: 0.5735 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5142  | total loss: \u001b[1m\u001b[32m1.96306\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5142 | loss: 1.96306 - acc: 0.5233 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5143  | total loss: \u001b[1m\u001b[32m1.83265\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5143 | loss: 1.83265 - acc: 0.5709 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5144  | total loss: \u001b[1m\u001b[32m1.98991\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5144 | loss: 1.98991 - acc: 0.5138 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5145  | total loss: \u001b[1m\u001b[32m1.85704\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5145 | loss: 1.85704 - acc: 0.5625 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5146  | total loss: \u001b[1m\u001b[32m2.00822\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5146 | loss: 2.00822 - acc: 0.5062 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5147  | total loss: \u001b[1m\u001b[32m1.87398\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5147 | loss: 1.87398 - acc: 0.5556 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5148  | total loss: \u001b[1m\u001b[32m1.75304\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5148 | loss: 1.75304 - acc: 0.6000 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5149  | total loss: \u001b[1m\u001b[32m1.64331\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5149 | loss: 1.64331 - acc: 0.6400 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5150  | total loss: \u001b[1m\u001b[32m1.74986\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5150 | loss: 1.74986 - acc: 0.5975 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5151  | total loss: \u001b[1m\u001b[32m1.63802\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5151 | loss: 1.63802 - acc: 0.6377 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5152  | total loss: \u001b[1m\u001b[32m1.79158\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5152 | loss: 1.79158 - acc: 0.5811 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5153  | total loss: \u001b[1m\u001b[32m1.67340\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5153 | loss: 1.67340 - acc: 0.6230 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5154  | total loss: \u001b[1m\u001b[32m1.82440\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5154 | loss: 1.82440 - acc: 0.5678 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5155  | total loss: \u001b[1m\u001b[32m1.70136\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5155 | loss: 1.70136 - acc: 0.6110 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5156  | total loss: \u001b[1m\u001b[32m1.87735\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5156 | loss: 1.87735 - acc: 0.5499 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5157  | total loss: \u001b[1m\u001b[32m1.74817\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5157 | loss: 1.74817 - acc: 0.5949 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5158  | total loss: \u001b[1m\u001b[32m1.87533\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 5158 | loss: 1.87533 - acc: 0.5497 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5159  | total loss: \u001b[1m\u001b[32m1.74589\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 5159 | loss: 1.74589 - acc: 0.5948 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5160  | total loss: \u001b[1m\u001b[32m1.93570\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5160 | loss: 1.93570 - acc: 0.5353 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5161  | total loss: \u001b[1m\u001b[32m1.80019\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5161 | loss: 1.80019 - acc: 0.5818 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5162  | total loss: \u001b[1m\u001b[32m1.93819\u001b[0m\u001b[0m | time: 0.025s\n",
      "| Adam | epoch: 5162 | loss: 1.93819 - acc: 0.5307 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5163  | total loss: \u001b[1m\u001b[32m1.80283\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5163 | loss: 1.80283 - acc: 0.5777 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5164  | total loss: \u001b[1m\u001b[32m1.96766\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5164 | loss: 1.96766 - acc: 0.5270 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5165  | total loss: \u001b[1m\u001b[32m1.82989\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5165 | loss: 1.82989 - acc: 0.5743 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5166  | total loss: \u001b[1m\u001b[32m1.99628\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 5166 | loss: 1.99628 - acc: 0.5169 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5167  | total loss: \u001b[1m\u001b[32m1.85645\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5167 | loss: 1.85645 - acc: 0.5652 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5168  | total loss: \u001b[1m\u001b[32m2.00044\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 5168 | loss: 2.00044 - acc: 0.5087 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5169  | total loss: \u001b[1m\u001b[32m1.86119\u001b[0m\u001b[0m | time: 0.016s\n",
      "| Adam | epoch: 5169 | loss: 1.86119 - acc: 0.5578 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5170  | total loss: \u001b[1m\u001b[32m1.97032\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5170 | loss: 1.97032 - acc: 0.5163 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5171  | total loss: \u001b[1m\u001b[32m1.83499\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5171 | loss: 1.83499 - acc: 0.5647 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5172  | total loss: \u001b[1m\u001b[32m1.71321\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5172 | loss: 1.71321 - acc: 0.6082 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5173  | total loss: \u001b[1m\u001b[32m1.60293\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5173 | loss: 1.60293 - acc: 0.6474 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5174  | total loss: \u001b[1m\u001b[32m1.78361\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5174 | loss: 1.78361 - acc: 0.5827 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5175  | total loss: \u001b[1m\u001b[32m1.66473\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5175 | loss: 1.66473 - acc: 0.6244 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5176  | total loss: \u001b[1m\u001b[32m1.55683\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5176 | loss: 1.55683 - acc: 0.6620 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5177  | total loss: \u001b[1m\u001b[32m1.45824\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5177 | loss: 1.45824 - acc: 0.6958 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5178  | total loss: \u001b[1m\u001b[32m1.65840\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5178 | loss: 1.65840 - acc: 0.6262 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5179  | total loss: \u001b[1m\u001b[32m1.54681\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5179 | loss: 1.54681 - acc: 0.6636 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5180  | total loss: \u001b[1m\u001b[32m1.73047\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5180 | loss: 1.73047 - acc: 0.6043 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5181  | total loss: \u001b[1m\u001b[32m1.60991\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5181 | loss: 1.60991 - acc: 0.6439 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5182  | total loss: \u001b[1m\u001b[32m1.80009\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5182 | loss: 1.80009 - acc: 0.5867 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5183  | total loss: \u001b[1m\u001b[32m1.67167\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5183 | loss: 1.67167 - acc: 0.6280 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5184  | total loss: \u001b[1m\u001b[32m1.87054\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5184 | loss: 1.87054 - acc: 0.5652 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5185  | total loss: \u001b[1m\u001b[32m1.73493\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5185 | loss: 1.73493 - acc: 0.6087 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5186  | total loss: \u001b[1m\u001b[32m1.89436\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5186 | loss: 1.89436 - acc: 0.5550 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5187  | total loss: \u001b[1m\u001b[32m1.75681\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5187 | loss: 1.75681 - acc: 0.5995 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5188  | total loss: \u001b[1m\u001b[32m1.93795\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5188 | loss: 1.93795 - acc: 0.5467 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5189  | total loss: \u001b[1m\u001b[32m1.79690\u001b[0m\u001b[0m | time: 0.026s\n",
      "| Adam | epoch: 5189 | loss: 1.79690 - acc: 0.5920 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5190  | total loss: \u001b[1m\u001b[32m1.95519\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5190 | loss: 1.95519 - acc: 0.5399 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5191  | total loss: \u001b[1m\u001b[32m1.81361\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5191 | loss: 1.81361 - acc: 0.5859 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5192  | total loss: \u001b[1m\u001b[32m1.99978\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5192 | loss: 1.99978 - acc: 0.5273 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5193  | total loss: \u001b[1m\u001b[32m1.85523\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5193 | loss: 1.85523 - acc: 0.5746 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5194  | total loss: \u001b[1m\u001b[32m1.72559\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5194 | loss: 1.72559 - acc: 0.6172 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5195  | total loss: \u001b[1m\u001b[32m1.60870\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5195 | loss: 1.60870 - acc: 0.6554 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5196  | total loss: \u001b[1m\u001b[32m1.50265\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5196 | loss: 1.50265 - acc: 0.6899 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5197  | total loss: \u001b[1m\u001b[32m1.40585\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5197 | loss: 1.40585 - acc: 0.7209 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5198  | total loss: \u001b[1m\u001b[32m1.31691\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5198 | loss: 1.31691 - acc: 0.7488 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5199  | total loss: \u001b[1m\u001b[32m1.23469\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5199 | loss: 1.23469 - acc: 0.7739 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5200  | total loss: \u001b[1m\u001b[32m1.42435\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5200 | loss: 1.42435 - acc: 0.7108 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5201  | total loss: \u001b[1m\u001b[32m1.32749\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5201 | loss: 1.32749 - acc: 0.7397 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5202  | total loss: \u001b[1m\u001b[32m1.23855\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5202 | loss: 1.23855 - acc: 0.7658 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5203  | total loss: \u001b[1m\u001b[32m1.15649\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5203 | loss: 1.15649 - acc: 0.7892 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5204  | total loss: \u001b[1m\u001b[32m1.39979\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5204 | loss: 1.39979 - acc: 0.7174 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5205  | total loss: \u001b[1m\u001b[32m1.29828\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5205 | loss: 1.29828 - acc: 0.7457 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5206  | total loss: \u001b[1m\u001b[32m1.55555\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5206 | loss: 1.55555 - acc: 0.6711 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5207  | total loss: \u001b[1m\u001b[32m1.43679\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5207 | loss: 1.43679 - acc: 0.7040 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5208  | total loss: \u001b[1m\u001b[32m1.64353\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5208 | loss: 1.64353 - acc: 0.6479 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5209  | total loss: \u001b[1m\u001b[32m1.51562\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5209 | loss: 1.51562 - acc: 0.6831 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5210  | total loss: \u001b[1m\u001b[32m1.40040\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5210 | loss: 1.40040 - acc: 0.7148 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5211  | total loss: \u001b[1m\u001b[32m1.29621\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5211 | loss: 1.29621 - acc: 0.7433 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5212  | total loss: \u001b[1m\u001b[32m1.42346\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5212 | loss: 1.42346 - acc: 0.7047 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5213  | total loss: \u001b[1m\u001b[32m1.31589\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5213 | loss: 1.31589 - acc: 0.7342 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5214  | total loss: \u001b[1m\u001b[32m1.54186\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5214 | loss: 1.54186 - acc: 0.6679 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5215  | total loss: \u001b[1m\u001b[32m1.42214\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5215 | loss: 1.42214 - acc: 0.7011 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5216  | total loss: \u001b[1m\u001b[32m1.31430\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5216 | loss: 1.31430 - acc: 0.7310 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5217  | total loss: \u001b[1m\u001b[32m1.21682\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5217 | loss: 1.21682 - acc: 0.7579 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5218  | total loss: \u001b[1m\u001b[32m1.12836\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5218 | loss: 1.12836 - acc: 0.7821 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5219  | total loss: \u001b[1m\u001b[32m1.04777\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5219 | loss: 1.04777 - acc: 0.8039 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5220  | total loss: \u001b[1m\u001b[32m1.33620\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5220 | loss: 1.33620 - acc: 0.7235 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5221  | total loss: \u001b[1m\u001b[32m1.23349\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5221 | loss: 1.23349 - acc: 0.7512 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5222  | total loss: \u001b[1m\u001b[32m1.51033\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5222 | loss: 1.51033 - acc: 0.6832 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5223  | total loss: \u001b[1m\u001b[32m1.39028\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5223 | loss: 1.39028 - acc: 0.7149 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5224  | total loss: \u001b[1m\u001b[32m1.28238\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5224 | loss: 1.28238 - acc: 0.7434 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5225  | total loss: \u001b[1m\u001b[32m1.18511\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5225 | loss: 1.18511 - acc: 0.7691 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5226  | total loss: \u001b[1m\u001b[32m1.39745\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5226 | loss: 1.39745 - acc: 0.7136 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5227  | total loss: \u001b[1m\u001b[32m1.28851\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5227 | loss: 1.28851 - acc: 0.7422 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5228  | total loss: \u001b[1m\u001b[32m1.52443\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5228 | loss: 1.52443 - acc: 0.6751 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5229  | total loss: \u001b[1m\u001b[32m1.40352\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5229 | loss: 1.40352 - acc: 0.7076 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5230  | total loss: \u001b[1m\u001b[32m1.66053\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5230 | loss: 1.66053 - acc: 0.6369 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5231  | total loss: \u001b[1m\u001b[32m1.52763\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5231 | loss: 1.52763 - acc: 0.6732 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5232  | total loss: \u001b[1m\u001b[32m1.73660\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5232 | loss: 1.73660 - acc: 0.6130 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5233  | total loss: \u001b[1m\u001b[32m1.59854\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5233 | loss: 1.59854 - acc: 0.6517 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5234  | total loss: \u001b[1m\u001b[32m1.79158\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5234 | loss: 1.79158 - acc: 0.5937 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5235  | total loss: \u001b[1m\u001b[32m1.65109\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5235 | loss: 1.65109 - acc: 0.6343 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5236  | total loss: \u001b[1m\u001b[32m1.82704\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5236 | loss: 1.82704 - acc: 0.5780 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5237  | total loss: \u001b[1m\u001b[32m1.68651\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5237 | loss: 1.68651 - acc: 0.6202 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5238  | total loss: \u001b[1m\u001b[32m1.87294\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5238 | loss: 1.87294 - acc: 0.5582 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5239  | total loss: \u001b[1m\u001b[32m1.73164\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5239 | loss: 1.73164 - acc: 0.6024 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5240  | total loss: \u001b[1m\u001b[32m1.60609\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5240 | loss: 1.60609 - acc: 0.6421 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5241  | total loss: \u001b[1m\u001b[32m1.49400\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5241 | loss: 1.49400 - acc: 0.6779 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5242  | total loss: \u001b[1m\u001b[32m1.67152\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5242 | loss: 1.67152 - acc: 0.6244 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5243  | total loss: \u001b[1m\u001b[32m1.55412\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5243 | loss: 1.55412 - acc: 0.6620 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5244  | total loss: \u001b[1m\u001b[32m1.44877\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5244 | loss: 1.44877 - acc: 0.6958 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5245  | total loss: \u001b[1m\u001b[32m1.35363\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5245 | loss: 1.35363 - acc: 0.7262 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5246  | total loss: \u001b[1m\u001b[32m1.53004\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5246 | loss: 1.53004 - acc: 0.6607 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5247  | total loss: \u001b[1m\u001b[32m1.42596\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5247 | loss: 1.42596 - acc: 0.6947 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5248  | total loss: \u001b[1m\u001b[32m1.62624\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5248 | loss: 1.62624 - acc: 0.6323 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5249  | total loss: \u001b[1m\u001b[32m1.51236\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5249 | loss: 1.51236 - acc: 0.6691 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5250  | total loss: \u001b[1m\u001b[32m1.69211\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5250 | loss: 1.69211 - acc: 0.6093 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5251  | total loss: \u001b[1m\u001b[32m1.57207\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5251 | loss: 1.57207 - acc: 0.6484 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5252  | total loss: \u001b[1m\u001b[32m1.73386\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5252 | loss: 1.73386 - acc: 0.5978 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5253  | total loss: \u001b[1m\u001b[32m1.61035\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5253 | loss: 1.61035 - acc: 0.6381 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5254  | total loss: \u001b[1m\u001b[32m1.80147\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5254 | loss: 1.80147 - acc: 0.5814 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5255  | total loss: \u001b[1m\u001b[32m1.67212\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5255 | loss: 1.67212 - acc: 0.6233 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5256  | total loss: \u001b[1m\u001b[32m1.55592\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5256 | loss: 1.55592 - acc: 0.6609 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5257  | total loss: \u001b[1m\u001b[32m1.45095\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5257 | loss: 1.45095 - acc: 0.6948 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5258  | total loss: \u001b[1m\u001b[32m1.62809\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5258 | loss: 1.62809 - acc: 0.6325 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5259  | total loss: \u001b[1m\u001b[32m1.51491\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5259 | loss: 1.51491 - acc: 0.6692 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5260  | total loss: \u001b[1m\u001b[32m1.41244\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5260 | loss: 1.41244 - acc: 0.7023 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5261  | total loss: \u001b[1m\u001b[32m1.31911\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5261 | loss: 1.31911 - acc: 0.7321 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5262  | total loss: \u001b[1m\u001b[32m1.23360\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5262 | loss: 1.23360 - acc: 0.7589 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5263  | total loss: \u001b[1m\u001b[32m1.15478\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5263 | loss: 1.15478 - acc: 0.7830 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5264  | total loss: \u001b[1m\u001b[32m1.37126\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5264 | loss: 1.37126 - acc: 0.7190 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5265  | total loss: \u001b[1m\u001b[32m1.27543\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5265 | loss: 1.27543 - acc: 0.7471 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5266  | total loss: \u001b[1m\u001b[32m1.18776\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5266 | loss: 1.18776 - acc: 0.7724 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5267  | total loss: \u001b[1m\u001b[32m1.10718\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5267 | loss: 1.10718 - acc: 0.7951 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5268  | total loss: \u001b[1m\u001b[32m1.34534\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5268 | loss: 1.34534 - acc: 0.7299 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5269  | total loss: \u001b[1m\u001b[32m1.24629\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5269 | loss: 1.24629 - acc: 0.7569 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5270  | total loss: \u001b[1m\u001b[32m1.15604\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5270 | loss: 1.15604 - acc: 0.7812 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5271  | total loss: \u001b[1m\u001b[32m1.07349\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5271 | loss: 1.07349 - acc: 0.8031 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5272  | total loss: \u001b[1m\u001b[32m1.25310\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5272 | loss: 1.25310 - acc: 0.7514 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5273  | total loss: \u001b[1m\u001b[32m1.15864\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5273 | loss: 1.15864 - acc: 0.7762 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5274  | total loss: \u001b[1m\u001b[32m1.44591\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5274 | loss: 1.44591 - acc: 0.6986 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5275  | total loss: \u001b[1m\u001b[32m1.33132\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5275 | loss: 1.33132 - acc: 0.7287 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5276  | total loss: \u001b[1m\u001b[32m1.59926\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5276 | loss: 1.59926 - acc: 0.6630 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5277  | total loss: \u001b[1m\u001b[32m1.46982\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5277 | loss: 1.46982 - acc: 0.6967 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5278  | total loss: \u001b[1m\u001b[32m1.70755\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5278 | loss: 1.70755 - acc: 0.6342 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5279  | total loss: \u001b[1m\u001b[32m1.56879\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5279 | loss: 1.56879 - acc: 0.6708 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5280  | total loss: \u001b[1m\u001b[32m1.77567\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5280 | loss: 1.77567 - acc: 0.6108 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5281  | total loss: \u001b[1m\u001b[32m1.63237\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5281 | loss: 1.63237 - acc: 0.6497 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5282  | total loss: \u001b[1m\u001b[32m1.79971\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5282 | loss: 1.79971 - acc: 0.5991 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5283  | total loss: \u001b[1m\u001b[32m1.65675\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5283 | loss: 1.65675 - acc: 0.6392 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5284  | total loss: \u001b[1m\u001b[32m1.80445\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5284 | loss: 1.80445 - acc: 0.5895 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5285  | total loss: \u001b[1m\u001b[32m1.66412\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5285 | loss: 1.66412 - acc: 0.6306 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5286  | total loss: \u001b[1m\u001b[32m1.86259\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5286 | loss: 1.86259 - acc: 0.5747 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5287  | total loss: \u001b[1m\u001b[32m1.71986\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5287 | loss: 1.71986 - acc: 0.6172 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5288  | total loss: \u001b[1m\u001b[32m1.93146\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5288 | loss: 1.93146 - acc: 0.5555 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5289  | total loss: \u001b[1m\u001b[32m1.78552\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5289 | loss: 1.78552 - acc: 0.5999 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5290  | total loss: \u001b[1m\u001b[32m1.94036\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5290 | loss: 1.94036 - acc: 0.5471 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5291  | total loss: \u001b[1m\u001b[32m1.79719\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5291 | loss: 1.79719 - acc: 0.5924 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5292  | total loss: \u001b[1m\u001b[32m1.94105\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5292 | loss: 1.94105 - acc: 0.5474 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5293  | total loss: \u001b[1m\u001b[32m1.80111\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5293 | loss: 1.80111 - acc: 0.5927 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5294  | total loss: \u001b[1m\u001b[32m1.96881\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5294 | loss: 1.96881 - acc: 0.5334 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5295  | total loss: \u001b[1m\u001b[32m1.82920\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5295 | loss: 1.82920 - acc: 0.5801 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5296  | total loss: \u001b[1m\u001b[32m1.95093\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5296 | loss: 1.95093 - acc: 0.5363 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5297  | total loss: \u001b[1m\u001b[32m1.81592\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5297 | loss: 1.81592 - acc: 0.5827 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5298  | total loss: \u001b[1m\u001b[32m1.94070\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5298 | loss: 1.94070 - acc: 0.5387 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5299  | total loss: \u001b[1m\u001b[32m1.80899\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5299 | loss: 1.80899 - acc: 0.5849 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5300  | total loss: \u001b[1m\u001b[32m1.94277\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5300 | loss: 1.94277 - acc: 0.5335 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5301  | total loss: \u001b[1m\u001b[32m1.81281\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5301 | loss: 1.81281 - acc: 0.5802 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5302  | total loss: \u001b[1m\u001b[32m1.69634\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5302 | loss: 1.69634 - acc: 0.6221 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5303  | total loss: \u001b[1m\u001b[32m1.59120\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5303 | loss: 1.59120 - acc: 0.6599 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5304  | total loss: \u001b[1m\u001b[32m1.76902\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5304 | loss: 1.76902 - acc: 0.5939 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5305  | total loss: \u001b[1m\u001b[32m1.65563\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5305 | loss: 1.65563 - acc: 0.6345 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5306  | total loss: \u001b[1m\u001b[32m1.80940\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5306 | loss: 1.80940 - acc: 0.5711 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5307  | total loss: \u001b[1m\u001b[32m1.69148\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5307 | loss: 1.69148 - acc: 0.6140 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5308  | total loss: \u001b[1m\u001b[32m1.83912\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5308 | loss: 1.83912 - acc: 0.5597 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5309  | total loss: \u001b[1m\u001b[32m1.71801\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5309 | loss: 1.71801 - acc: 0.6038 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5310  | total loss: \u001b[1m\u001b[32m1.87581\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5310 | loss: 1.87581 - acc: 0.5505 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5311  | total loss: \u001b[1m\u001b[32m1.75090\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5311 | loss: 1.75090 - acc: 0.5955 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5312  | total loss: \u001b[1m\u001b[32m1.90689\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5312 | loss: 1.90689 - acc: 0.5359 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5313  | total loss: \u001b[1m\u001b[32m1.77890\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5313 | loss: 1.77890 - acc: 0.5823 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5314  | total loss: \u001b[1m\u001b[32m1.94471\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5314 | loss: 1.94471 - acc: 0.5241 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5315  | total loss: \u001b[1m\u001b[32m1.81324\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5315 | loss: 1.81324 - acc: 0.5717 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5316  | total loss: \u001b[1m\u001b[32m1.95501\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5316 | loss: 1.95501 - acc: 0.5217 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5317  | total loss: \u001b[1m\u001b[32m1.82296\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 5317 | loss: 1.82296 - acc: 0.5695 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5318  | total loss: \u001b[1m\u001b[32m1.97220\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5318 | loss: 1.97220 - acc: 0.5125 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5319  | total loss: \u001b[1m\u001b[32m1.83907\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5319 | loss: 1.83907 - acc: 0.5613 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5320  | total loss: \u001b[1m\u001b[32m1.99420\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5320 | loss: 1.99420 - acc: 0.5052 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5321  | total loss: \u001b[1m\u001b[32m1.85971\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5321 | loss: 1.85971 - acc: 0.5546 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5322  | total loss: \u001b[1m\u001b[32m2.00805\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5322 | loss: 2.00805 - acc: 0.4992 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5323  | total loss: \u001b[1m\u001b[32m1.87306\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5323 | loss: 1.87306 - acc: 0.5493 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5324  | total loss: \u001b[1m\u001b[32m1.97335\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5324 | loss: 1.97335 - acc: 0.5086 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5325  | total loss: \u001b[1m\u001b[32m1.84253\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5325 | loss: 1.84253 - acc: 0.5578 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5326  | total loss: \u001b[1m\u001b[32m1.99425\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5326 | loss: 1.99425 - acc: 0.5091 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5327  | total loss: \u001b[1m\u001b[32m1.86179\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5327 | loss: 1.86179 - acc: 0.5582 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5328  | total loss: \u001b[1m\u001b[32m2.02357\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5328 | loss: 2.02357 - acc: 0.5024 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5329  | total loss: \u001b[1m\u001b[32m1.88869\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5329 | loss: 1.88869 - acc: 0.5522 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5330  | total loss: \u001b[1m\u001b[32m1.76716\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5330 | loss: 1.76716 - acc: 0.5969 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5331  | total loss: \u001b[1m\u001b[32m1.65689\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5331 | loss: 1.65689 - acc: 0.6372 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5332  | total loss: \u001b[1m\u001b[32m1.79936\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5332 | loss: 1.79936 - acc: 0.5878 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5333  | total loss: \u001b[1m\u001b[32m1.68363\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5333 | loss: 1.68363 - acc: 0.6290 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5334  | total loss: \u001b[1m\u001b[32m1.77950\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5334 | loss: 1.77950 - acc: 0.5876 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5335  | total loss: \u001b[1m\u001b[32m1.66364\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5335 | loss: 1.66364 - acc: 0.6288 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5336  | total loss: \u001b[1m\u001b[32m1.77624\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5336 | loss: 1.77624 - acc: 0.5873 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5337  | total loss: \u001b[1m\u001b[32m1.65868\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5337 | loss: 1.65868 - acc: 0.6286 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5338  | total loss: \u001b[1m\u001b[32m1.84649\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5338 | loss: 1.84649 - acc: 0.5657 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5339  | total loss: \u001b[1m\u001b[32m1.72045\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5339 | loss: 1.72045 - acc: 0.6092 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5340  | total loss: \u001b[1m\u001b[32m1.87664\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5340 | loss: 1.87664 - acc: 0.5625 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5341  | total loss: \u001b[1m\u001b[32m1.74662\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5341 | loss: 1.74662 - acc: 0.6063 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5342  | total loss: \u001b[1m\u001b[32m1.90414\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5342 | loss: 1.90414 - acc: 0.5528 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5343  | total loss: \u001b[1m\u001b[32m1.77075\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5343 | loss: 1.77075 - acc: 0.5975 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5344  | total loss: \u001b[1m\u001b[32m1.93838\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5344 | loss: 1.93838 - acc: 0.5378 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5345  | total loss: \u001b[1m\u001b[32m1.80140\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5345 | loss: 1.80140 - acc: 0.5840 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5346  | total loss: \u001b[1m\u001b[32m1.97590\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5346 | loss: 1.97590 - acc: 0.5256 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5347  | total loss: \u001b[1m\u001b[32m1.83558\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5347 | loss: 1.83558 - acc: 0.5730 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5348  | total loss: \u001b[1m\u001b[32m2.00091\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5348 | loss: 2.00091 - acc: 0.5157 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5349  | total loss: \u001b[1m\u001b[32m1.85900\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5349 | loss: 1.85900 - acc: 0.5642 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5350  | total loss: \u001b[1m\u001b[32m1.95392\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5350 | loss: 1.95392 - acc: 0.5292 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5351  | total loss: \u001b[1m\u001b[32m1.81755\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5351 | loss: 1.81755 - acc: 0.5763 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5352  | total loss: \u001b[1m\u001b[32m1.95481\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5352 | loss: 1.95481 - acc: 0.5329 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5353  | total loss: \u001b[1m\u001b[32m1.81889\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5353 | loss: 1.81889 - acc: 0.5796 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5354  | total loss: \u001b[1m\u001b[32m1.69643\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5354 | loss: 1.69643 - acc: 0.6217 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5355  | total loss: \u001b[1m\u001b[32m1.58542\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5355 | loss: 1.58542 - acc: 0.6595 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5356  | total loss: \u001b[1m\u001b[32m1.77127\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5356 | loss: 1.77127 - acc: 0.5935 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5357  | total loss: \u001b[1m\u001b[32m1.65106\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5357 | loss: 1.65106 - acc: 0.6342 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5358  | total loss: \u001b[1m\u001b[32m1.54191\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5358 | loss: 1.54191 - acc: 0.6708 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5359  | total loss: \u001b[1m\u001b[32m1.44219\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5359 | loss: 1.44219 - acc: 0.7037 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5360  | total loss: \u001b[1m\u001b[32m1.63729\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5360 | loss: 1.63729 - acc: 0.6405 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5361  | total loss: \u001b[1m\u001b[32m1.52525\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5361 | loss: 1.52525 - acc: 0.6764 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5362  | total loss: \u001b[1m\u001b[32m1.72270\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5362 | loss: 1.72270 - acc: 0.6088 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5363  | total loss: \u001b[1m\u001b[32m1.60048\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5363 | loss: 1.60048 - acc: 0.6479 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5364  | total loss: \u001b[1m\u001b[32m1.48964\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5364 | loss: 1.48964 - acc: 0.6831 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5365  | total loss: \u001b[1m\u001b[32m1.38862\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5365 | loss: 1.38862 - acc: 0.7148 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5366  | total loss: \u001b[1m\u001b[32m1.54136\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5366 | loss: 1.54136 - acc: 0.6647 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5367  | total loss: \u001b[1m\u001b[32m1.43264\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5367 | loss: 1.43264 - acc: 0.6983 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5368  | total loss: \u001b[1m\u001b[32m1.58412\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5368 | loss: 1.58412 - acc: 0.6499 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5369  | total loss: \u001b[1m\u001b[32m1.46942\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5369 | loss: 1.46942 - acc: 0.6849 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5370  | total loss: \u001b[1m\u001b[32m1.63515\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5370 | loss: 1.63515 - acc: 0.6378 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5371  | total loss: \u001b[1m\u001b[32m1.51439\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5371 | loss: 1.51439 - acc: 0.6740 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5372  | total loss: \u001b[1m\u001b[32m1.68782\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5372 | loss: 1.68782 - acc: 0.6209 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5373  | total loss: \u001b[1m\u001b[32m1.56154\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5373 | loss: 1.56154 - acc: 0.6588 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5374  | total loss: \u001b[1m\u001b[32m1.44767\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5374 | loss: 1.44767 - acc: 0.6929 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5375  | total loss: \u001b[1m\u001b[32m1.34454\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5375 | loss: 1.34454 - acc: 0.7237 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5376  | total loss: \u001b[1m\u001b[32m1.54375\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5376 | loss: 1.54375 - acc: 0.6584 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5377  | total loss: \u001b[1m\u001b[32m1.42981\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5377 | loss: 1.42981 - acc: 0.6926 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5378  | total loss: \u001b[1m\u001b[32m1.32667\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5378 | loss: 1.32667 - acc: 0.7233 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5379  | total loss: \u001b[1m\u001b[32m1.23289\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5379 | loss: 1.23289 - acc: 0.7510 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5380  | total loss: \u001b[1m\u001b[32m1.45178\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5380 | loss: 1.45178 - acc: 0.6902 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5381  | total loss: \u001b[1m\u001b[32m1.34385\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5381 | loss: 1.34385 - acc: 0.7212 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5382  | total loss: \u001b[1m\u001b[32m1.58572\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5382 | loss: 1.58572 - acc: 0.6490 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5383  | total loss: \u001b[1m\u001b[32m1.46395\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5383 | loss: 1.46395 - acc: 0.6841 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5384  | total loss: \u001b[1m\u001b[32m1.35423\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5384 | loss: 1.35423 - acc: 0.7157 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5385  | total loss: \u001b[1m\u001b[32m1.25498\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5385 | loss: 1.25498 - acc: 0.7442 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5386  | total loss: \u001b[1m\u001b[32m1.46498\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5386 | loss: 1.46498 - acc: 0.6840 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5387  | total loss: \u001b[1m\u001b[32m1.35390\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5387 | loss: 1.35390 - acc: 0.7156 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5388  | total loss: \u001b[1m\u001b[32m1.56635\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5388 | loss: 1.56635 - acc: 0.6512 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5389  | total loss: \u001b[1m\u001b[32m1.44541\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5389 | loss: 1.44541 - acc: 0.6861 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5390  | total loss: \u001b[1m\u001b[32m1.65108\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5390 | loss: 1.65108 - acc: 0.6246 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5391  | total loss: \u001b[1m\u001b[32m1.52290\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5391 | loss: 1.52290 - acc: 0.6622 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5392  | total loss: \u001b[1m\u001b[32m1.75802\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5392 | loss: 1.75802 - acc: 0.5959 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5393  | total loss: \u001b[1m\u001b[32m1.62130\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5393 | loss: 1.62130 - acc: 0.6363 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5394  | total loss: \u001b[1m\u001b[32m1.80881\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5394 | loss: 1.80881 - acc: 0.5870 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5395  | total loss: \u001b[1m\u001b[32m1.66979\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5395 | loss: 1.66979 - acc: 0.6283 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5396  | total loss: \u001b[1m\u001b[32m1.74293\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5396 | loss: 1.74293 - acc: 0.6012 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5397  | total loss: \u001b[1m\u001b[32m1.61317\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5397 | loss: 1.61317 - acc: 0.6411 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5398  | total loss: \u001b[1m\u001b[32m1.82491\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5398 | loss: 1.82491 - acc: 0.5770 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5399  | total loss: \u001b[1m\u001b[32m1.68955\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5399 | loss: 1.68955 - acc: 0.6193 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5400  | total loss: \u001b[1m\u001b[32m1.82467\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5400 | loss: 1.82467 - acc: 0.5716 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5401  | total loss: \u001b[1m\u001b[32m1.69210\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5401 | loss: 1.69210 - acc: 0.6145 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5402  | total loss: \u001b[1m\u001b[32m1.83087\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5402 | loss: 1.83087 - acc: 0.5673 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5403  | total loss: \u001b[1m\u001b[32m1.70035\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5403 | loss: 1.70035 - acc: 0.6106 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5404  | total loss: \u001b[1m\u001b[32m1.86100\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5404 | loss: 1.86100 - acc: 0.5495 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5405  | total loss: \u001b[1m\u001b[32m1.73006\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5405 | loss: 1.73006 - acc: 0.5946 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5406  | total loss: \u001b[1m\u001b[32m1.90414\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5406 | loss: 1.90414 - acc: 0.5422 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5407  | total loss: \u001b[1m\u001b[32m1.77137\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5407 | loss: 1.77137 - acc: 0.5880 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5408  | total loss: \u001b[1m\u001b[32m1.89533\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5408 | loss: 1.89533 - acc: 0.5435 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5409  | total loss: \u001b[1m\u001b[32m1.76551\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5409 | loss: 1.76551 - acc: 0.5892 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5410  | total loss: \u001b[1m\u001b[32m1.91365\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5410 | loss: 1.91365 - acc: 0.5374 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5411  | total loss: \u001b[1m\u001b[32m1.78374\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5411 | loss: 1.78374 - acc: 0.5836 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5412  | total loss: \u001b[1m\u001b[32m1.93355\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5412 | loss: 1.93355 - acc: 0.5324 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5413  | total loss: \u001b[1m\u001b[32m1.80325\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5413 | loss: 1.80325 - acc: 0.5792 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5414  | total loss: \u001b[1m\u001b[32m1.94441\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5414 | loss: 1.94441 - acc: 0.5284 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5415  | total loss: \u001b[1m\u001b[32m1.81440\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5415 | loss: 1.81440 - acc: 0.5756 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5416  | total loss: \u001b[1m\u001b[32m1.98169\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5416 | loss: 1.98169 - acc: 0.5180 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5417  | total loss: \u001b[1m\u001b[32m1.84919\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 5417 | loss: 1.84919 - acc: 0.5662 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5418  | total loss: \u001b[1m\u001b[32m1.98328\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5418 | loss: 1.98328 - acc: 0.5167 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5419  | total loss: \u001b[1m\u001b[32m1.85169\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 5419 | loss: 1.85169 - acc: 0.5651 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5420  | total loss: \u001b[1m\u001b[32m1.98399\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5420 | loss: 1.98399 - acc: 0.5157 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5421  | total loss: \u001b[1m\u001b[32m1.85316\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5421 | loss: 1.85316 - acc: 0.5641 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5422  | total loss: \u001b[1m\u001b[32m1.99395\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5422 | loss: 1.99395 - acc: 0.5149 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5423  | total loss: \u001b[1m\u001b[32m1.86285\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5423 | loss: 1.86285 - acc: 0.5634 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5424  | total loss: \u001b[1m\u001b[32m1.97505\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5424 | loss: 1.97505 - acc: 0.5142 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5425  | total loss: \u001b[1m\u001b[32m1.84652\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5425 | loss: 1.84652 - acc: 0.5628 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5426  | total loss: \u001b[1m\u001b[32m1.95142\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5426 | loss: 1.95142 - acc: 0.5279 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5427  | total loss: \u001b[1m\u001b[32m1.82562\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5427 | loss: 1.82562 - acc: 0.5751 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5428  | total loss: \u001b[1m\u001b[32m1.97632\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5428 | loss: 1.97632 - acc: 0.5248 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5429  | total loss: \u001b[1m\u001b[32m1.84807\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5429 | loss: 1.84807 - acc: 0.5723 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5430  | total loss: \u001b[1m\u001b[32m1.73220\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5430 | loss: 1.73220 - acc: 0.6150 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5431  | total loss: \u001b[1m\u001b[32m1.62676\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5431 | loss: 1.62676 - acc: 0.6535 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5432  | total loss: \u001b[1m\u001b[32m1.77116\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5432 | loss: 1.77116 - acc: 0.5953 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5433  | total loss: \u001b[1m\u001b[32m1.65918\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5433 | loss: 1.65918 - acc: 0.6358 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5434  | total loss: \u001b[1m\u001b[32m1.77989\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5434 | loss: 1.77989 - acc: 0.5936 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5435  | total loss: \u001b[1m\u001b[32m1.66477\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5435 | loss: 1.66477 - acc: 0.6343 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5436  | total loss: \u001b[1m\u001b[32m1.55976\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5436 | loss: 1.55976 - acc: 0.6709 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5437  | total loss: \u001b[1m\u001b[32m1.46330\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5437 | loss: 1.46330 - acc: 0.7038 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5438  | total loss: \u001b[1m\u001b[32m1.61441\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5438 | loss: 1.61441 - acc: 0.6548 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5439  | total loss: \u001b[1m\u001b[32m1.50855\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5439 | loss: 1.50855 - acc: 0.6893 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5440  | total loss: \u001b[1m\u001b[32m1.71707\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5440 | loss: 1.71707 - acc: 0.6204 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5441  | total loss: \u001b[1m\u001b[32m1.59814\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5441 | loss: 1.59814 - acc: 0.6584 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5442  | total loss: \u001b[1m\u001b[32m1.81589\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5442 | loss: 1.81589 - acc: 0.5925 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5443  | total loss: \u001b[1m\u001b[32m1.68553\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5443 | loss: 1.68553 - acc: 0.6333 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5444  | total loss: \u001b[1m\u001b[32m1.56743\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5444 | loss: 1.56743 - acc: 0.6699 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5445  | total loss: \u001b[1m\u001b[32m1.45991\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5445 | loss: 1.45991 - acc: 0.7030 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5446  | total loss: \u001b[1m\u001b[32m1.60708\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5446 | loss: 1.60708 - acc: 0.6469 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5447  | total loss: \u001b[1m\u001b[32m1.49322\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5447 | loss: 1.49322 - acc: 0.6822 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5448  | total loss: \u001b[1m\u001b[32m1.38959\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5448 | loss: 1.38959 - acc: 0.7140 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5449  | total loss: \u001b[1m\u001b[32m1.29479\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5449 | loss: 1.29479 - acc: 0.7426 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5450  | total loss: \u001b[1m\u001b[32m1.20768\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5450 | loss: 1.20768 - acc: 0.7684 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5451  | total loss: \u001b[1m\u001b[32m1.12724\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5451 | loss: 1.12724 - acc: 0.7915 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5452  | total loss: \u001b[1m\u001b[32m1.05266\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5452 | loss: 1.05266 - acc: 0.8124 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5453  | total loss: \u001b[1m\u001b[32m0.98324\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5453 | loss: 0.98324 - acc: 0.8311 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5454  | total loss: \u001b[1m\u001b[32m0.91840\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5454 | loss: 0.91840 - acc: 0.8480 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5455  | total loss: \u001b[1m\u001b[32m0.85769\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5455 | loss: 0.85769 - acc: 0.8632 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5456  | total loss: \u001b[1m\u001b[32m1.17338\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5456 | loss: 1.17338 - acc: 0.7840 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5457  | total loss: \u001b[1m\u001b[32m1.08365\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 5457 | loss: 1.08365 - acc: 0.8056 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5458  | total loss: \u001b[1m\u001b[32m1.00161\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5458 | loss: 1.00161 - acc: 0.8251 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5459  | total loss: \u001b[1m\u001b[32m0.92640\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5459 | loss: 0.92640 - acc: 0.8426 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5460  | total loss: \u001b[1m\u001b[32m1.26548\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5460 | loss: 1.26548 - acc: 0.7583 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5461  | total loss: \u001b[1m\u001b[32m1.16208\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5461 | loss: 1.16208 - acc: 0.7825 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5462  | total loss: \u001b[1m\u001b[32m1.46049\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5462 | loss: 1.46049 - acc: 0.7114 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5463  | total loss: \u001b[1m\u001b[32m1.33728\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5463 | loss: 1.33728 - acc: 0.7402 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5464  | total loss: \u001b[1m\u001b[32m1.55085\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5464 | loss: 1.55085 - acc: 0.6805 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5465  | total loss: \u001b[1m\u001b[32m1.41935\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5465 | loss: 1.41935 - acc: 0.7124 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5466  | total loss: \u001b[1m\u001b[32m1.66233\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5466 | loss: 1.66233 - acc: 0.6483 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5467  | total loss: \u001b[1m\u001b[32m1.52134\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5467 | loss: 1.52134 - acc: 0.6835 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5468  | total loss: \u001b[1m\u001b[32m1.75833\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5468 | loss: 1.75833 - acc: 0.6223 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5469  | total loss: \u001b[1m\u001b[32m1.61029\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5469 | loss: 1.61029 - acc: 0.6601 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5470  | total loss: \u001b[1m\u001b[32m1.83193\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5470 | loss: 1.83193 - acc: 0.6012 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5471  | total loss: \u001b[1m\u001b[32m1.67990\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5471 | loss: 1.67990 - acc: 0.6411 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5472  | total loss: \u001b[1m\u001b[32m1.87514\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5472 | loss: 1.87514 - acc: 0.5841 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5473  | total loss: \u001b[1m\u001b[32m1.72285\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5473 | loss: 1.72285 - acc: 0.6257 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5474  | total loss: \u001b[1m\u001b[32m1.86725\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5474 | loss: 1.86725 - acc: 0.5846 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5475  | total loss: \u001b[1m\u001b[32m1.72022\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5475 | loss: 1.72022 - acc: 0.6261 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5476  | total loss: \u001b[1m\u001b[32m1.89594\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5476 | loss: 1.89594 - acc: 0.5706 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5477  | total loss: \u001b[1m\u001b[32m1.75072\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5477 | loss: 1.75072 - acc: 0.6136 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5478  | total loss: \u001b[1m\u001b[32m1.87511\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5478 | loss: 1.87511 - acc: 0.5736 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5479  | total loss: \u001b[1m\u001b[32m1.73655\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5479 | loss: 1.73655 - acc: 0.6163 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5480  | total loss: \u001b[1m\u001b[32m1.85887\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5480 | loss: 1.85887 - acc: 0.5761 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5481  | total loss: \u001b[1m\u001b[32m1.72595\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5481 | loss: 1.72595 - acc: 0.6185 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5482  | total loss: \u001b[1m\u001b[32m1.86437\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5482 | loss: 1.86437 - acc: 0.5709 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5483  | total loss: \u001b[1m\u001b[32m1.73430\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5483 | loss: 1.73430 - acc: 0.6138 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5484  | total loss: \u001b[1m\u001b[32m1.84819\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 5484 | loss: 1.84819 - acc: 0.5739 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5485  | total loss: \u001b[1m\u001b[32m1.72239\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5485 | loss: 1.72239 - acc: 0.6165 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5486  | total loss: \u001b[1m\u001b[32m1.89206\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5486 | loss: 1.89206 - acc: 0.5548 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5487  | total loss: \u001b[1m\u001b[32m1.76418\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5487 | loss: 1.76418 - acc: 0.5994 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5488  | total loss: \u001b[1m\u001b[32m1.93181\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5488 | loss: 1.93181 - acc: 0.5394 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5489  | total loss: \u001b[1m\u001b[32m1.80224\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5489 | loss: 1.80224 - acc: 0.5855 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5490  | total loss: \u001b[1m\u001b[32m1.98891\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5490 | loss: 1.98891 - acc: 0.5269 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5491  | total loss: \u001b[1m\u001b[32m1.85573\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5491 | loss: 1.85573 - acc: 0.5742 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5492  | total loss: \u001b[1m\u001b[32m1.98697\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5492 | loss: 1.98697 - acc: 0.5240 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5493  | total loss: \u001b[1m\u001b[32m1.85566\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5493 | loss: 1.85566 - acc: 0.5716 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5494  | total loss: \u001b[1m\u001b[32m1.97378\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5494 | loss: 1.97378 - acc: 0.5287 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5495  | total loss: \u001b[1m\u001b[32m1.84496\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5495 | loss: 1.84496 - acc: 0.5758 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5496  | total loss: \u001b[1m\u001b[32m1.97411\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5496 | loss: 1.97411 - acc: 0.5254 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5497  | total loss: \u001b[1m\u001b[32m1.84600\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5497 | loss: 1.84600 - acc: 0.5728 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5498  | total loss: \u001b[1m\u001b[32m1.97163\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5498 | loss: 1.97163 - acc: 0.5227 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5499  | total loss: \u001b[1m\u001b[32m1.84420\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5499 | loss: 1.84420 - acc: 0.5704 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5500  | total loss: \u001b[1m\u001b[32m1.72928\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5500 | loss: 1.72928 - acc: 0.6134 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5501  | total loss: \u001b[1m\u001b[32m1.62486\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5501 | loss: 1.62486 - acc: 0.6520 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5502  | total loss: \u001b[1m\u001b[32m1.77913\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5502 | loss: 1.77913 - acc: 0.5940 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5503  | total loss: \u001b[1m\u001b[32m1.66730\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5503 | loss: 1.66730 - acc: 0.6346 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5504  | total loss: \u001b[1m\u001b[32m1.56522\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5504 | loss: 1.56522 - acc: 0.6711 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5505  | total loss: \u001b[1m\u001b[32m1.47136\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5505 | loss: 1.47136 - acc: 0.7040 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5506  | total loss: \u001b[1m\u001b[32m1.59359\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5506 | loss: 1.59359 - acc: 0.6622 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5507  | total loss: \u001b[1m\u001b[32m1.49273\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5507 | loss: 1.49273 - acc: 0.6960 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5508  | total loss: \u001b[1m\u001b[32m1.67856\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5508 | loss: 1.67856 - acc: 0.6264 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5509  | total loss: \u001b[1m\u001b[32m1.56600\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5509 | loss: 1.56600 - acc: 0.6637 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5510  | total loss: \u001b[1m\u001b[32m1.75685\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5510 | loss: 1.75685 - acc: 0.6045 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5511  | total loss: \u001b[1m\u001b[32m1.63441\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5511 | loss: 1.63441 - acc: 0.6441 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5512  | total loss: \u001b[1m\u001b[32m1.83661\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5512 | loss: 1.83661 - acc: 0.5796 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5513  | total loss: \u001b[1m\u001b[32m1.70519\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5513 | loss: 1.70519 - acc: 0.6217 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5514  | total loss: \u001b[1m\u001b[32m1.85078\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5514 | loss: 1.85078 - acc: 0.5667 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5515  | total loss: \u001b[1m\u001b[32m1.71772\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5515 | loss: 1.71772 - acc: 0.6100 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5516  | total loss: \u001b[1m\u001b[32m1.88076\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5516 | loss: 1.88076 - acc: 0.5561 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5517  | total loss: \u001b[1m\u001b[32m1.74497\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5517 | loss: 1.74497 - acc: 0.6005 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5518  | total loss: \u001b[1m\u001b[32m1.89474\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5518 | loss: 1.89474 - acc: 0.5476 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5519  | total loss: \u001b[1m\u001b[32m1.75824\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5519 | loss: 1.75824 - acc: 0.5929 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5520  | total loss: \u001b[1m\u001b[32m1.93202\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5520 | loss: 1.93202 - acc: 0.5336 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5521  | total loss: \u001b[1m\u001b[32m1.79294\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5521 | loss: 1.79294 - acc: 0.5802 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5522  | total loss: \u001b[1m\u001b[32m1.96688\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5522 | loss: 1.96688 - acc: 0.5293 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5523  | total loss: \u001b[1m\u001b[32m1.82582\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 5523 | loss: 1.82582 - acc: 0.5764 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5524  | total loss: \u001b[1m\u001b[32m1.98997\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5524 | loss: 1.98997 - acc: 0.5188 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5525  | total loss: \u001b[1m\u001b[32m1.84839\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5525 | loss: 1.84839 - acc: 0.5669 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5526  | total loss: \u001b[1m\u001b[32m1.72152\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5526 | loss: 1.72152 - acc: 0.6102 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5527  | total loss: \u001b[1m\u001b[32m1.60718\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5527 | loss: 1.60718 - acc: 0.6492 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5528  | total loss: \u001b[1m\u001b[32m1.50348\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5528 | loss: 1.50348 - acc: 0.6843 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5529  | total loss: \u001b[1m\u001b[32m1.40878\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5529 | loss: 1.40878 - acc: 0.7158 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5530  | total loss: \u001b[1m\u001b[32m1.32171\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5530 | loss: 1.32171 - acc: 0.7442 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5531  | total loss: \u001b[1m\u001b[32m1.24110\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5531 | loss: 1.24110 - acc: 0.7698 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5532  | total loss: \u001b[1m\u001b[32m1.43337\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5532 | loss: 1.43337 - acc: 0.7071 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5533  | total loss: \u001b[1m\u001b[32m1.33748\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5533 | loss: 1.33748 - acc: 0.7364 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5534  | total loss: \u001b[1m\u001b[32m1.54161\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5534 | loss: 1.54161 - acc: 0.6699 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5535  | total loss: \u001b[1m\u001b[32m1.43217\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5535 | loss: 1.43217 - acc: 0.7029 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5536  | total loss: \u001b[1m\u001b[32m1.66213\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5536 | loss: 1.66213 - acc: 0.6326 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5537  | total loss: \u001b[1m\u001b[32m1.53925\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5537 | loss: 1.53925 - acc: 0.6694 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5538  | total loss: \u001b[1m\u001b[32m1.75186\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5538 | loss: 1.75186 - acc: 0.6024 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5539  | total loss: \u001b[1m\u001b[32m1.61978\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5539 | loss: 1.61978 - acc: 0.6422 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5540  | total loss: \u001b[1m\u001b[32m1.76915\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5540 | loss: 1.76915 - acc: 0.5994 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5541  | total loss: \u001b[1m\u001b[32m1.63593\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5541 | loss: 1.63593 - acc: 0.6395 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5542  | total loss: \u001b[1m\u001b[32m1.80211\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5542 | loss: 1.80211 - acc: 0.5827 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5543  | total loss: \u001b[1m\u001b[32m1.66668\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5543 | loss: 1.66668 - acc: 0.6244 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5544  | total loss: \u001b[1m\u001b[32m1.83286\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5544 | loss: 1.83286 - acc: 0.5691 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5545  | total loss: \u001b[1m\u001b[32m1.69583\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5545 | loss: 1.69583 - acc: 0.6122 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5546  | total loss: \u001b[1m\u001b[32m1.87268\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5546 | loss: 1.87268 - acc: 0.5510 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5547  | total loss: \u001b[1m\u001b[32m1.73344\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5547 | loss: 1.73344 - acc: 0.5959 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5548  | total loss: \u001b[1m\u001b[32m1.88344\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5548 | loss: 1.88344 - acc: 0.5434 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5549  | total loss: \u001b[1m\u001b[32m1.74510\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5549 | loss: 1.74510 - acc: 0.5891 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5550  | total loss: \u001b[1m\u001b[32m1.94364\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5550 | loss: 1.94364 - acc: 0.5302 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5551  | total loss: \u001b[1m\u001b[32m1.80155\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5551 | loss: 1.80155 - acc: 0.5772 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5552  | total loss: \u001b[1m\u001b[32m1.92515\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5552 | loss: 1.92515 - acc: 0.5337 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5553  | total loss: \u001b[1m\u001b[32m1.78719\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5553 | loss: 1.78719 - acc: 0.5804 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5554  | total loss: \u001b[1m\u001b[32m1.92654\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5554 | loss: 1.92654 - acc: 0.5295 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5555  | total loss: \u001b[1m\u001b[32m1.79057\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5555 | loss: 1.79057 - acc: 0.5765 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5556  | total loss: \u001b[1m\u001b[32m1.94011\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5556 | loss: 1.94011 - acc: 0.5189 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5557  | total loss: \u001b[1m\u001b[32m1.80498\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5557 | loss: 1.80498 - acc: 0.5670 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5558  | total loss: \u001b[1m\u001b[32m1.97879\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5558 | loss: 1.97879 - acc: 0.5103 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5559  | total loss: \u001b[1m\u001b[32m1.84215\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5559 | loss: 1.84215 - acc: 0.5593 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5560  | total loss: \u001b[1m\u001b[32m1.71992\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5560 | loss: 1.71992 - acc: 0.6033 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5561  | total loss: \u001b[1m\u001b[32m1.60986\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5561 | loss: 1.60986 - acc: 0.6430 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5562  | total loss: \u001b[1m\u001b[32m1.77697\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5562 | loss: 1.77697 - acc: 0.5858 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5563  | total loss: \u001b[1m\u001b[32m1.66057\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5563 | loss: 1.66057 - acc: 0.6273 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5564  | total loss: \u001b[1m\u001b[32m1.79699\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5564 | loss: 1.79699 - acc: 0.5788 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5565  | total loss: \u001b[1m\u001b[32m1.67821\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5565 | loss: 1.67821 - acc: 0.6209 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5566  | total loss: \u001b[1m\u001b[32m1.85136\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 5566 | loss: 1.85136 - acc: 0.5588 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5567  | total loss: \u001b[1m\u001b[32m1.72707\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 5567 | loss: 1.72707 - acc: 0.6030 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5568  | total loss: \u001b[1m\u001b[32m1.61490\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5568 | loss: 1.61490 - acc: 0.6427 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5569  | total loss: \u001b[1m\u001b[32m1.51294\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5569 | loss: 1.51294 - acc: 0.6784 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5570  | total loss: \u001b[1m\u001b[32m1.68540\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5570 | loss: 1.68540 - acc: 0.6177 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5571  | total loss: \u001b[1m\u001b[32m1.57418\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5571 | loss: 1.57418 - acc: 0.6559 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5572  | total loss: \u001b[1m\u001b[32m1.74986\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5572 | loss: 1.74986 - acc: 0.5903 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5573  | total loss: \u001b[1m\u001b[32m1.63075\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5573 | loss: 1.63075 - acc: 0.6313 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5574  | total loss: \u001b[1m\u001b[32m1.52271\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5574 | loss: 1.52271 - acc: 0.6682 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5575  | total loss: \u001b[1m\u001b[32m1.42411\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5575 | loss: 1.42411 - acc: 0.7014 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5576  | total loss: \u001b[1m\u001b[32m1.61582\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5576 | loss: 1.61582 - acc: 0.6384 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5577  | total loss: \u001b[1m\u001b[32m1.50532\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5577 | loss: 1.50532 - acc: 0.6745 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5578  | total loss: \u001b[1m\u001b[32m1.69565\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5578 | loss: 1.69565 - acc: 0.6142 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5579  | total loss: \u001b[1m\u001b[32m1.57565\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5579 | loss: 1.57565 - acc: 0.6528 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5580  | total loss: \u001b[1m\u001b[32m1.46684\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5580 | loss: 1.46684 - acc: 0.6875 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5581  | total loss: \u001b[1m\u001b[32m1.36765\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5581 | loss: 1.36765 - acc: 0.7188 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5582  | total loss: \u001b[1m\u001b[32m1.27672\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5582 | loss: 1.27672 - acc: 0.7469 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5583  | total loss: \u001b[1m\u001b[32m1.19290\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5583 | loss: 1.19290 - acc: 0.7722 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5584  | total loss: \u001b[1m\u001b[32m1.42253\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5584 | loss: 1.42253 - acc: 0.7021 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5585  | total loss: \u001b[1m\u001b[32m1.32081\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5585 | loss: 1.32081 - acc: 0.7319 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5586  | total loss: \u001b[1m\u001b[32m1.22785\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5586 | loss: 1.22785 - acc: 0.7587 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5587  | total loss: \u001b[1m\u001b[32m1.14251\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 5587 | loss: 1.14251 - acc: 0.7828 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5588  | total loss: \u001b[1m\u001b[32m1.35658\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 5588 | loss: 1.35658 - acc: 0.7188 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5589  | total loss: \u001b[1m\u001b[32m1.25562\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5589 | loss: 1.25562 - acc: 0.7470 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5590  | total loss: \u001b[1m\u001b[32m1.16361\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5590 | loss: 1.16361 - acc: 0.7723 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5591  | total loss: \u001b[1m\u001b[32m1.07944\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5591 | loss: 1.07944 - acc: 0.7950 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5592  | total loss: \u001b[1m\u001b[32m1.00218\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5592 | loss: 1.00218 - acc: 0.8155 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5593  | total loss: \u001b[1m\u001b[32m0.93102\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5593 | loss: 0.93102 - acc: 0.8340 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5594  | total loss: \u001b[1m\u001b[32m1.18545\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5594 | loss: 1.18545 - acc: 0.7649 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5595  | total loss: \u001b[1m\u001b[32m1.09354\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5595 | loss: 1.09354 - acc: 0.7884 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5596  | total loss: \u001b[1m\u001b[32m1.40669\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5596 | loss: 1.40669 - acc: 0.7095 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5597  | total loss: \u001b[1m\u001b[32m1.29189\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5597 | loss: 1.29189 - acc: 0.7386 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5598  | total loss: \u001b[1m\u001b[32m1.18843\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5598 | loss: 1.18843 - acc: 0.7647 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5599  | total loss: \u001b[1m\u001b[32m1.09495\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5599 | loss: 1.09495 - acc: 0.7883 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5600  | total loss: \u001b[1m\u001b[32m1.01024\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5600 | loss: 1.01024 - acc: 0.8094 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5601  | total loss: \u001b[1m\u001b[32m0.93326\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5601 | loss: 0.93326 - acc: 0.8285 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5602  | total loss: \u001b[1m\u001b[32m0.86311\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5602 | loss: 0.86311 - acc: 0.8456 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5603  | total loss: \u001b[1m\u001b[32m0.79898\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5603 | loss: 0.79898 - acc: 0.8611 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5604  | total loss: \u001b[1m\u001b[32m1.18643\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5604 | loss: 1.18643 - acc: 0.7750 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5605  | total loss: \u001b[1m\u001b[32m1.08875\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5605 | loss: 1.08875 - acc: 0.7975 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5606  | total loss: \u001b[1m\u001b[32m1.37794\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5606 | loss: 1.37794 - acc: 0.7320 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5607  | total loss: \u001b[1m\u001b[32m1.26115\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5607 | loss: 1.26115 - acc: 0.7588 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5608  | total loss: \u001b[1m\u001b[32m1.15619\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5608 | loss: 1.15619 - acc: 0.7829 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5609  | total loss: \u001b[1m\u001b[32m1.06168\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5609 | loss: 1.06168 - acc: 0.8046 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5610  | total loss: \u001b[1m\u001b[32m1.34170\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5610 | loss: 1.34170 - acc: 0.7313 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5611  | total loss: \u001b[1m\u001b[32m1.22896\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5611 | loss: 1.22896 - acc: 0.7582 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5612  | total loss: \u001b[1m\u001b[32m1.51466\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5612 | loss: 1.51466 - acc: 0.6895 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5613  | total loss: \u001b[1m\u001b[32m1.38598\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5613 | loss: 1.38598 - acc: 0.7206 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5614  | total loss: \u001b[1m\u001b[32m1.63331\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5614 | loss: 1.63331 - acc: 0.6556 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5615  | total loss: \u001b[1m\u001b[32m1.49503\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5615 | loss: 1.49503 - acc: 0.6901 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5616  | total loss: \u001b[1m\u001b[32m1.37174\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5616 | loss: 1.37174 - acc: 0.7211 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5617  | total loss: \u001b[1m\u001b[32m1.26159\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5617 | loss: 1.26159 - acc: 0.7490 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5618  | total loss: \u001b[1m\u001b[32m1.55261\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5618 | loss: 1.55261 - acc: 0.6741 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5619  | total loss: \u001b[1m\u001b[32m1.42624\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5619 | loss: 1.42624 - acc: 0.7067 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5620  | total loss: \u001b[1m\u001b[32m1.31350\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5620 | loss: 1.31350 - acc: 0.7360 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5621  | total loss: \u001b[1m\u001b[32m1.21263\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5621 | loss: 1.21263 - acc: 0.7624 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5622  | total loss: \u001b[1m\u001b[32m1.42832\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5622 | loss: 1.42832 - acc: 0.7004 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5623  | total loss: \u001b[1m\u001b[32m1.31719\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5623 | loss: 1.31719 - acc: 0.7304 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5624  | total loss: \u001b[1m\u001b[32m1.53608\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5624 | loss: 1.53608 - acc: 0.6716 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5625  | total loss: \u001b[1m\u001b[32m1.41607\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 5625 | loss: 1.41607 - acc: 0.7045 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5626  | total loss: \u001b[1m\u001b[32m1.30890\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5626 | loss: 1.30890 - acc: 0.7340 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5627  | total loss: \u001b[1m\u001b[32m1.21286\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5627 | loss: 1.21286 - acc: 0.7606 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5628  | total loss: \u001b[1m\u001b[32m1.12642\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5628 | loss: 1.12642 - acc: 0.7846 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5629  | total loss: \u001b[1m\u001b[32m1.04824\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5629 | loss: 1.04824 - acc: 0.8061 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5630  | total loss: \u001b[1m\u001b[32m1.24819\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5630 | loss: 1.24819 - acc: 0.7469 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5631  | total loss: \u001b[1m\u001b[32m1.15720\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5631 | loss: 1.15720 - acc: 0.7722 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5632  | total loss: \u001b[1m\u001b[32m1.34116\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5632 | loss: 1.34116 - acc: 0.7236 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5633  | total loss: \u001b[1m\u001b[32m1.24092\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5633 | loss: 1.24092 - acc: 0.7512 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5634  | total loss: \u001b[1m\u001b[32m1.15065\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5634 | loss: 1.15065 - acc: 0.7761 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5635  | total loss: \u001b[1m\u001b[32m1.06900\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5635 | loss: 1.06900 - acc: 0.7985 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5636  | total loss: \u001b[1m\u001b[32m0.99479\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5636 | loss: 0.99479 - acc: 0.8186 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5637  | total loss: \u001b[1m\u001b[32m0.92702\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5637 | loss: 0.92702 - acc: 0.8368 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5638  | total loss: \u001b[1m\u001b[32m1.16229\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5638 | loss: 1.16229 - acc: 0.7745 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5639  | total loss: \u001b[1m\u001b[32m1.07609\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5639 | loss: 1.07609 - acc: 0.7971 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5640  | total loss: \u001b[1m\u001b[32m1.36308\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5640 | loss: 1.36308 - acc: 0.7245 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5641  | total loss: \u001b[1m\u001b[32m1.25634\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5641 | loss: 1.25634 - acc: 0.7521 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5642  | total loss: \u001b[1m\u001b[32m1.53498\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5642 | loss: 1.53498 - acc: 0.6769 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5643  | total loss: \u001b[1m\u001b[32m1.41191\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5643 | loss: 1.41191 - acc: 0.7092 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5644  | total loss: \u001b[1m\u001b[32m1.59396\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5644 | loss: 1.59396 - acc: 0.6597 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5645  | total loss: \u001b[1m\u001b[32m1.46666\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5645 | loss: 1.46666 - acc: 0.6937 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5646  | total loss: \u001b[1m\u001b[32m1.69225\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 5646 | loss: 1.69225 - acc: 0.6243 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5647  | total loss: \u001b[1m\u001b[32m1.55740\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 5647 | loss: 1.55740 - acc: 0.6619 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5648  | total loss: \u001b[1m\u001b[32m1.80508\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5648 | loss: 1.80508 - acc: 0.5957 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5649  | total loss: \u001b[1m\u001b[32m1.66196\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 5649 | loss: 1.66196 - acc: 0.6361 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5650  | total loss: \u001b[1m\u001b[32m1.53452\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5650 | loss: 1.53452 - acc: 0.6725 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5651  | total loss: \u001b[1m\u001b[32m1.42065\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5651 | loss: 1.42065 - acc: 0.7053 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5652  | total loss: \u001b[1m\u001b[32m1.64128\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5652 | loss: 1.64128 - acc: 0.6347 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5653  | total loss: \u001b[1m\u001b[32m1.51829\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5653 | loss: 1.51829 - acc: 0.6713 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5654  | total loss: \u001b[1m\u001b[32m1.69379\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5654 | loss: 1.69379 - acc: 0.6113 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5655  | total loss: \u001b[1m\u001b[32m1.56772\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5655 | loss: 1.56772 - acc: 0.6502 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5656  | total loss: \u001b[1m\u001b[32m1.77117\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5656 | loss: 1.77117 - acc: 0.5923 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5657  | total loss: \u001b[1m\u001b[32m1.63994\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5657 | loss: 1.63994 - acc: 0.6331 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5658  | total loss: \u001b[1m\u001b[32m1.84233\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5658 | loss: 1.84233 - acc: 0.5698 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5659  | total loss: \u001b[1m\u001b[32m1.70687\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5659 | loss: 1.70687 - acc: 0.6128 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5660  | total loss: \u001b[1m\u001b[32m1.87497\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5660 | loss: 1.87497 - acc: 0.5586 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5661  | total loss: \u001b[1m\u001b[32m1.73922\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5661 | loss: 1.73922 - acc: 0.6028 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5662  | total loss: \u001b[1m\u001b[32m1.93480\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5662 | loss: 1.93480 - acc: 0.5425 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5663  | total loss: \u001b[1m\u001b[32m1.79606\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5663 | loss: 1.79606 - acc: 0.5883 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5664  | total loss: \u001b[1m\u001b[32m1.93389\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5664 | loss: 1.93389 - acc: 0.5366 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5665  | total loss: \u001b[1m\u001b[32m1.79822\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5665 | loss: 1.79822 - acc: 0.5829 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5666  | total loss: \u001b[1m\u001b[32m1.95149\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5666 | loss: 1.95149 - acc: 0.5246 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5667  | total loss: \u001b[1m\u001b[32m1.81686\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5667 | loss: 1.81686 - acc: 0.5722 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5668  | total loss: \u001b[1m\u001b[32m1.96267\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5668 | loss: 1.96267 - acc: 0.5221 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5669  | total loss: \u001b[1m\u001b[32m1.82944\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5669 | loss: 1.82944 - acc: 0.5699 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5670  | total loss: \u001b[1m\u001b[32m1.95416\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5670 | loss: 1.95416 - acc: 0.5272 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5671  | total loss: \u001b[1m\u001b[32m1.82389\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5671 | loss: 1.82389 - acc: 0.5745 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5672  | total loss: \u001b[1m\u001b[32m1.98011\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5672 | loss: 1.98011 - acc: 0.5170 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5673  | total loss: \u001b[1m\u001b[32m1.84901\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5673 | loss: 1.84901 - acc: 0.5653 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5674  | total loss: \u001b[1m\u001b[32m1.73138\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5674 | loss: 1.73138 - acc: 0.6088 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5675  | total loss: \u001b[1m\u001b[32m1.62504\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5675 | loss: 1.62504 - acc: 0.6479 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5676  | total loss: \u001b[1m\u001b[32m1.75790\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5676 | loss: 1.75790 - acc: 0.5903 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5677  | total loss: \u001b[1m\u001b[32m1.64727\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5677 | loss: 1.64727 - acc: 0.6312 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5678  | total loss: \u001b[1m\u001b[32m1.81684\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5678 | loss: 1.81684 - acc: 0.5752 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5679  | total loss: \u001b[1m\u001b[32m1.69876\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5679 | loss: 1.69876 - acc: 0.6177 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5680  | total loss: \u001b[1m\u001b[32m1.59140\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5680 | loss: 1.59140 - acc: 0.6560 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5681  | total loss: \u001b[1m\u001b[32m1.49309\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5681 | loss: 1.49309 - acc: 0.6904 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5682  | total loss: \u001b[1m\u001b[32m1.67062\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5682 | loss: 1.67062 - acc: 0.6285 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5683  | total loss: \u001b[1m\u001b[32m1.56101\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5683 | loss: 1.56101 - acc: 0.6656 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5684  | total loss: \u001b[1m\u001b[32m1.46065\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5684 | loss: 1.46065 - acc: 0.6991 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5685  | total loss: \u001b[1m\u001b[32m1.36821\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5685 | loss: 1.36821 - acc: 0.7291 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5686  | total loss: \u001b[1m\u001b[32m1.52593\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5686 | loss: 1.52593 - acc: 0.6705 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5687  | total loss: \u001b[1m\u001b[32m1.42298\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5687 | loss: 1.42298 - acc: 0.7035 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5688  | total loss: \u001b[1m\u001b[32m1.59259\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5688 | loss: 1.59259 - acc: 0.6474 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5689  | total loss: \u001b[1m\u001b[32m1.48018\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5689 | loss: 1.48018 - acc: 0.6827 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5690  | total loss: \u001b[1m\u001b[32m1.37771\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5690 | loss: 1.37771 - acc: 0.7144 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5691  | total loss: \u001b[1m\u001b[32m1.28387\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5691 | loss: 1.28387 - acc: 0.7430 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5692  | total loss: \u001b[1m\u001b[32m1.51135\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5692 | loss: 1.51135 - acc: 0.6758 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5693  | total loss: \u001b[1m\u001b[32m1.40148\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5693 | loss: 1.40148 - acc: 0.7082 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5694  | total loss: \u001b[1m\u001b[32m1.49647\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5694 | loss: 1.49647 - acc: 0.6803 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5695  | total loss: \u001b[1m\u001b[32m1.38642\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5695 | loss: 1.38642 - acc: 0.7122 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5696  | total loss: \u001b[1m\u001b[32m1.53451\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5696 | loss: 1.53451 - acc: 0.6624 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5697  | total loss: \u001b[1m\u001b[32m1.41958\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5697 | loss: 1.41958 - acc: 0.6962 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5698  | total loss: \u001b[1m\u001b[32m1.59944\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5698 | loss: 1.59944 - acc: 0.6409 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5699  | total loss: \u001b[1m\u001b[32m1.47777\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5699 | loss: 1.47777 - acc: 0.6768 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5700  | total loss: \u001b[1m\u001b[32m1.70332\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5700 | loss: 1.70332 - acc: 0.6162 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5701  | total loss: \u001b[1m\u001b[32m1.57201\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5701 | loss: 1.57201 - acc: 0.6546 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5702  | total loss: \u001b[1m\u001b[32m1.78480\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5702 | loss: 1.78480 - acc: 0.5963 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5703  | total loss: \u001b[1m\u001b[32m1.64696\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5703 | loss: 1.64696 - acc: 0.6367 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5704  | total loss: \u001b[1m\u001b[32m1.86232\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5704 | loss: 1.86232 - acc: 0.5730 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5705  | total loss: \u001b[1m\u001b[32m1.71905\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5705 | loss: 1.71905 - acc: 0.6157 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5706  | total loss: \u001b[1m\u001b[32m1.90092\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5706 | loss: 1.90092 - acc: 0.5613 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5707  | total loss: \u001b[1m\u001b[32m1.75658\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5707 | loss: 1.75658 - acc: 0.6051 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5708  | total loss: \u001b[1m\u001b[32m1.62781\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5708 | loss: 1.62781 - acc: 0.6446 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5709  | total loss: \u001b[1m\u001b[32m1.51240\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5709 | loss: 1.51240 - acc: 0.6802 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5710  | total loss: \u001b[1m\u001b[32m1.69281\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5710 | loss: 1.69281 - acc: 0.6193 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5711  | total loss: \u001b[1m\u001b[32m1.57150\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5711 | loss: 1.57150 - acc: 0.6574 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5712  | total loss: \u001b[1m\u001b[32m1.75594\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5712 | loss: 1.75594 - acc: 0.5916 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5713  | total loss: \u001b[1m\u001b[32m1.62944\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5713 | loss: 1.62944 - acc: 0.6325 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5714  | total loss: \u001b[1m\u001b[32m1.80512\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5714 | loss: 1.80512 - acc: 0.5764 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5715  | total loss: \u001b[1m\u001b[32m1.67512\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5715 | loss: 1.67512 - acc: 0.6187 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5716  | total loss: \u001b[1m\u001b[32m1.86715\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5716 | loss: 1.86715 - acc: 0.5569 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5717  | total loss: \u001b[1m\u001b[32m1.73263\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5717 | loss: 1.73263 - acc: 0.6012 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5718  | total loss: \u001b[1m\u001b[32m1.91005\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5718 | loss: 1.91005 - acc: 0.5482 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5719  | total loss: \u001b[1m\u001b[32m1.77321\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5719 | loss: 1.77321 - acc: 0.5934 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5720  | total loss: \u001b[1m\u001b[32m1.92152\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5720 | loss: 1.92152 - acc: 0.5340 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5721  | total loss: \u001b[1m\u001b[32m1.78568\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5721 | loss: 1.78568 - acc: 0.5806 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5722  | total loss: \u001b[1m\u001b[32m1.96398\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5722 | loss: 1.96398 - acc: 0.5226 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5723  | total loss: \u001b[1m\u001b[32m1.82612\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5723 | loss: 1.82612 - acc: 0.5703 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5724  | total loss: \u001b[1m\u001b[32m1.94521\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5724 | loss: 1.94521 - acc: 0.5204 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5725  | total loss: \u001b[1m\u001b[32m1.81144\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5725 | loss: 1.81144 - acc: 0.5684 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5726  | total loss: \u001b[1m\u001b[32m1.98338\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5726 | loss: 1.98338 - acc: 0.5187 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5727  | total loss: \u001b[1m\u001b[32m1.84779\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5727 | loss: 1.84779 - acc: 0.5668 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5728  | total loss: \u001b[1m\u001b[32m2.00950\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5728 | loss: 2.00950 - acc: 0.5101 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5729  | total loss: \u001b[1m\u001b[32m1.87308\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5729 | loss: 1.87308 - acc: 0.5591 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5730  | total loss: \u001b[1m\u001b[32m1.99563\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5730 | loss: 1.99563 - acc: 0.5175 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5731  | total loss: \u001b[1m\u001b[32m1.86201\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5731 | loss: 1.86201 - acc: 0.5657 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5732  | total loss: \u001b[1m\u001b[32m2.01713\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5732 | loss: 2.01713 - acc: 0.5092 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5733  | total loss: \u001b[1m\u001b[32m1.88245\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5733 | loss: 1.88245 - acc: 0.5583 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5734  | total loss: \u001b[1m\u001b[32m2.05281\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5734 | loss: 2.05281 - acc: 0.5024 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5735  | total loss: \u001b[1m\u001b[32m1.91556\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5735 | loss: 1.91556 - acc: 0.5522 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5736  | total loss: \u001b[1m\u001b[32m2.04788\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5736 | loss: 2.04788 - acc: 0.5041 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5737  | total loss: \u001b[1m\u001b[32m1.91192\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5737 | loss: 1.91192 - acc: 0.5537 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5738  | total loss: \u001b[1m\u001b[32m2.03669\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5738 | loss: 2.03669 - acc: 0.4983 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5739  | total loss: \u001b[1m\u001b[32m1.90249\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5739 | loss: 1.90249 - acc: 0.5485 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5740  | total loss: \u001b[1m\u001b[32m2.02860\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5740 | loss: 2.02860 - acc: 0.5008 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5741  | total loss: \u001b[1m\u001b[32m1.89562\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5741 | loss: 1.89562 - acc: 0.5507 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5742  | total loss: \u001b[1m\u001b[32m1.96302\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5742 | loss: 1.96302 - acc: 0.5242 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5743  | total loss: \u001b[1m\u001b[32m1.83649\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5743 | loss: 1.83649 - acc: 0.5718 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5744  | total loss: \u001b[1m\u001b[32m1.93541\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5744 | loss: 1.93541 - acc: 0.5289 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5745  | total loss: \u001b[1m\u001b[32m1.81115\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5745 | loss: 1.81115 - acc: 0.5760 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5746  | total loss: \u001b[1m\u001b[32m1.95189\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5746 | loss: 1.95189 - acc: 0.5255 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5747  | total loss: \u001b[1m\u001b[32m1.82547\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5747 | loss: 1.82547 - acc: 0.5730 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5748  | total loss: \u001b[1m\u001b[32m1.71109\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5748 | loss: 1.71109 - acc: 0.6157 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5749  | total loss: \u001b[1m\u001b[32m1.60685\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5749 | loss: 1.60685 - acc: 0.6541 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5750  | total loss: \u001b[1m\u001b[32m1.51115\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5750 | loss: 1.51115 - acc: 0.6887 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5751  | total loss: \u001b[1m\u001b[32m1.42264\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5751 | loss: 1.42264 - acc: 0.7198 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5752  | total loss: \u001b[1m\u001b[32m1.61365\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5752 | loss: 1.61365 - acc: 0.6479 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5753  | total loss: \u001b[1m\u001b[32m1.51059\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5753 | loss: 1.51059 - acc: 0.6831 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5754  | total loss: \u001b[1m\u001b[32m1.62805\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5754 | loss: 1.62805 - acc: 0.6433 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5755  | total loss: \u001b[1m\u001b[32m1.52030\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 5755 | loss: 1.52030 - acc: 0.6790 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5756  | total loss: \u001b[1m\u001b[32m1.42160\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5756 | loss: 1.42160 - acc: 0.7111 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5757  | total loss: \u001b[1m\u001b[32m1.33067\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5757 | loss: 1.33067 - acc: 0.7400 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5758  | total loss: \u001b[1m\u001b[32m1.55685\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5758 | loss: 1.55685 - acc: 0.6660 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5759  | total loss: \u001b[1m\u001b[32m1.44879\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5759 | loss: 1.44879 - acc: 0.6994 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5760  | total loss: \u001b[1m\u001b[32m1.65973\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5760 | loss: 1.65973 - acc: 0.6366 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5761  | total loss: \u001b[1m\u001b[32m1.53931\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5761 | loss: 1.53931 - acc: 0.6729 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5762  | total loss: \u001b[1m\u001b[32m1.73680\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5762 | loss: 1.73680 - acc: 0.6128 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5763  | total loss: \u001b[1m\u001b[32m1.60782\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5763 | loss: 1.60782 - acc: 0.6515 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5764  | total loss: \u001b[1m\u001b[32m1.73951\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5764 | loss: 1.73951 - acc: 0.6078 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5765  | total loss: \u001b[1m\u001b[32m1.61009\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5765 | loss: 1.61009 - acc: 0.6470 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5766  | total loss: \u001b[1m\u001b[32m1.49334\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5766 | loss: 1.49334 - acc: 0.6823 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5767  | total loss: \u001b[1m\u001b[32m1.38756\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5767 | loss: 1.38756 - acc: 0.7141 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5768  | total loss: \u001b[1m\u001b[32m1.54699\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5768 | loss: 1.54699 - acc: 0.6641 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5769  | total loss: \u001b[1m\u001b[32m1.43439\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5769 | loss: 1.43439 - acc: 0.6977 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5770  | total loss: \u001b[1m\u001b[32m1.61256\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5770 | loss: 1.61256 - acc: 0.6422 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5771  | total loss: \u001b[1m\u001b[32m1.49269\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5771 | loss: 1.49269 - acc: 0.6780 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5772  | total loss: \u001b[1m\u001b[32m1.72118\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5772 | loss: 1.72118 - acc: 0.6102 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5773  | total loss: \u001b[1m\u001b[32m1.59066\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5773 | loss: 1.59066 - acc: 0.6492 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5774  | total loss: \u001b[1m\u001b[32m1.78418\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5774 | loss: 1.78418 - acc: 0.5914 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5775  | total loss: \u001b[1m\u001b[32m1.64843\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5775 | loss: 1.64843 - acc: 0.6323 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5776  | total loss: \u001b[1m\u001b[32m1.80232\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5776 | loss: 1.80232 - acc: 0.5833 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5777  | total loss: \u001b[1m\u001b[32m1.66624\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5777 | loss: 1.66624 - acc: 0.6250 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5778  | total loss: \u001b[1m\u001b[32m1.82996\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5778 | loss: 1.82996 - acc: 0.5768 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5779  | total loss: \u001b[1m\u001b[32m1.69283\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5779 | loss: 1.69283 - acc: 0.6191 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5780  | total loss: \u001b[1m\u001b[32m1.57001\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5780 | loss: 1.57001 - acc: 0.6572 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5781  | total loss: \u001b[1m\u001b[32m1.45951\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5781 | loss: 1.45951 - acc: 0.6915 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5782  | total loss: \u001b[1m\u001b[32m1.64335\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5782 | loss: 1.64335 - acc: 0.6366 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5783  | total loss: \u001b[1m\u001b[32m1.52530\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5783 | loss: 1.52530 - acc: 0.6729 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5784  | total loss: \u001b[1m\u001b[32m1.70959\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5784 | loss: 1.70959 - acc: 0.6199 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5785  | total loss: \u001b[1m\u001b[32m1.58520\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5785 | loss: 1.58520 - acc: 0.6579 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5786  | total loss: \u001b[1m\u001b[32m1.79322\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5786 | loss: 1.79322 - acc: 0.5921 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5787  | total loss: \u001b[1m\u001b[32m1.66143\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5787 | loss: 1.66143 - acc: 0.6329 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5788  | total loss: \u001b[1m\u001b[32m1.81817\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5788 | loss: 1.81817 - acc: 0.5839 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5789  | total loss: \u001b[1m\u001b[32m1.68524\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5789 | loss: 1.68524 - acc: 0.6255 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5790  | total loss: \u001b[1m\u001b[32m1.88070\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5790 | loss: 1.88070 - acc: 0.5630 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5791  | total loss: \u001b[1m\u001b[32m1.74312\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5791 | loss: 1.74312 - acc: 0.6067 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5792  | total loss: \u001b[1m\u001b[32m1.90516\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5792 | loss: 1.90516 - acc: 0.5532 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5793  | total loss: \u001b[1m\u001b[32m1.76706\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5793 | loss: 1.76706 - acc: 0.5978 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5794  | total loss: \u001b[1m\u001b[32m1.90022\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5794 | loss: 1.90022 - acc: 0.5523 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5795  | total loss: \u001b[1m\u001b[32m1.76457\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5795 | loss: 1.76457 - acc: 0.5971 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5796  | total loss: \u001b[1m\u001b[32m1.93864\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5796 | loss: 1.93864 - acc: 0.5374 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5797  | total loss: \u001b[1m\u001b[32m1.80105\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5797 | loss: 1.80105 - acc: 0.5837 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5798  | total loss: \u001b[1m\u001b[32m1.93069\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5798 | loss: 1.93069 - acc: 0.5396 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5799  | total loss: \u001b[1m\u001b[32m1.79567\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5799 | loss: 1.79567 - acc: 0.5856 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5800  | total loss: \u001b[1m\u001b[32m1.67457\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5800 | loss: 1.67457 - acc: 0.6271 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5801  | total loss: \u001b[1m\u001b[32m1.56530\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 5801 | loss: 1.56530 - acc: 0.6644 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5802  | total loss: \u001b[1m\u001b[32m1.69174\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5802 | loss: 1.69174 - acc: 0.6122 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5803  | total loss: \u001b[1m\u001b[32m1.57962\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5803 | loss: 1.57962 - acc: 0.6510 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5804  | total loss: \u001b[1m\u001b[32m1.73975\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5804 | loss: 1.73975 - acc: 0.5930 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5805  | total loss: \u001b[1m\u001b[32m1.62199\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5805 | loss: 1.62199 - acc: 0.6337 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5806  | total loss: \u001b[1m\u001b[32m1.76612\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5806 | loss: 1.76612 - acc: 0.5846 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5807  | total loss: \u001b[1m\u001b[32m1.64520\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5807 | loss: 1.64520 - acc: 0.6262 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5808  | total loss: \u001b[1m\u001b[32m1.75137\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5808 | loss: 1.75137 - acc: 0.5921 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5809  | total loss: \u001b[1m\u001b[32m1.63131\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5809 | loss: 1.63131 - acc: 0.6329 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5810  | total loss: \u001b[1m\u001b[32m1.81562\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5810 | loss: 1.81562 - acc: 0.5768 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5811  | total loss: \u001b[1m\u001b[32m1.68868\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5811 | loss: 1.68868 - acc: 0.6191 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5812  | total loss: \u001b[1m\u001b[32m1.57403\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5812 | loss: 1.57403 - acc: 0.6572 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5813  | total loss: \u001b[1m\u001b[32m1.46987\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5813 | loss: 1.46987 - acc: 0.6915 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5814  | total loss: \u001b[1m\u001b[32m1.37464\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5814 | loss: 1.37464 - acc: 0.7223 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5815  | total loss: \u001b[1m\u001b[32m1.28705\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5815 | loss: 1.28705 - acc: 0.7501 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5816  | total loss: \u001b[1m\u001b[32m1.20599\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5816 | loss: 1.20599 - acc: 0.7751 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5817  | total loss: \u001b[1m\u001b[32m1.13056\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5817 | loss: 1.13056 - acc: 0.7976 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5818  | total loss: \u001b[1m\u001b[32m1.31323\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 5818 | loss: 1.31323 - acc: 0.7392 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5819  | total loss: \u001b[1m\u001b[32m1.22277\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5819 | loss: 1.22277 - acc: 0.7653 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5820  | total loss: \u001b[1m\u001b[32m1.47465\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5820 | loss: 1.47465 - acc: 0.6888 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5821  | total loss: \u001b[1m\u001b[32m1.36548\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5821 | loss: 1.36548 - acc: 0.7199 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5822  | total loss: \u001b[1m\u001b[32m1.57201\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5822 | loss: 1.57201 - acc: 0.6622 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5823  | total loss: \u001b[1m\u001b[32m1.45194\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5823 | loss: 1.45194 - acc: 0.6960 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5824  | total loss: \u001b[1m\u001b[32m1.67692\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5824 | loss: 1.67692 - acc: 0.6335 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5825  | total loss: \u001b[1m\u001b[32m1.54622\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5825 | loss: 1.54622 - acc: 0.6702 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5826  | total loss: \u001b[1m\u001b[32m1.76558\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5826 | loss: 1.76558 - acc: 0.6032 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5827  | total loss: \u001b[1m\u001b[32m1.62687\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5827 | loss: 1.62687 - acc: 0.6428 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5828  | total loss: \u001b[1m\u001b[32m1.83692\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5828 | loss: 1.83692 - acc: 0.5857 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5829  | total loss: \u001b[1m\u001b[32m1.69281\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5829 | loss: 1.69281 - acc: 0.6271 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5830  | total loss: \u001b[1m\u001b[32m1.56387\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5830 | loss: 1.56387 - acc: 0.6644 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5831  | total loss: \u001b[1m\u001b[32m1.44809\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5831 | loss: 1.44809 - acc: 0.6980 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5832  | total loss: \u001b[1m\u001b[32m1.34368\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5832 | loss: 1.34368 - acc: 0.7282 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5833  | total loss: \u001b[1m\u001b[32m1.24908\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5833 | loss: 1.24908 - acc: 0.7554 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5834  | total loss: \u001b[1m\u001b[32m1.49988\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5834 | loss: 1.49988 - acc: 0.6798 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5835  | total loss: \u001b[1m\u001b[32m1.38867\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5835 | loss: 1.38867 - acc: 0.7118 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5836  | total loss: \u001b[1m\u001b[32m1.28817\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5836 | loss: 1.28817 - acc: 0.7407 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5837  | total loss: \u001b[1m\u001b[32m1.19694\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5837 | loss: 1.19694 - acc: 0.7666 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5838  | total loss: \u001b[1m\u001b[32m1.44156\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5838 | loss: 1.44156 - acc: 0.6971 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5839  | total loss: \u001b[1m\u001b[32m1.33377\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5839 | loss: 1.33377 - acc: 0.7274 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5840  | total loss: \u001b[1m\u001b[32m1.23626\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5840 | loss: 1.23626 - acc: 0.7546 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5841  | total loss: \u001b[1m\u001b[32m1.14770\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5841 | loss: 1.14770 - acc: 0.7792 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5842  | total loss: \u001b[1m\u001b[32m1.39390\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5842 | loss: 1.39390 - acc: 0.7155 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5843  | total loss: \u001b[1m\u001b[32m1.28834\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5843 | loss: 1.28834 - acc: 0.7440 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5844  | total loss: \u001b[1m\u001b[32m1.53911\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5844 | loss: 1.53911 - acc: 0.6767 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5845  | total loss: \u001b[1m\u001b[32m1.41901\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5845 | loss: 1.41901 - acc: 0.7091 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5846  | total loss: \u001b[1m\u001b[32m1.57635\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5846 | loss: 1.57635 - acc: 0.6596 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5847  | total loss: \u001b[1m\u001b[32m1.45330\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 5847 | loss: 1.45330 - acc: 0.6936 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5848  | total loss: \u001b[1m\u001b[32m1.66529\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5848 | loss: 1.66529 - acc: 0.6314 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5849  | total loss: \u001b[1m\u001b[32m1.53483\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5849 | loss: 1.53483 - acc: 0.6683 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5850  | total loss: \u001b[1m\u001b[32m1.73078\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5850 | loss: 1.73078 - acc: 0.6086 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5851  | total loss: \u001b[1m\u001b[32m1.59600\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5851 | loss: 1.59600 - acc: 0.6477 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5852  | total loss: \u001b[1m\u001b[32m1.47569\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5852 | loss: 1.47569 - acc: 0.6829 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5853  | total loss: \u001b[1m\u001b[32m1.36788\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5853 | loss: 1.36788 - acc: 0.7147 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5854  | total loss: \u001b[1m\u001b[32m1.61105\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5854 | loss: 1.61105 - acc: 0.6432 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5855  | total loss: \u001b[1m\u001b[32m1.49078\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5855 | loss: 1.49078 - acc: 0.6789 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5856  | total loss: \u001b[1m\u001b[32m1.38306\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5856 | loss: 1.38306 - acc: 0.7110 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5857  | total loss: \u001b[1m\u001b[32m1.28613\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5857 | loss: 1.28613 - acc: 0.7399 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5858  | total loss: \u001b[1m\u001b[32m1.52842\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5858 | loss: 1.52842 - acc: 0.6659 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5859  | total loss: \u001b[1m\u001b[32m1.41704\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5859 | loss: 1.41704 - acc: 0.6993 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5860  | total loss: \u001b[1m\u001b[32m1.62928\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5860 | loss: 1.62928 - acc: 0.6365 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5861  | total loss: \u001b[1m\u001b[32m1.50871\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5861 | loss: 1.50871 - acc: 0.6729 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5862  | total loss: \u001b[1m\u001b[32m1.72440\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5862 | loss: 1.72440 - acc: 0.6056 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5863  | total loss: \u001b[1m\u001b[32m1.59594\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5863 | loss: 1.59594 - acc: 0.6450 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5864  | total loss: \u001b[1m\u001b[32m1.78809\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 5864 | loss: 1.78809 - acc: 0.5805 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5865  | total loss: \u001b[1m\u001b[32m1.65555\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5865 | loss: 1.65555 - acc: 0.6225 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5866  | total loss: \u001b[1m\u001b[32m1.86546\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5866 | loss: 1.86546 - acc: 0.5602 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5867  | total loss: \u001b[1m\u001b[32m1.72800\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5867 | loss: 1.72800 - acc: 0.6042 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5868  | total loss: \u001b[1m\u001b[32m1.90091\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5868 | loss: 1.90091 - acc: 0.5438 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5869  | total loss: \u001b[1m\u001b[32m1.76307\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5869 | loss: 1.76307 - acc: 0.5894 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5870  | total loss: \u001b[1m\u001b[32m1.94201\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5870 | loss: 1.94201 - acc: 0.5305 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5871  | total loss: \u001b[1m\u001b[32m1.80335\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5871 | loss: 1.80335 - acc: 0.5774 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5872  | total loss: \u001b[1m\u001b[32m1.67980\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5872 | loss: 1.67980 - acc: 0.6197 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5873  | total loss: \u001b[1m\u001b[32m1.56903\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5873 | loss: 1.56903 - acc: 0.6577 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5874  | total loss: \u001b[1m\u001b[32m1.72091\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5874 | loss: 1.72091 - acc: 0.6062 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5875  | total loss: \u001b[1m\u001b[32m1.60609\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5875 | loss: 1.60609 - acc: 0.6456 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5876  | total loss: \u001b[1m\u001b[32m1.78408\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5876 | loss: 1.78408 - acc: 0.5882 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5877  | total loss: \u001b[1m\u001b[32m1.66314\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5877 | loss: 1.66314 - acc: 0.6294 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5878  | total loss: \u001b[1m\u001b[32m1.80046\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5878 | loss: 1.80046 - acc: 0.5736 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5879  | total loss: \u001b[1m\u001b[32m1.67820\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5879 | loss: 1.67820 - acc: 0.6162 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5880  | total loss: \u001b[1m\u001b[32m1.80325\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5880 | loss: 1.80325 - acc: 0.5760 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5881  | total loss: \u001b[1m\u001b[32m1.68081\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5881 | loss: 1.68081 - acc: 0.6184 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5882  | total loss: \u001b[1m\u001b[32m1.57023\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5882 | loss: 1.57023 - acc: 0.6566 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5883  | total loss: \u001b[1m\u001b[32m1.46970\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5883 | loss: 1.46970 - acc: 0.6909 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5884  | total loss: \u001b[1m\u001b[32m1.69032\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5884 | loss: 1.69032 - acc: 0.6218 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5885  | total loss: \u001b[1m\u001b[32m1.57580\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5885 | loss: 1.57580 - acc: 0.6596 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5886  | total loss: \u001b[1m\u001b[32m1.75793\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5886 | loss: 1.75793 - acc: 0.5937 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5887  | total loss: \u001b[1m\u001b[32m1.63577\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5887 | loss: 1.63577 - acc: 0.6343 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5888  | total loss: \u001b[1m\u001b[32m1.77943\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5888 | loss: 1.77943 - acc: 0.5852 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5889  | total loss: \u001b[1m\u001b[32m1.65486\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5889 | loss: 1.65486 - acc: 0.6266 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5890  | total loss: \u001b[1m\u001b[32m1.82174\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5890 | loss: 1.82174 - acc: 0.5711 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5891  | total loss: \u001b[1m\u001b[32m1.69298\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5891 | loss: 1.69298 - acc: 0.6140 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5892  | total loss: \u001b[1m\u001b[32m1.87044\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5892 | loss: 1.87044 - acc: 0.5526 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5893  | total loss: \u001b[1m\u001b[32m1.73737\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5893 | loss: 1.73737 - acc: 0.5974 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5894  | total loss: \u001b[1m\u001b[32m1.91128\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5894 | loss: 1.91128 - acc: 0.5448 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5895  | total loss: \u001b[1m\u001b[32m1.77522\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5895 | loss: 1.77522 - acc: 0.5903 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5896  | total loss: \u001b[1m\u001b[32m1.95238\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5896 | loss: 1.95238 - acc: 0.5313 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5897  | total loss: \u001b[1m\u001b[32m1.81372\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5897 | loss: 1.81372 - acc: 0.5781 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5898  | total loss: \u001b[1m\u001b[32m2.00482\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5898 | loss: 2.00482 - acc: 0.5203 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5899  | total loss: \u001b[1m\u001b[32m1.86287\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5899 | loss: 1.86287 - acc: 0.5683 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5900  | total loss: \u001b[1m\u001b[32m1.98877\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5900 | loss: 1.98877 - acc: 0.5257 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5901  | total loss: \u001b[1m\u001b[32m1.85045\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5901 | loss: 1.85045 - acc: 0.5732 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5902  | total loss: \u001b[1m\u001b[32m1.97260\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5902 | loss: 1.97260 - acc: 0.5301 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5903  | total loss: \u001b[1m\u001b[32m1.83750\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5903 | loss: 1.83750 - acc: 0.5771 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5904  | total loss: \u001b[1m\u001b[32m2.00355\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5904 | loss: 2.00355 - acc: 0.5194 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5905  | total loss: \u001b[1m\u001b[32m1.86677\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5905 | loss: 1.86677 - acc: 0.5675 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5906  | total loss: \u001b[1m\u001b[32m1.97285\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5906 | loss: 1.97285 - acc: 0.5250 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5907  | total loss: \u001b[1m\u001b[32m1.84023\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5907 | loss: 1.84023 - acc: 0.5725 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5908  | total loss: \u001b[1m\u001b[32m1.95662\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5908 | loss: 1.95662 - acc: 0.5295 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5909  | total loss: \u001b[1m\u001b[32m1.82612\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5909 | loss: 1.82612 - acc: 0.5766 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5910  | total loss: \u001b[1m\u001b[32m1.99122\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5910 | loss: 1.99122 - acc: 0.5189 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5911  | total loss: \u001b[1m\u001b[32m1.85764\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5911 | loss: 1.85764 - acc: 0.5670 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5912  | total loss: \u001b[1m\u001b[32m2.01630\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5912 | loss: 2.01630 - acc: 0.5103 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5913  | total loss: \u001b[1m\u001b[32m1.88072\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5913 | loss: 1.88072 - acc: 0.5593 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5914  | total loss: \u001b[1m\u001b[32m2.03892\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5914 | loss: 2.03892 - acc: 0.5034 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5915  | total loss: \u001b[1m\u001b[32m1.90166\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5915 | loss: 1.90166 - acc: 0.5530 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5916  | total loss: \u001b[1m\u001b[32m1.77801\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5916 | loss: 1.77801 - acc: 0.5977 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5917  | total loss: \u001b[1m\u001b[32m1.66588\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5917 | loss: 1.66588 - acc: 0.6380 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5918  | total loss: \u001b[1m\u001b[32m1.56346\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5918 | loss: 1.56346 - acc: 0.6742 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5919  | total loss: \u001b[1m\u001b[32m1.46925\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5919 | loss: 1.46925 - acc: 0.7067 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5920  | total loss: \u001b[1m\u001b[32m1.65883\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5920 | loss: 1.65883 - acc: 0.6432 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5921  | total loss: \u001b[1m\u001b[32m1.55111\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5921 | loss: 1.55111 - acc: 0.6789 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5922  | total loss: \u001b[1m\u001b[32m1.73925\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5922 | loss: 1.73925 - acc: 0.6110 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5923  | total loss: \u001b[1m\u001b[32m1.62068\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5923 | loss: 1.62068 - acc: 0.6499 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5924  | total loss: \u001b[1m\u001b[32m1.76046\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5924 | loss: 1.76046 - acc: 0.6063 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5925  | total loss: \u001b[1m\u001b[32m1.63777\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5925 | loss: 1.63777 - acc: 0.6457 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5926  | total loss: \u001b[1m\u001b[32m1.52623\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5926 | loss: 1.52623 - acc: 0.6811 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5927  | total loss: \u001b[1m\u001b[32m1.42430\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5927 | loss: 1.42430 - acc: 0.7130 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5928  | total loss: \u001b[1m\u001b[32m1.62183\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5928 | loss: 1.62183 - acc: 0.6489 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5929  | total loss: \u001b[1m\u001b[32m1.50765\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5929 | loss: 1.50765 - acc: 0.6840 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5930  | total loss: \u001b[1m\u001b[32m1.71946\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5930 | loss: 1.71946 - acc: 0.6156 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5931  | total loss: \u001b[1m\u001b[32m1.59415\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5931 | loss: 1.59415 - acc: 0.6540 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5932  | total loss: \u001b[1m\u001b[32m1.48075\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5932 | loss: 1.48075 - acc: 0.6886 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5933  | total loss: \u001b[1m\u001b[32m1.37765\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5933 | loss: 1.37765 - acc: 0.7198 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5934  | total loss: \u001b[1m\u001b[32m1.58826\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5934 | loss: 1.58826 - acc: 0.6549 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5935  | total loss: \u001b[1m\u001b[32m1.47259\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5935 | loss: 1.47259 - acc: 0.6894 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5936  | total loss: \u001b[1m\u001b[32m1.70822\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5936 | loss: 1.70822 - acc: 0.6205 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5937  | total loss: \u001b[1m\u001b[32m1.57989\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5937 | loss: 1.57989 - acc: 0.6584 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5938  | total loss: \u001b[1m\u001b[32m1.81009\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5938 | loss: 1.81009 - acc: 0.5926 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5939  | total loss: \u001b[1m\u001b[32m1.67200\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5939 | loss: 1.67200 - acc: 0.6333 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5940  | total loss: \u001b[1m\u001b[32m1.54791\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5940 | loss: 1.54791 - acc: 0.6700 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5941  | total loss: \u001b[1m\u001b[32m1.43594\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5941 | loss: 1.43594 - acc: 0.7030 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5942  | total loss: \u001b[1m\u001b[32m1.33445\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5942 | loss: 1.33445 - acc: 0.7327 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5943  | total loss: \u001b[1m\u001b[32m1.24202\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5943 | loss: 1.24202 - acc: 0.7594 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5944  | total loss: \u001b[1m\u001b[32m1.41076\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5944 | loss: 1.41076 - acc: 0.7049 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5945  | total loss: \u001b[1m\u001b[32m1.30873\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5945 | loss: 1.30873 - acc: 0.7344 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5946  | total loss: \u001b[1m\u001b[32m1.54974\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5946 | loss: 1.54974 - acc: 0.6610 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5947  | total loss: \u001b[1m\u001b[32m1.43298\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5947 | loss: 1.43298 - acc: 0.6949 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5948  | total loss: \u001b[1m\u001b[32m1.32758\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5948 | loss: 1.32758 - acc: 0.7254 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5949  | total loss: \u001b[1m\u001b[32m1.23204\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5949 | loss: 1.23204 - acc: 0.7529 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5950  | total loss: \u001b[1m\u001b[32m1.46917\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5950 | loss: 1.46917 - acc: 0.6847 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5951  | total loss: \u001b[1m\u001b[32m1.35841\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5951 | loss: 1.35841 - acc: 0.7162 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5952  | total loss: \u001b[1m\u001b[32m1.25829\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5952 | loss: 1.25829 - acc: 0.7446 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5953  | total loss: \u001b[1m\u001b[32m1.16743\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5953 | loss: 1.16743 - acc: 0.7702 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5954  | total loss: \u001b[1m\u001b[32m1.37466\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5954 | loss: 1.37466 - acc: 0.7074 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5955  | total loss: \u001b[1m\u001b[32m1.27084\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5955 | loss: 1.27084 - acc: 0.7367 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5956  | total loss: \u001b[1m\u001b[32m1.51225\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5956 | loss: 1.51225 - acc: 0.6702 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5957  | total loss: \u001b[1m\u001b[32m1.39438\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5957 | loss: 1.39438 - acc: 0.7031 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5958  | total loss: \u001b[1m\u001b[32m1.63315\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5958 | loss: 1.63315 - acc: 0.6328 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5959  | total loss: \u001b[1m\u001b[32m1.50396\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5959 | loss: 1.50396 - acc: 0.6695 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5960  | total loss: \u001b[1m\u001b[32m1.38808\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5960 | loss: 1.38808 - acc: 0.7026 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5961  | total loss: \u001b[1m\u001b[32m1.28378\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5961 | loss: 1.28378 - acc: 0.7323 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5962  | total loss: \u001b[1m\u001b[32m1.56325\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5962 | loss: 1.56325 - acc: 0.6591 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5963  | total loss: \u001b[1m\u001b[32m1.44168\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5963 | loss: 1.44168 - acc: 0.6932 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5964  | total loss: \u001b[1m\u001b[32m1.33247\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5964 | loss: 1.33247 - acc: 0.7239 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5965  | total loss: \u001b[1m\u001b[32m1.23399\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5965 | loss: 1.23399 - acc: 0.7515 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5966  | total loss: \u001b[1m\u001b[32m1.48966\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5966 | loss: 1.48966 - acc: 0.6763 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5967  | total loss: \u001b[1m\u001b[32m1.37531\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5967 | loss: 1.37531 - acc: 0.7087 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5968  | total loss: \u001b[1m\u001b[32m1.60112\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5968 | loss: 1.60112 - acc: 0.6450 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5969  | total loss: \u001b[1m\u001b[32m1.47644\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5969 | loss: 1.47644 - acc: 0.6805 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5970  | total loss: \u001b[1m\u001b[32m1.68265\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5970 | loss: 1.68265 - acc: 0.6196 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5971  | total loss: \u001b[1m\u001b[32m1.55144\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5971 | loss: 1.55144 - acc: 0.6576 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5972  | total loss: \u001b[1m\u001b[32m1.76071\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5972 | loss: 1.76071 - acc: 0.5919 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5973  | total loss: \u001b[1m\u001b[32m1.62404\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5973 | loss: 1.62404 - acc: 0.6327 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5974  | total loss: \u001b[1m\u001b[32m1.84389\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5974 | loss: 1.84389 - acc: 0.5694 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5975  | total loss: \u001b[1m\u001b[32m1.70185\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5975 | loss: 1.70185 - acc: 0.6125 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5976  | total loss: \u001b[1m\u001b[32m1.88079\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5976 | loss: 1.88079 - acc: 0.5512 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5977  | total loss: \u001b[1m\u001b[32m1.73847\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5977 | loss: 1.73847 - acc: 0.5961 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5978  | total loss: \u001b[1m\u001b[32m1.90774\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5978 | loss: 1.90774 - acc: 0.5436 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5979  | total loss: \u001b[1m\u001b[32m1.76626\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5979 | loss: 1.76626 - acc: 0.5893 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5980  | total loss: \u001b[1m\u001b[32m1.90593\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5980 | loss: 1.90593 - acc: 0.5375 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5981  | total loss: \u001b[1m\u001b[32m1.76806\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5981 | loss: 1.76806 - acc: 0.5837 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5982  | total loss: \u001b[1m\u001b[32m1.94263\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5982 | loss: 1.94263 - acc: 0.5254 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5983  | total loss: \u001b[1m\u001b[32m1.80447\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5983 | loss: 1.80447 - acc: 0.5728 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5984  | total loss: \u001b[1m\u001b[32m1.92438\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5984 | loss: 1.92438 - acc: 0.5298 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5985  | total loss: \u001b[1m\u001b[32m1.79122\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5985 | loss: 1.79122 - acc: 0.5768 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5986  | total loss: \u001b[1m\u001b[32m1.93057\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5986 | loss: 1.93057 - acc: 0.5192 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5987  | total loss: \u001b[1m\u001b[32m1.79972\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5987 | loss: 1.79972 - acc: 0.5672 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5988  | total loss: \u001b[1m\u001b[32m1.68295\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5988 | loss: 1.68295 - acc: 0.6105 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5989  | total loss: \u001b[1m\u001b[32m1.57797\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5989 | loss: 1.57797 - acc: 0.6495 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5990  | total loss: \u001b[1m\u001b[32m1.48284\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5990 | loss: 1.48284 - acc: 0.6845 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5991  | total loss: \u001b[1m\u001b[32m1.39587\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5991 | loss: 1.39587 - acc: 0.7161 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5992  | total loss: \u001b[1m\u001b[32m1.60368\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5992 | loss: 1.60368 - acc: 0.6445 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5993  | total loss: \u001b[1m\u001b[32m1.50176\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5993 | loss: 1.50176 - acc: 0.6800 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5994  | total loss: \u001b[1m\u001b[32m1.67483\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5994 | loss: 1.67483 - acc: 0.6120 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5995  | total loss: \u001b[1m\u001b[32m1.56378\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5995 | loss: 1.56378 - acc: 0.6508 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5996  | total loss: \u001b[1m\u001b[32m1.77306\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5996 | loss: 1.77306 - acc: 0.5857 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5997  | total loss: \u001b[1m\u001b[32m1.65097\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 5997 | loss: 1.65097 - acc: 0.6272 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5998  | total loss: \u001b[1m\u001b[32m1.54036\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 5998 | loss: 1.54036 - acc: 0.6644 -- iter: 14/14\n",
      "--\n",
      "Training Step: 5999  | total loss: \u001b[1m\u001b[32m1.43955\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 5999 | loss: 1.43955 - acc: 0.6980 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6000  | total loss: \u001b[1m\u001b[32m1.66255\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6000 | loss: 1.66255 - acc: 0.6353 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6001  | total loss: \u001b[1m\u001b[32m1.54720\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6001 | loss: 1.54720 - acc: 0.6718 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6002  | total loss: \u001b[1m\u001b[32m1.72949\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6002 | loss: 1.72949 - acc: 0.6118 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 6003  | total loss: \u001b[1m\u001b[32m1.60610\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6003 | loss: 1.60610 - acc: 0.6506 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6004  | total loss: \u001b[1m\u001b[32m1.76813\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6004 | loss: 1.76813 - acc: 0.5998 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6005  | total loss: \u001b[1m\u001b[32m1.64015\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6005 | loss: 1.64015 - acc: 0.6398 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6006  | total loss: \u001b[1m\u001b[32m1.52449\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6006 | loss: 1.52449 - acc: 0.6759 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6007  | total loss: \u001b[1m\u001b[32m1.41945\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6007 | loss: 1.41945 - acc: 0.7083 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6008  | total loss: \u001b[1m\u001b[32m1.60939\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6008 | loss: 1.60939 - acc: 0.6517 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6009  | total loss: \u001b[1m\u001b[32m1.49405\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6009 | loss: 1.49405 - acc: 0.6866 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6010  | total loss: \u001b[1m\u001b[32m1.67716\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6010 | loss: 1.67716 - acc: 0.6250 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6011  | total loss: \u001b[1m\u001b[32m1.55416\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6011 | loss: 1.55416 - acc: 0.6625 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6012  | total loss: \u001b[1m\u001b[32m1.74795\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6012 | loss: 1.74795 - acc: 0.5963 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6013  | total loss: \u001b[1m\u001b[32m1.61789\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6013 | loss: 1.61789 - acc: 0.6367 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6014  | total loss: \u001b[1m\u001b[32m1.83556\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6014 | loss: 1.83556 - acc: 0.5730 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6015  | total loss: \u001b[1m\u001b[32m1.69776\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6015 | loss: 1.69776 - acc: 0.6157 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6016  | total loss: \u001b[1m\u001b[32m1.88006\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6016 | loss: 1.88006 - acc: 0.5613 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6017  | total loss: \u001b[1m\u001b[32m1.73956\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6017 | loss: 1.73956 - acc: 0.6051 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6018  | total loss: \u001b[1m\u001b[32m1.84782\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6018 | loss: 1.84782 - acc: 0.5661 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6019  | total loss: \u001b[1m\u001b[32m1.71229\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6019 | loss: 1.71229 - acc: 0.6094 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6020  | total loss: \u001b[1m\u001b[32m1.59077\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6020 | loss: 1.59077 - acc: 0.6485 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6021  | total loss: \u001b[1m\u001b[32m1.48127\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6021 | loss: 1.48127 - acc: 0.6837 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6022  | total loss: \u001b[1m\u001b[32m1.38204\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6022 | loss: 1.38204 - acc: 0.7153 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6023  | total loss: \u001b[1m\u001b[32m1.29161\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6023 | loss: 1.29161 - acc: 0.7438 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6024  | total loss: \u001b[1m\u001b[32m1.51202\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6024 | loss: 1.51202 - acc: 0.6765 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6025  | total loss: \u001b[1m\u001b[32m1.40660\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6025 | loss: 1.40660 - acc: 0.7089 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6026  | total loss: \u001b[1m\u001b[32m1.62151\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6026 | loss: 1.62151 - acc: 0.6451 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6027  | total loss: \u001b[1m\u001b[32m1.50434\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6027 | loss: 1.50434 - acc: 0.6806 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6028  | total loss: \u001b[1m\u001b[32m1.66354\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6028 | loss: 1.66354 - acc: 0.6268 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6029  | total loss: \u001b[1m\u001b[32m1.54210\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6029 | loss: 1.54210 - acc: 0.6642 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6030  | total loss: \u001b[1m\u001b[32m1.75101\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6030 | loss: 1.75101 - acc: 0.5977 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6031  | total loss: \u001b[1m\u001b[32m1.62145\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6031 | loss: 1.62145 - acc: 0.6380 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6032  | total loss: \u001b[1m\u001b[32m1.77451\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6032 | loss: 1.77451 - acc: 0.5956 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6033  | total loss: \u001b[1m\u001b[32m1.64370\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6033 | loss: 1.64370 - acc: 0.6360 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6034  | total loss: \u001b[1m\u001b[32m1.81328\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6034 | loss: 1.81328 - acc: 0.5867 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6035  | total loss: \u001b[1m\u001b[32m1.67985\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6035 | loss: 1.67985 - acc: 0.6280 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6036  | total loss: \u001b[1m\u001b[32m1.86854\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6036 | loss: 1.86854 - acc: 0.5652 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6037  | total loss: \u001b[1m\u001b[32m1.73117\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6037 | loss: 1.73117 - acc: 0.6087 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6038  | total loss: \u001b[1m\u001b[32m1.89128\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6038 | loss: 1.89128 - acc: 0.5550 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6039  | total loss: \u001b[1m\u001b[32m1.75335\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6039 | loss: 1.75335 - acc: 0.5995 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6040  | total loss: \u001b[1m\u001b[32m1.91591\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6040 | loss: 1.91591 - acc: 0.5467 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6041  | total loss: \u001b[1m\u001b[32m1.77730\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6041 | loss: 1.77730 - acc: 0.5920 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6042  | total loss: \u001b[1m\u001b[32m1.95856\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6042 | loss: 1.95856 - acc: 0.5328 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6043  | total loss: \u001b[1m\u001b[32m1.81784\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6043 | loss: 1.81784 - acc: 0.5795 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6044  | total loss: \u001b[1m\u001b[32m1.96840\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 6044 | loss: 1.96840 - acc: 0.5287 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6045  | total loss: \u001b[1m\u001b[32m1.82898\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6045 | loss: 1.82898 - acc: 0.5759 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6046  | total loss: \u001b[1m\u001b[32m1.95578\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6046 | loss: 1.95578 - acc: 0.5326 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6047  | total loss: \u001b[1m\u001b[32m1.81962\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6047 | loss: 1.81962 - acc: 0.5793 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6048  | total loss: \u001b[1m\u001b[32m1.94279\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6048 | loss: 1.94279 - acc: 0.5357 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6049  | total loss: \u001b[1m\u001b[32m1.80951\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6049 | loss: 1.80951 - acc: 0.5821 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6050  | total loss: \u001b[1m\u001b[32m1.95758\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6050 | loss: 1.95758 - acc: 0.5310 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6051  | total loss: \u001b[1m\u001b[32m1.82428\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6051 | loss: 1.82428 - acc: 0.5779 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6052  | total loss: \u001b[1m\u001b[32m1.70468\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6052 | loss: 1.70468 - acc: 0.6201 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6053  | total loss: \u001b[1m\u001b[32m1.59667\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6053 | loss: 1.59667 - acc: 0.6581 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6054  | total loss: \u001b[1m\u001b[32m1.77642\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6054 | loss: 1.77642 - acc: 0.5994 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6055  | total loss: \u001b[1m\u001b[32m1.66006\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6055 | loss: 1.66006 - acc: 0.6395 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6056  | total loss: \u001b[1m\u001b[32m1.81229\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6056 | loss: 1.81229 - acc: 0.5898 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6057  | total loss: \u001b[1m\u001b[32m1.69150\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6057 | loss: 1.69150 - acc: 0.6309 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6058  | total loss: \u001b[1m\u001b[32m1.83762\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6058 | loss: 1.83762 - acc: 0.5821 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6059  | total loss: \u001b[1m\u001b[32m1.71364\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6059 | loss: 1.71364 - acc: 0.6238 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 6060  | total loss: \u001b[1m\u001b[32m1.86562\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6060 | loss: 1.86562 - acc: 0.5686 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6061  | total loss: \u001b[1m\u001b[32m1.73838\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6061 | loss: 1.73838 - acc: 0.6117 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6062  | total loss: \u001b[1m\u001b[32m1.89795\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6062 | loss: 1.89795 - acc: 0.5577 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6063  | total loss: \u001b[1m\u001b[32m1.76724\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6063 | loss: 1.76724 - acc: 0.6019 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6064  | total loss: \u001b[1m\u001b[32m1.91431\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6064 | loss: 1.91431 - acc: 0.5489 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6065  | total loss: \u001b[1m\u001b[32m1.78202\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6065 | loss: 1.78202 - acc: 0.5940 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6066  | total loss: \u001b[1m\u001b[32m1.86826\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6066 | loss: 1.86826 - acc: 0.5632 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6067  | total loss: \u001b[1m\u001b[32m1.74050\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6067 | loss: 1.74050 - acc: 0.6069 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6068  | total loss: \u001b[1m\u001b[32m1.86199\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6068 | loss: 1.86199 - acc: 0.5605 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6069  | total loss: \u001b[1m\u001b[32m1.73454\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6069 | loss: 1.73454 - acc: 0.6044 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6070  | total loss: \u001b[1m\u001b[32m1.89721\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6070 | loss: 1.89721 - acc: 0.5440 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6071  | total loss: \u001b[1m\u001b[32m1.76612\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6071 | loss: 1.76612 - acc: 0.5896 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6072  | total loss: \u001b[1m\u001b[32m1.91559\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6072 | loss: 1.91559 - acc: 0.5378 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6073  | total loss: \u001b[1m\u001b[32m1.78285\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6073 | loss: 1.78285 - acc: 0.5840 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6074  | total loss: \u001b[1m\u001b[32m1.96364\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6074 | loss: 1.96364 - acc: 0.5256 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6075  | total loss: \u001b[1m\u001b[32m1.82654\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6075 | loss: 1.82654 - acc: 0.5730 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6076  | total loss: \u001b[1m\u001b[32m1.94600\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6076 | loss: 1.94600 - acc: 0.5300 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6077  | total loss: \u001b[1m\u001b[32m1.81115\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6077 | loss: 1.81115 - acc: 0.5770 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6078  | total loss: \u001b[1m\u001b[32m1.68963\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6078 | loss: 1.68963 - acc: 0.6193 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6079  | total loss: \u001b[1m\u001b[32m1.57950\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6079 | loss: 1.57950 - acc: 0.6574 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6080  | total loss: \u001b[1m\u001b[32m1.75853\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6080 | loss: 1.75853 - acc: 0.5916 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6081  | total loss: \u001b[1m\u001b[32m1.63981\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6081 | loss: 1.63981 - acc: 0.6325 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6082  | total loss: \u001b[1m\u001b[32m1.78431\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6082 | loss: 1.78431 - acc: 0.5835 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6083  | total loss: \u001b[1m\u001b[32m1.66194\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 6083 | loss: 1.66194 - acc: 0.6252 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6084  | total loss: \u001b[1m\u001b[32m1.80845\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6084 | loss: 1.80845 - acc: 0.5769 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6085  | total loss: \u001b[1m\u001b[32m1.68300\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6085 | loss: 1.68300 - acc: 0.6192 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6086  | total loss: \u001b[1m\u001b[32m1.56954\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6086 | loss: 1.56954 - acc: 0.6573 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6087  | total loss: \u001b[1m\u001b[32m1.46634\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6087 | loss: 1.46634 - acc: 0.6916 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6088  | total loss: \u001b[1m\u001b[32m1.64297\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6088 | loss: 1.64297 - acc: 0.6367 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6089  | total loss: \u001b[1m\u001b[32m1.53022\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6089 | loss: 1.53022 - acc: 0.6730 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6090  | total loss: \u001b[1m\u001b[32m1.68435\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6090 | loss: 1.68435 - acc: 0.6200 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6091  | total loss: \u001b[1m\u001b[32m1.56596\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6091 | loss: 1.56596 - acc: 0.6580 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6092  | total loss: \u001b[1m\u001b[32m1.74965\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6092 | loss: 1.74965 - acc: 0.5994 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6093  | total loss: \u001b[1m\u001b[32m1.62390\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6093 | loss: 1.62390 - acc: 0.6394 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6094  | total loss: \u001b[1m\u001b[32m1.78515\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6094 | loss: 1.78515 - acc: 0.5898 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6095  | total loss: \u001b[1m\u001b[32m1.65556\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6095 | loss: 1.65556 - acc: 0.6308 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6096  | total loss: \u001b[1m\u001b[32m1.53861\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6096 | loss: 1.53861 - acc: 0.6677 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6097  | total loss: \u001b[1m\u001b[32m1.43255\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6097 | loss: 1.43255 - acc: 0.7009 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6098  | total loss: \u001b[1m\u001b[32m1.57619\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6098 | loss: 1.57619 - acc: 0.6523 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6099  | total loss: \u001b[1m\u001b[32m1.46462\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6099 | loss: 1.46462 - acc: 0.6870 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6100  | total loss: \u001b[1m\u001b[32m1.61643\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6100 | loss: 1.61643 - acc: 0.6326 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6101  | total loss: \u001b[1m\u001b[32m1.49973\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6101 | loss: 1.49973 - acc: 0.6694 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6102  | total loss: \u001b[1m\u001b[32m1.69845\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6102 | loss: 1.69845 - acc: 0.6096 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6103  | total loss: \u001b[1m\u001b[32m1.57319\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6103 | loss: 1.57319 - acc: 0.6486 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6104  | total loss: \u001b[1m\u001b[32m1.46020\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6104 | loss: 1.46020 - acc: 0.6838 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6105  | total loss: \u001b[1m\u001b[32m1.35780\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6105 | loss: 1.35780 - acc: 0.7154 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6106  | total loss: \u001b[1m\u001b[32m1.26456\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6106 | loss: 1.26456 - acc: 0.7438 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6107  | total loss: \u001b[1m\u001b[32m1.17923\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6107 | loss: 1.17923 - acc: 0.7695 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6108  | total loss: \u001b[1m\u001b[32m1.41785\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6108 | loss: 1.41785 - acc: 0.6997 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6109  | total loss: \u001b[1m\u001b[32m1.31486\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6109 | loss: 1.31486 - acc: 0.7297 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6110  | total loss: \u001b[1m\u001b[32m1.22119\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6110 | loss: 1.22119 - acc: 0.7567 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6111  | total loss: \u001b[1m\u001b[32m1.13563\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6111 | loss: 1.13563 - acc: 0.7810 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6112  | total loss: \u001b[1m\u001b[32m1.32873\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6112 | loss: 1.32873 - acc: 0.7244 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6113  | total loss: \u001b[1m\u001b[32m1.23022\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6113 | loss: 1.23022 - acc: 0.7519 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6114  | total loss: \u001b[1m\u001b[32m1.14058\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6114 | loss: 1.14058 - acc: 0.7767 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6115  | total loss: \u001b[1m\u001b[32m1.05871\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6115 | loss: 1.05871 - acc: 0.7991 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6116  | total loss: \u001b[1m\u001b[32m1.27485\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6116 | loss: 1.27485 - acc: 0.7406 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 6117  | total loss: \u001b[1m\u001b[32m1.17764\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6117 | loss: 1.17764 - acc: 0.7665 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6118  | total loss: \u001b[1m\u001b[32m1.08935\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6118 | loss: 1.08935 - acc: 0.7899 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6119  | total loss: \u001b[1m\u001b[32m1.00892\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6119 | loss: 1.00892 - acc: 0.8109 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6120  | total loss: \u001b[1m\u001b[32m0.93540\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 6120 | loss: 0.93540 - acc: 0.8298 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6121  | total loss: \u001b[1m\u001b[32m0.86799\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6121 | loss: 0.86799 - acc: 0.8468 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6122  | total loss: \u001b[1m\u001b[32m1.19981\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6122 | loss: 1.19981 - acc: 0.7621 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6123  | total loss: \u001b[1m\u001b[32m1.10427\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6123 | loss: 1.10427 - acc: 0.7859 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6124  | total loss: \u001b[1m\u001b[32m1.40872\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6124 | loss: 1.40872 - acc: 0.7145 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6125  | total loss: \u001b[1m\u001b[32m1.29209\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6125 | loss: 1.29209 - acc: 0.7430 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6126  | total loss: \u001b[1m\u001b[32m1.18722\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6126 | loss: 1.18722 - acc: 0.7687 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6127  | total loss: \u001b[1m\u001b[32m1.09271\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6127 | loss: 1.09271 - acc: 0.7919 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6128  | total loss: \u001b[1m\u001b[32m1.36812\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6128 | loss: 1.36812 - acc: 0.7198 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6129  | total loss: \u001b[1m\u001b[32m1.25569\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6129 | loss: 1.25569 - acc: 0.7478 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6130  | total loss: \u001b[1m\u001b[32m1.15472\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6130 | loss: 1.15472 - acc: 0.7730 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6131  | total loss: \u001b[1m\u001b[32m1.06385\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6131 | loss: 1.06385 - acc: 0.7957 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6132  | total loss: \u001b[1m\u001b[32m0.98184\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6132 | loss: 0.98184 - acc: 0.8162 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6133  | total loss: \u001b[1m\u001b[32m0.90761\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6133 | loss: 0.90761 - acc: 0.8346 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6134  | total loss: \u001b[1m\u001b[32m1.16397\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6134 | loss: 1.16397 - acc: 0.7725 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6135  | total loss: \u001b[1m\u001b[32m1.07102\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6135 | loss: 1.07102 - acc: 0.7953 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6136  | total loss: \u001b[1m\u001b[32m1.34505\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6136 | loss: 1.34505 - acc: 0.7229 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6137  | total loss: \u001b[1m\u001b[32m1.23452\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6137 | loss: 1.23452 - acc: 0.7506 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6138  | total loss: \u001b[1m\u001b[32m1.13543\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6138 | loss: 1.13543 - acc: 0.7755 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6139  | total loss: \u001b[1m\u001b[32m1.04639\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6139 | loss: 1.04639 - acc: 0.7980 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6140  | total loss: \u001b[1m\u001b[32m1.39303\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6140 | loss: 1.39303 - acc: 0.7182 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6141  | total loss: \u001b[1m\u001b[32m1.27894\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6141 | loss: 1.27894 - acc: 0.7464 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6142  | total loss: \u001b[1m\u001b[32m1.58581\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6142 | loss: 1.58581 - acc: 0.6717 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6143  | total loss: \u001b[1m\u001b[32m1.45428\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6143 | loss: 1.45428 - acc: 0.7046 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6144  | total loss: \u001b[1m\u001b[32m1.70040\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6144 | loss: 1.70040 - acc: 0.6412 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6145  | total loss: \u001b[1m\u001b[32m1.56022\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6145 | loss: 1.56022 - acc: 0.6771 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6146  | total loss: \u001b[1m\u001b[32m1.75120\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6146 | loss: 1.75120 - acc: 0.6237 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6147  | total loss: \u001b[1m\u001b[32m1.60949\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6147 | loss: 1.60949 - acc: 0.6613 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6148  | total loss: \u001b[1m\u001b[32m1.48361\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6148 | loss: 1.48361 - acc: 0.6952 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6149  | total loss: \u001b[1m\u001b[32m1.37145\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6149 | loss: 1.37145 - acc: 0.7257 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6150  | total loss: \u001b[1m\u001b[32m1.61133\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6150 | loss: 1.61133 - acc: 0.6531 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6151  | total loss: \u001b[1m\u001b[32m1.48861\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6151 | loss: 1.48861 - acc: 0.6878 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6152  | total loss: \u001b[1m\u001b[32m1.73719\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6152 | loss: 1.73719 - acc: 0.6190 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6153  | total loss: \u001b[1m\u001b[32m1.60479\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6153 | loss: 1.60479 - acc: 0.6571 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6154  | total loss: \u001b[1m\u001b[32m1.48691\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6154 | loss: 1.48691 - acc: 0.6914 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6155  | total loss: \u001b[1m\u001b[32m1.38150\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6155 | loss: 1.38150 - acc: 0.7223 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6156  | total loss: \u001b[1m\u001b[32m1.28676\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6156 | loss: 1.28676 - acc: 0.7500 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6157  | total loss: \u001b[1m\u001b[32m1.20110\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6157 | loss: 1.20110 - acc: 0.7750 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6158  | total loss: \u001b[1m\u001b[32m1.39283\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6158 | loss: 1.39283 - acc: 0.7118 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6159  | total loss: \u001b[1m\u001b[32m1.29573\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6159 | loss: 1.29573 - acc: 0.7406 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6160  | total loss: \u001b[1m\u001b[32m1.50115\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6160 | loss: 1.50115 - acc: 0.6809 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6161  | total loss: \u001b[1m\u001b[32m1.39297\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6161 | loss: 1.39297 - acc: 0.7128 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6162  | total loss: \u001b[1m\u001b[32m1.29536\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6162 | loss: 1.29536 - acc: 0.7415 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6163  | total loss: \u001b[1m\u001b[32m1.20684\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6163 | loss: 1.20684 - acc: 0.7673 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6164  | total loss: \u001b[1m\u001b[32m1.45688\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6164 | loss: 1.45688 - acc: 0.6906 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6165  | total loss: \u001b[1m\u001b[32m1.35126\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6165 | loss: 1.35126 - acc: 0.7215 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6166  | total loss: \u001b[1m\u001b[32m1.57445\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6166 | loss: 1.57445 - acc: 0.6494 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6167  | total loss: \u001b[1m\u001b[32m1.45738\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6167 | loss: 1.45738 - acc: 0.6845 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6168  | total loss: \u001b[1m\u001b[32m1.35218\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6168 | loss: 1.35218 - acc: 0.7160 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6169  | total loss: \u001b[1m\u001b[32m1.25722\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6169 | loss: 1.25722 - acc: 0.7444 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6170  | total loss: \u001b[1m\u001b[32m1.17108\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6170 | loss: 1.17108 - acc: 0.7700 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6171  | total loss: \u001b[1m\u001b[32m1.09252\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6171 | loss: 1.09252 - acc: 0.7930 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6172  | total loss: \u001b[1m\u001b[32m1.30575\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6172 | loss: 1.30575 - acc: 0.7280 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6173  | total loss: \u001b[1m\u001b[32m1.21193\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6173 | loss: 1.21193 - acc: 0.7552 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 6174  | total loss: \u001b[1m\u001b[32m1.40654\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6174 | loss: 1.40654 - acc: 0.6939 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6175  | total loss: \u001b[1m\u001b[32m1.30183\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6175 | loss: 1.30183 - acc: 0.7245 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6176  | total loss: \u001b[1m\u001b[32m1.55424\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6176 | loss: 1.55424 - acc: 0.6592 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6177  | total loss: \u001b[1m\u001b[32m1.43502\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6177 | loss: 1.43502 - acc: 0.6933 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6178  | total loss: \u001b[1m\u001b[32m1.32791\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6178 | loss: 1.32791 - acc: 0.7240 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6179  | total loss: \u001b[1m\u001b[32m1.23131\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6179 | loss: 1.23131 - acc: 0.7516 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6180  | total loss: \u001b[1m\u001b[32m1.45898\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6180 | loss: 1.45898 - acc: 0.6907 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6181  | total loss: \u001b[1m\u001b[32m1.34897\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6181 | loss: 1.34897 - acc: 0.7216 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6182  | total loss: \u001b[1m\u001b[32m1.59755\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6182 | loss: 1.59755 - acc: 0.6566 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6183  | total loss: \u001b[1m\u001b[32m1.47423\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6183 | loss: 1.47423 - acc: 0.6910 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6184  | total loss: \u001b[1m\u001b[32m1.66503\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6184 | loss: 1.66503 - acc: 0.6361 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6185  | total loss: \u001b[1m\u001b[32m1.53619\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6185 | loss: 1.53619 - acc: 0.6725 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6186  | total loss: \u001b[1m\u001b[32m1.72991\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6186 | loss: 1.72991 - acc: 0.6124 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6187  | total loss: \u001b[1m\u001b[32m1.59634\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6187 | loss: 1.59634 - acc: 0.6512 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6188  | total loss: \u001b[1m\u001b[32m1.47689\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6188 | loss: 1.47689 - acc: 0.6861 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6189  | total loss: \u001b[1m\u001b[32m1.36962\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6189 | loss: 1.36962 - acc: 0.7175 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6190  | total loss: \u001b[1m\u001b[32m1.27288\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6190 | loss: 1.27288 - acc: 0.7457 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6191  | total loss: \u001b[1m\u001b[32m1.18518\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6191 | loss: 1.18518 - acc: 0.7711 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6192  | total loss: \u001b[1m\u001b[32m1.46142\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6192 | loss: 1.46142 - acc: 0.6940 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6193  | total loss: \u001b[1m\u001b[32m1.35401\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6193 | loss: 1.35401 - acc: 0.7246 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6194  | total loss: \u001b[1m\u001b[32m1.56443\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6194 | loss: 1.56443 - acc: 0.6593 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6195  | total loss: \u001b[1m\u001b[32m1.44695\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6195 | loss: 1.44695 - acc: 0.6934 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6196  | total loss: \u001b[1m\u001b[32m1.66161\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6196 | loss: 1.66161 - acc: 0.6312 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6197  | total loss: \u001b[1m\u001b[32m1.53540\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6197 | loss: 1.53540 - acc: 0.6681 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6198  | total loss: \u001b[1m\u001b[32m1.72141\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6198 | loss: 1.72141 - acc: 0.6084 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6199  | total loss: \u001b[1m\u001b[32m1.59078\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6199 | loss: 1.59078 - acc: 0.6476 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6200  | total loss: \u001b[1m\u001b[32m1.80735\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6200 | loss: 1.80735 - acc: 0.5828 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6201  | total loss: \u001b[1m\u001b[32m1.67026\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6201 | loss: 1.67026 - acc: 0.6245 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6202  | total loss: \u001b[1m\u001b[32m1.84723\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6202 | loss: 1.84723 - acc: 0.5692 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6203  | total loss: \u001b[1m\u001b[32m1.70870\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6203 | loss: 1.70870 - acc: 0.6123 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6204  | total loss: \u001b[1m\u001b[32m1.84058\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6204 | loss: 1.84058 - acc: 0.5653 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6205  | total loss: \u001b[1m\u001b[32m1.70529\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6205 | loss: 1.70529 - acc: 0.6088 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6206  | total loss: \u001b[1m\u001b[32m1.58443\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6206 | loss: 1.58443 - acc: 0.6479 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6207  | total loss: \u001b[1m\u001b[32m1.47592\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6207 | loss: 1.47592 - acc: 0.6831 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6208  | total loss: \u001b[1m\u001b[32m1.37793\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6208 | loss: 1.37793 - acc: 0.7148 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6209  | total loss: \u001b[1m\u001b[32m1.28888\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6209 | loss: 1.28888 - acc: 0.7433 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6210  | total loss: \u001b[1m\u001b[32m1.52074\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6210 | loss: 1.52074 - acc: 0.6762 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6211  | total loss: \u001b[1m\u001b[32m1.41575\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6211 | loss: 1.41575 - acc: 0.7085 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6212  | total loss: \u001b[1m\u001b[32m1.32043\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6212 | loss: 1.32043 - acc: 0.7377 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6213  | total loss: \u001b[1m\u001b[32m1.23339\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6213 | loss: 1.23339 - acc: 0.7639 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6214  | total loss: \u001b[1m\u001b[32m1.43988\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6214 | loss: 1.43988 - acc: 0.7018 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6215  | total loss: \u001b[1m\u001b[32m1.33853\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6215 | loss: 1.33853 - acc: 0.7316 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6216  | total loss: \u001b[1m\u001b[32m1.53655\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6216 | loss: 1.53655 - acc: 0.6656 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6217  | total loss: \u001b[1m\u001b[32m1.42427\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6217 | loss: 1.42427 - acc: 0.6990 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6218  | total loss: \u001b[1m\u001b[32m1.64467\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6218 | loss: 1.64467 - acc: 0.6363 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6219  | total loss: \u001b[1m\u001b[32m1.52135\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6219 | loss: 1.52135 - acc: 0.6727 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6220  | total loss: \u001b[1m\u001b[32m1.71391\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6220 | loss: 1.71391 - acc: 0.6125 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6221  | total loss: \u001b[1m\u001b[32m1.58418\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6221 | loss: 1.58418 - acc: 0.6513 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6222  | total loss: \u001b[1m\u001b[32m1.79666\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6222 | loss: 1.79666 - acc: 0.5862 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6223  | total loss: \u001b[1m\u001b[32m1.65981\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6223 | loss: 1.65981 - acc: 0.6275 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6224  | total loss: \u001b[1m\u001b[32m1.53713\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6224 | loss: 1.53713 - acc: 0.6648 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6225  | total loss: \u001b[1m\u001b[32m1.42670\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6225 | loss: 1.42670 - acc: 0.6983 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6226  | total loss: \u001b[1m\u001b[32m1.60637\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6226 | loss: 1.60637 - acc: 0.6428 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6227  | total loss: \u001b[1m\u001b[32m1.48886\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6227 | loss: 1.48886 - acc: 0.6785 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6228  | total loss: \u001b[1m\u001b[32m1.38294\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6228 | loss: 1.38294 - acc: 0.7106 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6229  | total loss: \u001b[1m\u001b[32m1.28702\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6229 | loss: 1.28702 - acc: 0.7396 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6230  | total loss: \u001b[1m\u001b[32m1.48941\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6230 | loss: 1.48941 - acc: 0.6728 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 6231  | total loss: \u001b[1m\u001b[32m1.38182\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6231 | loss: 1.38182 - acc: 0.7055 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6232  | total loss: \u001b[1m\u001b[32m1.28451\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6232 | loss: 1.28451 - acc: 0.7349 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6233  | total loss: \u001b[1m\u001b[32m1.19607\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6233 | loss: 1.19607 - acc: 0.7614 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6234  | total loss: \u001b[1m\u001b[32m1.36256\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6234 | loss: 1.36256 - acc: 0.7067 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6235  | total loss: \u001b[1m\u001b[32m1.26473\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6235 | loss: 1.26473 - acc: 0.7361 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6236  | total loss: \u001b[1m\u001b[32m1.51882\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6236 | loss: 1.51882 - acc: 0.6624 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6237  | total loss: \u001b[1m\u001b[32m1.40475\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6237 | loss: 1.40475 - acc: 0.6962 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6238  | total loss: \u001b[1m\u001b[32m1.66512\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6238 | loss: 1.66512 - acc: 0.6266 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6239  | total loss: \u001b[1m\u001b[32m1.53683\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6239 | loss: 1.53683 - acc: 0.6639 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6240  | total loss: \u001b[1m\u001b[32m1.74385\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6240 | loss: 1.74385 - acc: 0.6047 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6241  | total loss: \u001b[1m\u001b[32m1.60901\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6241 | loss: 1.60901 - acc: 0.6442 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6242  | total loss: \u001b[1m\u001b[32m1.78737\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6242 | loss: 1.78737 - acc: 0.5941 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6243  | total loss: \u001b[1m\u001b[32m1.65019\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6243 | loss: 1.65019 - acc: 0.6347 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6244  | total loss: \u001b[1m\u001b[32m1.81433\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6244 | loss: 1.81433 - acc: 0.5855 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6245  | total loss: \u001b[1m\u001b[32m1.67691\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6245 | loss: 1.67691 - acc: 0.6269 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6246  | total loss: \u001b[1m\u001b[32m1.55422\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6246 | loss: 1.55422 - acc: 0.6642 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6247  | total loss: \u001b[1m\u001b[32m1.44419\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6247 | loss: 1.44419 - acc: 0.6978 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6248  | total loss: \u001b[1m\u001b[32m1.66759\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6248 | loss: 1.66759 - acc: 0.6280 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6249  | total loss: \u001b[1m\u001b[32m1.54683\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6249 | loss: 1.54683 - acc: 0.6652 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6250  | total loss: \u001b[1m\u001b[32m1.72383\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6250 | loss: 1.72383 - acc: 0.6130 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6251  | total loss: \u001b[1m\u001b[32m1.59849\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6251 | loss: 1.59849 - acc: 0.6517 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6252  | total loss: \u001b[1m\u001b[32m1.48593\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6252 | loss: 1.48593 - acc: 0.6865 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6253  | total loss: \u001b[1m\u001b[32m1.38431\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6253 | loss: 1.38431 - acc: 0.7179 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6254  | total loss: \u001b[1m\u001b[32m1.57354\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6254 | loss: 1.57354 - acc: 0.6532 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6255  | total loss: \u001b[1m\u001b[32m1.46240\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6255 | loss: 1.46240 - acc: 0.6879 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6256  | total loss: \u001b[1m\u001b[32m1.63089\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6256 | loss: 1.63089 - acc: 0.6334 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6257  | total loss: \u001b[1m\u001b[32m1.51380\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6257 | loss: 1.51380 - acc: 0.6701 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6258  | total loss: \u001b[1m\u001b[32m1.70182\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6258 | loss: 1.70182 - acc: 0.6102 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6259  | total loss: \u001b[1m\u001b[32m1.57796\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6259 | loss: 1.57796 - acc: 0.6492 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6260  | total loss: \u001b[1m\u001b[32m1.46653\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6260 | loss: 1.46653 - acc: 0.6843 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6261  | total loss: \u001b[1m\u001b[32m1.36575\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6261 | loss: 1.36575 - acc: 0.7158 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6262  | total loss: \u001b[1m\u001b[32m1.58137\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6262 | loss: 1.58137 - acc: 0.6514 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6263  | total loss: \u001b[1m\u001b[32m1.46810\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6263 | loss: 1.46810 - acc: 0.6863 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6264  | total loss: \u001b[1m\u001b[32m1.67625\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6264 | loss: 1.67625 - acc: 0.6176 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6265  | total loss: \u001b[1m\u001b[32m1.55339\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6265 | loss: 1.55339 - acc: 0.6559 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6266  | total loss: \u001b[1m\u001b[32m1.75132\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6266 | loss: 1.75132 - acc: 0.5974 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6267  | total loss: \u001b[1m\u001b[32m1.62157\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6267 | loss: 1.62157 - acc: 0.6377 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6268  | total loss: \u001b[1m\u001b[32m1.76273\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6268 | loss: 1.76273 - acc: 0.5882 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6269  | total loss: \u001b[1m\u001b[32m1.63292\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6269 | loss: 1.63292 - acc: 0.6294 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6270  | total loss: \u001b[1m\u001b[32m1.84814\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6270 | loss: 1.84814 - acc: 0.5664 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6271  | total loss: \u001b[1m\u001b[32m1.71142\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6271 | loss: 1.71142 - acc: 0.6098 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6272  | total loss: \u001b[1m\u001b[32m1.81850\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6272 | loss: 1.81850 - acc: 0.5702 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6273  | total loss: \u001b[1m\u001b[32m1.68659\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6273 | loss: 1.68659 - acc: 0.6132 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6274  | total loss: \u001b[1m\u001b[32m1.86250\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6274 | loss: 1.86250 - acc: 0.5519 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6275  | total loss: \u001b[1m\u001b[32m1.72824\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6275 | loss: 1.72824 - acc: 0.5967 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6276  | total loss: \u001b[1m\u001b[32m1.84773\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6276 | loss: 1.84773 - acc: 0.5585 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6277  | total loss: \u001b[1m\u001b[32m1.71707\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6277 | loss: 1.71707 - acc: 0.6026 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6278  | total loss: \u001b[1m\u001b[32m1.87344\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6278 | loss: 1.87344 - acc: 0.5566 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6279  | total loss: \u001b[1m\u001b[32m1.74211\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6279 | loss: 1.74211 - acc: 0.6010 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6280  | total loss: \u001b[1m\u001b[32m1.87567\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6280 | loss: 1.87567 - acc: 0.5552 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6281  | total loss: \u001b[1m\u001b[32m1.74581\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6281 | loss: 1.74581 - acc: 0.5997 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6282  | total loss: \u001b[1m\u001b[32m1.89380\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6282 | loss: 1.89380 - acc: 0.5540 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6283  | total loss: \u001b[1m\u001b[32m1.76345\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6283 | loss: 1.76345 - acc: 0.5986 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6284  | total loss: \u001b[1m\u001b[32m1.89587\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6284 | loss: 1.89587 - acc: 0.5530 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6285  | total loss: \u001b[1m\u001b[32m1.76634\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6285 | loss: 1.76634 - acc: 0.5977 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6286  | total loss: \u001b[1m\u001b[32m1.88542\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6286 | loss: 1.88542 - acc: 0.5522 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6287  | total loss: \u001b[1m\u001b[32m1.75778\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6287 | loss: 1.75778 - acc: 0.5970 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 6288  | total loss: \u001b[1m\u001b[32m1.92060\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6288 | loss: 1.92060 - acc: 0.5373 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6289  | total loss: \u001b[1m\u001b[32m1.79035\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6289 | loss: 1.79035 - acc: 0.5836 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6290  | total loss: \u001b[1m\u001b[32m1.93876\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6290 | loss: 1.93876 - acc: 0.5252 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6291  | total loss: \u001b[1m\u001b[32m1.80775\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6291 | loss: 1.80775 - acc: 0.5727 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6292  | total loss: \u001b[1m\u001b[32m1.93597\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6292 | loss: 1.93597 - acc: 0.5297 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6293  | total loss: \u001b[1m\u001b[32m1.80611\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6293 | loss: 1.80611 - acc: 0.5767 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6294  | total loss: \u001b[1m\u001b[32m1.96337\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6294 | loss: 1.96337 - acc: 0.5191 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6295  | total loss: \u001b[1m\u001b[32m1.83146\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6295 | loss: 1.83146 - acc: 0.5672 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6296  | total loss: \u001b[1m\u001b[32m1.97241\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6296 | loss: 1.97241 - acc: 0.5176 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6297  | total loss: \u001b[1m\u001b[32m1.84023\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6297 | loss: 1.84023 - acc: 0.5658 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6298  | total loss: \u001b[1m\u001b[32m1.99040\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6298 | loss: 1.99040 - acc: 0.5164 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6299  | total loss: \u001b[1m\u001b[32m1.85696\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6299 | loss: 1.85696 - acc: 0.5647 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6300  | total loss: \u001b[1m\u001b[32m2.00466\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6300 | loss: 2.00466 - acc: 0.5154 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6301  | total loss: \u001b[1m\u001b[32m1.87023\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6301 | loss: 1.87023 - acc: 0.5639 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6302  | total loss: \u001b[1m\u001b[32m2.01587\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6302 | loss: 2.01587 - acc: 0.5075 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6303  | total loss: \u001b[1m\u001b[32m1.88079\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6303 | loss: 1.88079 - acc: 0.5567 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6304  | total loss: \u001b[1m\u001b[32m1.75905\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6304 | loss: 1.75905 - acc: 0.6011 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6305  | total loss: \u001b[1m\u001b[32m1.64857\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6305 | loss: 1.64857 - acc: 0.6410 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6306  | total loss: \u001b[1m\u001b[32m1.83275\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6306 | loss: 1.83275 - acc: 0.5769 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6307  | total loss: \u001b[1m\u001b[32m1.71289\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6307 | loss: 1.71289 - acc: 0.6192 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6308  | total loss: \u001b[1m\u001b[32m1.88284\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6308 | loss: 1.88284 - acc: 0.5573 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6309  | total loss: \u001b[1m\u001b[32m1.75683\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6309 | loss: 1.75683 - acc: 0.6015 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6310  | total loss: \u001b[1m\u001b[32m1.89391\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6310 | loss: 1.89391 - acc: 0.5485 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6311  | total loss: \u001b[1m\u001b[32m1.76627\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6311 | loss: 1.76627 - acc: 0.5937 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6312  | total loss: \u001b[1m\u001b[32m1.89575\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6312 | loss: 1.89575 - acc: 0.5486 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6313  | total loss: \u001b[1m\u001b[32m1.76749\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6313 | loss: 1.76749 - acc: 0.5937 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6314  | total loss: \u001b[1m\u001b[32m1.94387\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6314 | loss: 1.94387 - acc: 0.5344 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6315  | total loss: \u001b[1m\u001b[32m1.81057\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6315 | loss: 1.81057 - acc: 0.5809 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6316  | total loss: \u001b[1m\u001b[32m1.97577\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6316 | loss: 1.97577 - acc: 0.5228 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6317  | total loss: \u001b[1m\u001b[32m1.83944\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6317 | loss: 1.83944 - acc: 0.5705 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6318  | total loss: \u001b[1m\u001b[32m1.99252\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6318 | loss: 1.99252 - acc: 0.5135 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6319  | total loss: \u001b[1m\u001b[32m1.85495\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6319 | loss: 1.85495 - acc: 0.5621 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6320  | total loss: \u001b[1m\u001b[32m2.01074\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6320 | loss: 2.01074 - acc: 0.5059 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6321  | total loss: \u001b[1m\u001b[32m1.87199\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6321 | loss: 1.87199 - acc: 0.5553 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6322  | total loss: \u001b[1m\u001b[32m1.97518\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6322 | loss: 1.97518 - acc: 0.5141 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6323  | total loss: \u001b[1m\u001b[32m1.84057\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6323 | loss: 1.84057 - acc: 0.5627 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6324  | total loss: \u001b[1m\u001b[32m1.93017\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6324 | loss: 1.93017 - acc: 0.5278 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6325  | total loss: \u001b[1m\u001b[32m1.80035\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6325 | loss: 1.80035 - acc: 0.5751 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6326  | total loss: \u001b[1m\u001b[32m1.68322\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6326 | loss: 1.68322 - acc: 0.6175 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6327  | total loss: \u001b[1m\u001b[32m1.57682\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6327 | loss: 1.57682 - acc: 0.6558 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6328  | total loss: \u001b[1m\u001b[32m1.73228\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6328 | loss: 1.73228 - acc: 0.5974 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6329  | total loss: \u001b[1m\u001b[32m1.61863\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6329 | loss: 1.61863 - acc: 0.6376 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6330  | total loss: \u001b[1m\u001b[32m1.78686\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6330 | loss: 1.78686 - acc: 0.5810 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6331  | total loss: \u001b[1m\u001b[32m1.66601\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6331 | loss: 1.66601 - acc: 0.6229 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6332  | total loss: \u001b[1m\u001b[32m1.81620\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 6332 | loss: 1.81620 - acc: 0.5749 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6333  | total loss: \u001b[1m\u001b[32m1.69118\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6333 | loss: 1.69118 - acc: 0.6174 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6334  | total loss: \u001b[1m\u001b[32m1.86081\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6334 | loss: 1.86081 - acc: 0.5557 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6335  | total loss: \u001b[1m\u001b[32m1.73070\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6335 | loss: 1.73070 - acc: 0.6001 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6336  | total loss: \u001b[1m\u001b[32m1.88995\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6336 | loss: 1.88995 - acc: 0.5472 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6337  | total loss: \u001b[1m\u001b[32m1.75696\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6337 | loss: 1.75696 - acc: 0.5925 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6338  | total loss: \u001b[1m\u001b[32m1.90569\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6338 | loss: 1.90569 - acc: 0.5404 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6339  | total loss: \u001b[1m\u001b[32m1.77150\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6339 | loss: 1.77150 - acc: 0.5864 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6340  | total loss: \u001b[1m\u001b[32m1.95270\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6340 | loss: 1.95270 - acc: 0.5277 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6341  | total loss: \u001b[1m\u001b[32m1.81458\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6341 | loss: 1.81458 - acc: 0.5750 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6342  | total loss: \u001b[1m\u001b[32m1.92551\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6342 | loss: 1.92551 - acc: 0.5317 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6343  | total loss: \u001b[1m\u001b[32m1.79113\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6343 | loss: 1.79113 - acc: 0.5786 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6344  | total loss: \u001b[1m\u001b[32m1.94061\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6344 | loss: 1.94061 - acc: 0.5279 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 6345  | total loss: \u001b[1m\u001b[32m1.80581\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6345 | loss: 1.80581 - acc: 0.5751 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6346  | total loss: \u001b[1m\u001b[32m1.96568\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 6346 | loss: 1.96568 - acc: 0.5247 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6347  | total loss: \u001b[1m\u001b[32m1.82950\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 6347 | loss: 1.82950 - acc: 0.5722 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6348  | total loss: \u001b[1m\u001b[32m1.97281\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6348 | loss: 1.97281 - acc: 0.5222 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6349  | total loss: \u001b[1m\u001b[32m1.83692\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6349 | loss: 1.83692 - acc: 0.5699 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6350  | total loss: \u001b[1m\u001b[32m1.97533\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6350 | loss: 1.97533 - acc: 0.5272 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6351  | total loss: \u001b[1m\u001b[32m1.84000\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 6351 | loss: 1.84000 - acc: 0.5745 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6352  | total loss: \u001b[1m\u001b[32m2.01098\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6352 | loss: 2.01098 - acc: 0.5171 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6353  | total loss: \u001b[1m\u001b[32m1.87290\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6353 | loss: 1.87290 - acc: 0.5654 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6354  | total loss: \u001b[1m\u001b[32m2.03113\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6354 | loss: 2.03113 - acc: 0.5088 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6355  | total loss: \u001b[1m\u001b[32m1.89196\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6355 | loss: 1.89196 - acc: 0.5579 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6356  | total loss: \u001b[1m\u001b[32m1.76679\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6356 | loss: 1.76679 - acc: 0.6021 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6357  | total loss: \u001b[1m\u001b[32m1.65349\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6357 | loss: 1.65349 - acc: 0.6419 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6358  | total loss: \u001b[1m\u001b[32m1.55023\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6358 | loss: 1.55023 - acc: 0.6777 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6359  | total loss: \u001b[1m\u001b[32m1.45546\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 6359 | loss: 1.45546 - acc: 0.7100 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6360  | total loss: \u001b[1m\u001b[32m1.66027\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6360 | loss: 1.66027 - acc: 0.6390 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6361  | total loss: \u001b[1m\u001b[32m1.55098\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6361 | loss: 1.55098 - acc: 0.6751 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6362  | total loss: \u001b[1m\u001b[32m1.68998\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6362 | loss: 1.68998 - acc: 0.6218 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6363  | total loss: \u001b[1m\u001b[32m1.57512\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6363 | loss: 1.57512 - acc: 0.6597 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6364  | total loss: \u001b[1m\u001b[32m1.78093\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6364 | loss: 1.78093 - acc: 0.5937 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6365  | total loss: \u001b[1m\u001b[32m1.65524\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6365 | loss: 1.65524 - acc: 0.6343 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6366  | total loss: \u001b[1m\u001b[32m1.84845\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6366 | loss: 1.84845 - acc: 0.5709 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6367  | total loss: \u001b[1m\u001b[32m1.71530\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6367 | loss: 1.71530 - acc: 0.6138 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6368  | total loss: \u001b[1m\u001b[32m1.89394\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6368 | loss: 1.89394 - acc: 0.5524 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6369  | total loss: \u001b[1m\u001b[32m1.75639\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6369 | loss: 1.75639 - acc: 0.5972 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6370  | total loss: \u001b[1m\u001b[32m1.63254\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6370 | loss: 1.63254 - acc: 0.6375 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6371  | total loss: \u001b[1m\u001b[32m1.52048\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6371 | loss: 1.52048 - acc: 0.6737 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6372  | total loss: \u001b[1m\u001b[32m1.67268\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6372 | loss: 1.67268 - acc: 0.6278 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6373  | total loss: \u001b[1m\u001b[32m1.55520\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6373 | loss: 1.55520 - acc: 0.6650 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6374  | total loss: \u001b[1m\u001b[32m1.73226\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6374 | loss: 1.73226 - acc: 0.6128 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6375  | total loss: \u001b[1m\u001b[32m1.60798\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6375 | loss: 1.60798 - acc: 0.6515 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6376  | total loss: \u001b[1m\u001b[32m1.77164\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6376 | loss: 1.77164 - acc: 0.5935 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6377  | total loss: \u001b[1m\u001b[32m1.64321\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6377 | loss: 1.64321 - acc: 0.6341 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6378  | total loss: \u001b[1m\u001b[32m1.81561\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6378 | loss: 1.81561 - acc: 0.5779 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6379  | total loss: \u001b[1m\u001b[32m1.68324\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6379 | loss: 1.68324 - acc: 0.6201 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6380  | total loss: \u001b[1m\u001b[32m1.56419\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6380 | loss: 1.56419 - acc: 0.6581 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6381  | total loss: \u001b[1m\u001b[32m1.45660\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6381 | loss: 1.45660 - acc: 0.6923 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6382  | total loss: \u001b[1m\u001b[32m1.65049\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6382 | loss: 1.65049 - acc: 0.6230 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6383  | total loss: \u001b[1m\u001b[32m1.53327\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6383 | loss: 1.53327 - acc: 0.6607 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6384  | total loss: \u001b[1m\u001b[32m1.70269\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6384 | loss: 1.70269 - acc: 0.6090 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6385  | total loss: \u001b[1m\u001b[32m1.57994\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6385 | loss: 1.57994 - acc: 0.6481 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6386  | total loss: \u001b[1m\u001b[32m1.74794\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6386 | loss: 1.74794 - acc: 0.5975 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6387  | total loss: \u001b[1m\u001b[32m1.62084\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6387 | loss: 1.62084 - acc: 0.6378 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6388  | total loss: \u001b[1m\u001b[32m1.83085\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6388 | loss: 1.83085 - acc: 0.5740 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6389  | total loss: \u001b[1m\u001b[32m1.69616\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6389 | loss: 1.69616 - acc: 0.6166 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6390  | total loss: \u001b[1m\u001b[32m1.90743\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6390 | loss: 1.90743 - acc: 0.5549 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6391  | total loss: \u001b[1m\u001b[32m1.76633\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6391 | loss: 1.76633 - acc: 0.5994 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6392  | total loss: \u001b[1m\u001b[32m1.89411\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6392 | loss: 1.89411 - acc: 0.5538 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6393  | total loss: \u001b[1m\u001b[32m1.75586\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6393 | loss: 1.75586 - acc: 0.5984 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6394  | total loss: \u001b[1m\u001b[32m1.88774\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6394 | loss: 1.88774 - acc: 0.5529 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6395  | total loss: \u001b[1m\u001b[32m1.75166\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6395 | loss: 1.75166 - acc: 0.5976 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6396  | total loss: \u001b[1m\u001b[32m1.88616\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 6396 | loss: 1.88616 - acc: 0.5450 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6397  | total loss: \u001b[1m\u001b[32m1.75181\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6397 | loss: 1.75181 - acc: 0.5905 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6398  | total loss: \u001b[1m\u001b[32m1.63133\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6398 | loss: 1.63133 - acc: 0.6314 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6399  | total loss: \u001b[1m\u001b[32m1.52267\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6399 | loss: 1.52267 - acc: 0.6683 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6400  | total loss: \u001b[1m\u001b[32m1.42404\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6400 | loss: 1.42404 - acc: 0.7014 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6401  | total loss: \u001b[1m\u001b[32m1.33394\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6401 | loss: 1.33394 - acc: 0.7313 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 6402  | total loss: \u001b[1m\u001b[32m1.25106\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6402 | loss: 1.25106 - acc: 0.7582 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6403  | total loss: \u001b[1m\u001b[32m1.17431\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6403 | loss: 1.17431 - acc: 0.7824 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6404  | total loss: \u001b[1m\u001b[32m1.10281\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6404 | loss: 1.10281 - acc: 0.8041 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6405  | total loss: \u001b[1m\u001b[32m1.03581\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6405 | loss: 1.03581 - acc: 0.8237 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6406  | total loss: \u001b[1m\u001b[32m1.27824\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6406 | loss: 1.27824 - acc: 0.7485 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6407  | total loss: \u001b[1m\u001b[32m1.18934\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6407 | loss: 1.18934 - acc: 0.7736 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6408  | total loss: \u001b[1m\u001b[32m1.10751\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6408 | loss: 1.10751 - acc: 0.7963 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6409  | total loss: \u001b[1m\u001b[32m1.03190\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6409 | loss: 1.03190 - acc: 0.8166 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6410  | total loss: \u001b[1m\u001b[32m0.96178\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6410 | loss: 0.96178 - acc: 0.8350 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6411  | total loss: \u001b[1m\u001b[32m0.89657\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6411 | loss: 0.89657 - acc: 0.8515 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6412  | total loss: \u001b[1m\u001b[32m0.83575\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6412 | loss: 0.83575 - acc: 0.8663 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6413  | total loss: \u001b[1m\u001b[32m0.77893\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6413 | loss: 0.77893 - acc: 0.8797 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6414  | total loss: \u001b[1m\u001b[32m1.08968\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6414 | loss: 1.08968 - acc: 0.7989 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6415  | total loss: \u001b[1m\u001b[32m1.00452\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6415 | loss: 1.00452 - acc: 0.8190 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6416  | total loss: \u001b[1m\u001b[32m0.92687\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6416 | loss: 0.92687 - acc: 0.8371 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6417  | total loss: \u001b[1m\u001b[32m0.85592\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6417 | loss: 0.85592 - acc: 0.8534 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6418  | total loss: \u001b[1m\u001b[32m1.16243\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6418 | loss: 1.16243 - acc: 0.7823 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6419  | total loss: \u001b[1m\u001b[32m1.06657\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6419 | loss: 1.06657 - acc: 0.8041 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6420  | total loss: \u001b[1m\u001b[32m1.40963\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6420 | loss: 1.40963 - acc: 0.7237 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6421  | total loss: \u001b[1m\u001b[32m1.28912\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6421 | loss: 1.28912 - acc: 0.7513 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6422  | total loss: \u001b[1m\u001b[32m1.18090\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6422 | loss: 1.18090 - acc: 0.7762 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6423  | total loss: \u001b[1m\u001b[32m1.08355\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6423 | loss: 1.08355 - acc: 0.7986 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6424  | total loss: \u001b[1m\u001b[32m1.38221\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6424 | loss: 1.38221 - acc: 0.7259 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6425  | total loss: \u001b[1m\u001b[32m1.26520\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6425 | loss: 1.26520 - acc: 0.7533 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6426  | total loss: \u001b[1m\u001b[32m1.56809\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6426 | loss: 1.56809 - acc: 0.6779 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6427  | total loss: \u001b[1m\u001b[32m1.43408\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6427 | loss: 1.43408 - acc: 0.7101 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6428  | total loss: \u001b[1m\u001b[32m1.73062\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6428 | loss: 1.73062 - acc: 0.6391 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6429  | total loss: \u001b[1m\u001b[32m1.58305\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6429 | loss: 1.58305 - acc: 0.6752 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6430  | total loss: \u001b[1m\u001b[32m1.45166\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6430 | loss: 1.45166 - acc: 0.7077 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6431  | total loss: \u001b[1m\u001b[32m1.33448\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6431 | loss: 1.33448 - acc: 0.7369 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6432  | total loss: \u001b[1m\u001b[32m1.51743\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6432 | loss: 1.51743 - acc: 0.6847 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6433  | total loss: \u001b[1m\u001b[32m1.39564\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6433 | loss: 1.39564 - acc: 0.7162 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6434  | total loss: \u001b[1m\u001b[32m1.60268\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6434 | loss: 1.60268 - acc: 0.6589 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6435  | total loss: \u001b[1m\u001b[32m1.47474\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6435 | loss: 1.47474 - acc: 0.6930 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6436  | total loss: \u001b[1m\u001b[32m1.73127\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6436 | loss: 1.73127 - acc: 0.6237 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6437  | total loss: \u001b[1m\u001b[32m1.59351\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6437 | loss: 1.59351 - acc: 0.6613 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6438  | total loss: \u001b[1m\u001b[32m1.81344\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6438 | loss: 1.81344 - acc: 0.5952 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6439  | total loss: \u001b[1m\u001b[32m1.67115\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6439 | loss: 1.67115 - acc: 0.6357 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6440  | total loss: \u001b[1m\u001b[32m1.86998\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6440 | loss: 1.86998 - acc: 0.5792 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6441  | total loss: \u001b[1m\u001b[32m1.72610\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6441 | loss: 1.72610 - acc: 0.6213 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6442  | total loss: \u001b[1m\u001b[32m1.59835\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6442 | loss: 1.59835 - acc: 0.6592 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6443  | total loss: \u001b[1m\u001b[32m1.48446\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6443 | loss: 1.48446 - acc: 0.6933 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6444  | total loss: \u001b[1m\u001b[32m1.68799\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6444 | loss: 1.68799 - acc: 0.6311 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6445  | total loss: \u001b[1m\u001b[32m1.56685\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6445 | loss: 1.56685 - acc: 0.6680 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6446  | total loss: \u001b[1m\u001b[32m1.75919\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6446 | loss: 1.75919 - acc: 0.6012 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6447  | total loss: \u001b[1m\u001b[32m1.63297\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6447 | loss: 1.63297 - acc: 0.6411 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6448  | total loss: \u001b[1m\u001b[32m1.75632\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6448 | loss: 1.75632 - acc: 0.5912 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6449  | total loss: \u001b[1m\u001b[32m1.63246\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6449 | loss: 1.63246 - acc: 0.6321 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6450  | total loss: \u001b[1m\u001b[32m1.78517\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6450 | loss: 1.78517 - acc: 0.5760 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6451  | total loss: \u001b[1m\u001b[32m1.66040\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6451 | loss: 1.66040 - acc: 0.6184 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6452  | total loss: \u001b[1m\u001b[32m1.80557\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6452 | loss: 1.80557 - acc: 0.5637 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6453  | total loss: \u001b[1m\u001b[32m1.68066\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6453 | loss: 1.68066 - acc: 0.6074 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6454  | total loss: \u001b[1m\u001b[32m1.85872\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6454 | loss: 1.85872 - acc: 0.5466 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6455  | total loss: \u001b[1m\u001b[32m1.73039\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6455 | loss: 1.73039 - acc: 0.5920 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6456  | total loss: \u001b[1m\u001b[32m1.89642\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6456 | loss: 1.89642 - acc: 0.5328 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6457  | total loss: \u001b[1m\u001b[32m1.76632\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6457 | loss: 1.76632 - acc: 0.5795 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6458  | total loss: \u001b[1m\u001b[32m1.85901\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6458 | loss: 1.85901 - acc: 0.5430 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 6459  | total loss: \u001b[1m\u001b[32m1.73429\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6459 | loss: 1.73429 - acc: 0.5887 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6460  | total loss: \u001b[1m\u001b[32m1.92334\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6460 | loss: 1.92334 - acc: 0.5298 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6461  | total loss: \u001b[1m\u001b[32m1.79353\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6461 | loss: 1.79353 - acc: 0.5768 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6462  | total loss: \u001b[1m\u001b[32m1.93590\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6462 | loss: 1.93590 - acc: 0.5334 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6463  | total loss: \u001b[1m\u001b[32m1.80595\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6463 | loss: 1.80595 - acc: 0.5801 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6464  | total loss: \u001b[1m\u001b[32m1.68901\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6464 | loss: 1.68901 - acc: 0.6221 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6465  | total loss: \u001b[1m\u001b[32m1.58306\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6465 | loss: 1.58306 - acc: 0.6599 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6466  | total loss: \u001b[1m\u001b[32m1.48635\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6466 | loss: 1.48635 - acc: 0.6939 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6467  | total loss: \u001b[1m\u001b[32m1.39743\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6467 | loss: 1.39743 - acc: 0.7245 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6468  | total loss: \u001b[1m\u001b[32m1.31506\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6468 | loss: 1.31506 - acc: 0.7520 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6469  | total loss: \u001b[1m\u001b[32m1.23825\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6469 | loss: 1.23825 - acc: 0.7768 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6470  | total loss: \u001b[1m\u001b[32m1.42977\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6470 | loss: 1.42977 - acc: 0.7134 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6471  | total loss: \u001b[1m\u001b[32m1.33669\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6471 | loss: 1.33669 - acc: 0.7421 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6472  | total loss: \u001b[1m\u001b[32m1.55019\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6472 | loss: 1.55019 - acc: 0.6750 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6473  | total loss: \u001b[1m\u001b[32m1.44178\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6473 | loss: 1.44178 - acc: 0.7075 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6474  | total loss: \u001b[1m\u001b[32m1.64527\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6474 | loss: 1.64527 - acc: 0.6439 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6475  | total loss: \u001b[1m\u001b[32m1.52529\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6475 | loss: 1.52529 - acc: 0.6795 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6476  | total loss: \u001b[1m\u001b[32m1.41635\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6476 | loss: 1.41635 - acc: 0.7116 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6477  | total loss: \u001b[1m\u001b[32m1.31701\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6477 | loss: 1.31701 - acc: 0.7404 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6478  | total loss: \u001b[1m\u001b[32m1.22602\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6478 | loss: 1.22602 - acc: 0.7664 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6479  | total loss: \u001b[1m\u001b[32m1.14235\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6479 | loss: 1.14235 - acc: 0.7897 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6480  | total loss: \u001b[1m\u001b[32m1.40931\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6480 | loss: 1.40931 - acc: 0.7108 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6481  | total loss: \u001b[1m\u001b[32m1.30451\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6481 | loss: 1.30451 - acc: 0.7397 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6482  | total loss: \u001b[1m\u001b[32m1.58004\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6482 | loss: 1.58004 - acc: 0.6657 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6483  | total loss: \u001b[1m\u001b[32m1.45697\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6483 | loss: 1.45697 - acc: 0.6991 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6484  | total loss: \u001b[1m\u001b[32m1.69016\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6484 | loss: 1.69016 - acc: 0.6292 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6485  | total loss: \u001b[1m\u001b[32m1.55614\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6485 | loss: 1.55614 - acc: 0.6663 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6486  | total loss: \u001b[1m\u001b[32m1.73723\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6486 | loss: 1.73723 - acc: 0.6140 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6487  | total loss: \u001b[1m\u001b[32m1.59944\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6487 | loss: 1.59944 - acc: 0.6526 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6488  | total loss: \u001b[1m\u001b[32m1.82299\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6488 | loss: 1.82299 - acc: 0.5873 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6489  | total loss: \u001b[1m\u001b[32m1.67836\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6489 | loss: 1.67836 - acc: 0.6286 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6490  | total loss: \u001b[1m\u001b[32m1.87812\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6490 | loss: 1.87812 - acc: 0.5657 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6491  | total loss: \u001b[1m\u001b[32m1.73042\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6491 | loss: 1.73042 - acc: 0.6091 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6492  | total loss: \u001b[1m\u001b[32m1.59856\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6492 | loss: 1.59856 - acc: 0.6482 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6493  | total loss: \u001b[1m\u001b[32m1.48042\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6493 | loss: 1.48042 - acc: 0.6834 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6494  | total loss: \u001b[1m\u001b[32m1.68530\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6494 | loss: 1.68530 - acc: 0.6294 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6495  | total loss: \u001b[1m\u001b[32m1.55932\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6495 | loss: 1.55932 - acc: 0.6664 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6496  | total loss: \u001b[1m\u001b[32m1.44620\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6496 | loss: 1.44620 - acc: 0.6998 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6497  | total loss: \u001b[1m\u001b[32m1.34417\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6497 | loss: 1.34417 - acc: 0.7298 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6498  | total loss: \u001b[1m\u001b[32m1.57580\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6498 | loss: 1.57580 - acc: 0.6568 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6499  | total loss: \u001b[1m\u001b[32m1.46049\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6499 | loss: 1.46049 - acc: 0.6911 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6500  | total loss: \u001b[1m\u001b[32m1.61651\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6500 | loss: 1.61651 - acc: 0.6363 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6501  | total loss: \u001b[1m\u001b[32m1.49754\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6501 | loss: 1.49754 - acc: 0.6727 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6502  | total loss: \u001b[1m\u001b[32m1.39054\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6502 | loss: 1.39054 - acc: 0.7054 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6503  | total loss: \u001b[1m\u001b[32m1.29385\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6503 | loss: 1.29385 - acc: 0.7349 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6504  | total loss: \u001b[1m\u001b[32m1.20604\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6504 | loss: 1.20604 - acc: 0.7614 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6505  | total loss: \u001b[1m\u001b[32m1.12587\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6505 | loss: 1.12587 - acc: 0.7852 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6506  | total loss: \u001b[1m\u001b[32m1.05228\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6506 | loss: 1.05228 - acc: 0.8067 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6507  | total loss: \u001b[1m\u001b[32m0.98439\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6507 | loss: 0.98439 - acc: 0.8260 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6508  | total loss: \u001b[1m\u001b[32m1.25497\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6508 | loss: 1.25497 - acc: 0.7506 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6509  | total loss: \u001b[1m\u001b[32m1.16419\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6509 | loss: 1.16419 - acc: 0.7755 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6510  | total loss: \u001b[1m\u001b[32m1.41335\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6510 | loss: 1.41335 - acc: 0.7051 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6511  | total loss: \u001b[1m\u001b[32m1.30561\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6511 | loss: 1.30561 - acc: 0.7346 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6512  | total loss: \u001b[1m\u001b[32m1.20824\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6512 | loss: 1.20824 - acc: 0.7611 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6513  | total loss: \u001b[1m\u001b[32m1.11993\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6513 | loss: 1.11993 - acc: 0.7850 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6514  | total loss: \u001b[1m\u001b[32m1.03951\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6514 | loss: 1.03951 - acc: 0.8065 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6515  | total loss: \u001b[1m\u001b[32m0.96601\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6515 | loss: 0.96601 - acc: 0.8259 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 6516  | total loss: \u001b[1m\u001b[32m0.89858\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6516 | loss: 0.89858 - acc: 0.8433 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6517  | total loss: \u001b[1m\u001b[32m0.83650\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6517 | loss: 0.83650 - acc: 0.8590 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6518  | total loss: \u001b[1m\u001b[32m1.11165\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6518 | loss: 1.11165 - acc: 0.7873 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6519  | total loss: \u001b[1m\u001b[32m1.02616\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6519 | loss: 1.02616 - acc: 0.8086 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6520  | total loss: \u001b[1m\u001b[32m0.94845\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6520 | loss: 0.94845 - acc: 0.8278 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6521  | total loss: \u001b[1m\u001b[32m0.87760\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6521 | loss: 0.87760 - acc: 0.8450 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6522  | total loss: \u001b[1m\u001b[32m0.81283\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6522 | loss: 0.81283 - acc: 0.8605 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6523  | total loss: \u001b[1m\u001b[32m0.75347\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6523 | loss: 0.75347 - acc: 0.8744 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6524  | total loss: \u001b[1m\u001b[32m1.05468\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6524 | loss: 1.05468 - acc: 0.8013 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6525  | total loss: \u001b[1m\u001b[32m0.96966\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6525 | loss: 0.96966 - acc: 0.8211 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6526  | total loss: \u001b[1m\u001b[32m1.31654\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6526 | loss: 1.31654 - acc: 0.7390 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6527  | total loss: \u001b[1m\u001b[32m1.20517\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6527 | loss: 1.20517 - acc: 0.7651 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6528  | total loss: \u001b[1m\u001b[32m1.51430\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6528 | loss: 1.51430 - acc: 0.6886 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6529  | total loss: \u001b[1m\u001b[32m1.38412\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6529 | loss: 1.38412 - acc: 0.7198 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6530  | total loss: \u001b[1m\u001b[32m1.56277\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6530 | loss: 1.56277 - acc: 0.6764 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6531  | total loss: \u001b[1m\u001b[32m1.42948\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6531 | loss: 1.42948 - acc: 0.7087 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6532  | total loss: \u001b[1m\u001b[32m1.70584\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6532 | loss: 1.70584 - acc: 0.6378 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6533  | total loss: \u001b[1m\u001b[32m1.56076\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6533 | loss: 1.56076 - acc: 0.6741 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6534  | total loss: \u001b[1m\u001b[32m1.77198\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6534 | loss: 1.77198 - acc: 0.6138 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6535  | total loss: \u001b[1m\u001b[32m1.62363\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6535 | loss: 1.62363 - acc: 0.6524 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6536  | total loss: \u001b[1m\u001b[32m1.49173\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6536 | loss: 1.49173 - acc: 0.6872 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6537  | total loss: \u001b[1m\u001b[32m1.37424\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6537 | loss: 1.37424 - acc: 0.7185 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6538  | total loss: \u001b[1m\u001b[32m1.58782\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6538 | loss: 1.58782 - acc: 0.6609 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6539  | total loss: \u001b[1m\u001b[32m1.46303\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6539 | loss: 1.46303 - acc: 0.6948 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6540  | total loss: \u001b[1m\u001b[32m1.59525\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6540 | loss: 1.59525 - acc: 0.6610 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6541  | total loss: \u001b[1m\u001b[32m1.47223\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6541 | loss: 1.47223 - acc: 0.6949 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6542  | total loss: \u001b[1m\u001b[32m1.68573\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6542 | loss: 1.68573 - acc: 0.6326 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6543  | total loss: \u001b[1m\u001b[32m1.55633\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6543 | loss: 1.55633 - acc: 0.6693 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6544  | total loss: \u001b[1m\u001b[32m1.80727\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6544 | loss: 1.80727 - acc: 0.6024 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6545  | total loss: \u001b[1m\u001b[32m1.66893\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6545 | loss: 1.66893 - acc: 0.6422 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6546  | total loss: \u001b[1m\u001b[32m1.83852\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6546 | loss: 1.83852 - acc: 0.5851 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6547  | total loss: \u001b[1m\u001b[32m1.70061\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6547 | loss: 1.70061 - acc: 0.6266 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6548  | total loss: \u001b[1m\u001b[32m1.89400\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6548 | loss: 1.89400 - acc: 0.5711 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6549  | total loss: \u001b[1m\u001b[32m1.75406\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6549 | loss: 1.75406 - acc: 0.6140 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6550  | total loss: \u001b[1m\u001b[32m1.92303\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6550 | loss: 1.92303 - acc: 0.5597 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6551  | total loss: \u001b[1m\u001b[32m1.78358\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6551 | loss: 1.78358 - acc: 0.6037 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6552  | total loss: \u001b[1m\u001b[32m1.90655\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6552 | loss: 1.90655 - acc: 0.5576 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6553  | total loss: \u001b[1m\u001b[32m1.77183\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6553 | loss: 1.77183 - acc: 0.6019 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6554  | total loss: \u001b[1m\u001b[32m1.65159\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6554 | loss: 1.65159 - acc: 0.6417 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6555  | total loss: \u001b[1m\u001b[32m1.54368\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6555 | loss: 1.54368 - acc: 0.6775 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6556  | total loss: \u001b[1m\u001b[32m1.69242\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6556 | loss: 1.69242 - acc: 0.6241 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6557  | total loss: \u001b[1m\u001b[32m1.58032\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6557 | loss: 1.58032 - acc: 0.6617 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6558  | total loss: \u001b[1m\u001b[32m1.72590\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6558 | loss: 1.72590 - acc: 0.6098 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6559  | total loss: \u001b[1m\u001b[32m1.61040\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6559 | loss: 1.61040 - acc: 0.6488 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6560  | total loss: \u001b[1m\u001b[32m1.50615\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6560 | loss: 1.50615 - acc: 0.6839 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6561  | total loss: \u001b[1m\u001b[32m1.41143\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6561 | loss: 1.41143 - acc: 0.7155 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6562  | total loss: \u001b[1m\u001b[32m1.63250\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6562 | loss: 1.63250 - acc: 0.6440 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6563  | total loss: \u001b[1m\u001b[32m1.52341\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6563 | loss: 1.52341 - acc: 0.6796 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6564  | total loss: \u001b[1m\u001b[32m1.42433\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6564 | loss: 1.42433 - acc: 0.7116 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6565  | total loss: \u001b[1m\u001b[32m1.33379\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6565 | loss: 1.33379 - acc: 0.7405 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6566  | total loss: \u001b[1m\u001b[32m1.53196\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6566 | loss: 1.53196 - acc: 0.6736 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6567  | total loss: \u001b[1m\u001b[32m1.42812\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6567 | loss: 1.42812 - acc: 0.7062 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6568  | total loss: \u001b[1m\u001b[32m1.62662\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6568 | loss: 1.62662 - acc: 0.6427 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6569  | total loss: \u001b[1m\u001b[32m1.51184\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6569 | loss: 1.51184 - acc: 0.6784 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6570  | total loss: \u001b[1m\u001b[32m1.72299\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6570 | loss: 1.72299 - acc: 0.6106 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6571  | total loss: \u001b[1m\u001b[32m1.59805\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6571 | loss: 1.59805 - acc: 0.6495 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6572  | total loss: \u001b[1m\u001b[32m1.82409\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6572 | loss: 1.82409 - acc: 0.5846 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 6573  | total loss: \u001b[1m\u001b[32m1.68937\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6573 | loss: 1.68937 - acc: 0.6261 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6574  | total loss: \u001b[1m\u001b[32m1.87406\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6574 | loss: 1.87406 - acc: 0.5707 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6575  | total loss: \u001b[1m\u001b[32m1.73526\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6575 | loss: 1.73526 - acc: 0.6136 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6576  | total loss: \u001b[1m\u001b[32m1.90998\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6576 | loss: 1.90998 - acc: 0.5522 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6577  | total loss: \u001b[1m\u001b[32m1.76893\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6577 | loss: 1.76893 - acc: 0.5970 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6578  | total loss: \u001b[1m\u001b[32m1.64246\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6578 | loss: 1.64246 - acc: 0.6373 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6579  | total loss: \u001b[1m\u001b[32m1.52851\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6579 | loss: 1.52851 - acc: 0.6736 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6580  | total loss: \u001b[1m\u001b[32m1.42532\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6580 | loss: 1.42532 - acc: 0.7062 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6581  | total loss: \u001b[1m\u001b[32m1.33133\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6581 | loss: 1.33133 - acc: 0.7356 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6582  | total loss: \u001b[1m\u001b[32m1.53380\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6582 | loss: 1.53380 - acc: 0.6692 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6583  | total loss: \u001b[1m\u001b[32m1.42684\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6583 | loss: 1.42684 - acc: 0.7023 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6584  | total loss: \u001b[1m\u001b[32m1.32953\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6584 | loss: 1.32953 - acc: 0.7320 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6585  | total loss: \u001b[1m\u001b[32m1.24055\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6585 | loss: 1.24055 - acc: 0.7588 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6586  | total loss: \u001b[1m\u001b[32m1.48180\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6586 | loss: 1.48180 - acc: 0.6901 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6587  | total loss: \u001b[1m\u001b[32m1.37521\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6587 | loss: 1.37521 - acc: 0.7211 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6588  | total loss: \u001b[1m\u001b[32m1.60618\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6588 | loss: 1.60618 - acc: 0.6561 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6589  | total loss: \u001b[1m\u001b[32m1.48601\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6589 | loss: 1.48601 - acc: 0.6905 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6590  | total loss: \u001b[1m\u001b[32m1.67357\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6590 | loss: 1.67357 - acc: 0.6286 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6591  | total loss: \u001b[1m\u001b[32m1.54643\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6591 | loss: 1.54643 - acc: 0.6657 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6592  | total loss: \u001b[1m\u001b[32m1.75922\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6592 | loss: 1.75922 - acc: 0.5992 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6593  | total loss: \u001b[1m\u001b[32m1.62403\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6593 | loss: 1.62403 - acc: 0.6392 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6594  | total loss: \u001b[1m\u001b[32m1.80313\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6594 | loss: 1.80313 - acc: 0.5825 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6595  | total loss: \u001b[1m\u001b[32m1.66472\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6595 | loss: 1.66472 - acc: 0.6242 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6596  | total loss: \u001b[1m\u001b[32m1.88236\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6596 | loss: 1.88236 - acc: 0.5618 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6597  | total loss: \u001b[1m\u001b[32m1.73786\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6597 | loss: 1.73786 - acc: 0.6056 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6598  | total loss: \u001b[1m\u001b[32m1.60862\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6598 | loss: 1.60862 - acc: 0.6451 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6599  | total loss: \u001b[1m\u001b[32m1.49257\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6599 | loss: 1.49257 - acc: 0.6806 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6600  | total loss: \u001b[1m\u001b[32m1.68809\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6600 | loss: 1.68809 - acc: 0.6196 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6601  | total loss: \u001b[1m\u001b[32m1.56446\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6601 | loss: 1.56446 - acc: 0.6577 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6602  | total loss: \u001b[1m\u001b[32m1.45324\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6602 | loss: 1.45324 - acc: 0.6919 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6603  | total loss: \u001b[1m\u001b[32m1.35272\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6603 | loss: 1.35272 - acc: 0.7227 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6604  | total loss: \u001b[1m\u001b[32m1.26142\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6604 | loss: 1.26142 - acc: 0.7504 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6605  | total loss: \u001b[1m\u001b[32m1.17804\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6605 | loss: 1.17804 - acc: 0.7754 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6606  | total loss: \u001b[1m\u001b[32m1.10150\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6606 | loss: 1.10150 - acc: 0.7979 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6607  | total loss: \u001b[1m\u001b[32m1.03088\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6607 | loss: 1.03088 - acc: 0.8181 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6608  | total loss: \u001b[1m\u001b[32m1.27216\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6608 | loss: 1.27216 - acc: 0.7434 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6609  | total loss: \u001b[1m\u001b[32m1.18166\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6609 | loss: 1.18166 - acc: 0.7691 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6610  | total loss: \u001b[1m\u001b[32m1.09907\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6610 | loss: 1.09907 - acc: 0.7922 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6611  | total loss: \u001b[1m\u001b[32m1.02338\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6611 | loss: 1.02338 - acc: 0.8129 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6612  | total loss: \u001b[1m\u001b[32m1.31699\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6612 | loss: 1.31699 - acc: 0.7317 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6613  | total loss: \u001b[1m\u001b[32m1.21754\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 6613 | loss: 1.21754 - acc: 0.7585 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6614  | total loss: \u001b[1m\u001b[32m1.12732\u001b[0m\u001b[0m | time: 0.031s\n",
      "| Adam | epoch: 6614 | loss: 1.12732 - acc: 0.7826 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6615  | total loss: \u001b[1m\u001b[32m1.04519\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 6615 | loss: 1.04519 - acc: 0.8044 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6616  | total loss: \u001b[1m\u001b[32m1.29208\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 6616 | loss: 1.29208 - acc: 0.7382 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6617  | total loss: \u001b[1m\u001b[32m1.19211\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6617 | loss: 1.19211 - acc: 0.7644 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6618  | total loss: \u001b[1m\u001b[32m1.10163\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 6618 | loss: 1.10163 - acc: 0.7880 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6619  | total loss: \u001b[1m\u001b[32m1.01950\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6619 | loss: 1.01950 - acc: 0.8092 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6620  | total loss: \u001b[1m\u001b[32m1.26706\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6620 | loss: 1.26706 - acc: 0.7425 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6621  | total loss: \u001b[1m\u001b[32m1.16742\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6621 | loss: 1.16742 - acc: 0.7683 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6622  | total loss: \u001b[1m\u001b[32m1.43948\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6622 | loss: 1.43948 - acc: 0.6986 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6623  | total loss: \u001b[1m\u001b[32m1.32280\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 6623 | loss: 1.32280 - acc: 0.7287 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6624  | total loss: \u001b[1m\u001b[32m1.57030\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6624 | loss: 1.57030 - acc: 0.6630 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6625  | total loss: \u001b[1m\u001b[32m1.44177\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 6625 | loss: 1.44177 - acc: 0.6967 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6626  | total loss: \u001b[1m\u001b[32m1.32677\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6626 | loss: 1.32677 - acc: 0.7270 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6627  | total loss: \u001b[1m\u001b[32m1.22361\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6627 | loss: 1.22361 - acc: 0.7543 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6628  | total loss: \u001b[1m\u001b[32m1.46767\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6628 | loss: 1.46767 - acc: 0.6932 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6629  | total loss: \u001b[1m\u001b[32m1.35119\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6629 | loss: 1.35119 - acc: 0.7239 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 6630  | total loss: \u001b[1m\u001b[32m1.61767\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6630 | loss: 1.61767 - acc: 0.6586 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6631  | total loss: \u001b[1m\u001b[32m1.48775\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6631 | loss: 1.48775 - acc: 0.6928 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6632  | total loss: \u001b[1m\u001b[32m1.63784\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6632 | loss: 1.63784 - acc: 0.6449 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6633  | total loss: \u001b[1m\u001b[32m1.50797\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6633 | loss: 1.50797 - acc: 0.6804 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6634  | total loss: \u001b[1m\u001b[32m1.39194\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6634 | loss: 1.39194 - acc: 0.7124 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6635  | total loss: \u001b[1m\u001b[32m1.28795\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6635 | loss: 1.28795 - acc: 0.7411 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6636  | total loss: \u001b[1m\u001b[32m1.53635\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6636 | loss: 1.53635 - acc: 0.6742 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6637  | total loss: \u001b[1m\u001b[32m1.41884\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6637 | loss: 1.41884 - acc: 0.7068 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6638  | total loss: \u001b[1m\u001b[32m1.68204\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6638 | loss: 1.68204 - acc: 0.6361 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6639  | total loss: \u001b[1m\u001b[32m1.55166\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6639 | loss: 1.55166 - acc: 0.6725 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6640  | total loss: \u001b[1m\u001b[32m1.73663\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6640 | loss: 1.73663 - acc: 0.6124 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6641  | total loss: \u001b[1m\u001b[32m1.60311\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6641 | loss: 1.60311 - acc: 0.6511 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6642  | total loss: \u001b[1m\u001b[32m1.48391\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6642 | loss: 1.48391 - acc: 0.6860 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6643  | total loss: \u001b[1m\u001b[32m1.37708\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6643 | loss: 1.37708 - acc: 0.7174 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6644  | total loss: \u001b[1m\u001b[32m1.62689\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6644 | loss: 1.62689 - acc: 0.6457 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6645  | total loss: \u001b[1m\u001b[32m1.50662\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6645 | loss: 1.50662 - acc: 0.6811 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6646  | total loss: \u001b[1m\u001b[32m1.73178\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6646 | loss: 1.73178 - acc: 0.6130 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6647  | total loss: \u001b[1m\u001b[32m1.60258\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 6647 | loss: 1.60258 - acc: 0.6517 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6648  | total loss: \u001b[1m\u001b[32m1.76676\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6648 | loss: 1.76676 - acc: 0.6008 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6649  | total loss: \u001b[1m\u001b[32m1.63600\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6649 | loss: 1.63600 - acc: 0.6407 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6650  | total loss: \u001b[1m\u001b[32m1.83455\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6650 | loss: 1.83455 - acc: 0.5767 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6651  | total loss: \u001b[1m\u001b[32m1.69918\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 6651 | loss: 1.69918 - acc: 0.6190 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6652  | total loss: \u001b[1m\u001b[32m1.87956\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6652 | loss: 1.87956 - acc: 0.5642 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6653  | total loss: \u001b[1m\u001b[32m1.74205\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6653 | loss: 1.74205 - acc: 0.6078 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6654  | total loss: \u001b[1m\u001b[32m1.61916\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6654 | loss: 1.61916 - acc: 0.6470 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6655  | total loss: \u001b[1m\u001b[32m1.50879\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6655 | loss: 1.50879 - acc: 0.6823 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6656  | total loss: \u001b[1m\u001b[32m1.40914\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6656 | loss: 1.40914 - acc: 0.7141 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6657  | total loss: \u001b[1m\u001b[32m1.31863\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6657 | loss: 1.31863 - acc: 0.7427 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6658  | total loss: \u001b[1m\u001b[32m1.53562\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6658 | loss: 1.53562 - acc: 0.6756 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6659  | total loss: \u001b[1m\u001b[32m1.43083\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6659 | loss: 1.43083 - acc: 0.7080 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6660  | total loss: \u001b[1m\u001b[32m1.33569\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 6660 | loss: 1.33569 - acc: 0.7372 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6661  | total loss: \u001b[1m\u001b[32m1.24883\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6661 | loss: 1.24883 - acc: 0.7635 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6662  | total loss: \u001b[1m\u001b[32m1.16909\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6662 | loss: 1.16909 - acc: 0.7871 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6663  | total loss: \u001b[1m\u001b[32m1.09548\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6663 | loss: 1.09548 - acc: 0.8084 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6664  | total loss: \u001b[1m\u001b[32m1.02718\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6664 | loss: 1.02718 - acc: 0.8276 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6665  | total loss: \u001b[1m\u001b[32m0.96350\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6665 | loss: 0.96350 - acc: 0.8448 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6666  | total loss: \u001b[1m\u001b[32m0.90389\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6666 | loss: 0.90389 - acc: 0.8603 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6667  | total loss: \u001b[1m\u001b[32m0.84789\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6667 | loss: 0.84789 - acc: 0.8743 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6668  | total loss: \u001b[1m\u001b[32m0.79514\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6668 | loss: 0.79514 - acc: 0.8869 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6669  | total loss: \u001b[1m\u001b[32m0.74535\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6669 | loss: 0.74535 - acc: 0.8982 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6670  | total loss: \u001b[1m\u001b[32m0.69829\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 6670 | loss: 0.69829 - acc: 0.9084 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6671  | total loss: \u001b[1m\u001b[32m0.65377\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6671 | loss: 0.65377 - acc: 0.9175 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6672  | total loss: \u001b[1m\u001b[32m0.61166\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6672 | loss: 0.61166 - acc: 0.9258 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6673  | total loss: \u001b[1m\u001b[32m0.57182\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6673 | loss: 0.57182 - acc: 0.9332 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6674  | total loss: \u001b[1m\u001b[32m0.53417\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6674 | loss: 0.53417 - acc: 0.9399 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6675  | total loss: \u001b[1m\u001b[32m0.49862\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6675 | loss: 0.49862 - acc: 0.9459 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6676  | total loss: \u001b[1m\u001b[32m0.88840\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6676 | loss: 0.88840 - acc: 0.8513 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6677  | total loss: \u001b[1m\u001b[32m0.81518\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6677 | loss: 0.81518 - acc: 0.8662 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6678  | total loss: \u001b[1m\u001b[32m0.74858\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6678 | loss: 0.74858 - acc: 0.8796 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6679  | total loss: \u001b[1m\u001b[32m0.68791\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6679 | loss: 0.68791 - acc: 0.8916 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6680  | total loss: \u001b[1m\u001b[32m1.07770\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6680 | loss: 1.07770 - acc: 0.8024 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6681  | total loss: \u001b[1m\u001b[32m0.98331\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6681 | loss: 0.98331 - acc: 0.8222 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6682  | total loss: \u001b[1m\u001b[32m1.35313\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6682 | loss: 1.35313 - acc: 0.7471 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6683  | total loss: \u001b[1m\u001b[32m1.23147\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6683 | loss: 1.23147 - acc: 0.7724 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6684  | total loss: \u001b[1m\u001b[32m1.12230\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6684 | loss: 1.12230 - acc: 0.7952 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6685  | total loss: \u001b[1m\u001b[32m1.02423\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6685 | loss: 1.02423 - acc: 0.8156 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6686  | total loss: \u001b[1m\u001b[32m0.93607\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6686 | loss: 0.93607 - acc: 0.8341 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 6687  | total loss: \u001b[1m\u001b[32m0.85672\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 6687 | loss: 0.85672 - acc: 0.8507 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6688  | total loss: \u001b[1m\u001b[32m1.20190\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6688 | loss: 1.20190 - acc: 0.7728 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6689  | total loss: \u001b[1m\u001b[32m1.09633\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6689 | loss: 1.09633 - acc: 0.7955 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6690  | total loss: \u001b[1m\u001b[32m1.00164\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6690 | loss: 1.00164 - acc: 0.8159 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6691  | total loss: \u001b[1m\u001b[32m0.91662\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6691 | loss: 0.91662 - acc: 0.8343 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6692  | total loss: \u001b[1m\u001b[32m0.84019\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6692 | loss: 0.84019 - acc: 0.8509 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6693  | total loss: \u001b[1m\u001b[32m0.77136\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6693 | loss: 0.77136 - acc: 0.8658 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6694  | total loss: \u001b[1m\u001b[32m1.08848\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6694 | loss: 1.08848 - acc: 0.7935 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6695  | total loss: \u001b[1m\u001b[32m0.99509\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6695 | loss: 0.99509 - acc: 0.8142 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6696  | total loss: \u001b[1m\u001b[32m0.91131\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6696 | loss: 0.91131 - acc: 0.8327 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6697  | total loss: \u001b[1m\u001b[32m0.83605\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6697 | loss: 0.83605 - acc: 0.8495 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6698  | total loss: \u001b[1m\u001b[32m0.76832\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6698 | loss: 0.76832 - acc: 0.8645 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6699  | total loss: \u001b[1m\u001b[32m0.70725\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6699 | loss: 0.70725 - acc: 0.8781 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6700  | total loss: \u001b[1m\u001b[32m0.98868\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6700 | loss: 0.98868 - acc: 0.8188 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6701  | total loss: \u001b[1m\u001b[32m0.90566\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6701 | loss: 0.90566 - acc: 0.8370 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6702  | total loss: \u001b[1m\u001b[32m1.23548\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6702 | loss: 1.23548 - acc: 0.7604 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6703  | total loss: \u001b[1m\u001b[32m1.12870\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6703 | loss: 1.12870 - acc: 0.7844 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6704  | total loss: \u001b[1m\u001b[32m1.03320\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6704 | loss: 1.03320 - acc: 0.8059 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6705  | total loss: \u001b[1m\u001b[32m0.94765\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6705 | loss: 0.94765 - acc: 0.8253 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6706  | total loss: \u001b[1m\u001b[32m0.87090\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6706 | loss: 0.87090 - acc: 0.8428 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6707  | total loss: \u001b[1m\u001b[32m0.80189\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6707 | loss: 0.80189 - acc: 0.8585 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6708  | total loss: \u001b[1m\u001b[32m1.16377\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6708 | loss: 1.16377 - acc: 0.7727 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6709  | total loss: \u001b[1m\u001b[32m1.06599\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6709 | loss: 1.06599 - acc: 0.7954 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6710  | total loss: \u001b[1m\u001b[32m1.39289\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6710 | loss: 1.39289 - acc: 0.7159 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6711  | total loss: \u001b[1m\u001b[32m1.27367\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6711 | loss: 1.27367 - acc: 0.7443 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6712  | total loss: \u001b[1m\u001b[32m1.16722\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6712 | loss: 1.16722 - acc: 0.7698 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6713  | total loss: \u001b[1m\u001b[32m1.07201\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6713 | loss: 1.07201 - acc: 0.7929 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6714  | total loss: \u001b[1m\u001b[32m0.98668\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6714 | loss: 0.98668 - acc: 0.8136 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6715  | total loss: \u001b[1m\u001b[32m0.91001\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6715 | loss: 0.91001 - acc: 0.8322 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6716  | total loss: \u001b[1m\u001b[32m1.20974\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6716 | loss: 1.20974 - acc: 0.7561 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6717  | total loss: \u001b[1m\u001b[32m1.11132\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6717 | loss: 1.11132 - acc: 0.7805 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6718  | total loss: \u001b[1m\u001b[32m1.02312\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6718 | loss: 1.02312 - acc: 0.8025 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6719  | total loss: \u001b[1m\u001b[32m0.94387\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6719 | loss: 0.94387 - acc: 0.8222 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6720  | total loss: \u001b[1m\u001b[32m1.23650\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 6720 | loss: 1.23650 - acc: 0.7471 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6721  | total loss: \u001b[1m\u001b[32m1.13648\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6721 | loss: 1.13648 - acc: 0.7724 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6722  | total loss: \u001b[1m\u001b[32m1.39080\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6722 | loss: 1.39080 - acc: 0.7095 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6723  | total loss: \u001b[1m\u001b[32m1.27681\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6723 | loss: 1.27681 - acc: 0.7385 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6724  | total loss: \u001b[1m\u001b[32m1.17498\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6724 | loss: 1.17498 - acc: 0.7647 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6725  | total loss: \u001b[1m\u001b[32m1.08378\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6725 | loss: 1.08378 - acc: 0.7882 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6726  | total loss: \u001b[1m\u001b[32m1.32751\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6726 | loss: 1.32751 - acc: 0.7237 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6727  | total loss: \u001b[1m\u001b[32m1.22200\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6727 | loss: 1.22200 - acc: 0.7513 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6728  | total loss: \u001b[1m\u001b[32m1.48174\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6728 | loss: 1.48174 - acc: 0.6833 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6729  | total loss: \u001b[1m\u001b[32m1.36247\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6729 | loss: 1.36247 - acc: 0.7150 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6730  | total loss: \u001b[1m\u001b[32m1.61254\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6730 | loss: 1.61254 - acc: 0.6435 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6731  | total loss: \u001b[1m\u001b[32m1.48264\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6731 | loss: 1.48264 - acc: 0.6791 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6732  | total loss: \u001b[1m\u001b[32m1.36692\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6732 | loss: 1.36692 - acc: 0.7112 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6733  | total loss: \u001b[1m\u001b[32m1.26355\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 6733 | loss: 1.26355 - acc: 0.7401 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6734  | total loss: \u001b[1m\u001b[32m1.54152\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6734 | loss: 1.54152 - acc: 0.6661 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6735  | total loss: \u001b[1m\u001b[32m1.42236\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6735 | loss: 1.42236 - acc: 0.6995 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6736  | total loss: \u001b[1m\u001b[32m1.31593\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6736 | loss: 1.31593 - acc: 0.7295 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6737  | total loss: \u001b[1m\u001b[32m1.22051\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6737 | loss: 1.22051 - acc: 0.7566 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6738  | total loss: \u001b[1m\u001b[32m1.46327\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6738 | loss: 1.46327 - acc: 0.6881 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6739  | total loss: \u001b[1m\u001b[32m1.35383\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6739 | loss: 1.35383 - acc: 0.7193 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6740  | total loss: \u001b[1m\u001b[32m1.25565\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6740 | loss: 1.25565 - acc: 0.7473 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6741  | total loss: \u001b[1m\u001b[32m1.16718\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6741 | loss: 1.16718 - acc: 0.7726 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6742  | total loss: \u001b[1m\u001b[32m1.40518\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6742 | loss: 1.40518 - acc: 0.7025 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6743  | total loss: \u001b[1m\u001b[32m1.30178\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6743 | loss: 1.30178 - acc: 0.7322 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 6744  | total loss: \u001b[1m\u001b[32m1.53538\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6744 | loss: 1.53538 - acc: 0.6662 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6745  | total loss: \u001b[1m\u001b[32m1.41984\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6745 | loss: 1.41984 - acc: 0.6995 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6746  | total loss: \u001b[1m\u001b[32m1.65377\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6746 | loss: 1.65377 - acc: 0.6296 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6747  | total loss: \u001b[1m\u001b[32m1.52796\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6747 | loss: 1.52796 - acc: 0.6666 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6748  | total loss: \u001b[1m\u001b[32m1.71684\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6748 | loss: 1.71684 - acc: 0.6071 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6749  | total loss: \u001b[1m\u001b[32m1.58686\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6749 | loss: 1.58686 - acc: 0.6464 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6750  | total loss: \u001b[1m\u001b[32m1.47074\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6750 | loss: 1.47074 - acc: 0.6818 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6751  | total loss: \u001b[1m\u001b[32m1.36656\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6751 | loss: 1.36656 - acc: 0.7136 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6752  | total loss: \u001b[1m\u001b[32m1.27262\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6752 | loss: 1.27262 - acc: 0.7422 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6753  | total loss: \u001b[1m\u001b[32m1.18745\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6753 | loss: 1.18745 - acc: 0.7680 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6754  | total loss: \u001b[1m\u001b[32m1.10980\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6754 | loss: 1.10980 - acc: 0.7912 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6755  | total loss: \u001b[1m\u001b[32m1.03859\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6755 | loss: 1.03859 - acc: 0.8121 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6756  | total loss: \u001b[1m\u001b[32m0.97292\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6756 | loss: 0.97292 - acc: 0.8309 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6757  | total loss: \u001b[1m\u001b[32m0.91204\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6757 | loss: 0.91204 - acc: 0.8478 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6758  | total loss: \u001b[1m\u001b[32m1.17644\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6758 | loss: 1.17644 - acc: 0.7701 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6759  | total loss: \u001b[1m\u001b[32m1.09241\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6759 | loss: 1.09241 - acc: 0.7931 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6760  | total loss: \u001b[1m\u001b[32m1.34518\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6760 | loss: 1.34518 - acc: 0.7210 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6761  | total loss: \u001b[1m\u001b[32m1.24298\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6761 | loss: 1.24298 - acc: 0.7489 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6762  | total loss: \u001b[1m\u001b[32m1.49077\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6762 | loss: 1.49077 - acc: 0.6811 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6763  | total loss: \u001b[1m\u001b[32m1.37397\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6763 | loss: 1.37397 - acc: 0.7130 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6764  | total loss: \u001b[1m\u001b[32m1.26892\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6764 | loss: 1.26892 - acc: 0.7417 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6765  | total loss: \u001b[1m\u001b[32m1.17415\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6765 | loss: 1.17415 - acc: 0.7675 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6766  | total loss: \u001b[1m\u001b[32m1.43050\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6766 | loss: 1.43050 - acc: 0.6979 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6767  | total loss: \u001b[1m\u001b[32m1.31942\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6767 | loss: 1.31942 - acc: 0.7281 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6768  | total loss: \u001b[1m\u001b[32m1.21947\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6768 | loss: 1.21947 - acc: 0.7553 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6769  | total loss: \u001b[1m\u001b[32m1.12925\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6769 | loss: 1.12925 - acc: 0.7798 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6770  | total loss: \u001b[1m\u001b[32m1.35535\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6770 | loss: 1.35535 - acc: 0.7161 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6771  | total loss: \u001b[1m\u001b[32m1.25118\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6771 | loss: 1.25118 - acc: 0.7445 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6772  | total loss: \u001b[1m\u001b[32m1.44494\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6772 | loss: 1.44494 - acc: 0.6915 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6773  | total loss: \u001b[1m\u001b[32m1.33216\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6773 | loss: 1.33216 - acc: 0.7223 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6774  | total loss: \u001b[1m\u001b[32m1.57767\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6774 | loss: 1.57767 - acc: 0.6572 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6775  | total loss: \u001b[1m\u001b[32m1.45268\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6775 | loss: 1.45268 - acc: 0.6915 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6776  | total loss: \u001b[1m\u001b[32m1.71329\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6776 | loss: 1.71329 - acc: 0.6224 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6777  | total loss: \u001b[1m\u001b[32m1.57670\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6777 | loss: 1.57670 - acc: 0.6601 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6778  | total loss: \u001b[1m\u001b[32m1.81945\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6778 | loss: 1.81945 - acc: 0.5941 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6779  | total loss: \u001b[1m\u001b[32m1.67502\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6779 | loss: 1.67502 - acc: 0.6347 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6780  | total loss: \u001b[1m\u001b[32m1.54631\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6780 | loss: 1.54631 - acc: 0.6712 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6781  | total loss: \u001b[1m\u001b[32m1.43126\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6781 | loss: 1.43126 - acc: 0.7041 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6782  | total loss: \u001b[1m\u001b[32m1.60195\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6782 | loss: 1.60195 - acc: 0.6551 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6783  | total loss: \u001b[1m\u001b[32m1.48253\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6783 | loss: 1.48253 - acc: 0.6896 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6784  | total loss: \u001b[1m\u001b[32m1.66979\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6784 | loss: 1.66979 - acc: 0.6278 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6785  | total loss: \u001b[1m\u001b[32m1.54516\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6785 | loss: 1.54516 - acc: 0.6650 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6786  | total loss: \u001b[1m\u001b[32m1.72254\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6786 | loss: 1.72254 - acc: 0.6128 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6787  | total loss: \u001b[1m\u001b[32m1.59455\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6787 | loss: 1.59455 - acc: 0.6515 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6788  | total loss: \u001b[1m\u001b[32m1.48007\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6788 | loss: 1.48007 - acc: 0.6864 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6789  | total loss: \u001b[1m\u001b[32m1.37721\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6789 | loss: 1.37721 - acc: 0.7177 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6790  | total loss: \u001b[1m\u001b[32m1.63716\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6790 | loss: 1.63716 - acc: 0.6460 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6791  | total loss: \u001b[1m\u001b[32m1.51887\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6791 | loss: 1.51887 - acc: 0.6814 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6792  | total loss: \u001b[1m\u001b[32m1.41246\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6792 | loss: 1.41246 - acc: 0.7132 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6793  | total loss: \u001b[1m\u001b[32m1.31626\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6793 | loss: 1.31626 - acc: 0.7419 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6794  | total loss: \u001b[1m\u001b[32m1.52359\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6794 | loss: 1.52359 - acc: 0.6749 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6795  | total loss: \u001b[1m\u001b[32m1.41544\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6795 | loss: 1.41544 - acc: 0.7074 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6796  | total loss: \u001b[1m\u001b[32m1.60231\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6796 | loss: 1.60231 - acc: 0.6509 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6797  | total loss: \u001b[1m\u001b[32m1.48604\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6797 | loss: 1.48604 - acc: 0.6858 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6798  | total loss: \u001b[1m\u001b[32m1.68038\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6798 | loss: 1.68038 - acc: 0.6244 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6799  | total loss: \u001b[1m\u001b[32m1.55660\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6799 | loss: 1.55660 - acc: 0.6619 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6800  | total loss: \u001b[1m\u001b[32m1.72304\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6800 | loss: 1.72304 - acc: 0.6029 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 6801  | total loss: \u001b[1m\u001b[32m1.59583\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6801 | loss: 1.59583 - acc: 0.6426 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6802  | total loss: \u001b[1m\u001b[32m1.48158\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6802 | loss: 1.48158 - acc: 0.6783 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6803  | total loss: \u001b[1m\u001b[32m1.37853\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6803 | loss: 1.37853 - acc: 0.7105 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6804  | total loss: \u001b[1m\u001b[32m1.55527\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6804 | loss: 1.55527 - acc: 0.6537 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6805  | total loss: \u001b[1m\u001b[32m1.44429\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6805 | loss: 1.44429 - acc: 0.6884 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6806  | total loss: \u001b[1m\u001b[32m1.67425\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6806 | loss: 1.67425 - acc: 0.6195 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6807  | total loss: \u001b[1m\u001b[32m1.55156\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6807 | loss: 1.55156 - acc: 0.6576 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6808  | total loss: \u001b[1m\u001b[32m1.44116\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6808 | loss: 1.44116 - acc: 0.6918 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6809  | total loss: \u001b[1m\u001b[32m1.34136\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6809 | loss: 1.34136 - acc: 0.7226 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6810  | total loss: \u001b[1m\u001b[32m1.25069\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 6810 | loss: 1.25069 - acc: 0.7504 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6811  | total loss: \u001b[1m\u001b[32m1.16790\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6811 | loss: 1.16790 - acc: 0.7753 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6812  | total loss: \u001b[1m\u001b[32m1.09190\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6812 | loss: 1.09190 - acc: 0.7978 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6813  | total loss: \u001b[1m\u001b[32m1.02179\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6813 | loss: 1.02179 - acc: 0.8180 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6814  | total loss: \u001b[1m\u001b[32m1.27891\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6814 | loss: 1.27891 - acc: 0.7434 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6815  | total loss: \u001b[1m\u001b[32m1.18727\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6815 | loss: 1.18727 - acc: 0.7690 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6816  | total loss: \u001b[1m\u001b[32m1.45887\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6816 | loss: 1.45887 - acc: 0.6921 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6817  | total loss: \u001b[1m\u001b[32m1.34788\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6817 | loss: 1.34788 - acc: 0.7229 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6818  | total loss: \u001b[1m\u001b[32m1.55709\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6818 | loss: 1.55709 - acc: 0.6649 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6819  | total loss: \u001b[1m\u001b[32m1.43597\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6819 | loss: 1.43597 - acc: 0.6984 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6820  | total loss: \u001b[1m\u001b[32m1.32680\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6820 | loss: 1.32680 - acc: 0.7286 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6821  | total loss: \u001b[1m\u001b[32m1.22810\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6821 | loss: 1.22810 - acc: 0.7557 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6822  | total loss: \u001b[1m\u001b[32m1.13855\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6822 | loss: 1.13855 - acc: 0.7801 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6823  | total loss: \u001b[1m\u001b[32m1.05702\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6823 | loss: 1.05702 - acc: 0.8021 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6824  | total loss: \u001b[1m\u001b[32m1.34640\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6824 | loss: 1.34640 - acc: 0.7219 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6825  | total loss: \u001b[1m\u001b[32m1.24282\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6825 | loss: 1.24282 - acc: 0.7497 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6826  | total loss: \u001b[1m\u001b[32m1.14919\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6826 | loss: 1.14919 - acc: 0.7748 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6827  | total loss: \u001b[1m\u001b[32m1.06429\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6827 | loss: 1.06429 - acc: 0.7973 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6828  | total loss: \u001b[1m\u001b[32m0.98705\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6828 | loss: 0.98705 - acc: 0.8176 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6829  | total loss: \u001b[1m\u001b[32m0.91655\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6829 | loss: 0.91655 - acc: 0.8358 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6830  | total loss: \u001b[1m\u001b[32m0.85199\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6830 | loss: 0.85199 - acc: 0.8522 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6831  | total loss: \u001b[1m\u001b[32m0.79268\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6831 | loss: 0.79268 - acc: 0.8670 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6832  | total loss: \u001b[1m\u001b[32m1.11429\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6832 | loss: 1.11429 - acc: 0.7874 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6833  | total loss: \u001b[1m\u001b[32m1.02711\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6833 | loss: 1.02711 - acc: 0.8087 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6834  | total loss: \u001b[1m\u001b[32m1.28785\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6834 | loss: 1.28785 - acc: 0.7421 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6835  | total loss: \u001b[1m\u001b[32m1.18299\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6835 | loss: 1.18299 - acc: 0.7679 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6836  | total loss: \u001b[1m\u001b[32m1.08859\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6836 | loss: 1.08859 - acc: 0.7911 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6837  | total loss: \u001b[1m\u001b[32m1.00341\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6837 | loss: 1.00341 - acc: 0.8120 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6838  | total loss: \u001b[1m\u001b[32m0.92635\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6838 | loss: 0.92635 - acc: 0.8308 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6839  | total loss: \u001b[1m\u001b[32m0.85645\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6839 | loss: 0.85645 - acc: 0.8477 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6840  | total loss: \u001b[1m\u001b[32m1.20109\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6840 | loss: 1.20109 - acc: 0.7629 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6841  | total loss: \u001b[1m\u001b[32m1.10319\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6841 | loss: 1.10319 - acc: 0.7867 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6842  | total loss: \u001b[1m\u001b[32m1.01503\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6842 | loss: 1.01503 - acc: 0.8080 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6843  | total loss: \u001b[1m\u001b[32m0.93547\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6843 | loss: 0.93547 - acc: 0.8272 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6844  | total loss: \u001b[1m\u001b[32m0.86348\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 6844 | loss: 0.86348 - acc: 0.8445 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6845  | total loss: \u001b[1m\u001b[32m0.79818\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6845 | loss: 0.79818 - acc: 0.8600 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6846  | total loss: \u001b[1m\u001b[32m1.16015\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6846 | loss: 1.16015 - acc: 0.7740 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6847  | total loss: \u001b[1m\u001b[32m1.06470\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6847 | loss: 1.06470 - acc: 0.7966 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6848  | total loss: \u001b[1m\u001b[32m0.97874\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6848 | loss: 0.97874 - acc: 0.8170 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6849  | total loss: \u001b[1m\u001b[32m0.90119\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6849 | loss: 0.90119 - acc: 0.8353 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6850  | total loss: \u001b[1m\u001b[32m0.83106\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6850 | loss: 0.83106 - acc: 0.8517 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6851  | total loss: \u001b[1m\u001b[32m0.76748\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6851 | loss: 0.76748 - acc: 0.8666 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6852  | total loss: \u001b[1m\u001b[32m0.70971\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6852 | loss: 0.70971 - acc: 0.8799 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6853  | total loss: \u001b[1m\u001b[32m0.65708\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6853 | loss: 0.65708 - acc: 0.8919 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6854  | total loss: \u001b[1m\u001b[32m1.01788\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6854 | loss: 1.01788 - acc: 0.8099 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6855  | total loss: \u001b[1m\u001b[32m0.93373\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6855 | loss: 0.93373 - acc: 0.8289 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6856  | total loss: \u001b[1m\u001b[32m0.85786\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6856 | loss: 0.85786 - acc: 0.8460 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6857  | total loss: \u001b[1m\u001b[32m0.78932\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6857 | loss: 0.78932 - acc: 0.8614 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 6858  | total loss: \u001b[1m\u001b[32m1.09316\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6858 | loss: 1.09316 - acc: 0.7967 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6859  | total loss: \u001b[1m\u001b[32m1.00103\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6859 | loss: 1.00103 - acc: 0.8170 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6860  | total loss: \u001b[1m\u001b[32m1.32762\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6860 | loss: 1.32762 - acc: 0.7425 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6861  | total loss: \u001b[1m\u001b[32m1.21293\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6861 | loss: 1.21293 - acc: 0.7682 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6862  | total loss: \u001b[1m\u001b[32m1.45887\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6862 | loss: 1.45887 - acc: 0.7128 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6863  | total loss: \u001b[1m\u001b[32m1.33274\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6863 | loss: 1.33274 - acc: 0.7415 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6864  | total loss: \u001b[1m\u001b[32m1.63240\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6864 | loss: 1.63240 - acc: 0.6674 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6865  | total loss: \u001b[1m\u001b[32m1.49137\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6865 | loss: 1.49137 - acc: 0.7006 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6866  | total loss: \u001b[1m\u001b[32m1.73531\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6866 | loss: 1.73531 - acc: 0.6449 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6867  | total loss: \u001b[1m\u001b[32m1.58723\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6867 | loss: 1.58723 - acc: 0.6804 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6868  | total loss: \u001b[1m\u001b[32m1.82982\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6868 | loss: 1.82982 - acc: 0.6123 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6869  | total loss: \u001b[1m\u001b[32m1.67630\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6869 | loss: 1.67630 - acc: 0.6511 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6870  | total loss: \u001b[1m\u001b[32m1.54011\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6870 | loss: 1.54011 - acc: 0.6860 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6871  | total loss: \u001b[1m\u001b[32m1.41907\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6871 | loss: 1.41907 - acc: 0.7174 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6872  | total loss: \u001b[1m\u001b[32m1.60539\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6872 | loss: 1.60539 - acc: 0.6599 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6873  | total loss: \u001b[1m\u001b[32m1.48055\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6873 | loss: 1.48055 - acc: 0.6939 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6874  | total loss: \u001b[1m\u001b[32m1.71412\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6874 | loss: 1.71412 - acc: 0.6317 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6875  | total loss: \u001b[1m\u001b[32m1.58143\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6875 | loss: 1.58143 - acc: 0.6685 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6876  | total loss: \u001b[1m\u001b[32m1.46334\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6876 | loss: 1.46334 - acc: 0.7017 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6877  | total loss: \u001b[1m\u001b[32m1.35786\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6877 | loss: 1.35786 - acc: 0.7315 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6878  | total loss: \u001b[1m\u001b[32m1.26323\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6878 | loss: 1.26323 - acc: 0.7584 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6879  | total loss: \u001b[1m\u001b[32m1.17790\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6879 | loss: 1.17790 - acc: 0.7825 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6880  | total loss: \u001b[1m\u001b[32m1.36763\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6880 | loss: 1.36763 - acc: 0.7257 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6881  | total loss: \u001b[1m\u001b[32m1.27141\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6881 | loss: 1.27141 - acc: 0.7531 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6882  | total loss: \u001b[1m\u001b[32m1.51655\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6882 | loss: 1.51655 - acc: 0.6778 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6883  | total loss: \u001b[1m\u001b[32m1.40571\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6883 | loss: 1.40571 - acc: 0.7100 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6884  | total loss: \u001b[1m\u001b[32m1.30607\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6884 | loss: 1.30607 - acc: 0.7390 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6885  | total loss: \u001b[1m\u001b[32m1.21607\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6885 | loss: 1.21607 - acc: 0.7651 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6886  | total loss: \u001b[1m\u001b[32m1.45857\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6886 | loss: 1.45857 - acc: 0.6958 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6887  | total loss: \u001b[1m\u001b[32m1.35277\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6887 | loss: 1.35277 - acc: 0.7262 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6888  | total loss: \u001b[1m\u001b[32m1.25726\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6888 | loss: 1.25726 - acc: 0.7536 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6889  | total loss: \u001b[1m\u001b[32m1.17065\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6889 | loss: 1.17065 - acc: 0.7782 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6890  | total loss: \u001b[1m\u001b[32m1.40962\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6890 | loss: 1.40962 - acc: 0.7075 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6891  | total loss: \u001b[1m\u001b[32m1.30675\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6891 | loss: 1.30675 - acc: 0.7368 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6892  | total loss: \u001b[1m\u001b[32m1.55388\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 6892 | loss: 1.55388 - acc: 0.6631 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6893  | total loss: \u001b[1m\u001b[32m1.43668\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 6893 | loss: 1.43668 - acc: 0.6968 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6894  | total loss: \u001b[1m\u001b[32m1.65372\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6894 | loss: 1.65372 - acc: 0.6343 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6895  | total loss: \u001b[1m\u001b[32m1.52750\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6895 | loss: 1.52750 - acc: 0.6708 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6896  | total loss: \u001b[1m\u001b[32m1.41429\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 6896 | loss: 1.41429 - acc: 0.7037 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6897  | total loss: \u001b[1m\u001b[32m1.31234\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6897 | loss: 1.31234 - acc: 0.7334 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6898  | total loss: \u001b[1m\u001b[32m1.22015\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6898 | loss: 1.22015 - acc: 0.7600 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6899  | total loss: \u001b[1m\u001b[32m1.13639\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6899 | loss: 1.13639 - acc: 0.7840 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6900  | total loss: \u001b[1m\u001b[32m1.05991\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 6900 | loss: 1.05991 - acc: 0.8056 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6901  | total loss: \u001b[1m\u001b[32m0.98975\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6901 | loss: 0.98975 - acc: 0.8251 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6902  | total loss: \u001b[1m\u001b[32m1.21113\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 6902 | loss: 1.21113 - acc: 0.7640 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6903  | total loss: \u001b[1m\u001b[32m1.12360\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6903 | loss: 1.12360 - acc: 0.7876 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6904  | total loss: \u001b[1m\u001b[32m1.38785\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6904 | loss: 1.38785 - acc: 0.7160 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6905  | total loss: \u001b[1m\u001b[32m1.28162\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6905 | loss: 1.28162 - acc: 0.7444 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6906  | total loss: \u001b[1m\u001b[32m1.50863\u001b[0m\u001b[0m | time: 0.015s\n",
      "| Adam | epoch: 6906 | loss: 1.50863 - acc: 0.6771 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6907  | total loss: \u001b[1m\u001b[32m1.39036\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6907 | loss: 1.39036 - acc: 0.7094 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6908  | total loss: \u001b[1m\u001b[32m1.28400\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6908 | loss: 1.28400 - acc: 0.7384 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6909  | total loss: \u001b[1m\u001b[32m1.18803\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6909 | loss: 1.18803 - acc: 0.7646 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6910  | total loss: \u001b[1m\u001b[32m1.42363\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6910 | loss: 1.42363 - acc: 0.6953 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6911  | total loss: \u001b[1m\u001b[32m1.31341\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6911 | loss: 1.31341 - acc: 0.7257 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6912  | total loss: \u001b[1m\u001b[32m1.51541\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 6912 | loss: 1.51541 - acc: 0.6675 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6913  | total loss: \u001b[1m\u001b[32m1.39654\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 6913 | loss: 1.39654 - acc: 0.7007 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6914  | total loss: \u001b[1m\u001b[32m1.64062\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 6914 | loss: 1.64062 - acc: 0.6306 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 6915  | total loss: \u001b[1m\u001b[32m1.51049\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 6915 | loss: 1.51049 - acc: 0.6676 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6916  | total loss: \u001b[1m\u001b[32m1.73301\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 6916 | loss: 1.73301 - acc: 0.6080 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6917  | total loss: \u001b[1m\u001b[32m1.59560\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6917 | loss: 1.59560 - acc: 0.6472 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6918  | total loss: \u001b[1m\u001b[32m1.81682\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6918 | loss: 1.81682 - acc: 0.5824 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6919  | total loss: \u001b[1m\u001b[32m1.67355\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6919 | loss: 1.67355 - acc: 0.6242 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6920  | total loss: \u001b[1m\u001b[32m1.90041\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6920 | loss: 1.90041 - acc: 0.5618 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6921  | total loss: \u001b[1m\u001b[32m1.75183\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6921 | loss: 1.75183 - acc: 0.6056 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6922  | total loss: \u001b[1m\u001b[32m1.92295\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 6922 | loss: 1.92295 - acc: 0.5522 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6923  | total loss: \u001b[1m\u001b[32m1.77552\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 6923 | loss: 1.77552 - acc: 0.5970 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6924  | total loss: \u001b[1m\u001b[32m1.93550\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 6924 | loss: 1.93550 - acc: 0.5444 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6925  | total loss: \u001b[1m\u001b[32m1.79029\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 6925 | loss: 1.79029 - acc: 0.5900 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6926  | total loss: \u001b[1m\u001b[32m1.66098\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6926 | loss: 1.66098 - acc: 0.6310 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6927  | total loss: \u001b[1m\u001b[32m1.54534\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 6927 | loss: 1.54534 - acc: 0.6679 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6928  | total loss: \u001b[1m\u001b[32m1.74881\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6928 | loss: 1.74881 - acc: 0.6011 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6929  | total loss: \u001b[1m\u001b[32m1.62551\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6929 | loss: 1.62551 - acc: 0.6410 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6930  | total loss: \u001b[1m\u001b[32m1.82159\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6930 | loss: 1.82159 - acc: 0.5769 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6931  | total loss: \u001b[1m\u001b[32m1.69258\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6931 | loss: 1.69258 - acc: 0.6192 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6932  | total loss: \u001b[1m\u001b[32m1.57700\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6932 | loss: 1.57700 - acc: 0.6573 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6933  | total loss: \u001b[1m\u001b[32m1.47289\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6933 | loss: 1.47289 - acc: 0.6915 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6934  | total loss: \u001b[1m\u001b[32m1.69163\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6934 | loss: 1.69163 - acc: 0.6224 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6935  | total loss: \u001b[1m\u001b[32m1.57571\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6935 | loss: 1.57571 - acc: 0.6602 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6936  | total loss: \u001b[1m\u001b[32m1.47108\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6936 | loss: 1.47108 - acc: 0.6941 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6937  | total loss: \u001b[1m\u001b[32m1.37610\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6937 | loss: 1.37610 - acc: 0.7247 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6938  | total loss: \u001b[1m\u001b[32m1.28934\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6938 | loss: 1.28934 - acc: 0.7523 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6939  | total loss: \u001b[1m\u001b[32m1.20957\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6939 | loss: 1.20957 - acc: 0.7770 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6940  | total loss: \u001b[1m\u001b[32m1.41377\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6940 | loss: 1.41377 - acc: 0.7065 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6941  | total loss: \u001b[1m\u001b[32m1.31852\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6941 | loss: 1.31852 - acc: 0.7358 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6942  | total loss: \u001b[1m\u001b[32m1.23140\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6942 | loss: 1.23140 - acc: 0.7622 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6943  | total loss: \u001b[1m\u001b[32m1.15129\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6943 | loss: 1.15129 - acc: 0.7860 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6944  | total loss: \u001b[1m\u001b[32m1.07728\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6944 | loss: 1.07728 - acc: 0.8074 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6945  | total loss: \u001b[1m\u001b[32m1.00859\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6945 | loss: 1.00859 - acc: 0.8267 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6946  | total loss: \u001b[1m\u001b[32m1.28984\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6946 | loss: 1.28984 - acc: 0.7440 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6947  | total loss: \u001b[1m\u001b[32m1.19662\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6947 | loss: 1.19662 - acc: 0.7696 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6948  | total loss: \u001b[1m\u001b[32m1.11145\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6948 | loss: 1.11145 - acc: 0.7926 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6949  | total loss: \u001b[1m\u001b[32m1.03334\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6949 | loss: 1.03334 - acc: 0.8134 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6950  | total loss: \u001b[1m\u001b[32m0.96146\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6950 | loss: 0.96146 - acc: 0.8320 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6951  | total loss: \u001b[1m\u001b[32m0.89511\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6951 | loss: 0.89511 - acc: 0.8488 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6952  | total loss: \u001b[1m\u001b[32m0.83369\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6952 | loss: 0.83369 - acc: 0.8640 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6953  | total loss: \u001b[1m\u001b[32m0.77668\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6953 | loss: 0.77668 - acc: 0.8776 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6954  | total loss: \u001b[1m\u001b[32m1.08233\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6954 | loss: 1.08233 - acc: 0.7969 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6955  | total loss: \u001b[1m\u001b[32m0.99794\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6955 | loss: 0.99794 - acc: 0.8173 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6956  | total loss: \u001b[1m\u001b[32m1.31835\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6956 | loss: 1.31835 - acc: 0.7355 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6957  | total loss: \u001b[1m\u001b[32m1.20943\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6957 | loss: 1.20943 - acc: 0.7620 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6958  | total loss: \u001b[1m\u001b[32m1.51279\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6958 | loss: 1.51279 - acc: 0.6858 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6959  | total loss: \u001b[1m\u001b[32m1.38481\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6959 | loss: 1.38481 - acc: 0.7172 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6960  | total loss: \u001b[1m\u001b[32m1.26995\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6960 | loss: 1.26995 - acc: 0.7455 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6961  | total loss: \u001b[1m\u001b[32m1.16668\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6961 | loss: 1.16668 - acc: 0.7709 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6962  | total loss: \u001b[1m\u001b[32m1.07364\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6962 | loss: 1.07364 - acc: 0.7938 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6963  | total loss: \u001b[1m\u001b[32m0.98961\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6963 | loss: 0.98961 - acc: 0.8145 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6964  | total loss: \u001b[1m\u001b[32m1.31258\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6964 | loss: 1.31258 - acc: 0.7330 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6965  | total loss: \u001b[1m\u001b[32m1.20460\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6965 | loss: 1.20460 - acc: 0.7597 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6966  | total loss: \u001b[1m\u001b[32m1.46606\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6966 | loss: 1.46606 - acc: 0.6909 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6967  | total loss: \u001b[1m\u001b[32m1.34372\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6967 | loss: 1.34372 - acc: 0.7218 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6968  | total loss: \u001b[1m\u001b[32m1.23416\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6968 | loss: 1.23416 - acc: 0.7496 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6969  | total loss: \u001b[1m\u001b[32m1.13585\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6969 | loss: 1.13585 - acc: 0.7747 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6970  | total loss: \u001b[1m\u001b[32m1.04741\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6970 | loss: 1.04741 - acc: 0.7972 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6971  | total loss: \u001b[1m\u001b[32m0.96763\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6971 | loss: 0.96763 - acc: 0.8175 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 6972  | total loss: \u001b[1m\u001b[32m1.26974\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6972 | loss: 1.26974 - acc: 0.7429 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6973  | total loss: \u001b[1m\u001b[32m1.16777\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6973 | loss: 1.16777 - acc: 0.7686 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6974  | total loss: \u001b[1m\u001b[32m1.07617\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6974 | loss: 1.07617 - acc: 0.7917 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6975  | total loss: \u001b[1m\u001b[32m0.99366\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6975 | loss: 0.99366 - acc: 0.8125 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6976  | total loss: \u001b[1m\u001b[32m0.91912\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6976 | loss: 0.91912 - acc: 0.8313 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6977  | total loss: \u001b[1m\u001b[32m0.85157\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6977 | loss: 0.85157 - acc: 0.8482 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6978  | total loss: \u001b[1m\u001b[32m1.15978\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6978 | loss: 1.15978 - acc: 0.7705 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6979  | total loss: \u001b[1m\u001b[32m1.06769\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6979 | loss: 1.06769 - acc: 0.7934 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6980  | total loss: \u001b[1m\u001b[32m0.98476\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6980 | loss: 0.98476 - acc: 0.8141 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6981  | total loss: \u001b[1m\u001b[32m0.90988\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6981 | loss: 0.90988 - acc: 0.8327 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6982  | total loss: \u001b[1m\u001b[32m1.22446\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6982 | loss: 1.22446 - acc: 0.7566 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6983  | total loss: \u001b[1m\u001b[32m1.12557\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6983 | loss: 1.12557 - acc: 0.7809 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6984  | total loss: \u001b[1m\u001b[32m1.40548\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6984 | loss: 1.40548 - acc: 0.7100 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6985  | total loss: \u001b[1m\u001b[32m1.28958\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6985 | loss: 1.28958 - acc: 0.7390 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6986  | total loss: \u001b[1m\u001b[32m1.18594\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6986 | loss: 1.18594 - acc: 0.7651 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6987  | total loss: \u001b[1m\u001b[32m1.09305\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6987 | loss: 1.09305 - acc: 0.7886 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6988  | total loss: \u001b[1m\u001b[32m1.00957\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 6988 | loss: 1.00957 - acc: 0.8097 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6989  | total loss: \u001b[1m\u001b[32m0.93431\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6989 | loss: 0.93431 - acc: 0.8287 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6990  | total loss: \u001b[1m\u001b[32m1.24448\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6990 | loss: 1.24448 - acc: 0.7530 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6991  | total loss: \u001b[1m\u001b[32m1.14589\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6991 | loss: 1.14589 - acc: 0.7777 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6992  | total loss: \u001b[1m\u001b[32m1.36863\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6992 | loss: 1.36863 - acc: 0.7214 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6993  | total loss: \u001b[1m\u001b[32m1.25871\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6993 | loss: 1.25871 - acc: 0.7492 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6994  | total loss: \u001b[1m\u001b[32m1.51245\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6994 | loss: 1.51245 - acc: 0.6814 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6995  | total loss: \u001b[1m\u001b[32m1.39003\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6995 | loss: 1.39003 - acc: 0.7133 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6996  | total loss: \u001b[1m\u001b[32m1.28081\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 6996 | loss: 1.28081 - acc: 0.7420 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6997  | total loss: \u001b[1m\u001b[32m1.18309\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6997 | loss: 1.18309 - acc: 0.7678 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6998  | total loss: \u001b[1m\u001b[32m1.09539\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 6998 | loss: 1.09539 - acc: 0.7910 -- iter: 14/14\n",
      "--\n",
      "Training Step: 6999  | total loss: \u001b[1m\u001b[32m1.01635\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 6999 | loss: 1.01635 - acc: 0.8119 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7000  | total loss: \u001b[1m\u001b[32m1.28115\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7000 | loss: 1.28115 - acc: 0.7378 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7001  | total loss: \u001b[1m\u001b[32m1.18361\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7001 | loss: 1.18361 - acc: 0.7641 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7002  | total loss: \u001b[1m\u001b[32m1.48644\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7002 | loss: 1.48644 - acc: 0.6877 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7003  | total loss: \u001b[1m\u001b[32m1.36947\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7003 | loss: 1.36947 - acc: 0.7189 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7004  | total loss: \u001b[1m\u001b[32m1.62987\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7004 | loss: 1.62987 - acc: 0.6541 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7005  | total loss: \u001b[1m\u001b[32m1.50051\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7005 | loss: 1.50051 - acc: 0.6887 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7006  | total loss: \u001b[1m\u001b[32m1.70320\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7006 | loss: 1.70320 - acc: 0.6270 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7007  | total loss: \u001b[1m\u001b[32m1.56911\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7007 | loss: 1.56911 - acc: 0.6643 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7008  | total loss: \u001b[1m\u001b[32m1.78946\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7008 | loss: 1.78946 - acc: 0.6050 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7009  | total loss: \u001b[1m\u001b[32m1.64980\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7009 | loss: 1.64980 - acc: 0.6445 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7010  | total loss: \u001b[1m\u001b[32m1.84629\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7010 | loss: 1.84629 - acc: 0.5801 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7011  | total loss: \u001b[1m\u001b[32m1.70438\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7011 | loss: 1.70438 - acc: 0.6221 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7012  | total loss: \u001b[1m\u001b[32m1.88645\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7012 | loss: 1.88645 - acc: 0.5670 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7013  | total loss: \u001b[1m\u001b[32m1.74421\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7013 | loss: 1.74421 - acc: 0.6103 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7014  | total loss: \u001b[1m\u001b[32m1.61771\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7014 | loss: 1.61771 - acc: 0.6493 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7015  | total loss: \u001b[1m\u001b[32m1.50468\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7015 | loss: 1.50468 - acc: 0.6843 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7016  | total loss: \u001b[1m\u001b[32m1.40316\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7016 | loss: 1.40316 - acc: 0.7159 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7017  | total loss: \u001b[1m\u001b[32m1.31140\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7017 | loss: 1.31140 - acc: 0.7443 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7018  | total loss: \u001b[1m\u001b[32m1.54302\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7018 | loss: 1.54302 - acc: 0.6699 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7019  | total loss: \u001b[1m\u001b[32m1.43651\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7019 | loss: 1.43651 - acc: 0.7029 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7020  | total loss: \u001b[1m\u001b[32m1.64428\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7020 | loss: 1.64428 - acc: 0.6326 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7021  | total loss: \u001b[1m\u001b[32m1.52767\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7021 | loss: 1.52767 - acc: 0.6693 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7022  | total loss: \u001b[1m\u001b[32m1.72361\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7022 | loss: 1.72361 - acc: 0.6096 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7023  | total loss: \u001b[1m\u001b[32m1.59969\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7023 | loss: 1.59969 - acc: 0.6486 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7024  | total loss: \u001b[1m\u001b[32m1.48832\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7024 | loss: 1.48832 - acc: 0.6837 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7025  | total loss: \u001b[1m\u001b[32m1.38769\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7025 | loss: 1.38769 - acc: 0.7154 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7026  | total loss: \u001b[1m\u001b[32m1.58237\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7026 | loss: 1.58237 - acc: 0.6581 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7027  | total loss: \u001b[1m\u001b[32m1.47137\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7027 | loss: 1.47137 - acc: 0.6923 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7028  | total loss: \u001b[1m\u001b[32m1.37089\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7028 | loss: 1.37089 - acc: 0.7231 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7029  | total loss: \u001b[1m\u001b[32m1.27945\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7029 | loss: 1.27945 - acc: 0.7508 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7030  | total loss: \u001b[1m\u001b[32m1.19575\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7030 | loss: 1.19575 - acc: 0.7757 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7031  | total loss: \u001b[1m\u001b[32m1.11874\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7031 | loss: 1.11874 - acc: 0.7981 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7032  | total loss: \u001b[1m\u001b[32m1.37679\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7032 | loss: 1.37679 - acc: 0.7183 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7033  | total loss: \u001b[1m\u001b[32m1.27894\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7033 | loss: 1.27894 - acc: 0.7465 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7034  | total loss: \u001b[1m\u001b[32m1.54202\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7034 | loss: 1.54202 - acc: 0.6718 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7035  | total loss: \u001b[1m\u001b[32m1.42643\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7035 | loss: 1.42643 - acc: 0.7046 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7036  | total loss: \u001b[1m\u001b[32m1.61380\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7036 | loss: 1.61380 - acc: 0.6485 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7037  | total loss: \u001b[1m\u001b[32m1.49086\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7037 | loss: 1.49086 - acc: 0.6836 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7038  | total loss: \u001b[1m\u001b[32m1.70255\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7038 | loss: 1.70255 - acc: 0.6224 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7039  | total loss: \u001b[1m\u001b[32m1.57138\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7039 | loss: 1.57138 - acc: 0.6602 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7040  | total loss: \u001b[1m\u001b[32m1.74643\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7040 | loss: 1.74643 - acc: 0.6084 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7041  | total loss: \u001b[1m\u001b[32m1.61211\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7041 | loss: 1.61211 - acc: 0.6476 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7042  | total loss: \u001b[1m\u001b[32m1.80952\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7042 | loss: 1.80952 - acc: 0.5900 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7043  | total loss: \u001b[1m\u001b[32m1.67062\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7043 | loss: 1.67062 - acc: 0.6310 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7044  | total loss: \u001b[1m\u001b[32m1.87097\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7044 | loss: 1.87097 - acc: 0.5679 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7045  | total loss: \u001b[1m\u001b[32m1.72823\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7045 | loss: 1.72823 - acc: 0.6111 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7046  | total loss: \u001b[1m\u001b[32m1.92307\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7046 | loss: 1.92307 - acc: 0.5500 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7047  | total loss: \u001b[1m\u001b[32m1.77796\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7047 | loss: 1.77796 - acc: 0.5950 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7048  | total loss: \u001b[1m\u001b[32m1.94762\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7048 | loss: 1.94762 - acc: 0.5426 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7049  | total loss: \u001b[1m\u001b[32m1.80299\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7049 | loss: 1.80299 - acc: 0.5884 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7050  | total loss: \u001b[1m\u001b[32m1.67387\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7050 | loss: 1.67387 - acc: 0.6295 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7051  | total loss: \u001b[1m\u001b[32m1.55802\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7051 | loss: 1.55802 - acc: 0.6666 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7052  | total loss: \u001b[1m\u001b[32m1.74117\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7052 | loss: 1.74117 - acc: 0.6071 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7053  | total loss: \u001b[1m\u001b[32m1.61883\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7053 | loss: 1.61883 - acc: 0.6464 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7054  | total loss: \u001b[1m\u001b[32m1.77415\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7054 | loss: 1.77415 - acc: 0.5960 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7055  | total loss: \u001b[1m\u001b[32m1.64889\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7055 | loss: 1.64889 - acc: 0.6364 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7056  | total loss: \u001b[1m\u001b[32m1.83592\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7056 | loss: 1.83592 - acc: 0.5799 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7057  | total loss: \u001b[1m\u001b[32m1.70503\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7057 | loss: 1.70503 - acc: 0.6219 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7058  | total loss: \u001b[1m\u001b[32m1.81616\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7058 | loss: 1.81616 - acc: 0.5812 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7059  | total loss: \u001b[1m\u001b[32m1.68782\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7059 | loss: 1.68782 - acc: 0.6230 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7060  | total loss: \u001b[1m\u001b[32m1.82587\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7060 | loss: 1.82587 - acc: 0.5822 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7061  | total loss: \u001b[1m\u001b[32m1.69694\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7061 | loss: 1.69694 - acc: 0.6239 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7062  | total loss: \u001b[1m\u001b[32m1.84456\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7062 | loss: 1.84456 - acc: 0.5758 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7063  | total loss: \u001b[1m\u001b[32m1.71401\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7063 | loss: 1.71401 - acc: 0.6183 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7064  | total loss: \u001b[1m\u001b[32m1.85825\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7064 | loss: 1.85825 - acc: 0.5707 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7065  | total loss: \u001b[1m\u001b[32m1.72669\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7065 | loss: 1.72669 - acc: 0.6136 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7066  | total loss: \u001b[1m\u001b[32m1.90159\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7066 | loss: 1.90159 - acc: 0.5523 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7067  | total loss: \u001b[1m\u001b[32m1.76641\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7067 | loss: 1.76641 - acc: 0.5971 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7068  | total loss: \u001b[1m\u001b[32m1.92818\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7068 | loss: 1.92818 - acc: 0.5373 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7069  | total loss: \u001b[1m\u001b[32m1.79142\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7069 | loss: 1.79142 - acc: 0.5836 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7070  | total loss: \u001b[1m\u001b[32m1.92724\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7070 | loss: 1.92724 - acc: 0.5324 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7071  | total loss: \u001b[1m\u001b[32m1.79181\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7071 | loss: 1.79181 - acc: 0.5792 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7072  | total loss: \u001b[1m\u001b[32m1.91749\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7072 | loss: 1.91749 - acc: 0.5355 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7073  | total loss: \u001b[1m\u001b[32m1.78416\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7073 | loss: 1.78416 - acc: 0.5820 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7074  | total loss: \u001b[1m\u001b[32m1.95268\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7074 | loss: 1.95268 - acc: 0.5238 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7075  | total loss: \u001b[1m\u001b[32m1.81696\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7075 | loss: 1.81696 - acc: 0.5714 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7076  | total loss: \u001b[1m\u001b[32m1.93153\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7076 | loss: 1.93153 - acc: 0.5285 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7077  | total loss: \u001b[1m\u001b[32m1.79905\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7077 | loss: 1.79905 - acc: 0.5757 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7078  | total loss: \u001b[1m\u001b[32m1.96176\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7078 | loss: 1.96176 - acc: 0.5181 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7079  | total loss: \u001b[1m\u001b[32m1.82740\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7079 | loss: 1.82740 - acc: 0.5663 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7080  | total loss: \u001b[1m\u001b[32m2.00520\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7080 | loss: 2.00520 - acc: 0.5097 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7081  | total loss: \u001b[1m\u001b[32m1.86781\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7081 | loss: 1.86781 - acc: 0.5587 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7082  | total loss: \u001b[1m\u001b[32m2.00496\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7082 | loss: 2.00496 - acc: 0.5100 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7083  | total loss: \u001b[1m\u001b[32m1.86887\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7083 | loss: 1.86887 - acc: 0.5590 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7084  | total loss: \u001b[1m\u001b[32m1.98960\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 7084 | loss: 1.98960 - acc: 0.5102 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7085  | total loss: \u001b[1m\u001b[32m1.85617\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 7085 | loss: 1.85617 - acc: 0.5592 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7086  | total loss: \u001b[1m\u001b[32m1.96704\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7086 | loss: 1.96704 - acc: 0.5176 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7087  | total loss: \u001b[1m\u001b[32m1.83664\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7087 | loss: 1.83664 - acc: 0.5658 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7088  | total loss: \u001b[1m\u001b[32m1.71918\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7088 | loss: 1.71918 - acc: 0.6092 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7089  | total loss: \u001b[1m\u001b[32m1.61268\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7089 | loss: 1.61268 - acc: 0.6483 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7090  | total loss: \u001b[1m\u001b[32m1.51545\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7090 | loss: 1.51545 - acc: 0.6835 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7091  | total loss: \u001b[1m\u001b[32m1.42605\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 7091 | loss: 1.42605 - acc: 0.7151 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7092  | total loss: \u001b[1m\u001b[32m1.55001\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7092 | loss: 1.55001 - acc: 0.6722 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7093  | total loss: \u001b[1m\u001b[32m1.45320\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 7093 | loss: 1.45320 - acc: 0.7050 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7094  | total loss: \u001b[1m\u001b[32m1.65580\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7094 | loss: 1.65580 - acc: 0.6345 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7095  | total loss: \u001b[1m\u001b[32m1.54535\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 7095 | loss: 1.54535 - acc: 0.6710 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7096  | total loss: \u001b[1m\u001b[32m1.44449\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7096 | loss: 1.44449 - acc: 0.7039 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7097  | total loss: \u001b[1m\u001b[32m1.35186\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7097 | loss: 1.35186 - acc: 0.7335 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7098  | total loss: \u001b[1m\u001b[32m1.26635\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7098 | loss: 1.26635 - acc: 0.7602 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7099  | total loss: \u001b[1m\u001b[32m1.18699\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7099 | loss: 1.18699 - acc: 0.7842 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7100  | total loss: \u001b[1m\u001b[32m1.43342\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7100 | loss: 1.43342 - acc: 0.7057 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7101  | total loss: \u001b[1m\u001b[32m1.33337\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7101 | loss: 1.33337 - acc: 0.7352 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7102  | total loss: \u001b[1m\u001b[32m1.53269\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7102 | loss: 1.53269 - acc: 0.6759 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7103  | total loss: \u001b[1m\u001b[32m1.42029\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7103 | loss: 1.42029 - acc: 0.7083 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7104  | total loss: \u001b[1m\u001b[32m1.31807\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7104 | loss: 1.31807 - acc: 0.7375 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7105  | total loss: \u001b[1m\u001b[32m1.22476\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7105 | loss: 1.22476 - acc: 0.7638 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7106  | total loss: \u001b[1m\u001b[32m1.13926\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7106 | loss: 1.13926 - acc: 0.7874 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7107  | total loss: \u001b[1m\u001b[32m1.06061\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7107 | loss: 1.06061 - acc: 0.8086 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7108  | total loss: \u001b[1m\u001b[32m1.33598\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7108 | loss: 1.33598 - acc: 0.7278 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7109  | total loss: \u001b[1m\u001b[32m1.23513\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7109 | loss: 1.23513 - acc: 0.7550 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7110  | total loss: \u001b[1m\u001b[32m1.49044\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7110 | loss: 1.49044 - acc: 0.6795 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7111  | total loss: \u001b[1m\u001b[32m1.37319\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7111 | loss: 1.37319 - acc: 0.7116 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7112  | total loss: \u001b[1m\u001b[32m1.61785\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7112 | loss: 1.61785 - acc: 0.6404 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7113  | total loss: \u001b[1m\u001b[32m1.48817\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7113 | loss: 1.48817 - acc: 0.6764 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7114  | total loss: \u001b[1m\u001b[32m1.37173\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7114 | loss: 1.37173 - acc: 0.7087 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7115  | total loss: \u001b[1m\u001b[32m1.26688\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7115 | loss: 1.26688 - acc: 0.7378 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7116  | total loss: \u001b[1m\u001b[32m1.51101\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7116 | loss: 1.51101 - acc: 0.6712 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7117  | total loss: \u001b[1m\u001b[32m1.39232\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 7117 | loss: 1.39232 - acc: 0.7041 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7118  | total loss: \u001b[1m\u001b[32m1.60833\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7118 | loss: 1.60833 - acc: 0.6408 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7119  | total loss: \u001b[1m\u001b[32m1.48086\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7119 | loss: 1.48086 - acc: 0.6767 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7120  | total loss: \u001b[1m\u001b[32m1.68155\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7120 | loss: 1.68155 - acc: 0.6234 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7121  | total loss: \u001b[1m\u001b[32m1.54835\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7121 | loss: 1.54835 - acc: 0.6610 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7122  | total loss: \u001b[1m\u001b[32m1.74857\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 7122 | loss: 1.74857 - acc: 0.6092 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7123  | total loss: \u001b[1m\u001b[32m1.61071\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7123 | loss: 1.61071 - acc: 0.6483 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7124  | total loss: \u001b[1m\u001b[32m1.82663\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7124 | loss: 1.82663 - acc: 0.5906 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7125  | total loss: \u001b[1m\u001b[32m1.68345\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7125 | loss: 1.68345 - acc: 0.6315 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7126  | total loss: \u001b[1m\u001b[32m1.84318\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7126 | loss: 1.84318 - acc: 0.5827 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7127  | total loss: \u001b[1m\u001b[32m1.70103\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7127 | loss: 1.70103 - acc: 0.6244 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7128  | total loss: \u001b[1m\u001b[32m1.57413\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7128 | loss: 1.57413 - acc: 0.6620 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7129  | total loss: \u001b[1m\u001b[32m1.46039\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7129 | loss: 1.46039 - acc: 0.6958 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7130  | total loss: \u001b[1m\u001b[32m1.67029\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7130 | loss: 1.67029 - acc: 0.6262 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7131  | total loss: \u001b[1m\u001b[32m1.54772\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7131 | loss: 1.54772 - acc: 0.6636 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7132  | total loss: \u001b[1m\u001b[32m1.72508\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7132 | loss: 1.72508 - acc: 0.6115 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7133  | total loss: \u001b[1m\u001b[32m1.59832\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7133 | loss: 1.59832 - acc: 0.6503 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7134  | total loss: \u001b[1m\u001b[32m1.48464\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7134 | loss: 1.48464 - acc: 0.6853 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7135  | total loss: \u001b[1m\u001b[32m1.38220\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7135 | loss: 1.38220 - acc: 0.7168 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7136  | total loss: \u001b[1m\u001b[32m1.28938\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7136 | loss: 1.28938 - acc: 0.7451 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7137  | total loss: \u001b[1m\u001b[32m1.20481\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7137 | loss: 1.20481 - acc: 0.7706 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7138  | total loss: \u001b[1m\u001b[32m1.43704\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7138 | loss: 1.43704 - acc: 0.7007 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7139  | total loss: \u001b[1m\u001b[32m1.33579\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7139 | loss: 1.33579 - acc: 0.7306 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7140  | total loss: \u001b[1m\u001b[32m1.53395\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 7140 | loss: 1.53395 - acc: 0.6718 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7141  | total loss: \u001b[1m\u001b[32m1.42192\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7141 | loss: 1.42192 - acc: 0.7047 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7142  | total loss: \u001b[1m\u001b[32m1.64864\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7142 | loss: 1.64864 - acc: 0.6413 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7143  | total loss: \u001b[1m\u001b[32m1.52491\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7143 | loss: 1.52491 - acc: 0.6772 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7144  | total loss: \u001b[1m\u001b[32m1.71726\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7144 | loss: 1.71726 - acc: 0.6238 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7145  | total loss: \u001b[1m\u001b[32m1.58719\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7145 | loss: 1.58719 - acc: 0.6614 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7146  | total loss: \u001b[1m\u001b[32m1.47029\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7146 | loss: 1.47029 - acc: 0.6952 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7147  | total loss: \u001b[1m\u001b[32m1.36479\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7147 | loss: 1.36479 - acc: 0.7257 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7148  | total loss: \u001b[1m\u001b[32m1.61702\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7148 | loss: 1.61702 - acc: 0.6532 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7149  | total loss: \u001b[1m\u001b[32m1.49649\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7149 | loss: 1.49649 - acc: 0.6878 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7150  | total loss: \u001b[1m\u001b[32m1.72609\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7150 | loss: 1.72609 - acc: 0.6191 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7151  | total loss: \u001b[1m\u001b[32m1.59529\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7151 | loss: 1.59529 - acc: 0.6571 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7152  | total loss: \u001b[1m\u001b[32m1.76845\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7152 | loss: 1.76845 - acc: 0.5986 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7153  | total loss: \u001b[1m\u001b[32m1.63477\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7153 | loss: 1.63477 - acc: 0.6387 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7154  | total loss: \u001b[1m\u001b[32m1.51502\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7154 | loss: 1.51502 - acc: 0.6748 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7155  | total loss: \u001b[1m\u001b[32m1.40731\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7155 | loss: 1.40731 - acc: 0.7074 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7156  | total loss: \u001b[1m\u001b[32m1.64200\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7156 | loss: 1.64200 - acc: 0.6366 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7157  | total loss: \u001b[1m\u001b[32m1.52177\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7157 | loss: 1.52177 - acc: 0.6730 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7158  | total loss: \u001b[1m\u001b[32m1.41362\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7158 | loss: 1.41362 - acc: 0.7057 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7159  | total loss: \u001b[1m\u001b[32m1.31587\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7159 | loss: 1.31587 - acc: 0.7351 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7160  | total loss: \u001b[1m\u001b[32m1.22707\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7160 | loss: 1.22707 - acc: 0.7616 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7161  | total loss: \u001b[1m\u001b[32m1.14596\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7161 | loss: 1.14596 - acc: 0.7854 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7162  | total loss: \u001b[1m\u001b[32m1.07148\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7162 | loss: 1.07148 - acc: 0.8069 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7163  | total loss: \u001b[1m\u001b[32m1.00273\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7163 | loss: 1.00273 - acc: 0.8262 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7164  | total loss: \u001b[1m\u001b[32m0.93896\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7164 | loss: 0.93896 - acc: 0.8436 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7165  | total loss: \u001b[1m\u001b[32m0.87955\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7165 | loss: 0.87955 - acc: 0.8592 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7166  | total loss: \u001b[1m\u001b[32m1.15192\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7166 | loss: 1.15192 - acc: 0.7804 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7167  | total loss: \u001b[1m\u001b[32m1.06808\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7167 | loss: 1.06808 - acc: 0.8024 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7168  | total loss: \u001b[1m\u001b[32m1.32257\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7168 | loss: 1.32257 - acc: 0.7293 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7169  | total loss: \u001b[1m\u001b[32m1.22016\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7169 | loss: 1.22016 - acc: 0.7564 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7170  | total loss: \u001b[1m\u001b[32m1.12745\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7170 | loss: 1.12745 - acc: 0.7807 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7171  | total loss: \u001b[1m\u001b[32m1.04326\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7171 | loss: 1.04326 - acc: 0.8027 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7172  | total loss: \u001b[1m\u001b[32m1.26541\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7172 | loss: 1.26541 - acc: 0.7438 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7173  | total loss: \u001b[1m\u001b[32m1.16628\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7173 | loss: 1.16628 - acc: 0.7694 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7174  | total loss: \u001b[1m\u001b[32m1.44563\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7174 | loss: 1.44563 - acc: 0.6996 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7175  | total loss: \u001b[1m\u001b[32m1.32844\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7175 | loss: 1.32844 - acc: 0.7297 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7176  | total loss: \u001b[1m\u001b[32m1.22310\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7176 | loss: 1.22310 - acc: 0.7567 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7177  | total loss: \u001b[1m\u001b[32m1.12816\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7177 | loss: 1.12816 - acc: 0.7810 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7178  | total loss: \u001b[1m\u001b[32m1.38860\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7178 | loss: 1.38860 - acc: 0.7101 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7179  | total loss: \u001b[1m\u001b[32m1.27715\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7179 | loss: 1.27715 - acc: 0.7391 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7180  | total loss: \u001b[1m\u001b[32m1.58559\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7180 | loss: 1.58559 - acc: 0.6652 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7181  | total loss: \u001b[1m\u001b[32m1.45558\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7181 | loss: 1.45558 - acc: 0.6986 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7182  | total loss: \u001b[1m\u001b[32m1.64155\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7182 | loss: 1.64155 - acc: 0.6502 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7183  | total loss: \u001b[1m\u001b[32m1.50784\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7183 | loss: 1.50784 - acc: 0.6852 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7184  | total loss: \u001b[1m\u001b[32m1.38835\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7184 | loss: 1.38835 - acc: 0.7167 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7185  | total loss: \u001b[1m\u001b[32m1.28127\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7185 | loss: 1.28127 - acc: 0.7450 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7186  | total loss: \u001b[1m\u001b[32m1.18502\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7186 | loss: 1.18502 - acc: 0.7705 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7187  | total loss: \u001b[1m\u001b[32m1.09819\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7187 | loss: 1.09819 - acc: 0.7935 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7188  | total loss: \u001b[1m\u001b[32m1.01955\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7188 | loss: 1.01955 - acc: 0.8141 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7189  | total loss: \u001b[1m\u001b[32m0.94804\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7189 | loss: 0.94804 - acc: 0.8327 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7190  | total loss: \u001b[1m\u001b[32m1.25558\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7190 | loss: 1.25558 - acc: 0.7494 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7191  | total loss: \u001b[1m\u001b[32m1.15954\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7191 | loss: 1.15954 - acc: 0.7745 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7192  | total loss: \u001b[1m\u001b[32m1.07286\u001b[0m\u001b[0m | time: 0.034s\n",
      "| Adam | epoch: 7192 | loss: 1.07286 - acc: 0.7970 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7193  | total loss: \u001b[1m\u001b[32m0.99436\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 7193 | loss: 0.99436 - acc: 0.8173 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7194  | total loss: \u001b[1m\u001b[32m1.30473\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7194 | loss: 1.30473 - acc: 0.7356 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7195  | total loss: \u001b[1m\u001b[32m1.20250\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7195 | loss: 1.20250 - acc: 0.7620 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7196  | total loss: \u001b[1m\u001b[32m1.47616\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 7196 | loss: 1.47616 - acc: 0.6858 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7197  | total loss: \u001b[1m\u001b[32m1.35740\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7197 | loss: 1.35740 - acc: 0.7173 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7198  | total loss: \u001b[1m\u001b[32m1.55664\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7198 | loss: 1.55664 - acc: 0.6598 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7199  | total loss: \u001b[1m\u001b[32m1.43133\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7199 | loss: 1.43133 - acc: 0.6938 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7200  | total loss: \u001b[1m\u001b[32m1.65968\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7200 | loss: 1.65968 - acc: 0.6316 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7201  | total loss: \u001b[1m\u001b[32m1.52626\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7201 | loss: 1.52626 - acc: 0.6684 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7202  | total loss: \u001b[1m\u001b[32m1.74451\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7202 | loss: 1.74451 - acc: 0.6159 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7203  | total loss: \u001b[1m\u001b[32m1.60544\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7203 | loss: 1.60544 - acc: 0.6543 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7204  | total loss: \u001b[1m\u001b[32m1.82938\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7204 | loss: 1.82938 - acc: 0.5889 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7205  | total loss: \u001b[1m\u001b[32m1.68520\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7205 | loss: 1.68520 - acc: 0.6300 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7206  | total loss: \u001b[1m\u001b[32m1.90240\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7206 | loss: 1.90240 - acc: 0.5670 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7207  | total loss: \u001b[1m\u001b[32m1.75470\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7207 | loss: 1.75470 - acc: 0.6103 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7208  | total loss: \u001b[1m\u001b[32m1.88711\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7208 | loss: 1.88711 - acc: 0.5635 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7209  | total loss: \u001b[1m\u001b[32m1.74471\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7209 | loss: 1.74471 - acc: 0.6072 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7210  | total loss: \u001b[1m\u001b[32m1.87781\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7210 | loss: 1.87781 - acc: 0.5607 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7211  | total loss: \u001b[1m\u001b[32m1.73986\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7211 | loss: 1.73986 - acc: 0.6047 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7212  | total loss: \u001b[1m\u001b[32m1.61705\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7212 | loss: 1.61705 - acc: 0.6442 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7213  | total loss: \u001b[1m\u001b[32m1.50716\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7213 | loss: 1.50716 - acc: 0.6798 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7214  | total loss: \u001b[1m\u001b[32m1.68936\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7214 | loss: 1.68936 - acc: 0.6190 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7215  | total loss: \u001b[1m\u001b[32m1.57297\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7215 | loss: 1.57297 - acc: 0.6571 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7216  | total loss: \u001b[1m\u001b[32m1.74675\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7216 | loss: 1.74675 - acc: 0.5985 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7217  | total loss: \u001b[1m\u001b[32m1.62551\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7217 | loss: 1.62551 - acc: 0.6386 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7218  | total loss: \u001b[1m\u001b[32m1.51656\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7218 | loss: 1.51656 - acc: 0.6748 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7219  | total loss: \u001b[1m\u001b[32m1.41806\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7219 | loss: 1.41806 - acc: 0.7073 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7220  | total loss: \u001b[1m\u001b[32m1.59842\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7220 | loss: 1.59842 - acc: 0.6509 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7221  | total loss: \u001b[1m\u001b[32m1.49050\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7221 | loss: 1.49050 - acc: 0.6858 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7222  | total loss: \u001b[1m\u001b[32m1.67847\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7222 | loss: 1.67847 - acc: 0.6243 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7223  | total loss: \u001b[1m\u001b[32m1.56184\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7223 | loss: 1.56184 - acc: 0.6619 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7224  | total loss: \u001b[1m\u001b[32m1.69774\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7224 | loss: 1.69774 - acc: 0.6171 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7225  | total loss: \u001b[1m\u001b[32m1.57881\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7225 | loss: 1.57881 - acc: 0.6554 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7226  | total loss: \u001b[1m\u001b[32m1.77312\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7226 | loss: 1.77312 - acc: 0.5899 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7227  | total loss: \u001b[1m\u001b[32m1.64669\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7227 | loss: 1.64669 - acc: 0.6309 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7228  | total loss: \u001b[1m\u001b[32m1.53282\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7228 | loss: 1.53282 - acc: 0.6678 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7229  | total loss: \u001b[1m\u001b[32m1.42973\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7229 | loss: 1.42973 - acc: 0.7010 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7230  | total loss: \u001b[1m\u001b[32m1.33590\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7230 | loss: 1.33590 - acc: 0.7309 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7231  | total loss: \u001b[1m\u001b[32m1.25000\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7231 | loss: 1.25000 - acc: 0.7578 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7232  | total loss: \u001b[1m\u001b[32m1.46517\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7232 | loss: 1.46517 - acc: 0.6892 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7233  | total loss: \u001b[1m\u001b[32m1.36379\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7233 | loss: 1.36379 - acc: 0.7203 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7234  | total loss: \u001b[1m\u001b[32m1.57721\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7234 | loss: 1.57721 - acc: 0.6554 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7235  | total loss: \u001b[1m\u001b[32m1.46325\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7235 | loss: 1.46325 - acc: 0.6898 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7236  | total loss: \u001b[1m\u001b[32m1.65734\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7236 | loss: 1.65734 - acc: 0.6280 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7237  | total loss: \u001b[1m\u001b[32m1.53483\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7237 | loss: 1.53483 - acc: 0.6652 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7238  | total loss: \u001b[1m\u001b[32m1.75746\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7238 | loss: 1.75746 - acc: 0.5987 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7239  | total loss: \u001b[1m\u001b[32m1.62519\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7239 | loss: 1.62519 - acc: 0.6388 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7240  | total loss: \u001b[1m\u001b[32m1.81476\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7240 | loss: 1.81476 - acc: 0.5821 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7241  | total loss: \u001b[1m\u001b[32m1.67769\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7241 | loss: 1.67769 - acc: 0.6239 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7242  | total loss: \u001b[1m\u001b[32m1.87448\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7242 | loss: 1.87448 - acc: 0.5615 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7243  | total loss: \u001b[1m\u001b[32m1.73284\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7243 | loss: 1.73284 - acc: 0.6053 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7244  | total loss: \u001b[1m\u001b[32m1.91684\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7244 | loss: 1.91684 - acc: 0.5448 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7245  | total loss: \u001b[1m\u001b[32m1.77283\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7245 | loss: 1.77283 - acc: 0.5903 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7246  | total loss: \u001b[1m\u001b[32m1.88788\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7246 | loss: 1.88788 - acc: 0.5456 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7247  | total loss: \u001b[1m\u001b[32m1.74877\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7247 | loss: 1.74877 - acc: 0.5910 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7248  | total loss: \u001b[1m\u001b[32m1.94219\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7248 | loss: 1.94219 - acc: 0.5319 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7249  | total loss: \u001b[1m\u001b[32m1.79986\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7249 | loss: 1.79986 - acc: 0.5787 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7250  | total loss: \u001b[1m\u001b[32m1.96011\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7250 | loss: 1.96011 - acc: 0.5280 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7251  | total loss: \u001b[1m\u001b[32m1.81847\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7251 | loss: 1.81847 - acc: 0.5752 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7252  | total loss: \u001b[1m\u001b[32m1.96873\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7252 | loss: 1.96873 - acc: 0.5248 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7253  | total loss: \u001b[1m\u001b[32m1.82872\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7253 | loss: 1.82872 - acc: 0.5723 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7254  | total loss: \u001b[1m\u001b[32m1.70357\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7254 | loss: 1.70357 - acc: 0.6151 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7255  | total loss: \u001b[1m\u001b[32m1.59109\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7255 | loss: 1.59109 - acc: 0.6536 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7256  | total loss: \u001b[1m\u001b[32m1.78660\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7256 | loss: 1.78660 - acc: 0.5882 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7257  | total loss: \u001b[1m\u001b[32m1.66570\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7257 | loss: 1.66570 - acc: 0.6294 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7258  | total loss: \u001b[1m\u001b[32m1.81897\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7258 | loss: 1.81897 - acc: 0.5736 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7259  | total loss: \u001b[1m\u001b[32m1.69498\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7259 | loss: 1.69498 - acc: 0.6163 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7260  | total loss: \u001b[1m\u001b[32m1.86950\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7260 | loss: 1.86950 - acc: 0.5546 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7261  | total loss: \u001b[1m\u001b[32m1.74088\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7261 | loss: 1.74088 - acc: 0.5992 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7262  | total loss: \u001b[1m\u001b[32m1.89010\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7262 | loss: 1.89010 - acc: 0.5535 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7263  | total loss: \u001b[1m\u001b[32m1.75993\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7263 | loss: 1.75993 - acc: 0.5982 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7264  | total loss: \u001b[1m\u001b[32m1.64264\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7264 | loss: 1.64264 - acc: 0.6384 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7265  | total loss: \u001b[1m\u001b[32m1.53636\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7265 | loss: 1.53636 - acc: 0.6745 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7266  | total loss: \u001b[1m\u001b[32m1.43945\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7266 | loss: 1.43945 - acc: 0.7071 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7267  | total loss: \u001b[1m\u001b[32m1.35054\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7267 | loss: 1.35054 - acc: 0.7364 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7268  | total loss: \u001b[1m\u001b[32m1.56536\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7268 | loss: 1.56536 - acc: 0.6627 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7269  | total loss: \u001b[1m\u001b[32m1.46086\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7269 | loss: 1.46086 - acc: 0.6965 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7270  | total loss: \u001b[1m\u001b[32m1.68552\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7270 | loss: 1.68552 - acc: 0.6268 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7271  | total loss: \u001b[1m\u001b[32m1.56727\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7271 | loss: 1.56727 - acc: 0.6641 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7272  | total loss: \u001b[1m\u001b[32m1.72625\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7272 | loss: 1.72625 - acc: 0.6120 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7273  | total loss: \u001b[1m\u001b[32m1.60304\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7273 | loss: 1.60304 - acc: 0.6508 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7274  | total loss: \u001b[1m\u001b[32m1.79648\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7274 | loss: 1.79648 - acc: 0.5857 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7275  | total loss: \u001b[1m\u001b[32m1.66602\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7275 | loss: 1.66602 - acc: 0.6271 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7276  | total loss: \u001b[1m\u001b[32m1.85994\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7276 | loss: 1.85994 - acc: 0.5644 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7277  | total loss: \u001b[1m\u001b[32m1.72355\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7277 | loss: 1.72355 - acc: 0.6080 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7278  | total loss: \u001b[1m\u001b[32m1.91887\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7278 | loss: 1.91887 - acc: 0.5472 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7279  | total loss: \u001b[1m\u001b[32m1.77758\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7279 | loss: 1.77758 - acc: 0.5925 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7280  | total loss: \u001b[1m\u001b[32m1.88625\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7280 | loss: 1.88625 - acc: 0.5547 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7281  | total loss: \u001b[1m\u001b[32m1.74942\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7281 | loss: 1.74942 - acc: 0.5992 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7282  | total loss: \u001b[1m\u001b[32m1.62654\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7282 | loss: 1.62654 - acc: 0.6393 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7283  | total loss: \u001b[1m\u001b[32m1.51567\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7283 | loss: 1.51567 - acc: 0.6753 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7284  | total loss: \u001b[1m\u001b[32m1.41511\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7284 | loss: 1.41511 - acc: 0.7078 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7285  | total loss: \u001b[1m\u001b[32m1.32339\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7285 | loss: 1.32339 - acc: 0.7370 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7286  | total loss: \u001b[1m\u001b[32m1.55778\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7286 | loss: 1.55778 - acc: 0.6633 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7287  | total loss: \u001b[1m\u001b[32m1.44954\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7287 | loss: 1.44954 - acc: 0.6970 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7288  | total loss: \u001b[1m\u001b[32m1.66213\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7288 | loss: 1.66213 - acc: 0.6273 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7289  | total loss: \u001b[1m\u001b[32m1.54227\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7289 | loss: 1.54227 - acc: 0.6646 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7290  | total loss: \u001b[1m\u001b[32m1.43385\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7290 | loss: 1.43385 - acc: 0.6981 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7291  | total loss: \u001b[1m\u001b[32m1.33530\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7291 | loss: 1.33530 - acc: 0.7283 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7292  | total loss: \u001b[1m\u001b[32m1.57135\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7292 | loss: 1.57135 - acc: 0.6555 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7293  | total loss: \u001b[1m\u001b[32m1.45750\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7293 | loss: 1.45750 - acc: 0.6899 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7294  | total loss: \u001b[1m\u001b[32m1.35437\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7294 | loss: 1.35437 - acc: 0.7209 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7295  | total loss: \u001b[1m\u001b[32m1.26054\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7295 | loss: 1.26054 - acc: 0.7488 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7296  | total loss: \u001b[1m\u001b[32m1.45831\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7296 | loss: 1.45831 - acc: 0.6882 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7297  | total loss: \u001b[1m\u001b[32m1.35222\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7297 | loss: 1.35222 - acc: 0.7194 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7298  | total loss: \u001b[1m\u001b[32m1.61735\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7298 | loss: 1.61735 - acc: 0.6475 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7299  | total loss: \u001b[1m\u001b[32m1.49456\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7299 | loss: 1.49456 - acc: 0.6827 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7300  | total loss: \u001b[1m\u001b[32m1.73122\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7300 | loss: 1.73122 - acc: 0.6145 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7301  | total loss: \u001b[1m\u001b[32m1.59732\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7301 | loss: 1.59732 - acc: 0.6530 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7302  | total loss: \u001b[1m\u001b[32m1.80159\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7302 | loss: 1.80159 - acc: 0.5948 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7303  | total loss: \u001b[1m\u001b[32m1.66169\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 7303 | loss: 1.66169 - acc: 0.6354 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7304  | total loss: \u001b[1m\u001b[32m1.82188\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7304 | loss: 1.82188 - acc: 0.5861 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7305  | total loss: \u001b[1m\u001b[32m1.68142\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7305 | loss: 1.68142 - acc: 0.6275 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7306  | total loss: \u001b[1m\u001b[32m1.55555\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7306 | loss: 1.55555 - acc: 0.6648 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7307  | total loss: \u001b[1m\u001b[32m1.44235\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7307 | loss: 1.44235 - acc: 0.6983 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7308  | total loss: \u001b[1m\u001b[32m1.68274\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 7308 | loss: 1.68274 - acc: 0.6284 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7309  | total loss: \u001b[1m\u001b[32m1.55706\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7309 | loss: 1.55706 - acc: 0.6656 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7310  | total loss: \u001b[1m\u001b[32m1.44406\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7310 | loss: 1.44406 - acc: 0.6990 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7311  | total loss: \u001b[1m\u001b[32m1.34203\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7311 | loss: 1.34203 - acc: 0.7291 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7312  | total loss: \u001b[1m\u001b[32m1.24951\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7312 | loss: 1.24951 - acc: 0.7562 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7313  | total loss: \u001b[1m\u001b[32m1.16521\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7313 | loss: 1.16521 - acc: 0.7806 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7314  | total loss: \u001b[1m\u001b[32m1.08802\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7314 | loss: 1.08802 - acc: 0.8025 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7315  | total loss: \u001b[1m\u001b[32m1.01702\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7315 | loss: 1.01702 - acc: 0.8223 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7316  | total loss: \u001b[1m\u001b[32m0.95141\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7316 | loss: 0.95141 - acc: 0.8401 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7317  | total loss: \u001b[1m\u001b[32m0.89053\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7317 | loss: 0.89053 - acc: 0.8561 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7318  | total loss: \u001b[1m\u001b[32m1.16771\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7318 | loss: 1.16771 - acc: 0.7776 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7319  | total loss: \u001b[1m\u001b[32m1.08232\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7319 | loss: 1.08232 - acc: 0.7998 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7320  | total loss: \u001b[1m\u001b[32m1.34455\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7320 | loss: 1.34455 - acc: 0.7270 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7321  | total loss: \u001b[1m\u001b[32m1.24010\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7321 | loss: 1.24010 - acc: 0.7543 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7322  | total loss: \u001b[1m\u001b[32m1.14560\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7322 | loss: 1.14560 - acc: 0.7789 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7323  | total loss: \u001b[1m\u001b[32m1.05987\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7323 | loss: 1.05987 - acc: 0.8010 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7324  | total loss: \u001b[1m\u001b[32m0.98185\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7324 | loss: 0.98185 - acc: 0.8209 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7325  | total loss: \u001b[1m\u001b[32m0.91063\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7325 | loss: 0.91063 - acc: 0.8388 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7326  | total loss: \u001b[1m\u001b[32m0.84543\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7326 | loss: 0.84543 - acc: 0.8549 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7327  | total loss: \u001b[1m\u001b[32m0.78556\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7327 | loss: 0.78556 - acc: 0.8694 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7328  | total loss: \u001b[1m\u001b[32m0.73044\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7328 | loss: 0.73044 - acc: 0.8825 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7329  | total loss: \u001b[1m\u001b[32m0.67956\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7329 | loss: 0.67956 - acc: 0.8942 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7330  | total loss: \u001b[1m\u001b[32m1.02648\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7330 | loss: 1.02648 - acc: 0.8120 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7331  | total loss: \u001b[1m\u001b[32m0.94431\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7331 | loss: 0.94431 - acc: 0.8308 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7332  | total loss: \u001b[1m\u001b[32m1.23182\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7332 | loss: 1.23182 - acc: 0.7620 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7333  | total loss: \u001b[1m\u001b[32m1.12880\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7333 | loss: 1.12880 - acc: 0.7858 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7334  | total loss: \u001b[1m\u001b[32m1.42474\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7334 | loss: 1.42474 - acc: 0.7143 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7335  | total loss: \u001b[1m\u001b[32m1.30320\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7335 | loss: 1.30320 - acc: 0.7429 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7336  | total loss: \u001b[1m\u001b[32m1.52414\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7336 | loss: 1.52414 - acc: 0.6900 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7337  | total loss: \u001b[1m\u001b[32m1.39424\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7337 | loss: 1.39424 - acc: 0.7210 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7338  | total loss: \u001b[1m\u001b[32m1.64465\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7338 | loss: 1.64465 - acc: 0.6561 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7339  | total loss: \u001b[1m\u001b[32m1.50499\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7339 | loss: 1.50499 - acc: 0.6905 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7340  | total loss: \u001b[1m\u001b[32m1.72513\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7340 | loss: 1.72513 - acc: 0.6357 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7341  | total loss: \u001b[1m\u001b[32m1.58037\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7341 | loss: 1.58037 - acc: 0.6721 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7342  | total loss: \u001b[1m\u001b[32m1.45153\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7342 | loss: 1.45153 - acc: 0.7049 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7343  | total loss: \u001b[1m\u001b[32m1.33665\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7343 | loss: 1.33665 - acc: 0.7344 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7344  | total loss: \u001b[1m\u001b[32m1.59388\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7344 | loss: 1.59388 - acc: 0.6610 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7345  | total loss: \u001b[1m\u001b[32m1.46689\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7345 | loss: 1.46689 - acc: 0.6949 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7346  | total loss: \u001b[1m\u001b[32m1.61903\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7346 | loss: 1.61903 - acc: 0.6468 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7347  | total loss: \u001b[1m\u001b[32m1.49207\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7347 | loss: 1.49207 - acc: 0.6821 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7348  | total loss: \u001b[1m\u001b[32m1.75332\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7348 | loss: 1.75332 - acc: 0.6139 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7349  | total loss: \u001b[1m\u001b[32m1.61584\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7349 | loss: 1.61584 - acc: 0.6525 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7350  | total loss: \u001b[1m\u001b[32m1.81230\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7350 | loss: 1.81230 - acc: 0.5944 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7351  | total loss: \u001b[1m\u001b[32m1.67228\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7351 | loss: 1.67228 - acc: 0.6350 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7352  | total loss: \u001b[1m\u001b[32m1.86371\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7352 | loss: 1.86371 - acc: 0.5715 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7353  | total loss: \u001b[1m\u001b[32m1.72218\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7353 | loss: 1.72218 - acc: 0.6143 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7354  | total loss: \u001b[1m\u001b[32m1.59631\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7354 | loss: 1.59631 - acc: 0.6529 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7355  | total loss: \u001b[1m\u001b[32m1.48393\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7355 | loss: 1.48393 - acc: 0.6876 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7356  | total loss: \u001b[1m\u001b[32m1.63265\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7356 | loss: 1.63265 - acc: 0.6331 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7357  | total loss: \u001b[1m\u001b[32m1.51783\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7357 | loss: 1.51783 - acc: 0.6698 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7358  | total loss: \u001b[1m\u001b[32m1.41479\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7358 | loss: 1.41479 - acc: 0.7028 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7359  | total loss: \u001b[1m\u001b[32m1.32181\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7359 | loss: 1.32181 - acc: 0.7326 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7360  | total loss: \u001b[1m\u001b[32m1.50175\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7360 | loss: 1.50175 - acc: 0.6736 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7361  | total loss: \u001b[1m\u001b[32m1.39938\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7361 | loss: 1.39938 - acc: 0.7062 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7362  | total loss: \u001b[1m\u001b[32m1.59927\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7362 | loss: 1.59927 - acc: 0.6427 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7363  | total loss: \u001b[1m\u001b[32m1.48701\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7363 | loss: 1.48701 - acc: 0.6785 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7364  | total loss: \u001b[1m\u001b[32m1.71069\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7364 | loss: 1.71069 - acc: 0.6106 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7365  | total loss: \u001b[1m\u001b[32m1.58775\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7365 | loss: 1.58775 - acc: 0.6496 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7366  | total loss: \u001b[1m\u001b[32m1.76573\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7366 | loss: 1.76573 - acc: 0.5918 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7367  | total loss: \u001b[1m\u001b[32m1.63823\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7367 | loss: 1.63823 - acc: 0.6326 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7368  | total loss: \u001b[1m\u001b[32m1.80389\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7368 | loss: 1.80389 - acc: 0.5836 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7369  | total loss: \u001b[1m\u001b[32m1.67367\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7369 | loss: 1.67367 - acc: 0.6252 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7370  | total loss: \u001b[1m\u001b[32m1.55675\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7370 | loss: 1.55675 - acc: 0.6627 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7371  | total loss: \u001b[1m\u001b[32m1.45123\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7371 | loss: 1.45123 - acc: 0.6964 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7372  | total loss: \u001b[1m\u001b[32m1.35550\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7372 | loss: 1.35550 - acc: 0.7268 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7373  | total loss: \u001b[1m\u001b[32m1.26815\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7373 | loss: 1.26815 - acc: 0.7541 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7374  | total loss: \u001b[1m\u001b[32m1.18800\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7374 | loss: 1.18800 - acc: 0.7787 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7375  | total loss: \u001b[1m\u001b[32m1.11403\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7375 | loss: 1.11403 - acc: 0.8008 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7376  | total loss: \u001b[1m\u001b[32m1.37280\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7376 | loss: 1.37280 - acc: 0.7208 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7377  | total loss: \u001b[1m\u001b[32m1.27731\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7377 | loss: 1.27731 - acc: 0.7487 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7378  | total loss: \u001b[1m\u001b[32m1.43367\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7378 | loss: 1.43367 - acc: 0.6952 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7379  | total loss: \u001b[1m\u001b[32m1.33019\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7379 | loss: 1.33019 - acc: 0.7257 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7380  | total loss: \u001b[1m\u001b[32m1.49015\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7380 | loss: 1.49015 - acc: 0.6746 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7381  | total loss: \u001b[1m\u001b[32m1.37989\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7381 | loss: 1.37989 - acc: 0.7071 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7382  | total loss: \u001b[1m\u001b[32m1.62875\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7382 | loss: 1.62875 - acc: 0.6364 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7383  | total loss: \u001b[1m\u001b[32m1.50438\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7383 | loss: 1.50438 - acc: 0.6728 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7384  | total loss: \u001b[1m\u001b[32m1.66717\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7384 | loss: 1.66717 - acc: 0.6198 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7385  | total loss: \u001b[1m\u001b[32m1.53952\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7385 | loss: 1.53952 - acc: 0.6578 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7386  | total loss: \u001b[1m\u001b[32m1.74107\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7386 | loss: 1.74107 - acc: 0.5992 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7387  | total loss: \u001b[1m\u001b[32m1.60716\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7387 | loss: 1.60716 - acc: 0.6392 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7388  | total loss: \u001b[1m\u001b[32m1.48710\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7388 | loss: 1.48710 - acc: 0.6753 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7389  | total loss: \u001b[1m\u001b[32m1.37908\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7389 | loss: 1.37908 - acc: 0.7078 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7390  | total loss: \u001b[1m\u001b[32m1.59417\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7390 | loss: 1.59417 - acc: 0.6442 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7391  | total loss: \u001b[1m\u001b[32m1.47545\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7391 | loss: 1.47545 - acc: 0.6797 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7392  | total loss: \u001b[1m\u001b[32m1.67257\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7392 | loss: 1.67257 - acc: 0.6189 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7393  | total loss: \u001b[1m\u001b[32m1.54663\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7393 | loss: 1.54663 - acc: 0.6570 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7394  | total loss: \u001b[1m\u001b[32m1.68129\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7394 | loss: 1.68129 - acc: 0.6127 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7395  | total loss: \u001b[1m\u001b[32m1.55545\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7395 | loss: 1.55545 - acc: 0.6515 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7396  | total loss: \u001b[1m\u001b[32m1.75303\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7396 | loss: 1.75303 - acc: 0.5935 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7397  | total loss: \u001b[1m\u001b[32m1.62140\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7397 | loss: 1.62140 - acc: 0.6341 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7398  | total loss: \u001b[1m\u001b[32m1.81326\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7398 | loss: 1.81326 - acc: 0.5778 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7399  | total loss: \u001b[1m\u001b[32m1.67743\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7399 | loss: 1.67743 - acc: 0.6201 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7400  | total loss: \u001b[1m\u001b[32m1.86726\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7400 | loss: 1.86726 - acc: 0.5581 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7401  | total loss: \u001b[1m\u001b[32m1.72823\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7401 | loss: 1.72823 - acc: 0.6023 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7402  | total loss: \u001b[1m\u001b[32m1.87587\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7402 | loss: 1.87587 - acc: 0.5563 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7403  | total loss: \u001b[1m\u001b[32m1.73837\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7403 | loss: 1.73837 - acc: 0.6007 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7404  | total loss: \u001b[1m\u001b[32m1.89920\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7404 | loss: 1.89920 - acc: 0.5478 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7405  | total loss: \u001b[1m\u001b[32m1.76181\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7405 | loss: 1.76181 - acc: 0.5930 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7406  | total loss: \u001b[1m\u001b[32m1.94449\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7406 | loss: 1.94449 - acc: 0.5408 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7407  | total loss: \u001b[1m\u001b[32m1.80502\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7407 | loss: 1.80502 - acc: 0.5867 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7408  | total loss: \u001b[1m\u001b[32m1.96974\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7408 | loss: 1.96974 - acc: 0.5352 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7409  | total loss: \u001b[1m\u001b[32m1.83013\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7409 | loss: 1.83013 - acc: 0.5817 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7410  | total loss: \u001b[1m\u001b[32m1.96171\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7410 | loss: 1.96171 - acc: 0.5307 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7411  | total loss: \u001b[1m\u001b[32m1.82517\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7411 | loss: 1.82517 - acc: 0.5776 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7412  | total loss: \u001b[1m\u001b[32m1.98790\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7412 | loss: 1.98790 - acc: 0.5270 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7413  | total loss: \u001b[1m\u001b[32m1.85080\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7413 | loss: 1.85080 - acc: 0.5743 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7414  | total loss: \u001b[1m\u001b[32m1.97554\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7414 | loss: 1.97554 - acc: 0.5311 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7415  | total loss: \u001b[1m\u001b[32m1.84137\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7415 | loss: 1.84137 - acc: 0.5780 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7416  | total loss: \u001b[1m\u001b[32m1.98690\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7416 | loss: 1.98690 - acc: 0.5202 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7417  | total loss: \u001b[1m\u001b[32m1.85297\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7417 | loss: 1.85297 - acc: 0.5682 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7418  | total loss: \u001b[1m\u001b[32m1.99430\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7418 | loss: 1.99430 - acc: 0.5114 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7419  | total loss: \u001b[1m\u001b[32m1.86095\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7419 | loss: 1.86095 - acc: 0.5602 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7420  | total loss: \u001b[1m\u001b[32m1.74117\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7420 | loss: 1.74117 - acc: 0.6042 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7421  | total loss: \u001b[1m\u001b[32m1.63286\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7421 | loss: 1.63286 - acc: 0.6438 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7422  | total loss: \u001b[1m\u001b[32m1.74416\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7422 | loss: 1.74416 - acc: 0.5937 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7423  | total loss: \u001b[1m\u001b[32m1.63389\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7423 | loss: 1.63389 - acc: 0.6343 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7424  | total loss: \u001b[1m\u001b[32m1.78374\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7424 | loss: 1.78374 - acc: 0.5780 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7425  | total loss: \u001b[1m\u001b[32m1.66813\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7425 | loss: 1.66813 - acc: 0.6202 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7426  | total loss: \u001b[1m\u001b[32m1.82578\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7426 | loss: 1.82578 - acc: 0.5582 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7427  | total loss: \u001b[1m\u001b[32m1.70500\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7427 | loss: 1.70500 - acc: 0.6024 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7428  | total loss: \u001b[1m\u001b[32m1.86631\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7428 | loss: 1.86631 - acc: 0.5493 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7429  | total loss: \u001b[1m\u001b[32m1.74083\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7429 | loss: 1.74083 - acc: 0.5944 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7430  | total loss: \u001b[1m\u001b[32m1.92522\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7430 | loss: 1.92522 - acc: 0.5349 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7431  | total loss: \u001b[1m\u001b[32m1.79363\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7431 | loss: 1.79363 - acc: 0.5814 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7432  | total loss: \u001b[1m\u001b[32m1.67489\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7432 | loss: 1.67489 - acc: 0.6233 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7433  | total loss: \u001b[1m\u001b[32m1.56712\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7433 | loss: 1.56712 - acc: 0.6610 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7434  | total loss: \u001b[1m\u001b[32m1.75205\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7434 | loss: 1.75205 - acc: 0.5949 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7435  | total loss: \u001b[1m\u001b[32m1.63457\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7435 | loss: 1.63457 - acc: 0.6354 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7436  | total loss: \u001b[1m\u001b[32m1.78987\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7436 | loss: 1.78987 - acc: 0.5790 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7437  | total loss: \u001b[1m\u001b[32m1.66728\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7437 | loss: 1.66728 - acc: 0.6211 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7438  | total loss: \u001b[1m\u001b[32m1.81944\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7438 | loss: 1.81944 - acc: 0.5733 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7439  | total loss: \u001b[1m\u001b[32m1.69310\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7439 | loss: 1.69310 - acc: 0.6159 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7440  | total loss: \u001b[1m\u001b[32m1.85174\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7440 | loss: 1.85174 - acc: 0.5543 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7441  | total loss: \u001b[1m\u001b[32m1.72178\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7441 | loss: 1.72178 - acc: 0.5989 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7442  | total loss: \u001b[1m\u001b[32m1.85622\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7442 | loss: 1.85622 - acc: 0.5462 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7443  | total loss: \u001b[1m\u001b[32m1.72581\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7443 | loss: 1.72581 - acc: 0.5915 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7444  | total loss: \u001b[1m\u001b[32m1.86875\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7444 | loss: 1.86875 - acc: 0.5395 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7445  | total loss: \u001b[1m\u001b[32m1.73742\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7445 | loss: 1.73742 - acc: 0.5856 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7446  | total loss: \u001b[1m\u001b[32m1.61912\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7446 | loss: 1.61912 - acc: 0.6270 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7447  | total loss: \u001b[1m\u001b[32m1.51199\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7447 | loss: 1.51199 - acc: 0.6643 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7448  | total loss: \u001b[1m\u001b[32m1.72656\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7448 | loss: 1.72656 - acc: 0.5979 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7449  | total loss: \u001b[1m\u001b[32m1.60723\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7449 | loss: 1.60723 - acc: 0.6381 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7450  | total loss: \u001b[1m\u001b[32m1.49903\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7450 | loss: 1.49903 - acc: 0.6743 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7451  | total loss: \u001b[1m\u001b[32m1.40037\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7451 | loss: 1.40037 - acc: 0.7069 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7452  | total loss: \u001b[1m\u001b[32m1.61252\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7452 | loss: 1.61252 - acc: 0.6433 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7453  | total loss: \u001b[1m\u001b[32m1.50015\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7453 | loss: 1.50015 - acc: 0.6790 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7454  | total loss: \u001b[1m\u001b[32m1.39790\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7454 | loss: 1.39790 - acc: 0.7111 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7455  | total loss: \u001b[1m\u001b[32m1.30439\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7455 | loss: 1.30439 - acc: 0.7400 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7456  | total loss: \u001b[1m\u001b[32m1.21847\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7456 | loss: 1.21847 - acc: 0.7660 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7457  | total loss: \u001b[1m\u001b[32m1.13914\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7457 | loss: 1.13914 - acc: 0.7894 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7458  | total loss: \u001b[1m\u001b[32m1.35999\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7458 | loss: 1.35999 - acc: 0.7247 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7459  | total loss: \u001b[1m\u001b[32m1.26314\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7459 | loss: 1.26314 - acc: 0.7523 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7460  | total loss: \u001b[1m\u001b[32m1.48795\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7460 | loss: 1.48795 - acc: 0.6913 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7461  | total loss: \u001b[1m\u001b[32m1.37629\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7461 | loss: 1.37629 - acc: 0.7222 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7462  | total loss: \u001b[1m\u001b[32m1.27497\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7462 | loss: 1.27497 - acc: 0.7500 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7463  | total loss: \u001b[1m\u001b[32m1.18269\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7463 | loss: 1.18269 - acc: 0.7750 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7464  | total loss: \u001b[1m\u001b[32m1.45558\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7464 | loss: 1.45558 - acc: 0.6975 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7465  | total loss: \u001b[1m\u001b[32m1.34368\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7465 | loss: 1.34368 - acc: 0.7277 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7466  | total loss: \u001b[1m\u001b[32m1.24243\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7466 | loss: 1.24243 - acc: 0.7550 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7467  | total loss: \u001b[1m\u001b[32m1.15050\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7467 | loss: 1.15050 - acc: 0.7795 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7468  | total loss: \u001b[1m\u001b[32m1.06678\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7468 | loss: 1.06678 - acc: 0.8015 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7469  | total loss: \u001b[1m\u001b[32m0.99028\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7469 | loss: 0.99028 - acc: 0.8214 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7470  | total loss: \u001b[1m\u001b[32m1.28151\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7470 | loss: 1.28151 - acc: 0.7392 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7471  | total loss: \u001b[1m\u001b[32m1.18194\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7471 | loss: 1.18194 - acc: 0.7653 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7472  | total loss: \u001b[1m\u001b[32m1.49080\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7472 | loss: 1.49080 - acc: 0.6888 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7473  | total loss: \u001b[1m\u001b[32m1.37019\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7473 | loss: 1.37019 - acc: 0.7199 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7474  | total loss: \u001b[1m\u001b[32m1.59358\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7474 | loss: 1.59358 - acc: 0.6622 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7475  | total loss: \u001b[1m\u001b[32m1.46371\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7475 | loss: 1.46371 - acc: 0.6960 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7476  | total loss: \u001b[1m\u001b[32m1.34740\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7476 | loss: 1.34740 - acc: 0.7264 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7477  | total loss: \u001b[1m\u001b[32m1.24295\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7477 | loss: 1.24295 - acc: 0.7537 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7478  | total loss: \u001b[1m\u001b[32m1.52718\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7478 | loss: 1.52718 - acc: 0.6784 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7479  | total loss: \u001b[1m\u001b[32m1.40550\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7479 | loss: 1.40550 - acc: 0.7105 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7480  | total loss: \u001b[1m\u001b[32m1.29645\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7480 | loss: 1.29645 - acc: 0.7395 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7481  | total loss: \u001b[1m\u001b[32m1.19842\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7481 | loss: 1.19842 - acc: 0.7655 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7482  | total loss: \u001b[1m\u001b[32m1.45092\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7482 | loss: 1.45092 - acc: 0.6961 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7483  | total loss: \u001b[1m\u001b[32m1.33783\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7483 | loss: 1.33783 - acc: 0.7265 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7484  | total loss: \u001b[1m\u001b[32m1.23629\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7484 | loss: 1.23629 - acc: 0.7539 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7485  | total loss: \u001b[1m\u001b[32m1.14479\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7485 | loss: 1.14479 - acc: 0.7785 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7486  | total loss: \u001b[1m\u001b[32m1.42840\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7486 | loss: 1.42840 - acc: 0.7006 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7487  | total loss: \u001b[1m\u001b[32m1.31774\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7487 | loss: 1.31774 - acc: 0.7306 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7488  | total loss: \u001b[1m\u001b[32m1.59492\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7488 | loss: 1.59492 - acc: 0.6575 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7489  | total loss: \u001b[1m\u001b[32m1.46867\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7489 | loss: 1.46867 - acc: 0.6918 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7490  | total loss: \u001b[1m\u001b[32m1.68911\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7490 | loss: 1.68911 - acc: 0.6226 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7491  | total loss: \u001b[1m\u001b[32m1.55531\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7491 | loss: 1.55531 - acc: 0.6603 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7492  | total loss: \u001b[1m\u001b[32m1.78104\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7492 | loss: 1.78104 - acc: 0.5943 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7493  | total loss: \u001b[1m\u001b[32m1.64064\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7493 | loss: 1.64064 - acc: 0.6349 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7494  | total loss: \u001b[1m\u001b[32m1.82661\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7494 | loss: 1.82661 - acc: 0.5785 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7495  | total loss: \u001b[1m\u001b[32m1.68478\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7495 | loss: 1.68478 - acc: 0.6207 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7496  | total loss: \u001b[1m\u001b[32m1.88821\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7496 | loss: 1.88821 - acc: 0.5586 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7497  | total loss: \u001b[1m\u001b[32m1.74370\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7497 | loss: 1.74370 - acc: 0.6027 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7498  | total loss: \u001b[1m\u001b[32m1.93624\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7498 | loss: 1.93624 - acc: 0.5425 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7499  | total loss: \u001b[1m\u001b[32m1.79076\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7499 | loss: 1.79076 - acc: 0.5882 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7500  | total loss: \u001b[1m\u001b[32m1.93988\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7500 | loss: 1.93988 - acc: 0.5365 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7501  | total loss: \u001b[1m\u001b[32m1.79784\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7501 | loss: 1.79784 - acc: 0.5829 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7502  | total loss: \u001b[1m\u001b[32m1.91335\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7502 | loss: 1.91335 - acc: 0.5389 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7503  | total loss: \u001b[1m\u001b[32m1.77735\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7503 | loss: 1.77735 - acc: 0.5850 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7504  | total loss: \u001b[1m\u001b[32m1.65615\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7504 | loss: 1.65615 - acc: 0.6265 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7505  | total loss: \u001b[1m\u001b[32m1.54753\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7505 | loss: 1.54753 - acc: 0.6638 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7506  | total loss: \u001b[1m\u001b[32m1.44953\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7506 | loss: 1.44953 - acc: 0.6975 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7507  | total loss: \u001b[1m\u001b[32m1.36048\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7507 | loss: 1.36048 - acc: 0.7277 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7508  | total loss: \u001b[1m\u001b[32m1.56551\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7508 | loss: 1.56551 - acc: 0.6621 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7509  | total loss: \u001b[1m\u001b[32m1.46300\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7509 | loss: 1.46300 - acc: 0.6959 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7510  | total loss: \u001b[1m\u001b[32m1.62027\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7510 | loss: 1.62027 - acc: 0.6406 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7511  | total loss: \u001b[1m\u001b[32m1.51103\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7511 | loss: 1.51103 - acc: 0.6765 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7512  | total loss: \u001b[1m\u001b[32m1.71107\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7512 | loss: 1.71107 - acc: 0.6089 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7513  | total loss: \u001b[1m\u001b[32m1.59214\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7513 | loss: 1.59214 - acc: 0.6480 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7514  | total loss: \u001b[1m\u001b[32m1.75623\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7514 | loss: 1.75623 - acc: 0.5975 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7515  | total loss: \u001b[1m\u001b[32m1.63272\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7515 | loss: 1.63272 - acc: 0.6377 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7516  | total loss: \u001b[1m\u001b[32m1.76212\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7516 | loss: 1.76212 - acc: 0.5954 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7517  | total loss: \u001b[1m\u001b[32m1.63826\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7517 | loss: 1.63826 - acc: 0.6358 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7518  | total loss: \u001b[1m\u001b[32m1.78346\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7518 | loss: 1.78346 - acc: 0.5865 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7519  | total loss: \u001b[1m\u001b[32m1.65790\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7519 | loss: 1.65790 - acc: 0.6279 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7520  | total loss: \u001b[1m\u001b[32m1.84600\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7520 | loss: 1.84600 - acc: 0.5651 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7521  | total loss: \u001b[1m\u001b[32m1.71495\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7521 | loss: 1.71495 - acc: 0.6086 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7522  | total loss: \u001b[1m\u001b[32m1.85191\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7522 | loss: 1.85191 - acc: 0.5620 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7523  | total loss: \u001b[1m\u001b[32m1.72116\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7523 | loss: 1.72116 - acc: 0.6058 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7524  | total loss: \u001b[1m\u001b[32m1.89238\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7524 | loss: 1.89238 - acc: 0.5524 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7525  | total loss: \u001b[1m\u001b[32m1.75855\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 7525 | loss: 1.75855 - acc: 0.5971 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7526  | total loss: \u001b[1m\u001b[32m1.89906\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7526 | loss: 1.89906 - acc: 0.5446 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7527  | total loss: \u001b[1m\u001b[32m1.76554\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7527 | loss: 1.76554 - acc: 0.5901 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7528  | total loss: \u001b[1m\u001b[32m1.92993\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7528 | loss: 1.92993 - acc: 0.5311 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7529  | total loss: \u001b[1m\u001b[32m1.79424\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7529 | loss: 1.79424 - acc: 0.5780 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7530  | total loss: \u001b[1m\u001b[32m1.67225\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7530 | loss: 1.67225 - acc: 0.6202 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7531  | total loss: \u001b[1m\u001b[32m1.56196\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7531 | loss: 1.56196 - acc: 0.6582 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7532  | total loss: \u001b[1m\u001b[32m1.67259\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7532 | loss: 1.67259 - acc: 0.6209 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7533  | total loss: \u001b[1m\u001b[32m1.56075\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7533 | loss: 1.56075 - acc: 0.6588 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7534  | total loss: \u001b[1m\u001b[32m1.69694\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7534 | loss: 1.69694 - acc: 0.6072 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7535  | total loss: \u001b[1m\u001b[32m1.58144\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7535 | loss: 1.58144 - acc: 0.6465 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7536  | total loss: \u001b[1m\u001b[32m1.69792\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7536 | loss: 1.69792 - acc: 0.6033 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7537  | total loss: \u001b[1m\u001b[32m1.58139\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7537 | loss: 1.58139 - acc: 0.6430 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7538  | total loss: \u001b[1m\u001b[32m1.78262\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7538 | loss: 1.78262 - acc: 0.5858 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7539  | total loss: \u001b[1m\u001b[32m1.65706\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7539 | loss: 1.65706 - acc: 0.6272 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7540  | total loss: \u001b[1m\u001b[32m1.83982\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7540 | loss: 1.83982 - acc: 0.5716 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7541  | total loss: \u001b[1m\u001b[32m1.70858\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7541 | loss: 1.70858 - acc: 0.6145 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7542  | total loss: \u001b[1m\u001b[32m1.86814\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7542 | loss: 1.86814 - acc: 0.5602 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7543  | total loss: \u001b[1m\u001b[32m1.73448\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7543 | loss: 1.73448 - acc: 0.6042 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7544  | total loss: \u001b[1m\u001b[32m1.91427\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7544 | loss: 1.91427 - acc: 0.5509 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7545  | total loss: \u001b[1m\u001b[32m1.77668\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7545 | loss: 1.77668 - acc: 0.5958 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7546  | total loss: \u001b[1m\u001b[32m1.95948\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7546 | loss: 1.95948 - acc: 0.5362 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7547  | total loss: \u001b[1m\u001b[32m1.81830\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7547 | loss: 1.81830 - acc: 0.5826 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7548  | total loss: \u001b[1m\u001b[32m1.98158\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7548 | loss: 1.98158 - acc: 0.5315 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7549  | total loss: \u001b[1m\u001b[32m1.83926\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7549 | loss: 1.83926 - acc: 0.5783 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7550  | total loss: \u001b[1m\u001b[32m1.99039\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7550 | loss: 1.99039 - acc: 0.5276 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7551  | total loss: \u001b[1m\u001b[32m1.84832\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7551 | loss: 1.84832 - acc: 0.5749 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7552  | total loss: \u001b[1m\u001b[32m1.72071\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7552 | loss: 1.72071 - acc: 0.6174 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7553  | total loss: \u001b[1m\u001b[32m1.60550\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7553 | loss: 1.60550 - acc: 0.6557 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7554  | total loss: \u001b[1m\u001b[32m1.80071\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7554 | loss: 1.80071 - acc: 0.5901 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7555  | total loss: \u001b[1m\u001b[32m1.67657\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7555 | loss: 1.67657 - acc: 0.6311 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7556  | total loss: \u001b[1m\u001b[32m1.79842\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7556 | loss: 1.79842 - acc: 0.5894 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7557  | total loss: \u001b[1m\u001b[32m1.67390\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7557 | loss: 1.67390 - acc: 0.6305 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7558  | total loss: \u001b[1m\u001b[32m1.87257\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7558 | loss: 1.87257 - acc: 0.5674 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7559  | total loss: \u001b[1m\u001b[32m1.74040\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7559 | loss: 1.74040 - acc: 0.6107 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7560  | total loss: \u001b[1m\u001b[32m1.88482\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7560 | loss: 1.88482 - acc: 0.5567 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7561  | total loss: \u001b[1m\u001b[32m1.75153\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7561 | loss: 1.75153 - acc: 0.6011 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7562  | total loss: \u001b[1m\u001b[32m1.63138\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7562 | loss: 1.63138 - acc: 0.6410 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7563  | total loss: \u001b[1m\u001b[32m1.52255\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7563 | loss: 1.52255 - acc: 0.6769 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7564  | total loss: \u001b[1m\u001b[32m1.69387\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7564 | loss: 1.69387 - acc: 0.6163 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7565  | total loss: \u001b[1m\u001b[32m1.57729\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7565 | loss: 1.57729 - acc: 0.6547 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7566  | total loss: \u001b[1m\u001b[32m1.77401\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7566 | loss: 1.77401 - acc: 0.5892 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7567  | total loss: \u001b[1m\u001b[32m1.64869\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7567 | loss: 1.64869 - acc: 0.6303 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7568  | total loss: \u001b[1m\u001b[32m1.80387\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7568 | loss: 1.80387 - acc: 0.5816 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7569  | total loss: \u001b[1m\u001b[32m1.67533\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7569 | loss: 1.67533 - acc: 0.6234 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7570  | total loss: \u001b[1m\u001b[32m1.85914\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7570 | loss: 1.85914 - acc: 0.5611 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7571  | total loss: \u001b[1m\u001b[32m1.72529\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7571 | loss: 1.72529 - acc: 0.6050 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7572  | total loss: \u001b[1m\u001b[32m1.88952\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7572 | loss: 1.88952 - acc: 0.5516 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7573  | total loss: \u001b[1m\u001b[32m1.75341\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7573 | loss: 1.75341 - acc: 0.5964 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7574  | total loss: \u001b[1m\u001b[32m1.92638\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7574 | loss: 1.92638 - acc: 0.5368 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7575  | total loss: \u001b[1m\u001b[32m1.78772\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7575 | loss: 1.78772 - acc: 0.5831 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7576  | total loss: \u001b[1m\u001b[32m1.94757\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7576 | loss: 1.94757 - acc: 0.5248 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7577  | total loss: \u001b[1m\u001b[32m1.80821\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7577 | loss: 1.80821 - acc: 0.5723 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7578  | total loss: \u001b[1m\u001b[32m1.68322\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7578 | loss: 1.68322 - acc: 0.6151 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7579  | total loss: \u001b[1m\u001b[32m1.57056\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7579 | loss: 1.57056 - acc: 0.6536 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7580  | total loss: \u001b[1m\u001b[32m1.72844\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7580 | loss: 1.72844 - acc: 0.6025 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7581  | total loss: \u001b[1m\u001b[32m1.61053\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7581 | loss: 1.61053 - acc: 0.6423 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7582  | total loss: \u001b[1m\u001b[32m1.77745\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7582 | loss: 1.77745 - acc: 0.5852 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7583  | total loss: \u001b[1m\u001b[32m1.65426\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7583 | loss: 1.65426 - acc: 0.6267 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7584  | total loss: \u001b[1m\u001b[32m1.78903\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7584 | loss: 1.78903 - acc: 0.5783 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7585  | total loss: \u001b[1m\u001b[32m1.66454\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7585 | loss: 1.66454 - acc: 0.6205 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7586  | total loss: \u001b[1m\u001b[32m1.81133\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7586 | loss: 1.81133 - acc: 0.5727 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7587  | total loss: \u001b[1m\u001b[32m1.68455\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7587 | loss: 1.68455 - acc: 0.6154 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7588  | total loss: \u001b[1m\u001b[32m1.84230\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7588 | loss: 1.84230 - acc: 0.5610 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7589  | total loss: \u001b[1m\u001b[32m1.71254\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7589 | loss: 1.71254 - acc: 0.6049 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7590  | total loss: \u001b[1m\u001b[32m1.84650\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7590 | loss: 1.84650 - acc: 0.5587 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7591  | total loss: \u001b[1m\u001b[32m1.71659\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7591 | loss: 1.71659 - acc: 0.6028 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7592  | total loss: \u001b[1m\u001b[32m1.85293\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7592 | loss: 1.85293 - acc: 0.5568 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7593  | total loss: \u001b[1m\u001b[32m1.72272\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7593 | loss: 1.72272 - acc: 0.6012 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7594  | total loss: \u001b[1m\u001b[32m1.87707\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7594 | loss: 1.87707 - acc: 0.5482 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7595  | total loss: \u001b[1m\u001b[32m1.74499\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7595 | loss: 1.74499 - acc: 0.5934 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7596  | total loss: \u001b[1m\u001b[32m1.62613\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7596 | loss: 1.62613 - acc: 0.6340 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7597  | total loss: \u001b[1m\u001b[32m1.51857\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7597 | loss: 1.51857 - acc: 0.6706 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7598  | total loss: \u001b[1m\u001b[32m1.42069\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7598 | loss: 1.42069 - acc: 0.7036 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7599  | total loss: \u001b[1m\u001b[32m1.33106\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7599 | loss: 1.33106 - acc: 0.7332 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7600  | total loss: \u001b[1m\u001b[32m1.55876\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7600 | loss: 1.55876 - acc: 0.6599 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7601  | total loss: \u001b[1m\u001b[32m1.45265\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7601 | loss: 1.45265 - acc: 0.6939 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7602  | total loss: \u001b[1m\u001b[32m1.67236\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7602 | loss: 1.67236 - acc: 0.6245 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7603  | total loss: \u001b[1m\u001b[32m1.55346\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7603 | loss: 1.55346 - acc: 0.6621 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7604  | total loss: \u001b[1m\u001b[32m1.71904\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7604 | loss: 1.71904 - acc: 0.6030 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7605  | total loss: \u001b[1m\u001b[32m1.59493\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7605 | loss: 1.59493 - acc: 0.6427 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7606  | total loss: \u001b[1m\u001b[32m1.78243\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7606 | loss: 1.78243 - acc: 0.5927 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7607  | total loss: \u001b[1m\u001b[32m1.65200\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7607 | loss: 1.65200 - acc: 0.6334 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7608  | total loss: \u001b[1m\u001b[32m1.84648\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7608 | loss: 1.84648 - acc: 0.5701 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7609  | total loss: \u001b[1m\u001b[32m1.71025\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7609 | loss: 1.71025 - acc: 0.6131 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7610  | total loss: \u001b[1m\u001b[32m1.88455\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7610 | loss: 1.88455 - acc: 0.5518 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7611  | total loss: \u001b[1m\u001b[32m1.74580\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7611 | loss: 1.74580 - acc: 0.5966 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7612  | total loss: \u001b[1m\u001b[32m1.87736\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7612 | loss: 1.87736 - acc: 0.5512 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7613  | total loss: \u001b[1m\u001b[32m1.74090\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 7613 | loss: 1.74090 - acc: 0.5961 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7614  | total loss: \u001b[1m\u001b[32m1.88097\u001b[0m\u001b[0m | time: 0.015s\n",
      "| Adam | epoch: 7614 | loss: 1.88097 - acc: 0.5436 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7615  | total loss: \u001b[1m\u001b[32m1.74567\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7615 | loss: 1.74567 - acc: 0.5893 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7616  | total loss: \u001b[1m\u001b[32m1.89573\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7616 | loss: 1.89573 - acc: 0.5375 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7617  | total loss: \u001b[1m\u001b[32m1.76048\u001b[0m\u001b[0m | time: 0.023s\n",
      "| Adam | epoch: 7617 | loss: 1.76048 - acc: 0.5837 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7618  | total loss: \u001b[1m\u001b[32m1.88517\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7618 | loss: 1.88517 - acc: 0.5397 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7619  | total loss: \u001b[1m\u001b[32m1.75243\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7619 | loss: 1.75243 - acc: 0.5857 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7620  | total loss: \u001b[1m\u001b[32m1.87580\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7620 | loss: 1.87580 - acc: 0.5414 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7621  | total loss: \u001b[1m\u001b[32m1.74522\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 7621 | loss: 1.74522 - acc: 0.5873 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7622  | total loss: \u001b[1m\u001b[32m1.91161\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7622 | loss: 1.91161 - acc: 0.5285 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7623  | total loss: \u001b[1m\u001b[32m1.77860\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7623 | loss: 1.77860 - acc: 0.5757 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7624  | total loss: \u001b[1m\u001b[32m1.93093\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7624 | loss: 1.93093 - acc: 0.5253 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7625  | total loss: \u001b[1m\u001b[32m1.79723\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7625 | loss: 1.79723 - acc: 0.5727 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7626  | total loss: \u001b[1m\u001b[32m1.67711\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7626 | loss: 1.67711 - acc: 0.6155 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7627  | total loss: \u001b[1m\u001b[32m1.56850\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7627 | loss: 1.56850 - acc: 0.6539 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7628  | total loss: \u001b[1m\u001b[32m1.77490\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7628 | loss: 1.77490 - acc: 0.5885 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7629  | total loss: \u001b[1m\u001b[32m1.65518\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7629 | loss: 1.65518 - acc: 0.6297 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7630  | total loss: \u001b[1m\u001b[32m1.54661\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7630 | loss: 1.54661 - acc: 0.6667 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7631  | total loss: \u001b[1m\u001b[32m1.44754\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7631 | loss: 1.44754 - acc: 0.7000 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7632  | total loss: \u001b[1m\u001b[32m1.64672\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7632 | loss: 1.64672 - acc: 0.6372 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7633  | total loss: \u001b[1m\u001b[32m1.53503\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7633 | loss: 1.53503 - acc: 0.6735 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7634  | total loss: \u001b[1m\u001b[32m1.72401\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7634 | loss: 1.72401 - acc: 0.6133 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7635  | total loss: \u001b[1m\u001b[32m1.60304\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7635 | loss: 1.60304 - acc: 0.6519 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7636  | total loss: \u001b[1m\u001b[32m1.74196\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7636 | loss: 1.74196 - acc: 0.6082 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7637  | total loss: \u001b[1m\u001b[32m1.61840\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7637 | loss: 1.61840 - acc: 0.6473 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7638  | total loss: \u001b[1m\u001b[32m1.81288\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7638 | loss: 1.81288 - acc: 0.5826 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7639  | total loss: \u001b[1m\u001b[32m1.68218\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7639 | loss: 1.68218 - acc: 0.6244 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7640  | total loss: \u001b[1m\u001b[32m1.79485\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7640 | loss: 1.79485 - acc: 0.5833 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7641  | total loss: \u001b[1m\u001b[32m1.66655\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7641 | loss: 1.66655 - acc: 0.6250 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7642  | total loss: \u001b[1m\u001b[32m1.81555\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7642 | loss: 1.81555 - acc: 0.5768 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7643  | total loss: \u001b[1m\u001b[32m1.68606\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7643 | loss: 1.68606 - acc: 0.6191 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7644  | total loss: \u001b[1m\u001b[32m1.56968\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7644 | loss: 1.56968 - acc: 0.6572 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7645  | total loss: \u001b[1m\u001b[32m1.46452\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7645 | loss: 1.46452 - acc: 0.6915 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7646  | total loss: \u001b[1m\u001b[32m1.64813\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7646 | loss: 1.64813 - acc: 0.6295 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7647  | total loss: \u001b[1m\u001b[32m1.53415\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7647 | loss: 1.53415 - acc: 0.6665 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7648  | total loss: \u001b[1m\u001b[32m1.43097\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7648 | loss: 1.43097 - acc: 0.6999 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7649  | total loss: \u001b[1m\u001b[32m1.33703\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7649 | loss: 1.33703 - acc: 0.7299 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7650  | total loss: \u001b[1m\u001b[32m1.53362\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7650 | loss: 1.53362 - acc: 0.6640 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7651  | total loss: \u001b[1m\u001b[32m1.42738\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7651 | loss: 1.42738 - acc: 0.6976 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7652  | total loss: \u001b[1m\u001b[32m1.60526\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7652 | loss: 1.60526 - acc: 0.6422 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7653  | total loss: \u001b[1m\u001b[32m1.49067\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7653 | loss: 1.49067 - acc: 0.6779 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7654  | total loss: \u001b[1m\u001b[32m1.38687\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7654 | loss: 1.38687 - acc: 0.7102 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7655  | total loss: \u001b[1m\u001b[32m1.29239\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7655 | loss: 1.29239 - acc: 0.7391 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7656  | total loss: \u001b[1m\u001b[32m1.53053\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7656 | loss: 1.53053 - acc: 0.6652 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7657  | total loss: \u001b[1m\u001b[32m1.41985\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7657 | loss: 1.41985 - acc: 0.6987 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7658  | total loss: \u001b[1m\u001b[32m1.63841\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7658 | loss: 1.63841 - acc: 0.6288 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7659  | total loss: \u001b[1m\u001b[32m1.51621\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7659 | loss: 1.51621 - acc: 0.6659 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7660  | total loss: \u001b[1m\u001b[32m1.72361\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7660 | loss: 1.72361 - acc: 0.6065 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7661  | total loss: \u001b[1m\u001b[32m1.59313\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7661 | loss: 1.59313 - acc: 0.6458 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7662  | total loss: \u001b[1m\u001b[32m1.47580\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7662 | loss: 1.47580 - acc: 0.6813 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7663  | total loss: \u001b[1m\u001b[32m1.36986\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7663 | loss: 1.36986 - acc: 0.7131 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7664  | total loss: \u001b[1m\u001b[32m1.62891\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7664 | loss: 1.62891 - acc: 0.6418 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7665  | total loss: \u001b[1m\u001b[32m1.50716\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7665 | loss: 1.50716 - acc: 0.6776 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7666  | total loss: \u001b[1m\u001b[32m1.74182\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7666 | loss: 1.74182 - acc: 0.6099 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7667  | total loss: \u001b[1m\u001b[32m1.60926\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7667 | loss: 1.60926 - acc: 0.6489 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7668  | total loss: \u001b[1m\u001b[32m1.49015\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7668 | loss: 1.49015 - acc: 0.6840 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7669  | total loss: \u001b[1m\u001b[32m1.38269\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7669 | loss: 1.38269 - acc: 0.7156 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7670  | total loss: \u001b[1m\u001b[32m1.59645\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7670 | loss: 1.59645 - acc: 0.6512 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7671  | total loss: \u001b[1m\u001b[32m1.47783\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7671 | loss: 1.47783 - acc: 0.6861 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7672  | total loss: \u001b[1m\u001b[32m1.71026\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7672 | loss: 1.71026 - acc: 0.6175 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7673  | total loss: \u001b[1m\u001b[32m1.58060\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7673 | loss: 1.58060 - acc: 0.6557 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7674  | total loss: \u001b[1m\u001b[32m1.78179\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7674 | loss: 1.78179 - acc: 0.5901 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7675  | total loss: \u001b[1m\u001b[32m1.64618\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7675 | loss: 1.64618 - acc: 0.6311 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7676  | total loss: \u001b[1m\u001b[32m1.82587\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7676 | loss: 1.82587 - acc: 0.5752 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7677  | total loss: \u001b[1m\u001b[32m1.68772\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7677 | loss: 1.68772 - acc: 0.6176 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7678  | total loss: \u001b[1m\u001b[32m1.89912\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7678 | loss: 1.89912 - acc: 0.5559 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7679  | total loss: \u001b[1m\u001b[32m1.75597\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7679 | loss: 1.75597 - acc: 0.6003 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7680  | total loss: \u001b[1m\u001b[32m1.93107\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7680 | loss: 1.93107 - acc: 0.5474 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7681  | total loss: \u001b[1m\u001b[32m1.78734\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7681 | loss: 1.78734 - acc: 0.5927 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7682  | total loss: \u001b[1m\u001b[32m1.96231\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7682 | loss: 1.96231 - acc: 0.5334 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7683  | total loss: \u001b[1m\u001b[32m1.81825\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7683 | loss: 1.81825 - acc: 0.5801 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7684  | total loss: \u001b[1m\u001b[32m1.98307\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7684 | loss: 1.98307 - acc: 0.5292 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7685  | total loss: \u001b[1m\u001b[32m1.83973\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7685 | loss: 1.83973 - acc: 0.5763 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7686  | total loss: \u001b[1m\u001b[32m1.71170\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7686 | loss: 1.71170 - acc: 0.6186 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7687  | total loss: \u001b[1m\u001b[32m1.59674\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7687 | loss: 1.59674 - acc: 0.6568 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7688  | total loss: \u001b[1m\u001b[32m1.75721\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7688 | loss: 1.75721 - acc: 0.5982 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7689  | total loss: \u001b[1m\u001b[32m1.63779\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7689 | loss: 1.63779 - acc: 0.6384 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7690  | total loss: \u001b[1m\u001b[32m1.84829\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7690 | loss: 1.84829 - acc: 0.5746 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7691  | total loss: \u001b[1m\u001b[32m1.72024\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7691 | loss: 1.72024 - acc: 0.6171 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7692  | total loss: \u001b[1m\u001b[32m1.86112\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7692 | loss: 1.86112 - acc: 0.5697 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7693  | total loss: \u001b[1m\u001b[32m1.73243\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7693 | loss: 1.73243 - acc: 0.6127 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7694  | total loss: \u001b[1m\u001b[32m1.89948\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7694 | loss: 1.89948 - acc: 0.5586 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7695  | total loss: \u001b[1m\u001b[32m1.76764\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7695 | loss: 1.76764 - acc: 0.6027 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7696  | total loss: \u001b[1m\u001b[32m1.64901\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7696 | loss: 1.64901 - acc: 0.6425 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7697  | total loss: \u001b[1m\u001b[32m1.54168\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7697 | loss: 1.54168 - acc: 0.6782 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7698  | total loss: \u001b[1m\u001b[32m1.44398\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7698 | loss: 1.44398 - acc: 0.7104 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7699  | total loss: \u001b[1m\u001b[32m1.35449\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7699 | loss: 1.35449 - acc: 0.7394 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7700  | total loss: \u001b[1m\u001b[32m1.27201\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7700 | loss: 1.27201 - acc: 0.7654 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7701  | total loss: \u001b[1m\u001b[32m1.19553\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7701 | loss: 1.19553 - acc: 0.7889 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7702  | total loss: \u001b[1m\u001b[32m1.41243\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7702 | loss: 1.41243 - acc: 0.7171 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7703  | total loss: \u001b[1m\u001b[32m1.31795\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7703 | loss: 1.31795 - acc: 0.7454 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7704  | total loss: \u001b[1m\u001b[32m1.55105\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7704 | loss: 1.55105 - acc: 0.6709 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7705  | total loss: \u001b[1m\u001b[32m1.44020\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7705 | loss: 1.44020 - acc: 0.7038 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7706  | total loss: \u001b[1m\u001b[32m1.33936\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7706 | loss: 1.33936 - acc: 0.7334 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7707  | total loss: \u001b[1m\u001b[32m1.24722\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7707 | loss: 1.24722 - acc: 0.7601 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7708  | total loss: \u001b[1m\u001b[32m1.16268\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7708 | loss: 1.16268 - acc: 0.7841 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7709  | total loss: \u001b[1m\u001b[32m1.08478\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7709 | loss: 1.08478 - acc: 0.8057 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7710  | total loss: \u001b[1m\u001b[32m1.33267\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7710 | loss: 1.33267 - acc: 0.7322 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7711  | total loss: \u001b[1m\u001b[32m1.23492\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7711 | loss: 1.23492 - acc: 0.7590 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7712  | total loss: \u001b[1m\u001b[32m1.46979\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7712 | loss: 1.46979 - acc: 0.6903 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7713  | total loss: \u001b[1m\u001b[32m1.35696\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7713 | loss: 1.35696 - acc: 0.7212 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7714  | total loss: \u001b[1m\u001b[32m1.59216\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7714 | loss: 1.59216 - acc: 0.6562 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7715  | total loss: \u001b[1m\u001b[32m1.46689\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7715 | loss: 1.46689 - acc: 0.6906 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7716  | total loss: \u001b[1m\u001b[32m1.35412\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7716 | loss: 1.35412 - acc: 0.7216 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7717  | total loss: \u001b[1m\u001b[32m1.25228\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7717 | loss: 1.25228 - acc: 0.7494 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7718  | total loss: \u001b[1m\u001b[32m1.16001\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7718 | loss: 1.16001 - acc: 0.7745 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7719  | total loss: \u001b[1m\u001b[32m1.07611\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7719 | loss: 1.07611 - acc: 0.7970 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7720  | total loss: \u001b[1m\u001b[32m1.24611\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7720 | loss: 1.24611 - acc: 0.7530 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7721  | total loss: \u001b[1m\u001b[32m1.15205\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7721 | loss: 1.15205 - acc: 0.7777 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7722  | total loss: \u001b[1m\u001b[32m1.06668\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7722 | loss: 1.06668 - acc: 0.8000 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7723  | total loss: \u001b[1m\u001b[32m0.98895\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7723 | loss: 0.98895 - acc: 0.8200 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7724  | total loss: \u001b[1m\u001b[32m1.29076\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7724 | loss: 1.29076 - acc: 0.7380 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7725  | total loss: \u001b[1m\u001b[32m1.18949\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7725 | loss: 1.18949 - acc: 0.7642 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7726  | total loss: \u001b[1m\u001b[32m1.48229\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7726 | loss: 1.48229 - acc: 0.6877 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7727  | total loss: \u001b[1m\u001b[32m1.36210\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7727 | loss: 1.36210 - acc: 0.7190 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7728  | total loss: \u001b[1m\u001b[32m1.63703\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7728 | loss: 1.63703 - acc: 0.6471 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7729  | total loss: \u001b[1m\u001b[32m1.50271\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7729 | loss: 1.50271 - acc: 0.6824 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7730  | total loss: \u001b[1m\u001b[32m1.74224\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7730 | loss: 1.74224 - acc: 0.6141 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7731  | total loss: \u001b[1m\u001b[32m1.59967\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7731 | loss: 1.59967 - acc: 0.6527 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7732  | total loss: \u001b[1m\u001b[32m1.82859\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7732 | loss: 1.82859 - acc: 0.5874 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7733  | total loss: \u001b[1m\u001b[32m1.68036\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7733 | loss: 1.68036 - acc: 0.6287 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7734  | total loss: \u001b[1m\u001b[32m1.54832\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7734 | loss: 1.54832 - acc: 0.6658 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7735  | total loss: \u001b[1m\u001b[32m1.43041\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7735 | loss: 1.43041 - acc: 0.6992 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7736  | total loss: \u001b[1m\u001b[32m1.64274\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7736 | loss: 1.64274 - acc: 0.6436 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7737  | total loss: \u001b[1m\u001b[32m1.51692\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7737 | loss: 1.51692 - acc: 0.6792 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7738  | total loss: \u001b[1m\u001b[32m1.72819\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7738 | loss: 1.72819 - acc: 0.6113 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7739  | total loss: \u001b[1m\u001b[32m1.59581\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7739 | loss: 1.59581 - acc: 0.6502 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7740  | total loss: \u001b[1m\u001b[32m1.47753\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7740 | loss: 1.47753 - acc: 0.6852 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7741  | total loss: \u001b[1m\u001b[32m1.37146\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7741 | loss: 1.37146 - acc: 0.7167 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7742  | total loss: \u001b[1m\u001b[32m1.27592\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7742 | loss: 1.27592 - acc: 0.7450 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7743  | total loss: \u001b[1m\u001b[32m1.18946\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7743 | loss: 1.18946 - acc: 0.7705 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7744  | total loss: \u001b[1m\u001b[32m1.40405\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7744 | loss: 1.40405 - acc: 0.7006 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7745  | total loss: \u001b[1m\u001b[32m1.30389\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7745 | loss: 1.30389 - acc: 0.7305 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7746  | total loss: \u001b[1m\u001b[32m1.46572\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7746 | loss: 1.46572 - acc: 0.6789 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7747  | total loss: \u001b[1m\u001b[32m1.35913\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7747 | loss: 1.35913 - acc: 0.7110 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7748  | total loss: \u001b[1m\u001b[32m1.59973\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7748 | loss: 1.59973 - acc: 0.6399 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7749  | total loss: \u001b[1m\u001b[32m1.48012\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7749 | loss: 1.48012 - acc: 0.6759 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7750  | total loss: \u001b[1m\u001b[32m1.64811\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7750 | loss: 1.64811 - acc: 0.6226 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7751  | total loss: \u001b[1m\u001b[32m1.52461\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7751 | loss: 1.52461 - acc: 0.6604 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7752  | total loss: \u001b[1m\u001b[32m1.73884\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7752 | loss: 1.73884 - acc: 0.5943 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7753  | total loss: \u001b[1m\u001b[32m1.60773\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7753 | loss: 1.60773 - acc: 0.6349 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7754  | total loss: \u001b[1m\u001b[32m1.81338\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7754 | loss: 1.81338 - acc: 0.5714 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7755  | total loss: \u001b[1m\u001b[32m1.67684\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7755 | loss: 1.67684 - acc: 0.6143 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7756  | total loss: \u001b[1m\u001b[32m1.87300\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7756 | loss: 1.87300 - acc: 0.5528 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7757  | total loss: \u001b[1m\u001b[32m1.73300\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7757 | loss: 1.73300 - acc: 0.5975 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7758  | total loss: \u001b[1m\u001b[32m1.60803\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7758 | loss: 1.60803 - acc: 0.6378 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7759  | total loss: \u001b[1m\u001b[32m1.49599\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7759 | loss: 1.49599 - acc: 0.6740 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7760  | total loss: \u001b[1m\u001b[32m1.71630\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7760 | loss: 1.71630 - acc: 0.6066 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7761  | total loss: \u001b[1m\u001b[32m1.59412\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7761 | loss: 1.59412 - acc: 0.6460 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7762  | total loss: \u001b[1m\u001b[32m1.48437\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7762 | loss: 1.48437 - acc: 0.6814 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7763  | total loss: \u001b[1m\u001b[32m1.38526\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7763 | loss: 1.38526 - acc: 0.7132 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7764  | total loss: \u001b[1m\u001b[32m1.54095\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7764 | loss: 1.54095 - acc: 0.6633 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7765  | total loss: \u001b[1m\u001b[32m1.43521\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7765 | loss: 1.43521 - acc: 0.6970 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7766  | total loss: \u001b[1m\u001b[32m1.63203\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7766 | loss: 1.63203 - acc: 0.6344 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7767  | total loss: \u001b[1m\u001b[32m1.51670\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7767 | loss: 1.51670 - acc: 0.6710 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7768  | total loss: \u001b[1m\u001b[32m1.69665\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7768 | loss: 1.69665 - acc: 0.6110 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7769  | total loss: \u001b[1m\u001b[32m1.57491\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7769 | loss: 1.57491 - acc: 0.6499 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7770  | total loss: \u001b[1m\u001b[32m1.73074\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7770 | loss: 1.73074 - acc: 0.5992 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7771  | total loss: \u001b[1m\u001b[32m1.60602\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7771 | loss: 1.60602 - acc: 0.6393 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7772  | total loss: \u001b[1m\u001b[32m1.78203\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7772 | loss: 1.78203 - acc: 0.5825 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7773  | total loss: \u001b[1m\u001b[32m1.65290\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7773 | loss: 1.65290 - acc: 0.6243 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7774  | total loss: \u001b[1m\u001b[32m1.81260\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7774 | loss: 1.81260 - acc: 0.5690 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7775  | total loss: \u001b[1m\u001b[32m1.68149\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7775 | loss: 1.68149 - acc: 0.6121 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7776  | total loss: \u001b[1m\u001b[32m1.56382\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7776 | loss: 1.56382 - acc: 0.6509 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7777  | total loss: \u001b[1m\u001b[32m1.45768\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7777 | loss: 1.45768 - acc: 0.6858 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7778  | total loss: \u001b[1m\u001b[32m1.67375\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7778 | loss: 1.67375 - acc: 0.6172 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7779  | total loss: \u001b[1m\u001b[32m1.55607\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7779 | loss: 1.55607 - acc: 0.6555 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7780  | total loss: \u001b[1m\u001b[32m1.71962\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7780 | loss: 1.71962 - acc: 0.5971 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7781  | total loss: \u001b[1m\u001b[32m1.59743\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7781 | loss: 1.59743 - acc: 0.6374 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7782  | total loss: \u001b[1m\u001b[32m1.77511\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7782 | loss: 1.77511 - acc: 0.5808 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7783  | total loss: \u001b[1m\u001b[32m1.64785\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7783 | loss: 1.64785 - acc: 0.6227 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7784  | total loss: \u001b[1m\u001b[32m1.76270\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7784 | loss: 1.76270 - acc: 0.5890 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7785  | total loss: \u001b[1m\u001b[32m1.63721\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7785 | loss: 1.63721 - acc: 0.6301 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7786  | total loss: \u001b[1m\u001b[32m1.83882\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7786 | loss: 1.83882 - acc: 0.5671 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7787  | total loss: \u001b[1m\u001b[32m1.70632\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7787 | loss: 1.70632 - acc: 0.6104 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7788  | total loss: \u001b[1m\u001b[32m1.83113\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7788 | loss: 1.83113 - acc: 0.5636 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7789  | total loss: \u001b[1m\u001b[32m1.70033\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7789 | loss: 1.70033 - acc: 0.6073 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7790  | total loss: \u001b[1m\u001b[32m1.86024\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7790 | loss: 1.86024 - acc: 0.5465 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7791  | total loss: \u001b[1m\u001b[32m1.72780\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7791 | loss: 1.72780 - acc: 0.5919 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7792  | total loss: \u001b[1m\u001b[32m1.88985\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7792 | loss: 1.88985 - acc: 0.5398 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7793  | total loss: \u001b[1m\u001b[32m1.75595\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7793 | loss: 1.75595 - acc: 0.5859 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7794  | total loss: \u001b[1m\u001b[32m1.92554\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7794 | loss: 1.92554 - acc: 0.5273 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7795  | total loss: \u001b[1m\u001b[32m1.78972\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7795 | loss: 1.78972 - acc: 0.5745 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7796  | total loss: \u001b[1m\u001b[32m1.66799\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7796 | loss: 1.66799 - acc: 0.6171 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7797  | total loss: \u001b[1m\u001b[32m1.55825\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7797 | loss: 1.55825 - acc: 0.6554 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7798  | total loss: \u001b[1m\u001b[32m1.45868\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7798 | loss: 1.45868 - acc: 0.6898 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7799  | total loss: \u001b[1m\u001b[32m1.36772\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7799 | loss: 1.36772 - acc: 0.7209 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7800  | total loss: \u001b[1m\u001b[32m1.28407\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7800 | loss: 1.28407 - acc: 0.7488 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7801  | total loss: \u001b[1m\u001b[32m1.20664\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7801 | loss: 1.20664 - acc: 0.7739 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7802  | total loss: \u001b[1m\u001b[32m1.45042\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7802 | loss: 1.45042 - acc: 0.6965 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7803  | total loss: \u001b[1m\u001b[32m1.35266\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7803 | loss: 1.35266 - acc: 0.7269 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7804  | total loss: \u001b[1m\u001b[32m1.54480\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7804 | loss: 1.54480 - acc: 0.6685 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7805  | total loss: \u001b[1m\u001b[32m1.43528\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7805 | loss: 1.43528 - acc: 0.7016 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7806  | total loss: \u001b[1m\u001b[32m1.65261\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7806 | loss: 1.65261 - acc: 0.6314 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7807  | total loss: \u001b[1m\u001b[32m1.53112\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7807 | loss: 1.53112 - acc: 0.6683 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7808  | total loss: \u001b[1m\u001b[32m1.75740\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7808 | loss: 1.75740 - acc: 0.6015 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7809  | total loss: \u001b[1m\u001b[32m1.62542\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7809 | loss: 1.62542 - acc: 0.6413 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7810  | total loss: \u001b[1m\u001b[32m1.81832\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7810 | loss: 1.81832 - acc: 0.5772 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7811  | total loss: \u001b[1m\u001b[32m1.68120\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7811 | loss: 1.68120 - acc: 0.6195 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7812  | total loss: \u001b[1m\u001b[32m1.87417\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7812 | loss: 1.87417 - acc: 0.5575 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7813  | total loss: \u001b[1m\u001b[32m1.73311\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7813 | loss: 1.73311 - acc: 0.6018 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7814  | total loss: \u001b[1m\u001b[32m1.87002\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7814 | loss: 1.87002 - acc: 0.5559 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7815  | total loss: \u001b[1m\u001b[32m1.73130\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7815 | loss: 1.73130 - acc: 0.6003 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7816  | total loss: \u001b[1m\u001b[32m1.60710\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7816 | loss: 1.60710 - acc: 0.6403 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7817  | total loss: \u001b[1m\u001b[32m1.49537\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7817 | loss: 1.49537 - acc: 0.6762 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7818  | total loss: \u001b[1m\u001b[32m1.63372\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7818 | loss: 1.63372 - acc: 0.6300 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7819  | total loss: \u001b[1m\u001b[32m1.51895\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7819 | loss: 1.51895 - acc: 0.6670 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7820  | total loss: \u001b[1m\u001b[32m1.72947\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7820 | loss: 1.72947 - acc: 0.6003 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7821  | total loss: \u001b[1m\u001b[32m1.60524\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7821 | loss: 1.60524 - acc: 0.6403 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7822  | total loss: \u001b[1m\u001b[32m1.80100\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7822 | loss: 1.80100 - acc: 0.5834 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7823  | total loss: \u001b[1m\u001b[32m1.67036\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7823 | loss: 1.67036 - acc: 0.6251 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7824  | total loss: \u001b[1m\u001b[32m1.83452\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7824 | loss: 1.83452 - acc: 0.5697 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7825  | total loss: \u001b[1m\u001b[32m1.70167\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7825 | loss: 1.70167 - acc: 0.6127 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7826  | total loss: \u001b[1m\u001b[32m1.58241\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7826 | loss: 1.58241 - acc: 0.6515 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7827  | total loss: \u001b[1m\u001b[32m1.47482\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7827 | loss: 1.47482 - acc: 0.6863 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7828  | total loss: \u001b[1m\u001b[32m1.37718\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7828 | loss: 1.37718 - acc: 0.7177 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7829  | total loss: \u001b[1m\u001b[32m1.28806\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7829 | loss: 1.28806 - acc: 0.7459 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7830  | total loss: \u001b[1m\u001b[32m1.51438\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7830 | loss: 1.51438 - acc: 0.6713 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7831  | total loss: \u001b[1m\u001b[32m1.40934\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7831 | loss: 1.40934 - acc: 0.7042 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7832  | total loss: \u001b[1m\u001b[32m1.60864\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7832 | loss: 1.60864 - acc: 0.6409 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7833  | total loss: \u001b[1m\u001b[32m1.49312\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7833 | loss: 1.49312 - acc: 0.6768 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7834  | total loss: \u001b[1m\u001b[32m1.70345\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7834 | loss: 1.70345 - acc: 0.6163 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7835  | total loss: \u001b[1m\u001b[32m1.57817\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7835 | loss: 1.57817 - acc: 0.6547 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7836  | total loss: \u001b[1m\u001b[32m1.76447\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7836 | loss: 1.76447 - acc: 0.5963 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7837  | total loss: \u001b[1m\u001b[32m1.63347\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7837 | loss: 1.63347 - acc: 0.6367 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7838  | total loss: \u001b[1m\u001b[32m1.51565\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7838 | loss: 1.51565 - acc: 0.6730 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7839  | total loss: \u001b[1m\u001b[32m1.40921\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7839 | loss: 1.40921 - acc: 0.7057 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7840  | total loss: \u001b[1m\u001b[32m1.58609\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7840 | loss: 1.58609 - acc: 0.6566 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7841  | total loss: \u001b[1m\u001b[32m1.47163\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7841 | loss: 1.47163 - acc: 0.6909 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7842  | total loss: \u001b[1m\u001b[32m1.67638\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7842 | loss: 1.67638 - acc: 0.6290 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7843  | total loss: \u001b[1m\u001b[32m1.55268\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7843 | loss: 1.55268 - acc: 0.6661 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7844  | total loss: \u001b[1m\u001b[32m1.73132\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7844 | loss: 1.73132 - acc: 0.6066 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7845  | total loss: \u001b[1m\u001b[32m1.60261\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7845 | loss: 1.60261 - acc: 0.6460 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7846  | total loss: \u001b[1m\u001b[32m1.76504\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7846 | loss: 1.76504 - acc: 0.5885 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7847  | total loss: \u001b[1m\u001b[32m1.63388\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7847 | loss: 1.63388 - acc: 0.6296 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7848  | total loss: \u001b[1m\u001b[32m1.51612\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7848 | loss: 1.51612 - acc: 0.6667 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7849  | total loss: \u001b[1m\u001b[32m1.40991\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7849 | loss: 1.40991 - acc: 0.7000 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7850  | total loss: \u001b[1m\u001b[32m1.60304\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7850 | loss: 1.60304 - acc: 0.6372 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7851  | total loss: \u001b[1m\u001b[32m1.48756\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7851 | loss: 1.48756 - acc: 0.6734 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7852  | total loss: \u001b[1m\u001b[32m1.66880\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7852 | loss: 1.66880 - acc: 0.6132 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7853  | total loss: \u001b[1m\u001b[32m1.54681\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7853 | loss: 1.54681 - acc: 0.6519 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7854  | total loss: \u001b[1m\u001b[32m1.74771\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7854 | loss: 1.74771 - acc: 0.5867 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7855  | total loss: \u001b[1m\u001b[32m1.61861\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7855 | loss: 1.61861 - acc: 0.6281 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7856  | total loss: \u001b[1m\u001b[32m1.82356\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7856 | loss: 1.82356 - acc: 0.5652 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7857  | total loss: \u001b[1m\u001b[32m1.68834\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7857 | loss: 1.68834 - acc: 0.6087 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7858  | total loss: \u001b[1m\u001b[32m1.81854\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7858 | loss: 1.81854 - acc: 0.5621 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7859  | total loss: \u001b[1m\u001b[32m1.68560\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7859 | loss: 1.68560 - acc: 0.6059 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7860  | total loss: \u001b[1m\u001b[32m1.87728\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7860 | loss: 1.87728 - acc: 0.5453 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7861  | total loss: \u001b[1m\u001b[32m1.74052\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7861 | loss: 1.74052 - acc: 0.5908 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7862  | total loss: \u001b[1m\u001b[32m1.87545\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7862 | loss: 1.87545 - acc: 0.5389 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7863  | total loss: \u001b[1m\u001b[32m1.74118\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7863 | loss: 1.74118 - acc: 0.5850 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7864  | total loss: \u001b[1m\u001b[32m1.62110\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7864 | loss: 1.62110 - acc: 0.6265 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7865  | total loss: \u001b[1m\u001b[32m1.51311\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7865 | loss: 1.51311 - acc: 0.6638 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7866  | total loss: \u001b[1m\u001b[32m1.71578\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7866 | loss: 1.71578 - acc: 0.5974 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7867  | total loss: \u001b[1m\u001b[32m1.59820\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7867 | loss: 1.59820 - acc: 0.6377 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7868  | total loss: \u001b[1m\u001b[32m1.74792\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7868 | loss: 1.74792 - acc: 0.5882 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7869  | total loss: \u001b[1m\u001b[32m1.62742\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7869 | loss: 1.62742 - acc: 0.6294 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7870  | total loss: \u001b[1m\u001b[32m1.79612\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7870 | loss: 1.79612 - acc: 0.5736 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7871  | total loss: \u001b[1m\u001b[32m1.67127\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7871 | loss: 1.67127 - acc: 0.6162 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7872  | total loss: \u001b[1m\u001b[32m1.84984\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7872 | loss: 1.84984 - acc: 0.5618 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7873  | total loss: \u001b[1m\u001b[32m1.72031\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7873 | loss: 1.72031 - acc: 0.6056 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7874  | total loss: \u001b[1m\u001b[32m1.83817\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7874 | loss: 1.83817 - acc: 0.5593 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7875  | total loss: \u001b[1m\u001b[32m1.71055\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7875 | loss: 1.71055 - acc: 0.6034 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7876  | total loss: \u001b[1m\u001b[32m1.86074\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7876 | loss: 1.86074 - acc: 0.5573 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7877  | total loss: \u001b[1m\u001b[32m1.73160\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7877 | loss: 1.73160 - acc: 0.6016 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7878  | total loss: \u001b[1m\u001b[32m1.84302\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7878 | loss: 1.84302 - acc: 0.5629 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7879  | total loss: \u001b[1m\u001b[32m1.71624\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7879 | loss: 1.71624 - acc: 0.6066 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7880  | total loss: \u001b[1m\u001b[32m1.60199\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7880 | loss: 1.60199 - acc: 0.6459 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7881  | total loss: \u001b[1m\u001b[32m1.49838\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7881 | loss: 1.49838 - acc: 0.6813 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7882  | total loss: \u001b[1m\u001b[32m1.40380\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7882 | loss: 1.40380 - acc: 0.7132 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7883  | total loss: \u001b[1m\u001b[32m1.31686\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7883 | loss: 1.31686 - acc: 0.7419 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7884  | total loss: \u001b[1m\u001b[32m1.52431\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7884 | loss: 1.52431 - acc: 0.6748 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7885  | total loss: \u001b[1m\u001b[32m1.42200\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7885 | loss: 1.42200 - acc: 0.7073 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7886  | total loss: \u001b[1m\u001b[32m1.64909\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7886 | loss: 1.64909 - acc: 0.6366 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7887  | total loss: \u001b[1m\u001b[32m1.53225\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7887 | loss: 1.53225 - acc: 0.6730 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7888  | total loss: \u001b[1m\u001b[32m1.71059\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7888 | loss: 1.71059 - acc: 0.6128 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7889  | total loss: \u001b[1m\u001b[32m1.58657\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7889 | loss: 1.58657 - acc: 0.6515 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7890  | total loss: \u001b[1m\u001b[32m1.47440\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7890 | loss: 1.47440 - acc: 0.6864 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7891  | total loss: \u001b[1m\u001b[32m1.37248\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 7891 | loss: 1.37248 - acc: 0.7177 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7892  | total loss: \u001b[1m\u001b[32m1.61575\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 7892 | loss: 1.61575 - acc: 0.6460 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7893  | total loss: \u001b[1m\u001b[32m1.49808\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7893 | loss: 1.49808 - acc: 0.6814 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7894  | total loss: \u001b[1m\u001b[32m1.68968\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7894 | loss: 1.68968 - acc: 0.6204 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7895  | total loss: \u001b[1m\u001b[32m1.56396\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7895 | loss: 1.56396 - acc: 0.6583 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7896  | total loss: \u001b[1m\u001b[32m1.72866\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7896 | loss: 1.72866 - acc: 0.5996 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7897  | total loss: \u001b[1m\u001b[32m1.59909\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7897 | loss: 1.59909 - acc: 0.6397 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7898  | total loss: \u001b[1m\u001b[32m1.75414\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7898 | loss: 1.75414 - acc: 0.5900 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7899  | total loss: \u001b[1m\u001b[32m1.62254\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7899 | loss: 1.62254 - acc: 0.6310 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7900  | total loss: \u001b[1m\u001b[32m1.50419\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7900 | loss: 1.50419 - acc: 0.6679 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7901  | total loss: \u001b[1m\u001b[32m1.39731\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7901 | loss: 1.39731 - acc: 0.7011 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7902  | total loss: \u001b[1m\u001b[32m1.64134\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7902 | loss: 1.64134 - acc: 0.6310 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7903  | total loss: \u001b[1m\u001b[32m1.52017\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7903 | loss: 1.52017 - acc: 0.6679 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7904  | total loss: \u001b[1m\u001b[32m1.74101\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7904 | loss: 1.74101 - acc: 0.6011 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7905  | total loss: \u001b[1m\u001b[32m1.61023\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7905 | loss: 1.61023 - acc: 0.6410 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7906  | total loss: \u001b[1m\u001b[32m1.81458\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7906 | loss: 1.81458 - acc: 0.5769 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7907  | total loss: \u001b[1m\u001b[32m1.67761\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7907 | loss: 1.67761 - acc: 0.6192 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7908  | total loss: \u001b[1m\u001b[32m1.82710\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7908 | loss: 1.82710 - acc: 0.5716 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7909  | total loss: \u001b[1m\u001b[32m1.69052\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7909 | loss: 1.69052 - acc: 0.6144 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7910  | total loss: \u001b[1m\u001b[32m1.56819\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7910 | loss: 1.56819 - acc: 0.6530 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7911  | total loss: \u001b[1m\u001b[32m1.45815\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7911 | loss: 1.45815 - acc: 0.6877 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7912  | total loss: \u001b[1m\u001b[32m1.67295\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7912 | loss: 1.67295 - acc: 0.6189 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7913  | total loss: \u001b[1m\u001b[32m1.55249\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7913 | loss: 1.55249 - acc: 0.6570 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7914  | total loss: \u001b[1m\u001b[32m1.44402\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7914 | loss: 1.44402 - acc: 0.6913 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7915  | total loss: \u001b[1m\u001b[32m1.34586\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7915 | loss: 1.34586 - acc: 0.7222 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7916  | total loss: \u001b[1m\u001b[32m1.57030\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7916 | loss: 1.57030 - acc: 0.6571 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7917  | total loss: \u001b[1m\u001b[32m1.45842\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7917 | loss: 1.45842 - acc: 0.6914 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7918  | total loss: \u001b[1m\u001b[32m1.69551\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7918 | loss: 1.69551 - acc: 0.6223 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7919  | total loss: \u001b[1m\u001b[32m1.57090\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7919 | loss: 1.57090 - acc: 0.6600 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7920  | total loss: \u001b[1m\u001b[32m1.70290\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7920 | loss: 1.70290 - acc: 0.6155 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7921  | total loss: \u001b[1m\u001b[32m1.57791\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7921 | loss: 1.57791 - acc: 0.6539 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7922  | total loss: \u001b[1m\u001b[32m1.77203\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7922 | loss: 1.77203 - acc: 0.5957 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7923  | total loss: \u001b[1m\u001b[32m1.64090\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7923 | loss: 1.64090 - acc: 0.6361 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7924  | total loss: \u001b[1m\u001b[32m1.82426\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7924 | loss: 1.82426 - acc: 0.5725 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7925  | total loss: \u001b[1m\u001b[32m1.68919\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7925 | loss: 1.68919 - acc: 0.6152 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7926  | total loss: \u001b[1m\u001b[32m1.56808\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7926 | loss: 1.56808 - acc: 0.6537 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7927  | total loss: \u001b[1m\u001b[32m1.45899\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7927 | loss: 1.45899 - acc: 0.6883 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7928  | total loss: \u001b[1m\u001b[32m1.36025\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7928 | loss: 1.36025 - acc: 0.7195 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7929  | total loss: \u001b[1m\u001b[32m1.27039\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7929 | loss: 1.27039 - acc: 0.7476 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7930  | total loss: \u001b[1m\u001b[32m1.51396\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7930 | loss: 1.51396 - acc: 0.6728 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7931  | total loss: \u001b[1m\u001b[32m1.40696\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7931 | loss: 1.40696 - acc: 0.7055 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7932  | total loss: \u001b[1m\u001b[32m1.60838\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7932 | loss: 1.60838 - acc: 0.6421 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7933  | total loss: \u001b[1m\u001b[32m1.49112\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7933 | loss: 1.49112 - acc: 0.6779 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7934  | total loss: \u001b[1m\u001b[32m1.64627\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7934 | loss: 1.64627 - acc: 0.6315 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7935  | total loss: \u001b[1m\u001b[32m1.52496\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7935 | loss: 1.52496 - acc: 0.6684 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7936  | total loss: \u001b[1m\u001b[32m1.75638\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7936 | loss: 1.75638 - acc: 0.6015 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7937  | total loss: \u001b[1m\u001b[32m1.62432\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7937 | loss: 1.62432 - acc: 0.6414 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7938  | total loss: \u001b[1m\u001b[32m1.81204\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7938 | loss: 1.81204 - acc: 0.5844 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7939  | total loss: \u001b[1m\u001b[32m1.67526\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7939 | loss: 1.67526 - acc: 0.6260 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7940  | total loss: \u001b[1m\u001b[32m1.89325\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7940 | loss: 1.89325 - acc: 0.5634 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7941  | total loss: \u001b[1m\u001b[32m1.74970\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7941 | loss: 1.74970 - acc: 0.6070 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7942  | total loss: \u001b[1m\u001b[32m1.62104\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7942 | loss: 1.62104 - acc: 0.6463 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7943  | total loss: \u001b[1m\u001b[32m1.50526\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7943 | loss: 1.50526 - acc: 0.6817 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7944  | total loss: \u001b[1m\u001b[32m1.40060\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7944 | loss: 1.40060 - acc: 0.7135 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7945  | total loss: \u001b[1m\u001b[32m1.30554\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7945 | loss: 1.30554 - acc: 0.7422 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7946  | total loss: \u001b[1m\u001b[32m1.51546\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7946 | loss: 1.51546 - acc: 0.6751 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7947  | total loss: \u001b[1m\u001b[32m1.40730\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7947 | loss: 1.40730 - acc: 0.7076 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7948  | total loss: \u001b[1m\u001b[32m1.30917\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7948 | loss: 1.30917 - acc: 0.7368 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7949  | total loss: \u001b[1m\u001b[32m1.21974\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7949 | loss: 1.21974 - acc: 0.7631 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7950  | total loss: \u001b[1m\u001b[32m1.43619\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7950 | loss: 1.43619 - acc: 0.6940 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7951  | total loss: \u001b[1m\u001b[32m1.33214\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7951 | loss: 1.33214 - acc: 0.7246 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7952  | total loss: \u001b[1m\u001b[32m1.52419\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7952 | loss: 1.52419 - acc: 0.6664 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7953  | total loss: \u001b[1m\u001b[32m1.41038\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7953 | loss: 1.41038 - acc: 0.6998 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7954  | total loss: \u001b[1m\u001b[32m1.65452\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7954 | loss: 1.65452 - acc: 0.6298 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7955  | total loss: \u001b[1m\u001b[32m1.52771\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7955 | loss: 1.52771 - acc: 0.6668 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7956  | total loss: \u001b[1m\u001b[32m1.41364\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7956 | loss: 1.41364 - acc: 0.7001 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7957  | total loss: \u001b[1m\u001b[32m1.31064\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7957 | loss: 1.31064 - acc: 0.7301 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7958  | total loss: \u001b[1m\u001b[32m1.53507\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7958 | loss: 1.53507 - acc: 0.6642 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7959  | total loss: \u001b[1m\u001b[32m1.41943\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7959 | loss: 1.41943 - acc: 0.6978 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7960  | total loss: \u001b[1m\u001b[32m1.59095\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7960 | loss: 1.59095 - acc: 0.6423 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7961  | total loss: \u001b[1m\u001b[32m1.46985\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7961 | loss: 1.46985 - acc: 0.6781 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7962  | total loss: \u001b[1m\u001b[32m1.36083\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7962 | loss: 1.36083 - acc: 0.7103 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7963  | total loss: \u001b[1m\u001b[32m1.26232\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7963 | loss: 1.26232 - acc: 0.7393 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7964  | total loss: \u001b[1m\u001b[32m1.47593\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7964 | loss: 1.47593 - acc: 0.6796 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7965  | total loss: \u001b[1m\u001b[32m1.36536\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7965 | loss: 1.36536 - acc: 0.7117 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7966  | total loss: \u001b[1m\u001b[32m1.64136\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7966 | loss: 1.64136 - acc: 0.6405 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7967  | total loss: \u001b[1m\u001b[32m1.51481\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7967 | loss: 1.51481 - acc: 0.6764 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7968  | total loss: \u001b[1m\u001b[32m1.72418\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7968 | loss: 1.72418 - acc: 0.6159 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7969  | total loss: \u001b[1m\u001b[32m1.59080\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7969 | loss: 1.59080 - acc: 0.6543 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7970  | total loss: \u001b[1m\u001b[32m1.76669\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7970 | loss: 1.76669 - acc: 0.6032 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7971  | total loss: \u001b[1m\u001b[32m1.63100\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7971 | loss: 1.63100 - acc: 0.6429 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7972  | total loss: \u001b[1m\u001b[32m1.86065\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7972 | loss: 1.86065 - acc: 0.5786 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7973  | total loss: \u001b[1m\u001b[32m1.71792\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7973 | loss: 1.71792 - acc: 0.6207 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7974  | total loss: \u001b[1m\u001b[32m1.59048\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7974 | loss: 1.59048 - acc: 0.6587 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7975  | total loss: \u001b[1m\u001b[32m1.47625\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7975 | loss: 1.47625 - acc: 0.6928 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7976  | total loss: \u001b[1m\u001b[32m1.68274\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7976 | loss: 1.68274 - acc: 0.6307 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7977  | total loss: \u001b[1m\u001b[32m1.55988\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7977 | loss: 1.55988 - acc: 0.6676 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7978  | total loss: \u001b[1m\u001b[32m1.74884\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7978 | loss: 1.74884 - acc: 0.6080 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7979  | total loss: \u001b[1m\u001b[32m1.62026\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7979 | loss: 1.62026 - acc: 0.6472 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7980  | total loss: \u001b[1m\u001b[32m1.50478\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7980 | loss: 1.50478 - acc: 0.6825 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7981  | total loss: \u001b[1m\u001b[32m1.40060\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7981 | loss: 1.40060 - acc: 0.7142 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7982  | total loss: \u001b[1m\u001b[32m1.59030\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7982 | loss: 1.59030 - acc: 0.6499 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7983  | total loss: \u001b[1m\u001b[32m1.47702\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7983 | loss: 1.47702 - acc: 0.6849 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7984  | total loss: \u001b[1m\u001b[32m1.68365\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7984 | loss: 1.68365 - acc: 0.6236 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7985  | total loss: \u001b[1m\u001b[32m1.56115\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7985 | loss: 1.56115 - acc: 0.6612 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7986  | total loss: \u001b[1m\u001b[32m1.74529\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7986 | loss: 1.74529 - acc: 0.6023 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7987  | total loss: \u001b[1m\u001b[32m1.61722\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7987 | loss: 1.61722 - acc: 0.6420 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7988  | total loss: \u001b[1m\u001b[32m1.79088\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7988 | loss: 1.79088 - acc: 0.5850 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7989  | total loss: \u001b[1m\u001b[32m1.65924\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7989 | loss: 1.65924 - acc: 0.6265 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7990  | total loss: \u001b[1m\u001b[32m1.82505\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7990 | loss: 1.82505 - acc: 0.5710 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7991  | total loss: \u001b[1m\u001b[32m1.69134\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 7991 | loss: 1.69134 - acc: 0.6139 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7992  | total loss: \u001b[1m\u001b[32m1.82635\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7992 | loss: 1.82635 - acc: 0.5668 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7993  | total loss: \u001b[1m\u001b[32m1.69402\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7993 | loss: 1.69402 - acc: 0.6101 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7994  | total loss: \u001b[1m\u001b[32m1.85796\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 7994 | loss: 1.85796 - acc: 0.5562 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7995  | total loss: \u001b[1m\u001b[32m1.72409\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7995 | loss: 1.72409 - acc: 0.6006 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7996  | total loss: \u001b[1m\u001b[32m1.88351\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 7996 | loss: 1.88351 - acc: 0.5477 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7997  | total loss: \u001b[1m\u001b[32m1.74886\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7997 | loss: 1.74886 - acc: 0.5929 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7998  | total loss: \u001b[1m\u001b[32m1.90914\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7998 | loss: 1.90914 - acc: 0.5408 -- iter: 14/14\n",
      "--\n",
      "Training Step: 7999  | total loss: \u001b[1m\u001b[32m1.77369\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 7999 | loss: 1.77369 - acc: 0.5867 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8000  | total loss: \u001b[1m\u001b[32m1.93464\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8000 | loss: 1.93464 - acc: 0.5280 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8001  | total loss: \u001b[1m\u001b[32m1.79845\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8001 | loss: 1.79845 - acc: 0.5752 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8002  | total loss: \u001b[1m\u001b[32m1.67646\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8002 | loss: 1.67646 - acc: 0.6177 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8003  | total loss: \u001b[1m\u001b[32m1.56659\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8003 | loss: 1.56659 - acc: 0.6559 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8004  | total loss: \u001b[1m\u001b[32m1.72459\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8004 | loss: 1.72459 - acc: 0.6046 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8005  | total loss: \u001b[1m\u001b[32m1.60918\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8005 | loss: 1.60918 - acc: 0.6442 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8006  | total loss: \u001b[1m\u001b[32m1.79539\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8006 | loss: 1.79539 - acc: 0.5797 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8007  | total loss: \u001b[1m\u001b[32m1.67252\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8007 | loss: 1.67252 - acc: 0.6218 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8008  | total loss: \u001b[1m\u001b[32m1.83673\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8008 | loss: 1.83673 - acc: 0.5667 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8009  | total loss: \u001b[1m\u001b[32m1.70973\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8009 | loss: 1.70973 - acc: 0.6101 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8010  | total loss: \u001b[1m\u001b[32m1.88150\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8010 | loss: 1.88150 - acc: 0.5562 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8011  | total loss: \u001b[1m\u001b[32m1.75020\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8011 | loss: 1.75020 - acc: 0.6006 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8012  | total loss: \u001b[1m\u001b[32m1.63183\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8012 | loss: 1.63183 - acc: 0.6405 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8013  | total loss: \u001b[1m\u001b[32m1.52455\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8013 | loss: 1.52455 - acc: 0.6765 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8014  | total loss: \u001b[1m\u001b[32m1.66495\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8014 | loss: 1.66495 - acc: 0.6302 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8015  | total loss: \u001b[1m\u001b[32m1.55256\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8015 | loss: 1.55256 - acc: 0.6672 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8016  | total loss: \u001b[1m\u001b[32m1.74987\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8016 | loss: 1.74987 - acc: 0.6005 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8017  | total loss: \u001b[1m\u001b[32m1.62787\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8017 | loss: 1.62787 - acc: 0.6405 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8018  | total loss: \u001b[1m\u001b[32m1.83619\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8018 | loss: 1.83619 - acc: 0.5764 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8019  | total loss: \u001b[1m\u001b[32m1.70524\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8019 | loss: 1.70524 - acc: 0.6188 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8020  | total loss: \u001b[1m\u001b[32m1.79271\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8020 | loss: 1.79271 - acc: 0.5855 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8021  | total loss: \u001b[1m\u001b[32m1.66597\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8021 | loss: 1.66597 - acc: 0.6269 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8022  | total loss: \u001b[1m\u001b[32m1.80311\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8022 | loss: 1.80311 - acc: 0.5785 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8023  | total loss: \u001b[1m\u001b[32m1.67516\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8023 | loss: 1.67516 - acc: 0.6207 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8024  | total loss: \u001b[1m\u001b[32m1.85768\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8024 | loss: 1.85768 - acc: 0.5586 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8025  | total loss: \u001b[1m\u001b[32m1.72447\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8025 | loss: 1.72447 - acc: 0.6027 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8026  | total loss: \u001b[1m\u001b[32m1.88887\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8026 | loss: 1.88887 - acc: 0.5425 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8027  | total loss: \u001b[1m\u001b[32m1.75315\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8027 | loss: 1.75315 - acc: 0.5882 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8028  | total loss: \u001b[1m\u001b[32m1.90266\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8028 | loss: 1.90266 - acc: 0.5365 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8029  | total loss: \u001b[1m\u001b[32m1.76640\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8029 | loss: 1.76640 - acc: 0.5829 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8030  | total loss: \u001b[1m\u001b[32m1.87937\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8030 | loss: 1.87937 - acc: 0.5460 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8031  | total loss: \u001b[1m\u001b[32m1.74624\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8031 | loss: 1.74624 - acc: 0.5914 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8032  | total loss: \u001b[1m\u001b[32m1.86147\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8032 | loss: 1.86147 - acc: 0.5466 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8033  | total loss: \u001b[1m\u001b[32m1.73081\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8033 | loss: 1.73081 - acc: 0.5919 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8034  | total loss: \u001b[1m\u001b[32m1.88590\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8034 | loss: 1.88590 - acc: 0.5399 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8035  | total loss: \u001b[1m\u001b[32m1.75355\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8035 | loss: 1.75355 - acc: 0.5859 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8036  | total loss: \u001b[1m\u001b[32m1.90283\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8036 | loss: 1.90283 - acc: 0.5344 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8037  | total loss: \u001b[1m\u001b[32m1.76969\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8037 | loss: 1.76969 - acc: 0.5810 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8038  | total loss: \u001b[1m\u001b[32m1.93350\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8038 | loss: 1.93350 - acc: 0.5300 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8039  | total loss: \u001b[1m\u001b[32m1.79836\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8039 | loss: 1.79836 - acc: 0.5770 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8040  | total loss: \u001b[1m\u001b[32m1.95529\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8040 | loss: 1.95529 - acc: 0.5265 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8041  | total loss: \u001b[1m\u001b[32m1.81918\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8041 | loss: 1.81918 - acc: 0.5738 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8042  | total loss: \u001b[1m\u001b[32m1.98191\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8042 | loss: 1.98191 - acc: 0.5236 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8043  | total loss: \u001b[1m\u001b[32m1.84445\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8043 | loss: 1.84445 - acc: 0.5712 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8044  | total loss: \u001b[1m\u001b[32m1.95352\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8044 | loss: 1.95352 - acc: 0.5355 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8045  | total loss: \u001b[1m\u001b[32m1.81999\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8045 | loss: 1.81999 - acc: 0.5820 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8046  | total loss: \u001b[1m\u001b[32m1.69988\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8046 | loss: 1.69988 - acc: 0.6238 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8047  | total loss: \u001b[1m\u001b[32m1.59122\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8047 | loss: 1.59122 - acc: 0.6614 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8048  | total loss: \u001b[1m\u001b[32m1.49230\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8048 | loss: 1.49230 - acc: 0.6953 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8049  | total loss: \u001b[1m\u001b[32m1.40166\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8049 | loss: 1.40166 - acc: 0.7257 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8050  | total loss: \u001b[1m\u001b[32m1.57517\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8050 | loss: 1.57517 - acc: 0.6603 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8051  | total loss: \u001b[1m\u001b[32m1.47308\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8051 | loss: 1.47308 - acc: 0.6943 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8052  | total loss: \u001b[1m\u001b[32m1.65551\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8052 | loss: 1.65551 - acc: 0.6320 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8053  | total loss: \u001b[1m\u001b[32m1.54309\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8053 | loss: 1.54309 - acc: 0.6688 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8054  | total loss: \u001b[1m\u001b[32m1.72215\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8054 | loss: 1.72215 - acc: 0.6091 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 8055  | total loss: \u001b[1m\u001b[32m1.60153\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8055 | loss: 1.60153 - acc: 0.6481 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8056  | total loss: \u001b[1m\u001b[32m1.78252\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8056 | loss: 1.78252 - acc: 0.5833 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8057  | total loss: \u001b[1m\u001b[32m1.65491\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8057 | loss: 1.65491 - acc: 0.6250 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8058  | total loss: \u001b[1m\u001b[32m1.82203\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8058 | loss: 1.82203 - acc: 0.5696 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8059  | total loss: \u001b[1m\u001b[32m1.69008\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8059 | loss: 1.69008 - acc: 0.6127 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8060  | total loss: \u001b[1m\u001b[32m1.57102\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8060 | loss: 1.57102 - acc: 0.6514 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8061  | total loss: \u001b[1m\u001b[32m1.46311\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8061 | loss: 1.46311 - acc: 0.6863 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8062  | total loss: \u001b[1m\u001b[32m1.36483\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8062 | loss: 1.36483 - acc: 0.7176 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8063  | total loss: \u001b[1m\u001b[32m1.27489\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8063 | loss: 1.27489 - acc: 0.7459 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8064  | total loss: \u001b[1m\u001b[32m1.51059\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8064 | loss: 1.51059 - acc: 0.6713 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8065  | total loss: \u001b[1m\u001b[32m1.40352\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8065 | loss: 1.40352 - acc: 0.7042 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8066  | total loss: \u001b[1m\u001b[32m1.59175\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8066 | loss: 1.59175 - acc: 0.6409 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8067  | total loss: \u001b[1m\u001b[32m1.47522\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8067 | loss: 1.47522 - acc: 0.6768 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8068  | total loss: \u001b[1m\u001b[32m1.65268\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8068 | loss: 1.65268 - acc: 0.6234 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8069  | total loss: \u001b[1m\u001b[32m1.52957\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8069 | loss: 1.52957 - acc: 0.6611 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8070  | total loss: \u001b[1m\u001b[32m1.69606\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8070 | loss: 1.69606 - acc: 0.6021 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8071  | total loss: \u001b[1m\u001b[32m1.56870\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8071 | loss: 1.56870 - acc: 0.6419 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8072  | total loss: \u001b[1m\u001b[32m1.45402\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8072 | loss: 1.45402 - acc: 0.6777 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8073  | total loss: \u001b[1m\u001b[32m1.35037\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8073 | loss: 1.35037 - acc: 0.7099 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8074  | total loss: \u001b[1m\u001b[32m1.57617\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8074 | loss: 1.57617 - acc: 0.6461 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8075  | total loss: \u001b[1m\u001b[32m1.45959\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8075 | loss: 1.45959 - acc: 0.6815 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8076  | total loss: \u001b[1m\u001b[32m1.65713\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8076 | loss: 1.65713 - acc: 0.6205 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8077  | total loss: \u001b[1m\u001b[32m1.53257\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8077 | loss: 1.53257 - acc: 0.6584 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8078  | total loss: \u001b[1m\u001b[32m1.75598\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8078 | loss: 1.75598 - acc: 0.5926 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8079  | total loss: \u001b[1m\u001b[32m1.62246\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8079 | loss: 1.62246 - acc: 0.6333 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8080  | total loss: \u001b[1m\u001b[32m1.82787\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8080 | loss: 1.82787 - acc: 0.5700 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8081  | total loss: \u001b[1m\u001b[32m1.68887\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8081 | loss: 1.68887 - acc: 0.6130 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8082  | total loss: \u001b[1m\u001b[32m1.83245\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8082 | loss: 1.83245 - acc: 0.5660 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8083  | total loss: \u001b[1m\u001b[32m1.69508\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8083 | loss: 1.69508 - acc: 0.6094 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8084  | total loss: \u001b[1m\u001b[32m1.85623\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8084 | loss: 1.85623 - acc: 0.5556 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8085  | total loss: \u001b[1m\u001b[32m1.71875\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8085 | loss: 1.71875 - acc: 0.6000 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8086  | total loss: \u001b[1m\u001b[32m1.87744\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8086 | loss: 1.87744 - acc: 0.5472 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8087  | total loss: \u001b[1m\u001b[32m1.74030\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8087 | loss: 1.74030 - acc: 0.5924 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8088  | total loss: \u001b[1m\u001b[32m1.88183\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8088 | loss: 1.88183 - acc: 0.5475 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8089  | total loss: \u001b[1m\u001b[32m1.74680\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8089 | loss: 1.74680 - acc: 0.5927 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8090  | total loss: \u001b[1m\u001b[32m1.87886\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8090 | loss: 1.87886 - acc: 0.5406 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8091  | total loss: \u001b[1m\u001b[32m1.74663\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8091 | loss: 1.74663 - acc: 0.5865 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8092  | total loss: \u001b[1m\u001b[32m1.88535\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8092 | loss: 1.88535 - acc: 0.5422 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8093  | total loss: \u001b[1m\u001b[32m1.75474\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8093 | loss: 1.75474 - acc: 0.5880 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8094  | total loss: \u001b[1m\u001b[32m1.93860\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8094 | loss: 1.93860 - acc: 0.5292 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8095  | total loss: \u001b[1m\u001b[32m1.80480\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8095 | loss: 1.80480 - acc: 0.5762 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8096  | total loss: \u001b[1m\u001b[32m1.94487\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8096 | loss: 1.94487 - acc: 0.5258 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8097  | total loss: \u001b[1m\u001b[32m1.81249\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8097 | loss: 1.81249 - acc: 0.5732 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8098  | total loss: \u001b[1m\u001b[32m1.99400\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8098 | loss: 1.99400 - acc: 0.5159 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8099  | total loss: \u001b[1m\u001b[32m1.85857\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8099 | loss: 1.85857 - acc: 0.5643 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8100  | total loss: \u001b[1m\u001b[32m1.73716\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8100 | loss: 1.73716 - acc: 0.6079 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8101  | total loss: \u001b[1m\u001b[32m1.62765\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8101 | loss: 1.62765 - acc: 0.6471 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8102  | total loss: \u001b[1m\u001b[32m1.81541\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8102 | loss: 1.81541 - acc: 0.5824 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8103  | total loss: \u001b[1m\u001b[32m1.69717\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8103 | loss: 1.69717 - acc: 0.6241 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8104  | total loss: \u001b[1m\u001b[32m1.85494\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8104 | loss: 1.85494 - acc: 0.5689 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8105  | total loss: \u001b[1m\u001b[32m1.73218\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8105 | loss: 1.73218 - acc: 0.6120 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8106  | total loss: \u001b[1m\u001b[32m1.87823\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8106 | loss: 1.87823 - acc: 0.5579 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8107  | total loss: \u001b[1m\u001b[32m1.75271\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8107 | loss: 1.75271 - acc: 0.6021 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8108  | total loss: \u001b[1m\u001b[32m1.88022\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8108 | loss: 1.88022 - acc: 0.5562 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8109  | total loss: \u001b[1m\u001b[32m1.75406\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8109 | loss: 1.75406 - acc: 0.6006 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8110  | total loss: \u001b[1m\u001b[32m1.63998\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8110 | loss: 1.63998 - acc: 0.6405 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8111  | total loss: \u001b[1m\u001b[32m1.53618\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8111 | loss: 1.53618 - acc: 0.6765 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 8112  | total loss: \u001b[1m\u001b[32m1.65927\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8112 | loss: 1.65927 - acc: 0.6303 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8113  | total loss: \u001b[1m\u001b[32m1.55100\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8113 | loss: 1.55100 - acc: 0.6672 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8114  | total loss: \u001b[1m\u001b[32m1.70299\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8114 | loss: 1.70299 - acc: 0.6148 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8115  | total loss: \u001b[1m\u001b[32m1.58830\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8115 | loss: 1.58830 - acc: 0.6533 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8116  | total loss: \u001b[1m\u001b[32m1.73872\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8116 | loss: 1.73872 - acc: 0.6023 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8117  | total loss: \u001b[1m\u001b[32m1.61903\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8117 | loss: 1.61903 - acc: 0.6420 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8118  | total loss: \u001b[1m\u001b[32m1.75276\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8118 | loss: 1.75276 - acc: 0.5993 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8119  | total loss: \u001b[1m\u001b[32m1.63062\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8119 | loss: 1.63062 - acc: 0.6393 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8120  | total loss: \u001b[1m\u001b[32m1.82572\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8120 | loss: 1.82572 - acc: 0.5754 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8121  | total loss: \u001b[1m\u001b[32m1.69573\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8121 | loss: 1.69573 - acc: 0.6179 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8122  | total loss: \u001b[1m\u001b[32m1.84642\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8122 | loss: 1.84642 - acc: 0.5632 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8123  | total loss: \u001b[1m\u001b[32m1.71435\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8123 | loss: 1.71435 - acc: 0.6069 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8124  | total loss: \u001b[1m\u001b[32m1.89870\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8124 | loss: 1.89870 - acc: 0.5462 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8125  | total loss: \u001b[1m\u001b[32m1.76188\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8125 | loss: 1.76188 - acc: 0.5916 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8126  | total loss: \u001b[1m\u001b[32m1.88302\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8126 | loss: 1.88302 - acc: 0.5539 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8127  | total loss: \u001b[1m\u001b[32m1.74842\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8127 | loss: 1.74842 - acc: 0.5985 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8128  | total loss: \u001b[1m\u001b[32m1.93937\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8128 | loss: 1.93937 - acc: 0.5386 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8129  | total loss: \u001b[1m\u001b[32m1.79995\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8129 | loss: 1.79995 - acc: 0.5848 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8130  | total loss: \u001b[1m\u001b[32m1.96894\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8130 | loss: 1.96894 - acc: 0.5263 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8131  | total loss: \u001b[1m\u001b[32m1.82779\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8131 | loss: 1.82779 - acc: 0.5737 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8132  | total loss: \u001b[1m\u001b[32m1.70111\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8132 | loss: 1.70111 - acc: 0.6163 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8133  | total loss: \u001b[1m\u001b[32m1.58685\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8133 | loss: 1.58685 - acc: 0.6547 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8134  | total loss: \u001b[1m\u001b[32m1.78033\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8134 | loss: 1.78033 - acc: 0.5892 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8135  | total loss: \u001b[1m\u001b[32m1.65754\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8135 | loss: 1.65754 - acc: 0.6303 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8136  | total loss: \u001b[1m\u001b[32m1.80651\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8136 | loss: 1.80651 - acc: 0.5744 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8137  | total loss: \u001b[1m\u001b[32m1.68097\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8137 | loss: 1.68097 - acc: 0.6170 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8138  | total loss: \u001b[1m\u001b[32m1.56767\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8138 | loss: 1.56767 - acc: 0.6553 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8139  | total loss: \u001b[1m\u001b[32m1.46486\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8139 | loss: 1.46486 - acc: 0.6897 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8140  | total loss: \u001b[1m\u001b[32m1.65090\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8140 | loss: 1.65090 - acc: 0.6279 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8141  | total loss: \u001b[1m\u001b[32m1.53800\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8141 | loss: 1.53800 - acc: 0.6651 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8142  | total loss: \u001b[1m\u001b[32m1.74198\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8142 | loss: 1.74198 - acc: 0.5986 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8143  | total loss: \u001b[1m\u001b[32m1.61897\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8143 | loss: 1.61897 - acc: 0.6387 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8144  | total loss: \u001b[1m\u001b[32m1.79629\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8144 | loss: 1.79629 - acc: 0.5749 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8145  | total loss: \u001b[1m\u001b[32m1.66757\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8145 | loss: 1.66757 - acc: 0.6174 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8146  | total loss: \u001b[1m\u001b[32m1.86413\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8146 | loss: 1.86413 - acc: 0.5556 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8147  | total loss: \u001b[1m\u001b[32m1.72895\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8147 | loss: 1.72895 - acc: 0.6001 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8148  | total loss: \u001b[1m\u001b[32m1.86599\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8148 | loss: 1.86599 - acc: 0.5544 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8149  | total loss: \u001b[1m\u001b[32m1.73137\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8149 | loss: 1.73137 - acc: 0.5989 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8150  | total loss: \u001b[1m\u001b[32m1.61034\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8150 | loss: 1.61034 - acc: 0.6390 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8151  | total loss: \u001b[1m\u001b[32m1.50099\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8151 | loss: 1.50099 - acc: 0.6751 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8152  | total loss: \u001b[1m\u001b[32m1.40166\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8152 | loss: 1.40166 - acc: 0.7076 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8153  | total loss: \u001b[1m\u001b[32m1.31093\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8153 | loss: 1.31093 - acc: 0.7369 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8154  | total loss: \u001b[1m\u001b[32m1.48071\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8154 | loss: 1.48071 - acc: 0.6846 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8155  | total loss: \u001b[1m\u001b[32m1.37941\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8155 | loss: 1.37941 - acc: 0.7161 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8156  | total loss: \u001b[1m\u001b[32m1.61494\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8156 | loss: 1.61494 - acc: 0.6445 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8157  | total loss: \u001b[1m\u001b[32m1.49859\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8157 | loss: 1.49859 - acc: 0.6801 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8158  | total loss: \u001b[1m\u001b[32m1.70069\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8158 | loss: 1.70069 - acc: 0.6121 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8159  | total loss: \u001b[1m\u001b[32m1.57531\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8159 | loss: 1.57531 - acc: 0.6509 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8160  | total loss: \u001b[1m\u001b[32m1.46223\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8160 | loss: 1.46223 - acc: 0.6858 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8161  | total loss: \u001b[1m\u001b[32m1.35982\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8161 | loss: 1.35982 - acc: 0.7172 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8162  | total loss: \u001b[1m\u001b[32m1.58147\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8162 | loss: 1.58147 - acc: 0.6455 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8163  | total loss: \u001b[1m\u001b[32m1.46611\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8163 | loss: 1.46611 - acc: 0.6809 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8164  | total loss: \u001b[1m\u001b[32m1.36184\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8164 | loss: 1.36184 - acc: 0.7128 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8165  | total loss: \u001b[1m\u001b[32m1.26717\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8165 | loss: 1.26717 - acc: 0.7416 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8166  | total loss: \u001b[1m\u001b[32m1.18081\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8166 | loss: 1.18081 - acc: 0.7674 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8167  | total loss: \u001b[1m\u001b[32m1.10164\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8167 | loss: 1.10164 - acc: 0.7907 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8168  | total loss: \u001b[1m\u001b[32m1.38094\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8168 | loss: 1.38094 - acc: 0.7116 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 8169  | total loss: \u001b[1m\u001b[32m1.27950\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8169 | loss: 1.27950 - acc: 0.7404 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8170  | total loss: \u001b[1m\u001b[32m1.54508\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8170 | loss: 1.54508 - acc: 0.6664 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8171  | total loss: \u001b[1m\u001b[32m1.42642\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8171 | loss: 1.42642 - acc: 0.6997 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8172  | total loss: \u001b[1m\u001b[32m1.65828\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8172 | loss: 1.65828 - acc: 0.6298 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8173  | total loss: \u001b[1m\u001b[32m1.52874\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8173 | loss: 1.52874 - acc: 0.6668 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8174  | total loss: \u001b[1m\u001b[32m1.41243\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8174 | loss: 1.41243 - acc: 0.7001 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8175  | total loss: \u001b[1m\u001b[32m1.30765\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8175 | loss: 1.30765 - acc: 0.7301 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8176  | total loss: \u001b[1m\u001b[32m1.51405\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8176 | loss: 1.51405 - acc: 0.6714 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8177  | total loss: \u001b[1m\u001b[32m1.39896\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8177 | loss: 1.39896 - acc: 0.7042 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8178  | total loss: \u001b[1m\u001b[32m1.58325\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8178 | loss: 1.58325 - acc: 0.6481 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8179  | total loss: \u001b[1m\u001b[32m1.46175\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8179 | loss: 1.46175 - acc: 0.6833 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8180  | total loss: \u001b[1m\u001b[32m1.66450\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8180 | loss: 1.66450 - acc: 0.6221 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8181  | total loss: \u001b[1m\u001b[32m1.53605\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8181 | loss: 1.53605 - acc: 0.6599 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8182  | total loss: \u001b[1m\u001b[32m1.73814\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8182 | loss: 1.73814 - acc: 0.6010 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8183  | total loss: \u001b[1m\u001b[32m1.60412\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8183 | loss: 1.60412 - acc: 0.6409 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8184  | total loss: \u001b[1m\u001b[32m1.83749\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8184 | loss: 1.83749 - acc: 0.5769 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8185  | total loss: \u001b[1m\u001b[32m1.69593\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8185 | loss: 1.69593 - acc: 0.6192 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8186  | total loss: \u001b[1m\u001b[32m1.85617\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8186 | loss: 1.85617 - acc: 0.5644 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8187  | total loss: \u001b[1m\u001b[32m1.71549\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8187 | loss: 1.71549 - acc: 0.6080 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8188  | total loss: \u001b[1m\u001b[32m1.87275\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8188 | loss: 1.87275 - acc: 0.5543 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8189  | total loss: \u001b[1m\u001b[32m1.73316\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8189 | loss: 1.73316 - acc: 0.5989 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8190  | total loss: \u001b[1m\u001b[32m1.84944\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8190 | loss: 1.84944 - acc: 0.5604 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8191  | total loss: \u001b[1m\u001b[32m1.71463\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8191 | loss: 1.71463 - acc: 0.6044 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8192  | total loss: \u001b[1m\u001b[32m1.89810\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8192 | loss: 1.89810 - acc: 0.5439 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8193  | total loss: \u001b[1m\u001b[32m1.76075\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8193 | loss: 1.76075 - acc: 0.5895 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8194  | total loss: \u001b[1m\u001b[32m1.87747\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8194 | loss: 1.87747 - acc: 0.5449 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8195  | total loss: \u001b[1m\u001b[32m1.74447\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8195 | loss: 1.74447 - acc: 0.5904 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8196  | total loss: \u001b[1m\u001b[32m1.89970\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8196 | loss: 1.89970 - acc: 0.5385 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8197  | total loss: \u001b[1m\u001b[32m1.76661\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8197 | loss: 1.76661 - acc: 0.5846 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8198  | total loss: \u001b[1m\u001b[32m1.91474\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8198 | loss: 1.91474 - acc: 0.5333 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8199  | total loss: \u001b[1m\u001b[32m1.78212\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8199 | loss: 1.78212 - acc: 0.5800 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8200  | total loss: \u001b[1m\u001b[32m1.86947\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8200 | loss: 1.86947 - acc: 0.5434 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8201  | total loss: \u001b[1m\u001b[32m1.74301\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8201 | loss: 1.74301 - acc: 0.5891 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8202  | total loss: \u001b[1m\u001b[32m1.88951\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8202 | loss: 1.88951 - acc: 0.5373 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8203  | total loss: \u001b[1m\u001b[32m1.76242\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8203 | loss: 1.76242 - acc: 0.5836 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8204  | total loss: \u001b[1m\u001b[32m1.91731\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8204 | loss: 1.91731 - acc: 0.5324 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8205  | total loss: \u001b[1m\u001b[32m1.78871\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8205 | loss: 1.78871 - acc: 0.5791 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8206  | total loss: \u001b[1m\u001b[32m1.92320\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8206 | loss: 1.92320 - acc: 0.5284 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8207  | total loss: \u001b[1m\u001b[32m1.79510\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8207 | loss: 1.79510 - acc: 0.5755 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8208  | total loss: \u001b[1m\u001b[32m1.94725\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8208 | loss: 1.94725 - acc: 0.5180 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8209  | total loss: \u001b[1m\u001b[32m1.81780\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8209 | loss: 1.81780 - acc: 0.5662 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8210  | total loss: \u001b[1m\u001b[32m1.97671\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8210 | loss: 1.97671 - acc: 0.5096 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8211  | total loss: \u001b[1m\u001b[32m1.84532\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8211 | loss: 1.84532 - acc: 0.5586 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8212  | total loss: \u001b[1m\u001b[32m1.98210\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8212 | loss: 1.98210 - acc: 0.5099 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8213  | total loss: \u001b[1m\u001b[32m1.85087\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8213 | loss: 1.85087 - acc: 0.5589 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8214  | total loss: \u001b[1m\u001b[32m1.95515\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8214 | loss: 1.95515 - acc: 0.5244 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8215  | total loss: \u001b[1m\u001b[32m1.82687\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8215 | loss: 1.82687 - acc: 0.5720 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8216  | total loss: \u001b[1m\u001b[32m1.97529\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8216 | loss: 1.97529 - acc: 0.5219 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8217  | total loss: \u001b[1m\u001b[32m1.84510\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8217 | loss: 1.84510 - acc: 0.5697 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8218  | total loss: \u001b[1m\u001b[32m1.99082\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8218 | loss: 1.99082 - acc: 0.5128 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8219  | total loss: \u001b[1m\u001b[32m1.85917\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8219 | loss: 1.85917 - acc: 0.5615 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8220  | total loss: \u001b[1m\u001b[32m1.97574\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8220 | loss: 1.97574 - acc: 0.5125 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8221  | total loss: \u001b[1m\u001b[32m1.84562\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8221 | loss: 1.84562 - acc: 0.5612 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8222  | total loss: \u001b[1m\u001b[32m1.96081\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8222 | loss: 1.96081 - acc: 0.5194 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8223  | total loss: \u001b[1m\u001b[32m1.83196\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8223 | loss: 1.83196 - acc: 0.5675 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8224  | total loss: \u001b[1m\u001b[32m1.97475\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8224 | loss: 1.97475 - acc: 0.5179 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8225  | total loss: \u001b[1m\u001b[32m1.84409\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8225 | loss: 1.84409 - acc: 0.5661 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 8226  | total loss: \u001b[1m\u001b[32m1.99670\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8226 | loss: 1.99670 - acc: 0.5095 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8227  | total loss: \u001b[1m\u001b[32m1.86361\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8227 | loss: 1.86361 - acc: 0.5585 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8228  | total loss: \u001b[1m\u001b[32m1.98855\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8228 | loss: 1.98855 - acc: 0.5098 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8229  | total loss: \u001b[1m\u001b[32m1.85613\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8229 | loss: 1.85613 - acc: 0.5588 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8230  | total loss: \u001b[1m\u001b[32m2.02937\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8230 | loss: 2.02937 - acc: 0.5029 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8231  | total loss: \u001b[1m\u001b[32m1.89290\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8231 | loss: 1.89290 - acc: 0.5526 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8232  | total loss: \u001b[1m\u001b[32m2.02684\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8232 | loss: 2.02684 - acc: 0.5045 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8233  | total loss: \u001b[1m\u001b[32m1.89069\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8233 | loss: 1.89069 - acc: 0.5541 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8234  | total loss: \u001b[1m\u001b[32m2.03600\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8234 | loss: 2.03600 - acc: 0.4987 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8235  | total loss: \u001b[1m\u001b[32m1.89893\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8235 | loss: 1.89893 - acc: 0.5488 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8236  | total loss: \u001b[1m\u001b[32m2.01605\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8236 | loss: 2.01605 - acc: 0.5082 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8237  | total loss: \u001b[1m\u001b[32m1.88083\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8237 | loss: 1.88083 - acc: 0.5574 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8238  | total loss: \u001b[1m\u001b[32m1.98029\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8238 | loss: 1.98029 - acc: 0.5231 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8239  | total loss: \u001b[1m\u001b[32m1.84813\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8239 | loss: 1.84813 - acc: 0.5708 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8240  | total loss: \u001b[1m\u001b[32m1.99547\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8240 | loss: 1.99547 - acc: 0.5137 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8241  | total loss: \u001b[1m\u001b[32m1.86136\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8241 | loss: 1.86136 - acc: 0.5623 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8242  | total loss: \u001b[1m\u001b[32m1.74023\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8242 | loss: 1.74023 - acc: 0.6061 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8243  | total loss: \u001b[1m\u001b[32m1.63014\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8243 | loss: 1.63014 - acc: 0.6455 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8244  | total loss: \u001b[1m\u001b[32m1.79442\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8244 | loss: 1.79442 - acc: 0.5881 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8245  | total loss: \u001b[1m\u001b[32m1.67658\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8245 | loss: 1.67658 - acc: 0.6293 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8246  | total loss: \u001b[1m\u001b[32m1.56926\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8246 | loss: 1.56926 - acc: 0.6663 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8247  | total loss: \u001b[1m\u001b[32m1.47091\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8247 | loss: 1.47091 - acc: 0.6997 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8248  | total loss: \u001b[1m\u001b[32m1.67756\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8248 | loss: 1.67756 - acc: 0.6297 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8249  | total loss: \u001b[1m\u001b[32m1.56514\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8249 | loss: 1.56514 - acc: 0.6668 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8250  | total loss: \u001b[1m\u001b[32m1.77129\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8250 | loss: 1.77129 - acc: 0.6001 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8251  | total loss: \u001b[1m\u001b[32m1.64751\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8251 | loss: 1.64751 - acc: 0.6401 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8252  | total loss: \u001b[1m\u001b[32m1.85748\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8252 | loss: 1.85748 - acc: 0.5761 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8253  | total loss: \u001b[1m\u001b[32m1.72412\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8253 | loss: 1.72412 - acc: 0.6185 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8254  | total loss: \u001b[1m\u001b[32m1.85783\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8254 | loss: 1.85783 - acc: 0.5709 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8255  | total loss: \u001b[1m\u001b[32m1.72413\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8255 | loss: 1.72413 - acc: 0.6138 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8256  | total loss: \u001b[1m\u001b[32m1.91383\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8256 | loss: 1.91383 - acc: 0.5524 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8257  | total loss: \u001b[1m\u001b[32m1.77477\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8257 | loss: 1.77477 - acc: 0.5972 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8258  | total loss: \u001b[1m\u001b[32m1.64959\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8258 | loss: 1.64959 - acc: 0.6375 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8259  | total loss: \u001b[1m\u001b[32m1.53637\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8259 | loss: 1.53637 - acc: 0.6737 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8260  | total loss: \u001b[1m\u001b[32m1.72358\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8260 | loss: 1.72358 - acc: 0.6135 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8261  | total loss: \u001b[1m\u001b[32m1.60169\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8261 | loss: 1.60169 - acc: 0.6521 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8262  | total loss: \u001b[1m\u001b[32m1.81872\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8262 | loss: 1.81872 - acc: 0.5869 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8263  | total loss: \u001b[1m\u001b[32m1.68692\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8263 | loss: 1.68692 - acc: 0.6282 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8264  | total loss: \u001b[1m\u001b[32m1.85556\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8264 | loss: 1.85556 - acc: 0.5726 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8265  | total loss: \u001b[1m\u001b[32m1.72047\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8265 | loss: 1.72047 - acc: 0.6153 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8266  | total loss: \u001b[1m\u001b[32m1.59893\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8266 | loss: 1.59893 - acc: 0.6538 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8267  | total loss: \u001b[1m\u001b[32m1.48903\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8267 | loss: 1.48903 - acc: 0.6884 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8268  | total loss: \u001b[1m\u001b[32m1.61343\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8268 | loss: 1.61343 - acc: 0.6410 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8269  | total loss: \u001b[1m\u001b[32m1.50070\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8269 | loss: 1.50070 - acc: 0.6769 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8270  | total loss: \u001b[1m\u001b[32m1.70247\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8270 | loss: 1.70247 - acc: 0.6092 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8271  | total loss: \u001b[1m\u001b[32m1.58005\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8271 | loss: 1.58005 - acc: 0.6483 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8272  | total loss: \u001b[1m\u001b[32m1.76490\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8272 | loss: 1.76490 - acc: 0.5906 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8273  | total loss: \u001b[1m\u001b[32m1.63628\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8273 | loss: 1.63628 - acc: 0.6315 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8274  | total loss: \u001b[1m\u001b[32m1.80361\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8274 | loss: 1.80361 - acc: 0.5755 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8275  | total loss: \u001b[1m\u001b[32m1.67181\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8275 | loss: 1.67181 - acc: 0.6180 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8276  | total loss: \u001b[1m\u001b[32m1.84858\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8276 | loss: 1.84858 - acc: 0.5562 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8277  | total loss: \u001b[1m\u001b[32m1.71355\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8277 | loss: 1.71355 - acc: 0.6006 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8278  | total loss: \u001b[1m\u001b[32m1.89953\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8278 | loss: 1.89953 - acc: 0.5405 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8279  | total loss: \u001b[1m\u001b[32m1.76117\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8279 | loss: 1.76117 - acc: 0.5864 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8280  | total loss: \u001b[1m\u001b[32m1.92470\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8280 | loss: 1.92470 - acc: 0.5349 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8281  | total loss: \u001b[1m\u001b[32m1.78581\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8281 | loss: 1.78581 - acc: 0.5815 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8282  | total loss: \u001b[1m\u001b[32m1.93093\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8282 | loss: 1.93093 - acc: 0.5305 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 8283  | total loss: \u001b[1m\u001b[32m1.79334\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8283 | loss: 1.79334 - acc: 0.5774 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8284  | total loss: \u001b[1m\u001b[32m1.92552\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8284 | loss: 1.92552 - acc: 0.5268 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8285  | total loss: \u001b[1m\u001b[32m1.79033\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8285 | loss: 1.79033 - acc: 0.5741 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8286  | total loss: \u001b[1m\u001b[32m1.93202\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8286 | loss: 1.93202 - acc: 0.5239 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8287  | total loss: \u001b[1m\u001b[32m1.79807\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8287 | loss: 1.79807 - acc: 0.5715 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8288  | total loss: \u001b[1m\u001b[32m1.67803\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8288 | loss: 1.67803 - acc: 0.6143 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8289  | total loss: \u001b[1m\u001b[32m1.56977\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8289 | loss: 1.56977 - acc: 0.6529 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8290  | total loss: \u001b[1m\u001b[32m1.77954\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8290 | loss: 1.77954 - acc: 0.5876 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8291  | total loss: \u001b[1m\u001b[32m1.66033\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8291 | loss: 1.66033 - acc: 0.6288 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8292  | total loss: \u001b[1m\u001b[32m1.74847\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8292 | loss: 1.74847 - acc: 0.5945 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8293  | total loss: \u001b[1m\u001b[32m1.63167\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8293 | loss: 1.63167 - acc: 0.6351 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8294  | total loss: \u001b[1m\u001b[32m1.81843\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8294 | loss: 1.81843 - acc: 0.5716 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8295  | total loss: \u001b[1m\u001b[32m1.69411\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8295 | loss: 1.69411 - acc: 0.6144 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8296  | total loss: \u001b[1m\u001b[32m1.58176\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8296 | loss: 1.58176 - acc: 0.6530 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8297  | total loss: \u001b[1m\u001b[32m1.47958\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8297 | loss: 1.47958 - acc: 0.6877 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8298  | total loss: \u001b[1m\u001b[32m1.64693\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8298 | loss: 1.64693 - acc: 0.6260 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8299  | total loss: \u001b[1m\u001b[32m1.53603\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8299 | loss: 1.53603 - acc: 0.6634 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8300  | total loss: \u001b[1m\u001b[32m1.74318\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8300 | loss: 1.74318 - acc: 0.5971 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8301  | total loss: \u001b[1m\u001b[32m1.62145\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8301 | loss: 1.62145 - acc: 0.6374 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8302  | total loss: \u001b[1m\u001b[32m1.75945\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8302 | loss: 1.75945 - acc: 0.5879 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8303  | total loss: \u001b[1m\u001b[32m1.63554\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8303 | loss: 1.63554 - acc: 0.6291 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8304  | total loss: \u001b[1m\u001b[32m1.52352\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8304 | loss: 1.52352 - acc: 0.6662 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8305  | total loss: \u001b[1m\u001b[32m1.42165\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8305 | loss: 1.42165 - acc: 0.6996 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8306  | total loss: \u001b[1m\u001b[32m1.62351\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8306 | loss: 1.62351 - acc: 0.6368 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8307  | total loss: \u001b[1m\u001b[32m1.50971\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8307 | loss: 1.50971 - acc: 0.6731 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8308  | total loss: \u001b[1m\u001b[32m1.69384\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8308 | loss: 1.69384 - acc: 0.6129 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8309  | total loss: \u001b[1m\u001b[32m1.57209\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8309 | loss: 1.57209 - acc: 0.6516 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8310  | total loss: \u001b[1m\u001b[32m1.79769\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8310 | loss: 1.79769 - acc: 0.5865 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8311  | total loss: \u001b[1m\u001b[32m1.66560\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8311 | loss: 1.66560 - acc: 0.6278 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8312  | total loss: \u001b[1m\u001b[32m1.84076\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8312 | loss: 1.84076 - acc: 0.5722 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8313  | total loss: \u001b[1m\u001b[32m1.70493\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8313 | loss: 1.70493 - acc: 0.6150 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8314  | total loss: \u001b[1m\u001b[32m1.86763\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8314 | loss: 1.86763 - acc: 0.5606 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8315  | total loss: \u001b[1m\u001b[32m1.73004\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8315 | loss: 1.73004 - acc: 0.6046 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8316  | total loss: \u001b[1m\u001b[32m1.60647\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8316 | loss: 1.60647 - acc: 0.6441 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8317  | total loss: \u001b[1m\u001b[32m1.49491\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8317 | loss: 1.49491 - acc: 0.6797 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8318  | total loss: \u001b[1m\u001b[32m1.64196\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8318 | loss: 1.64196 - acc: 0.6332 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8319  | total loss: \u001b[1m\u001b[32m1.52582\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8319 | loss: 1.52582 - acc: 0.6698 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8320  | total loss: \u001b[1m\u001b[32m1.71598\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8320 | loss: 1.71598 - acc: 0.6100 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8321  | total loss: \u001b[1m\u001b[32m1.59190\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8321 | loss: 1.59190 - acc: 0.6490 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8322  | total loss: \u001b[1m\u001b[32m1.80893\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8322 | loss: 1.80893 - acc: 0.5841 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8323  | total loss: \u001b[1m\u001b[32m1.67582\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8323 | loss: 1.67582 - acc: 0.6257 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8324  | total loss: \u001b[1m\u001b[32m1.83642\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8324 | loss: 1.83642 - acc: 0.5774 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8325  | total loss: \u001b[1m\u001b[32m1.70139\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8325 | loss: 1.70139 - acc: 0.6197 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8326  | total loss: \u001b[1m\u001b[32m1.87920\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8326 | loss: 1.87920 - acc: 0.5577 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8327  | total loss: \u001b[1m\u001b[32m1.74116\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8327 | loss: 1.74116 - acc: 0.6019 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8328  | total loss: \u001b[1m\u001b[32m1.89790\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8328 | loss: 1.89790 - acc: 0.5489 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8329  | total loss: \u001b[1m\u001b[32m1.75954\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8329 | loss: 1.75954 - acc: 0.5940 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8330  | total loss: \u001b[1m\u001b[32m1.63545\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8330 | loss: 1.63545 - acc: 0.6346 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8331  | total loss: \u001b[1m\u001b[32m1.52356\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8331 | loss: 1.52356 - acc: 0.6711 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8332  | total loss: \u001b[1m\u001b[32m1.73484\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8332 | loss: 1.73484 - acc: 0.6040 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8333  | total loss: \u001b[1m\u001b[32m1.61244\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8333 | loss: 1.61244 - acc: 0.6436 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8334  | total loss: \u001b[1m\u001b[32m1.50186\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8334 | loss: 1.50186 - acc: 0.6793 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8335  | total loss: \u001b[1m\u001b[32m1.40143\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8335 | loss: 1.40143 - acc: 0.7113 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8336  | total loss: \u001b[1m\u001b[32m1.30970\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8336 | loss: 1.30970 - acc: 0.7402 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8337  | total loss: \u001b[1m\u001b[32m1.22544\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8337 | loss: 1.22544 - acc: 0.7662 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8338  | total loss: \u001b[1m\u001b[32m1.43809\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8338 | loss: 1.43809 - acc: 0.6967 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8339  | total loss: \u001b[1m\u001b[32m1.33803\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8339 | loss: 1.33803 - acc: 0.7270 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 8340  | total loss: \u001b[1m\u001b[32m1.24667\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8340 | loss: 1.24667 - acc: 0.7543 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8341  | total loss: \u001b[1m\u001b[32m1.16285\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8341 | loss: 1.16285 - acc: 0.7789 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8342  | total loss: \u001b[1m\u001b[32m1.41330\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8342 | loss: 1.41330 - acc: 0.7081 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8343  | total loss: \u001b[1m\u001b[32m1.31025\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8343 | loss: 1.31025 - acc: 0.7373 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8344  | total loss: \u001b[1m\u001b[32m1.21647\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8344 | loss: 1.21647 - acc: 0.7636 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8345  | total loss: \u001b[1m\u001b[32m1.13078\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8345 | loss: 1.13078 - acc: 0.7872 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8346  | total loss: \u001b[1m\u001b[32m1.05219\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8346 | loss: 1.05219 - acc: 0.8085 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8347  | total loss: \u001b[1m\u001b[32m0.97983\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8347 | loss: 0.97983 - acc: 0.8277 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8348  | total loss: \u001b[1m\u001b[32m0.91298\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8348 | loss: 0.91298 - acc: 0.8449 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8349  | total loss: \u001b[1m\u001b[32m0.85102\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8349 | loss: 0.85102 - acc: 0.8604 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8350  | total loss: \u001b[1m\u001b[32m1.09144\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8350 | loss: 1.09144 - acc: 0.7887 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8351  | total loss: \u001b[1m\u001b[32m1.00887\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8351 | loss: 1.00887 - acc: 0.8098 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8352  | total loss: \u001b[1m\u001b[32m1.23783\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8352 | loss: 1.23783 - acc: 0.7502 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8353  | total loss: \u001b[1m\u001b[32m1.13928\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8353 | loss: 1.13928 - acc: 0.7752 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8354  | total loss: \u001b[1m\u001b[32m1.35962\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8354 | loss: 1.35962 - acc: 0.7191 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8355  | total loss: \u001b[1m\u001b[32m1.24870\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8355 | loss: 1.24870 - acc: 0.7472 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8356  | total loss: \u001b[1m\u001b[32m1.14892\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8356 | loss: 1.14892 - acc: 0.7725 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8357  | total loss: \u001b[1m\u001b[32m1.05894\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8357 | loss: 1.05894 - acc: 0.7952 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8358  | total loss: \u001b[1m\u001b[32m1.34645\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8358 | loss: 1.34645 - acc: 0.7229 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8359  | total loss: \u001b[1m\u001b[32m1.23678\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8359 | loss: 1.23678 - acc: 0.7506 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8360  | total loss: \u001b[1m\u001b[32m1.48407\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8360 | loss: 1.48407 - acc: 0.6898 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8361  | total loss: \u001b[1m\u001b[32m1.36170\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8361 | loss: 1.36170 - acc: 0.7208 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8362  | total loss: \u001b[1m\u001b[32m1.60738\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8362 | loss: 1.60738 - acc: 0.6559 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8363  | total loss: \u001b[1m\u001b[32m1.47457\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8363 | loss: 1.47457 - acc: 0.6903 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8364  | total loss: \u001b[1m\u001b[32m1.73927\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8364 | loss: 1.73927 - acc: 0.6213 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8365  | total loss: \u001b[1m\u001b[32m1.59602\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8365 | loss: 1.59602 - acc: 0.6591 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8366  | total loss: \u001b[1m\u001b[32m1.76570\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8366 | loss: 1.76570 - acc: 0.6075 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8367  | total loss: \u001b[1m\u001b[32m1.62311\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8367 | loss: 1.62311 - acc: 0.6468 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8368  | total loss: \u001b[1m\u001b[32m1.49624\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8368 | loss: 1.49624 - acc: 0.6821 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8369  | total loss: \u001b[1m\u001b[32m1.38303\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8369 | loss: 1.38303 - acc: 0.7139 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8370  | total loss: \u001b[1m\u001b[32m1.63121\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8370 | loss: 1.63121 - acc: 0.6425 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8371  | total loss: \u001b[1m\u001b[32m1.50640\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8371 | loss: 1.50640 - acc: 0.6782 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8372  | total loss: \u001b[1m\u001b[32m1.70338\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8372 | loss: 1.70338 - acc: 0.6247 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8373  | total loss: \u001b[1m\u001b[32m1.57375\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8373 | loss: 1.57375 - acc: 0.6622 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8374  | total loss: \u001b[1m\u001b[32m1.45804\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8374 | loss: 1.45804 - acc: 0.6960 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8375  | total loss: \u001b[1m\u001b[32m1.35432\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8375 | loss: 1.35432 - acc: 0.7264 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8376  | total loss: \u001b[1m\u001b[32m1.54800\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8376 | loss: 1.54800 - acc: 0.6681 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8377  | total loss: \u001b[1m\u001b[32m1.43580\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8377 | loss: 1.43580 - acc: 0.7012 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8378  | total loss: \u001b[1m\u001b[32m1.59839\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8378 | loss: 1.59839 - acc: 0.6526 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8379  | total loss: \u001b[1m\u001b[32m1.48188\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8379 | loss: 1.48188 - acc: 0.6873 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8380  | total loss: \u001b[1m\u001b[32m1.66738\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8380 | loss: 1.66738 - acc: 0.6329 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8381  | total loss: \u001b[1m\u001b[32m1.54495\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8381 | loss: 1.54495 - acc: 0.6696 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8382  | total loss: \u001b[1m\u001b[32m1.75905\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8382 | loss: 1.75905 - acc: 0.6098 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8383  | total loss: \u001b[1m\u001b[32m1.62884\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8383 | loss: 1.62884 - acc: 0.6488 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8384  | total loss: \u001b[1m\u001b[32m1.51215\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8384 | loss: 1.51215 - acc: 0.6839 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8385  | total loss: \u001b[1m\u001b[32m1.40712\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8385 | loss: 1.40712 - acc: 0.7155 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8386  | total loss: \u001b[1m\u001b[32m1.31211\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8386 | loss: 1.31211 - acc: 0.7440 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8387  | total loss: \u001b[1m\u001b[32m1.22572\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8387 | loss: 1.22572 - acc: 0.7696 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8388  | total loss: \u001b[1m\u001b[32m1.14674\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8388 | loss: 1.14674 - acc: 0.7926 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8389  | total loss: \u001b[1m\u001b[32m1.07416\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8389 | loss: 1.07416 - acc: 0.8133 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8390  | total loss: \u001b[1m\u001b[32m1.00711\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8390 | loss: 1.00711 - acc: 0.8320 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8391  | total loss: \u001b[1m\u001b[32m0.94486\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8391 | loss: 0.94486 - acc: 0.8488 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8392  | total loss: \u001b[1m\u001b[32m1.23582\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8392 | loss: 1.23582 - acc: 0.7639 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8393  | total loss: \u001b[1m\u001b[32m1.14775\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8393 | loss: 1.14775 - acc: 0.7875 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8394  | total loss: \u001b[1m\u001b[32m1.37019\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8394 | loss: 1.37019 - acc: 0.7231 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8395  | total loss: \u001b[1m\u001b[32m1.26718\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8395 | loss: 1.26718 - acc: 0.7508 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8396  | total loss: \u001b[1m\u001b[32m1.46936\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8396 | loss: 1.46936 - acc: 0.6900 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 8397  | total loss: \u001b[1m\u001b[32m1.35597\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8397 | loss: 1.35597 - acc: 0.7210 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8398  | total loss: \u001b[1m\u001b[32m1.25376\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8398 | loss: 1.25376 - acc: 0.7489 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8399  | total loss: \u001b[1m\u001b[32m1.16132\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8399 | loss: 1.16132 - acc: 0.7740 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8400  | total loss: \u001b[1m\u001b[32m1.42766\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8400 | loss: 1.42766 - acc: 0.6966 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8401  | total loss: \u001b[1m\u001b[32m1.31737\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8401 | loss: 1.31737 - acc: 0.7269 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8402  | total loss: \u001b[1m\u001b[32m1.55161\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8402 | loss: 1.55161 - acc: 0.6614 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8403  | total loss: \u001b[1m\u001b[32m1.42948\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8403 | loss: 1.42948 - acc: 0.6952 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8404  | total loss: \u001b[1m\u001b[32m1.67360\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8404 | loss: 1.67360 - acc: 0.6257 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8405  | total loss: \u001b[1m\u001b[32m1.54060\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8405 | loss: 1.54060 - acc: 0.6631 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8406  | total loss: \u001b[1m\u001b[32m1.79136\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8406 | loss: 1.79136 - acc: 0.5968 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8407  | total loss: \u001b[1m\u001b[32m1.64874\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8407 | loss: 1.64874 - acc: 0.6371 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8408  | total loss: \u001b[1m\u001b[32m1.83152\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8408 | loss: 1.83152 - acc: 0.5806 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8409  | total loss: \u001b[1m\u001b[32m1.68763\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8409 | loss: 1.68763 - acc: 0.6225 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8410  | total loss: \u001b[1m\u001b[32m1.55931\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8410 | loss: 1.55931 - acc: 0.6603 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8411  | total loss: \u001b[1m\u001b[32m1.44451\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8411 | loss: 1.44451 - acc: 0.6942 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8412  | total loss: \u001b[1m\u001b[32m1.69496\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8412 | loss: 1.69496 - acc: 0.6248 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8413  | total loss: \u001b[1m\u001b[32m1.56786\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8413 | loss: 1.56786 - acc: 0.6623 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8414  | total loss: \u001b[1m\u001b[32m1.73908\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8414 | loss: 1.73908 - acc: 0.6104 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8415  | total loss: \u001b[1m\u001b[32m1.60931\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8415 | loss: 1.60931 - acc: 0.6493 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8416  | total loss: \u001b[1m\u001b[32m1.82137\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8416 | loss: 1.82137 - acc: 0.5844 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8417  | total loss: \u001b[1m\u001b[32m1.68545\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8417 | loss: 1.68545 - acc: 0.6260 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8418  | total loss: \u001b[1m\u001b[32m1.84044\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8418 | loss: 1.84044 - acc: 0.5777 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8419  | total loss: \u001b[1m\u001b[32m1.70488\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8419 | loss: 1.70488 - acc: 0.6199 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8420  | total loss: \u001b[1m\u001b[32m1.87784\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8420 | loss: 1.87784 - acc: 0.5650 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8421  | total loss: \u001b[1m\u001b[32m1.74086\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8421 | loss: 1.74086 - acc: 0.6085 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8422  | total loss: \u001b[1m\u001b[32m1.93968\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8422 | loss: 1.93968 - acc: 0.5477 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8423  | total loss: \u001b[1m\u001b[32m1.79891\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8423 | loss: 1.79891 - acc: 0.5929 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8424  | total loss: \u001b[1m\u001b[32m1.95013\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8424 | loss: 1.95013 - acc: 0.5408 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8425  | total loss: \u001b[1m\u001b[32m1.81066\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8425 | loss: 1.81066 - acc: 0.5867 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8426  | total loss: \u001b[1m\u001b[32m1.91446\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8426 | loss: 1.91446 - acc: 0.5495 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8427  | total loss: \u001b[1m\u001b[32m1.78051\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8427 | loss: 1.78051 - acc: 0.5945 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8428  | total loss: \u001b[1m\u001b[32m1.91517\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8428 | loss: 1.91517 - acc: 0.5493 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8429  | total loss: \u001b[1m\u001b[32m1.78260\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8429 | loss: 1.78260 - acc: 0.5944 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8430  | total loss: \u001b[1m\u001b[32m1.66355\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8430 | loss: 1.66355 - acc: 0.6350 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8431  | total loss: \u001b[1m\u001b[32m1.55605\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8431 | loss: 1.55605 - acc: 0.6715 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8432  | total loss: \u001b[1m\u001b[32m1.71559\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8432 | loss: 1.71559 - acc: 0.6115 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8433  | total loss: \u001b[1m\u001b[32m1.60185\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8433 | loss: 1.60185 - acc: 0.6503 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8434  | total loss: \u001b[1m\u001b[32m1.79550\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8434 | loss: 1.79550 - acc: 0.5853 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8435  | total loss: \u001b[1m\u001b[32m1.67322\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8435 | loss: 1.67322 - acc: 0.6268 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8436  | total loss: \u001b[1m\u001b[32m1.81564\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8436 | loss: 1.81564 - acc: 0.5712 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8437  | total loss: \u001b[1m\u001b[32m1.69118\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8437 | loss: 1.69118 - acc: 0.6141 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8438  | total loss: \u001b[1m\u001b[32m1.85649\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8438 | loss: 1.85649 - acc: 0.5527 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8439  | total loss: \u001b[1m\u001b[32m1.72815\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8439 | loss: 1.72815 - acc: 0.5974 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8440  | total loss: \u001b[1m\u001b[32m1.83931\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8440 | loss: 1.83931 - acc: 0.5591 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8441  | total loss: \u001b[1m\u001b[32m1.71296\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8441 | loss: 1.71296 - acc: 0.6032 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8442  | total loss: \u001b[1m\u001b[32m1.86898\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8442 | loss: 1.86898 - acc: 0.5500 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8443  | total loss: \u001b[1m\u001b[32m1.74000\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8443 | loss: 1.74000 - acc: 0.5950 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8444  | total loss: \u001b[1m\u001b[32m1.62383\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8444 | loss: 1.62383 - acc: 0.6355 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8445  | total loss: \u001b[1m\u001b[32m1.51863\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8445 | loss: 1.51863 - acc: 0.6720 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8446  | total loss: \u001b[1m\u001b[32m1.72318\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8446 | loss: 1.72318 - acc: 0.6048 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8447  | total loss: \u001b[1m\u001b[32m1.60663\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8447 | loss: 1.60663 - acc: 0.6443 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8448  | total loss: \u001b[1m\u001b[32m1.75297\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8448 | loss: 1.75297 - acc: 0.5941 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8449  | total loss: \u001b[1m\u001b[32m1.63250\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8449 | loss: 1.63250 - acc: 0.6347 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8450  | total loss: \u001b[1m\u001b[32m1.78738\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8450 | loss: 1.78738 - acc: 0.5784 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8451  | total loss: \u001b[1m\u001b[32m1.66295\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8451 | loss: 1.66295 - acc: 0.6206 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8452  | total loss: \u001b[1m\u001b[32m1.81882\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8452 | loss: 1.81882 - acc: 0.5656 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8453  | total loss: \u001b[1m\u001b[32m1.69118\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8453 | loss: 1.69118 - acc: 0.6091 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 8454  | total loss: \u001b[1m\u001b[32m1.83510\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8454 | loss: 1.83510 - acc: 0.5553 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8455  | total loss: \u001b[1m\u001b[32m1.70609\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8455 | loss: 1.70609 - acc: 0.5998 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8456  | total loss: \u001b[1m\u001b[32m1.83685\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8456 | loss: 1.83685 - acc: 0.5541 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8457  | total loss: \u001b[1m\u001b[32m1.70807\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8457 | loss: 1.70807 - acc: 0.5987 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8458  | total loss: \u001b[1m\u001b[32m1.86041\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8458 | loss: 1.86041 - acc: 0.5460 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8459  | total loss: \u001b[1m\u001b[32m1.72983\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8459 | loss: 1.72983 - acc: 0.5914 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8460  | total loss: \u001b[1m\u001b[32m1.61233\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8460 | loss: 1.61233 - acc: 0.6322 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8461  | total loss: \u001b[1m\u001b[32m1.50603\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8461 | loss: 1.50603 - acc: 0.6690 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8462  | total loss: \u001b[1m\u001b[32m1.71286\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8462 | loss: 1.71286 - acc: 0.6021 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8463  | total loss: \u001b[1m\u001b[32m1.59530\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8463 | loss: 1.59530 - acc: 0.6419 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8464  | total loss: \u001b[1m\u001b[32m1.80188\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8464 | loss: 1.80188 - acc: 0.5777 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8465  | total loss: \u001b[1m\u001b[32m1.67490\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8465 | loss: 1.67490 - acc: 0.6199 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8466  | total loss: \u001b[1m\u001b[32m1.86351\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8466 | loss: 1.86351 - acc: 0.5579 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8467  | total loss: \u001b[1m\u001b[32m1.73041\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8467 | loss: 1.73041 - acc: 0.6021 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8468  | total loss: \u001b[1m\u001b[32m1.88124\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8468 | loss: 1.88124 - acc: 0.5491 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8469  | total loss: \u001b[1m\u001b[32m1.74678\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8469 | loss: 1.74678 - acc: 0.5942 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8470  | total loss: \u001b[1m\u001b[32m1.62574\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8470 | loss: 1.62574 - acc: 0.6348 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8471  | total loss: \u001b[1m\u001b[32m1.51625\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8471 | loss: 1.51625 - acc: 0.6713 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8472  | total loss: \u001b[1m\u001b[32m1.70782\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8472 | loss: 1.70782 - acc: 0.6041 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8473  | total loss: \u001b[1m\u001b[32m1.58898\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8473 | loss: 1.58898 - acc: 0.6437 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8474  | total loss: \u001b[1m\u001b[32m1.72799\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8474 | loss: 1.72799 - acc: 0.6008 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8475  | total loss: \u001b[1m\u001b[32m1.60648\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8475 | loss: 1.60648 - acc: 0.6407 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8476  | total loss: \u001b[1m\u001b[32m1.79937\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8476 | loss: 1.79937 - acc: 0.5766 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8477  | total loss: \u001b[1m\u001b[32m1.67057\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8477 | loss: 1.67057 - acc: 0.6190 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8478  | total loss: \u001b[1m\u001b[32m1.84535\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8478 | loss: 1.84535 - acc: 0.5571 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8479  | total loss: \u001b[1m\u001b[32m1.71246\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8479 | loss: 1.71246 - acc: 0.6014 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8480  | total loss: \u001b[1m\u001b[32m1.84485\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8480 | loss: 1.84485 - acc: 0.5555 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8481  | total loss: \u001b[1m\u001b[32m1.71276\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8481 | loss: 1.71276 - acc: 0.6000 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8482  | total loss: \u001b[1m\u001b[32m1.84182\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8482 | loss: 1.84182 - acc: 0.5543 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8483  | total loss: \u001b[1m\u001b[32m1.71084\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8483 | loss: 1.71084 - acc: 0.5988 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8484  | total loss: \u001b[1m\u001b[32m1.87001\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8484 | loss: 1.87001 - acc: 0.5461 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8485  | total loss: \u001b[1m\u001b[32m1.73710\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8485 | loss: 1.73710 - acc: 0.5915 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8486  | total loss: \u001b[1m\u001b[32m1.92277\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8486 | loss: 1.92277 - acc: 0.5323 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8487  | total loss: \u001b[1m\u001b[32m1.78573\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8487 | loss: 1.78573 - acc: 0.5791 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8488  | total loss: \u001b[1m\u001b[32m1.94220\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8488 | loss: 1.94220 - acc: 0.5212 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8489  | total loss: \u001b[1m\u001b[32m1.80456\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8489 | loss: 1.80456 - acc: 0.5691 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8490  | total loss: \u001b[1m\u001b[32m1.98384\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8490 | loss: 1.98384 - acc: 0.5122 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8491  | total loss: \u001b[1m\u001b[32m1.84350\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8491 | loss: 1.84350 - acc: 0.5609 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8492  | total loss: \u001b[1m\u001b[32m1.99455\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8492 | loss: 1.99455 - acc: 0.5120 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8493  | total loss: \u001b[1m\u001b[32m1.85455\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8493 | loss: 1.85455 - acc: 0.5608 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8494  | total loss: \u001b[1m\u001b[32m1.72884\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8494 | loss: 1.72884 - acc: 0.6047 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8495  | total loss: \u001b[1m\u001b[32m1.61534\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8495 | loss: 1.61534 - acc: 0.6442 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8496  | total loss: \u001b[1m\u001b[32m1.77355\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8496 | loss: 1.77355 - acc: 0.5870 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8497  | total loss: \u001b[1m\u001b[32m1.65450\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8497 | loss: 1.65450 - acc: 0.6283 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8498  | total loss: \u001b[1m\u001b[32m1.81349\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8498 | loss: 1.81349 - acc: 0.5726 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8499  | total loss: \u001b[1m\u001b[32m1.68975\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8499 | loss: 1.68975 - acc: 0.6153 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8500  | total loss: \u001b[1m\u001b[32m1.86872\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8500 | loss: 1.86872 - acc: 0.5538 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8501  | total loss: \u001b[1m\u001b[32m1.73917\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8501 | loss: 1.73917 - acc: 0.5984 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8502  | total loss: \u001b[1m\u001b[32m1.92995\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8502 | loss: 1.92995 - acc: 0.5386 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8503  | total loss: \u001b[1m\u001b[32m1.79448\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8503 | loss: 1.79448 - acc: 0.5847 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8504  | total loss: \u001b[1m\u001b[32m1.96116\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8504 | loss: 1.96116 - acc: 0.5262 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8505  | total loss: \u001b[1m\u001b[32m1.82323\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8505 | loss: 1.82323 - acc: 0.5736 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8506  | total loss: \u001b[1m\u001b[32m1.96508\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8506 | loss: 1.96508 - acc: 0.5234 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8507  | total loss: \u001b[1m\u001b[32m1.82769\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8507 | loss: 1.82769 - acc: 0.5711 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8508  | total loss: \u001b[1m\u001b[32m1.99588\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8508 | loss: 1.99588 - acc: 0.5140 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8509  | total loss: \u001b[1m\u001b[32m1.85648\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8509 | loss: 1.85648 - acc: 0.5626 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8510  | total loss: \u001b[1m\u001b[32m1.97813\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8510 | loss: 1.97813 - acc: 0.5206 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 8511  | total loss: \u001b[1m\u001b[32m1.84155\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8511 | loss: 1.84155 - acc: 0.5685 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8512  | total loss: \u001b[1m\u001b[32m1.93764\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8512 | loss: 1.93764 - acc: 0.5331 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8513  | total loss: \u001b[1m\u001b[32m1.80586\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8513 | loss: 1.80586 - acc: 0.5798 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8514  | total loss: \u001b[1m\u001b[32m1.92717\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8514 | loss: 1.92717 - acc: 0.5361 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8515  | total loss: \u001b[1m\u001b[32m1.79687\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8515 | loss: 1.79687 - acc: 0.5825 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8516  | total loss: \u001b[1m\u001b[32m1.94241\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8516 | loss: 1.94241 - acc: 0.5314 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8517  | total loss: \u001b[1m\u001b[32m1.81094\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8517 | loss: 1.81094 - acc: 0.5782 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8518  | total loss: \u001b[1m\u001b[32m1.93766\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8518 | loss: 1.93766 - acc: 0.5276 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8519  | total loss: \u001b[1m\u001b[32m1.80695\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8519 | loss: 1.80695 - acc: 0.5748 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8520  | total loss: \u001b[1m\u001b[32m1.97180\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8520 | loss: 1.97180 - acc: 0.5173 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8521  | total loss: \u001b[1m\u001b[32m1.83806\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8521 | loss: 1.83806 - acc: 0.5656 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8522  | total loss: \u001b[1m\u001b[32m1.99482\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8522 | loss: 1.99482 - acc: 0.5090 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8523  | total loss: \u001b[1m\u001b[32m1.85936\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8523 | loss: 1.85936 - acc: 0.5581 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8524  | total loss: \u001b[1m\u001b[32m2.02704\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8524 | loss: 2.02704 - acc: 0.5023 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8525  | total loss: \u001b[1m\u001b[32m1.88901\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8525 | loss: 1.88901 - acc: 0.5521 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8526  | total loss: \u001b[1m\u001b[32m2.01622\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8526 | loss: 2.01622 - acc: 0.5040 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8527  | total loss: \u001b[1m\u001b[32m1.87983\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8527 | loss: 1.87983 - acc: 0.5536 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8528  | total loss: \u001b[1m\u001b[32m2.01103\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8528 | loss: 2.01103 - acc: 0.5054 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8529  | total loss: \u001b[1m\u001b[32m1.87556\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8529 | loss: 1.87556 - acc: 0.5549 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8530  | total loss: \u001b[1m\u001b[32m2.00966\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8530 | loss: 2.00966 - acc: 0.5065 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8531  | total loss: \u001b[1m\u001b[32m1.87466\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8531 | loss: 1.87466 - acc: 0.5559 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8532  | total loss: \u001b[1m\u001b[32m2.01452\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8532 | loss: 2.01452 - acc: 0.5074 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8533  | total loss: \u001b[1m\u001b[32m1.87933\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8533 | loss: 1.87933 - acc: 0.5567 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8534  | total loss: \u001b[1m\u001b[32m2.02540\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8534 | loss: 2.02540 - acc: 0.5010 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8535  | total loss: \u001b[1m\u001b[32m1.88951\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8535 | loss: 1.88951 - acc: 0.5509 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8536  | total loss: \u001b[1m\u001b[32m1.97885\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8536 | loss: 1.97885 - acc: 0.5101 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8537  | total loss: \u001b[1m\u001b[32m1.84792\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8537 | loss: 1.84792 - acc: 0.5591 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8538  | total loss: \u001b[1m\u001b[32m1.96759\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8538 | loss: 1.96759 - acc: 0.5103 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8539  | total loss: \u001b[1m\u001b[32m1.83792\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8539 | loss: 1.83792 - acc: 0.5593 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8540  | total loss: \u001b[1m\u001b[32m1.95790\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8540 | loss: 1.95790 - acc: 0.5177 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8541  | total loss: \u001b[1m\u001b[32m1.82911\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8541 | loss: 1.82911 - acc: 0.5659 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8542  | total loss: \u001b[1m\u001b[32m1.98038\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8542 | loss: 1.98038 - acc: 0.5093 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8543  | total loss: \u001b[1m\u001b[32m1.84913\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8543 | loss: 1.84913 - acc: 0.5584 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8544  | total loss: \u001b[1m\u001b[32m2.00476\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8544 | loss: 2.00476 - acc: 0.5025 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8545  | total loss: \u001b[1m\u001b[32m1.87096\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8545 | loss: 1.87096 - acc: 0.5523 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8546  | total loss: \u001b[1m\u001b[32m2.00497\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8546 | loss: 2.00497 - acc: 0.5042 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8547  | total loss: \u001b[1m\u001b[32m1.87105\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8547 | loss: 1.87105 - acc: 0.5538 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8548  | total loss: \u001b[1m\u001b[32m1.99476\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8548 | loss: 1.99476 - acc: 0.5127 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8549  | total loss: \u001b[1m\u001b[32m1.86167\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8549 | loss: 1.86167 - acc: 0.5614 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8550  | total loss: \u001b[1m\u001b[32m1.96295\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8550 | loss: 1.96295 - acc: 0.5196 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8551  | total loss: \u001b[1m\u001b[32m1.83280\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8551 | loss: 1.83280 - acc: 0.5676 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8552  | total loss: \u001b[1m\u001b[32m1.95641\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8552 | loss: 1.95641 - acc: 0.5180 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8553  | total loss: \u001b[1m\u001b[32m1.82665\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8553 | loss: 1.82665 - acc: 0.5662 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8554  | total loss: \u001b[1m\u001b[32m1.89124\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8554 | loss: 1.89124 - acc: 0.5453 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8555  | total loss: \u001b[1m\u001b[32m1.76731\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8555 | loss: 1.76731 - acc: 0.5908 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8556  | total loss: \u001b[1m\u001b[32m1.91681\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8556 | loss: 1.91681 - acc: 0.5388 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8557  | total loss: \u001b[1m\u001b[32m1.78941\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8557 | loss: 1.78941 - acc: 0.5849 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8558  | total loss: \u001b[1m\u001b[32m1.93344\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8558 | loss: 1.93344 - acc: 0.5336 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8559  | total loss: \u001b[1m\u001b[32m1.80368\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8559 | loss: 1.80368 - acc: 0.5802 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8560  | total loss: \u001b[1m\u001b[32m1.68627\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8560 | loss: 1.68627 - acc: 0.6222 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8561  | total loss: \u001b[1m\u001b[32m1.57941\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8561 | loss: 1.57941 - acc: 0.6600 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8562  | total loss: \u001b[1m\u001b[32m1.72425\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8562 | loss: 1.72425 - acc: 0.6011 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8563  | total loss: \u001b[1m\u001b[32m1.61100\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8563 | loss: 1.61100 - acc: 0.6410 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8564  | total loss: \u001b[1m\u001b[32m1.79121\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8564 | loss: 1.79121 - acc: 0.5769 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8565  | total loss: \u001b[1m\u001b[32m1.66938\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8565 | loss: 1.66938 - acc: 0.6192 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8566  | total loss: \u001b[1m\u001b[32m1.55872\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8566 | loss: 1.55872 - acc: 0.6573 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8567  | total loss: \u001b[1m\u001b[32m1.45765\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8567 | loss: 1.45765 - acc: 0.6916 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 8568  | total loss: \u001b[1m\u001b[32m1.65934\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8568 | loss: 1.65934 - acc: 0.6296 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8569  | total loss: \u001b[1m\u001b[32m1.54540\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8569 | loss: 1.54540 - acc: 0.6666 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8570  | total loss: \u001b[1m\u001b[32m1.73700\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8570 | loss: 1.73700 - acc: 0.5999 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8571  | total loss: \u001b[1m\u001b[32m1.61361\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8571 | loss: 1.61361 - acc: 0.6399 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8572  | total loss: \u001b[1m\u001b[32m1.50176\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8572 | loss: 1.50176 - acc: 0.6760 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8573  | total loss: \u001b[1m\u001b[32m1.39990\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8573 | loss: 1.39990 - acc: 0.7084 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8574  | total loss: \u001b[1m\u001b[32m1.62422\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8574 | loss: 1.62422 - acc: 0.6375 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8575  | total loss: \u001b[1m\u001b[32m1.50813\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8575 | loss: 1.50813 - acc: 0.6738 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8576  | total loss: \u001b[1m\u001b[32m1.72242\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8576 | loss: 1.72242 - acc: 0.6064 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8577  | total loss: \u001b[1m\u001b[32m1.59565\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8577 | loss: 1.59565 - acc: 0.6458 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8578  | total loss: \u001b[1m\u001b[32m1.48112\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8578 | loss: 1.48112 - acc: 0.6812 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8579  | total loss: \u001b[1m\u001b[32m1.37722\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8579 | loss: 1.37722 - acc: 0.7131 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8580  | total loss: \u001b[1m\u001b[32m1.62825\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8580 | loss: 1.62825 - acc: 0.6418 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8581  | total loss: \u001b[1m\u001b[32m1.50837\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8581 | loss: 1.50837 - acc: 0.6776 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8582  | total loss: \u001b[1m\u001b[32m1.69966\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8582 | loss: 1.69966 - acc: 0.6241 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8583  | total loss: \u001b[1m\u001b[32m1.57239\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8583 | loss: 1.57239 - acc: 0.6617 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8584  | total loss: \u001b[1m\u001b[32m1.73625\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8584 | loss: 1.73625 - acc: 0.6098 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8585  | total loss: \u001b[1m\u001b[32m1.60562\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8585 | loss: 1.60562 - acc: 0.6488 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8586  | total loss: \u001b[1m\u001b[32m1.80031\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8586 | loss: 1.80031 - acc: 0.5839 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8587  | total loss: \u001b[1m\u001b[32m1.66410\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8587 | loss: 1.66410 - acc: 0.6256 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8588  | total loss: \u001b[1m\u001b[32m1.81982\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8588 | loss: 1.81982 - acc: 0.5773 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8589  | total loss: \u001b[1m\u001b[32m1.68291\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8589 | loss: 1.68291 - acc: 0.6196 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8590  | total loss: \u001b[1m\u001b[32m1.85132\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8590 | loss: 1.85132 - acc: 0.5647 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8591  | total loss: \u001b[1m\u001b[32m1.71272\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8591 | loss: 1.71272 - acc: 0.6083 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8592  | total loss: \u001b[1m\u001b[32m1.88283\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8592 | loss: 1.88283 - acc: 0.5546 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8593  | total loss: \u001b[1m\u001b[32m1.74275\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8593 | loss: 1.74275 - acc: 0.5991 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8594  | total loss: \u001b[1m\u001b[32m1.92578\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8594 | loss: 1.92578 - acc: 0.5464 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8595  | total loss: \u001b[1m\u001b[32m1.78342\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8595 | loss: 1.78342 - acc: 0.5917 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8596  | total loss: \u001b[1m\u001b[32m1.93644\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8596 | loss: 1.93644 - acc: 0.5397 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8597  | total loss: \u001b[1m\u001b[32m1.79519\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8597 | loss: 1.79519 - acc: 0.5857 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8598  | total loss: \u001b[1m\u001b[32m1.89709\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8598 | loss: 1.89709 - acc: 0.5414 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8599  | total loss: \u001b[1m\u001b[32m1.76182\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8599 | loss: 1.76182 - acc: 0.5873 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8600  | total loss: \u001b[1m\u001b[32m1.89115\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8600 | loss: 1.89115 - acc: 0.5428 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8601  | total loss: \u001b[1m\u001b[32m1.75832\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8601 | loss: 1.75832 - acc: 0.5886 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8602  | total loss: \u001b[1m\u001b[32m1.88688\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8602 | loss: 1.88688 - acc: 0.5440 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8603  | total loss: \u001b[1m\u001b[32m1.75603\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8603 | loss: 1.75603 - acc: 0.5896 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8604  | total loss: \u001b[1m\u001b[32m1.63862\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8604 | loss: 1.63862 - acc: 0.6306 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8605  | total loss: \u001b[1m\u001b[32m1.53264\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8605 | loss: 1.53264 - acc: 0.6676 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8606  | total loss: \u001b[1m\u001b[32m1.72029\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8606 | loss: 1.72029 - acc: 0.6008 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8607  | total loss: \u001b[1m\u001b[32m1.60520\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8607 | loss: 1.60520 - acc: 0.6407 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8608  | total loss: \u001b[1m\u001b[32m1.74815\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8608 | loss: 1.74815 - acc: 0.5909 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8609  | total loss: \u001b[1m\u001b[32m1.62970\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8609 | loss: 1.62970 - acc: 0.6319 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8610  | total loss: \u001b[1m\u001b[32m1.80340\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8610 | loss: 1.80340 - acc: 0.5758 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8611  | total loss: \u001b[1m\u001b[32m1.67924\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8611 | loss: 1.67924 - acc: 0.6182 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8612  | total loss: \u001b[1m\u001b[32m1.84672\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8612 | loss: 1.84672 - acc: 0.5564 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8613  | total loss: \u001b[1m\u001b[32m1.71859\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8613 | loss: 1.71859 - acc: 0.6008 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8614  | total loss: \u001b[1m\u001b[32m1.60323\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8614 | loss: 1.60323 - acc: 0.6407 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8615  | total loss: \u001b[1m\u001b[32m1.49879\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8615 | loss: 1.49879 - acc: 0.6766 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8616  | total loss: \u001b[1m\u001b[32m1.69448\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8616 | loss: 1.69448 - acc: 0.6090 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8617  | total loss: \u001b[1m\u001b[32m1.57970\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8617 | loss: 1.57970 - acc: 0.6481 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8618  | total loss: \u001b[1m\u001b[32m1.74871\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8618 | loss: 1.74871 - acc: 0.5904 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8619  | total loss: \u001b[1m\u001b[32m1.62807\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8619 | loss: 1.62807 - acc: 0.6314 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8620  | total loss: \u001b[1m\u001b[32m1.78086\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8620 | loss: 1.78086 - acc: 0.5825 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8621  | total loss: \u001b[1m\u001b[32m1.65693\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8621 | loss: 1.65693 - acc: 0.6243 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8622  | total loss: \u001b[1m\u001b[32m1.84443\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8622 | loss: 1.84443 - acc: 0.5618 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8623  | total loss: \u001b[1m\u001b[32m1.71447\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8623 | loss: 1.71447 - acc: 0.6056 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8624  | total loss: \u001b[1m\u001b[32m1.88531\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8624 | loss: 1.88531 - acc: 0.5522 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 8625  | total loss: \u001b[1m\u001b[32m1.75211\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8625 | loss: 1.75211 - acc: 0.5970 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8626  | total loss: \u001b[1m\u001b[32m1.63241\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8626 | loss: 1.63241 - acc: 0.6373 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8627  | total loss: \u001b[1m\u001b[32m1.52421\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8627 | loss: 1.52421 - acc: 0.6736 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8628  | total loss: \u001b[1m\u001b[32m1.71166\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8628 | loss: 1.71166 - acc: 0.6134 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8629  | total loss: \u001b[1m\u001b[32m1.59438\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8629 | loss: 1.59438 - acc: 0.6520 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8630  | total loss: \u001b[1m\u001b[32m1.74147\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8630 | loss: 1.74147 - acc: 0.6011 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8631  | total loss: \u001b[1m\u001b[32m1.62056\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8631 | loss: 1.62056 - acc: 0.6410 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8632  | total loss: \u001b[1m\u001b[32m1.78734\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8632 | loss: 1.78734 - acc: 0.5840 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8633  | total loss: \u001b[1m\u001b[32m1.66161\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8633 | loss: 1.66161 - acc: 0.6256 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8634  | total loss: \u001b[1m\u001b[32m1.81203\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8634 | loss: 1.81203 - acc: 0.5774 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8635  | total loss: \u001b[1m\u001b[32m1.68386\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8635 | loss: 1.68386 - acc: 0.6196 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8636  | total loss: \u001b[1m\u001b[32m1.86955\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8636 | loss: 1.86955 - acc: 0.5577 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8637  | total loss: \u001b[1m\u001b[32m1.73598\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8637 | loss: 1.73598 - acc: 0.6019 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8638  | total loss: \u001b[1m\u001b[32m1.90518\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8638 | loss: 1.90518 - acc: 0.5488 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8639  | total loss: \u001b[1m\u001b[32m1.76871\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8639 | loss: 1.76871 - acc: 0.5940 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8640  | total loss: \u001b[1m\u001b[32m1.95200\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8640 | loss: 1.95200 - acc: 0.5346 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8641  | total loss: \u001b[1m\u001b[32m1.81185\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8641 | loss: 1.81185 - acc: 0.5811 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8642  | total loss: \u001b[1m\u001b[32m1.94809\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8642 | loss: 1.94809 - acc: 0.5373 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8643  | total loss: \u001b[1m\u001b[32m1.80950\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8643 | loss: 1.80950 - acc: 0.5836 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8644  | total loss: \u001b[1m\u001b[32m1.91360\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8644 | loss: 1.91360 - acc: 0.5466 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8645  | total loss: \u001b[1m\u001b[32m1.77952\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8645 | loss: 1.77952 - acc: 0.5920 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8646  | total loss: \u001b[1m\u001b[32m1.85116\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8646 | loss: 1.85116 - acc: 0.5542 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8647  | total loss: \u001b[1m\u001b[32m1.72403\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8647 | loss: 1.72403 - acc: 0.5988 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8648  | total loss: \u001b[1m\u001b[32m1.87279\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8648 | loss: 1.87279 - acc: 0.5460 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8649  | total loss: \u001b[1m\u001b[32m1.74401\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8649 | loss: 1.74401 - acc: 0.5914 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8650  | total loss: \u001b[1m\u001b[32m1.91563\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8650 | loss: 1.91563 - acc: 0.5323 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8651  | total loss: \u001b[1m\u001b[32m1.78331\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8651 | loss: 1.78331 - acc: 0.5791 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8652  | total loss: \u001b[1m\u001b[32m1.66434\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8652 | loss: 1.66434 - acc: 0.6212 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8653  | total loss: \u001b[1m\u001b[32m1.55676\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8653 | loss: 1.55676 - acc: 0.6590 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8654  | total loss: \u001b[1m\u001b[32m1.72261\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8654 | loss: 1.72261 - acc: 0.5931 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8655  | total loss: \u001b[1m\u001b[32m1.60789\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8655 | loss: 1.60789 - acc: 0.6338 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8656  | total loss: \u001b[1m\u001b[32m1.73493\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8656 | loss: 1.73493 - acc: 0.5919 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8657  | total loss: \u001b[1m\u001b[32m1.61811\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8657 | loss: 1.61811 - acc: 0.6327 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8658  | total loss: \u001b[1m\u001b[32m1.79525\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8658 | loss: 1.79525 - acc: 0.5766 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8659  | total loss: \u001b[1m\u001b[32m1.67190\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8659 | loss: 1.67190 - acc: 0.6189 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8660  | total loss: \u001b[1m\u001b[32m1.84040\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8660 | loss: 1.84040 - acc: 0.5570 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8661  | total loss: \u001b[1m\u001b[32m1.71251\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8661 | loss: 1.71251 - acc: 0.6013 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8662  | total loss: \u001b[1m\u001b[32m1.88921\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8662 | loss: 1.88921 - acc: 0.5412 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8663  | total loss: \u001b[1m\u001b[32m1.75699\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8663 | loss: 1.75699 - acc: 0.5871 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8664  | total loss: \u001b[1m\u001b[32m1.87767\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8664 | loss: 1.87767 - acc: 0.5426 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8665  | total loss: \u001b[1m\u001b[32m1.74738\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8665 | loss: 1.74738 - acc: 0.5884 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8666  | total loss: \u001b[1m\u001b[32m1.89106\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8666 | loss: 1.89106 - acc: 0.5367 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8667  | total loss: \u001b[1m\u001b[32m1.76024\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8667 | loss: 1.76024 - acc: 0.5830 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8668  | total loss: \u001b[1m\u001b[32m1.64257\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8668 | loss: 1.64257 - acc: 0.6247 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8669  | total loss: \u001b[1m\u001b[32m1.53612\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8669 | loss: 1.53612 - acc: 0.6622 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8670  | total loss: \u001b[1m\u001b[32m1.43920\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8670 | loss: 1.43920 - acc: 0.6960 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8671  | total loss: \u001b[1m\u001b[32m1.35038\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8671 | loss: 1.35038 - acc: 0.7264 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8672  | total loss: \u001b[1m\u001b[32m1.56642\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8672 | loss: 1.56642 - acc: 0.6538 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8673  | total loss: \u001b[1m\u001b[32m1.46188\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8673 | loss: 1.46188 - acc: 0.6884 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8674  | total loss: \u001b[1m\u001b[32m1.68682\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8674 | loss: 1.68682 - acc: 0.6196 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8675  | total loss: \u001b[1m\u001b[32m1.56832\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8675 | loss: 1.56832 - acc: 0.6576 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8676  | total loss: \u001b[1m\u001b[32m1.74106\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8676 | loss: 1.74106 - acc: 0.5990 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8677  | total loss: \u001b[1m\u001b[32m1.61620\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8677 | loss: 1.61620 - acc: 0.6391 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8678  | total loss: \u001b[1m\u001b[32m1.80333\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8678 | loss: 1.80333 - acc: 0.5823 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8679  | total loss: \u001b[1m\u001b[32m1.67200\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8679 | loss: 1.67200 - acc: 0.6241 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8680  | total loss: \u001b[1m\u001b[32m1.55358\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8680 | loss: 1.55358 - acc: 0.6617 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8681  | total loss: \u001b[1m\u001b[32m1.44632\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8681 | loss: 1.44632 - acc: 0.6955 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 8682  | total loss: \u001b[1m\u001b[32m1.34869\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8682 | loss: 1.34869 - acc: 0.7260 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8683  | total loss: \u001b[1m\u001b[32m1.25938\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8683 | loss: 1.25938 - acc: 0.7534 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8684  | total loss: \u001b[1m\u001b[32m1.17730\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8684 | loss: 1.17730 - acc: 0.7780 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8685  | total loss: \u001b[1m\u001b[32m1.10149\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8685 | loss: 1.10149 - acc: 0.8002 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8686  | total loss: \u001b[1m\u001b[32m1.03117\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8686 | loss: 1.03117 - acc: 0.8202 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8687  | total loss: \u001b[1m\u001b[32m0.96567\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8687 | loss: 0.96567 - acc: 0.8382 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8688  | total loss: \u001b[1m\u001b[32m1.20236\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8688 | loss: 1.20236 - acc: 0.7686 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8689  | total loss: \u001b[1m\u001b[32m1.11616\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8689 | loss: 1.11616 - acc: 0.7918 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8690  | total loss: \u001b[1m\u001b[32m1.03714\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8690 | loss: 1.03714 - acc: 0.8126 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8691  | total loss: \u001b[1m\u001b[32m0.96445\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8691 | loss: 0.96445 - acc: 0.8313 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8692  | total loss: \u001b[1m\u001b[32m1.25635\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8692 | loss: 1.25635 - acc: 0.7482 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8693  | total loss: \u001b[1m\u001b[32m1.15941\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8693 | loss: 1.15941 - acc: 0.7734 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8694  | total loss: \u001b[1m\u001b[32m1.42861\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8694 | loss: 1.42861 - acc: 0.7032 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8695  | total loss: \u001b[1m\u001b[32m1.31359\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8695 | loss: 1.31359 - acc: 0.7329 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8696  | total loss: \u001b[1m\u001b[32m1.57414\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8696 | loss: 1.57414 - acc: 0.6667 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8697  | total loss: \u001b[1m\u001b[32m1.44494\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8697 | loss: 1.44494 - acc: 0.7001 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8698  | total loss: \u001b[1m\u001b[32m1.61858\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8698 | loss: 1.61858 - acc: 0.6586 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8699  | total loss: \u001b[1m\u001b[32m1.48607\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8699 | loss: 1.48607 - acc: 0.6928 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8700  | total loss: \u001b[1m\u001b[32m1.66159\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8700 | loss: 1.66159 - acc: 0.6449 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8701  | total loss: \u001b[1m\u001b[32m1.52631\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8701 | loss: 1.52631 - acc: 0.6804 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8702  | total loss: \u001b[1m\u001b[32m1.73108\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8702 | loss: 1.73108 - acc: 0.6195 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8703  | total loss: \u001b[1m\u001b[32m1.59086\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8703 | loss: 1.59086 - acc: 0.6576 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8704  | total loss: \u001b[1m\u001b[32m1.79472\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8704 | loss: 1.79472 - acc: 0.5990 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8705  | total loss: \u001b[1m\u001b[32m1.65069\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8705 | loss: 1.65069 - acc: 0.6391 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8706  | total loss: \u001b[1m\u001b[32m1.86640\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8706 | loss: 1.86640 - acc: 0.5752 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8707  | total loss: \u001b[1m\u001b[32m1.71831\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8707 | loss: 1.71831 - acc: 0.6176 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8708  | total loss: \u001b[1m\u001b[32m1.91893\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8708 | loss: 1.91893 - acc: 0.5559 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8709  | total loss: \u001b[1m\u001b[32m1.76912\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8709 | loss: 1.76912 - acc: 0.6003 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8710  | total loss: \u001b[1m\u001b[32m1.63584\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8710 | loss: 1.63584 - acc: 0.6403 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8711  | total loss: \u001b[1m\u001b[32m1.51687\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8711 | loss: 1.51687 - acc: 0.6762 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8712  | total loss: \u001b[1m\u001b[32m1.74404\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8712 | loss: 1.74404 - acc: 0.6086 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8713  | total loss: \u001b[1m\u001b[32m1.61600\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8713 | loss: 1.61600 - acc: 0.6477 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8714  | total loss: \u001b[1m\u001b[32m1.50149\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8714 | loss: 1.50149 - acc: 0.6830 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8715  | total loss: \u001b[1m\u001b[32m1.39860\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8715 | loss: 1.39860 - acc: 0.7147 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8716  | total loss: \u001b[1m\u001b[32m1.57968\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8716 | loss: 1.57968 - acc: 0.6504 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8717  | total loss: \u001b[1m\u001b[32m1.46908\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8717 | loss: 1.46908 - acc: 0.6853 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8718  | total loss: \u001b[1m\u001b[32m1.60435\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8718 | loss: 1.60435 - acc: 0.6382 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8719  | total loss: \u001b[1m\u001b[32m1.49159\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8719 | loss: 1.49159 - acc: 0.6744 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8720  | total loss: \u001b[1m\u001b[32m1.69008\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8720 | loss: 1.69008 - acc: 0.6141 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8721  | total loss: \u001b[1m\u001b[32m1.56928\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8721 | loss: 1.56928 - acc: 0.6527 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8722  | total loss: \u001b[1m\u001b[32m1.71302\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8722 | loss: 1.71302 - acc: 0.6088 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8723  | total loss: \u001b[1m\u001b[32m1.59069\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8723 | loss: 1.59069 - acc: 0.6480 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8724  | total loss: \u001b[1m\u001b[32m1.75634\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8724 | loss: 1.75634 - acc: 0.5975 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8725  | total loss: \u001b[1m\u001b[32m1.63053\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8725 | loss: 1.63053 - acc: 0.6377 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8726  | total loss: \u001b[1m\u001b[32m1.81455\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8726 | loss: 1.81455 - acc: 0.5739 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8727  | total loss: \u001b[1m\u001b[32m1.68406\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8727 | loss: 1.68406 - acc: 0.6165 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8728  | total loss: \u001b[1m\u001b[32m1.86224\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8728 | loss: 1.86224 - acc: 0.5549 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8729  | total loss: \u001b[1m\u001b[32m1.72835\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8729 | loss: 1.72835 - acc: 0.5994 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8730  | total loss: \u001b[1m\u001b[32m1.88304\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8730 | loss: 1.88304 - acc: 0.5466 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8731  | total loss: \u001b[1m\u001b[32m1.74856\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8731 | loss: 1.74856 - acc: 0.5919 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8732  | total loss: \u001b[1m\u001b[32m1.88629\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8732 | loss: 1.88629 - acc: 0.5470 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8733  | total loss: \u001b[1m\u001b[32m1.75302\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8733 | loss: 1.75302 - acc: 0.5923 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8734  | total loss: \u001b[1m\u001b[32m1.92951\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8734 | loss: 1.92951 - acc: 0.5331 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8735  | total loss: \u001b[1m\u001b[32m1.79354\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8735 | loss: 1.79354 - acc: 0.5798 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8736  | total loss: \u001b[1m\u001b[32m1.96151\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8736 | loss: 1.96151 - acc: 0.5218 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8737  | total loss: \u001b[1m\u001b[32m1.82410\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8737 | loss: 1.82410 - acc: 0.5696 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8738  | total loss: \u001b[1m\u001b[32m2.00869\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8738 | loss: 2.00869 - acc: 0.5127 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 8739  | total loss: \u001b[1m\u001b[32m1.86841\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8739 | loss: 1.86841 - acc: 0.5614 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8740  | total loss: \u001b[1m\u001b[32m2.02201\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8740 | loss: 2.02201 - acc: 0.5124 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8741  | total loss: \u001b[1m\u001b[32m1.88219\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8741 | loss: 1.88219 - acc: 0.5612 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8742  | total loss: \u001b[1m\u001b[32m1.75679\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8742 | loss: 1.75679 - acc: 0.6050 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8743  | total loss: \u001b[1m\u001b[32m1.64366\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8743 | loss: 1.64366 - acc: 0.6445 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8744  | total loss: \u001b[1m\u001b[32m1.82131\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8744 | loss: 1.82131 - acc: 0.5801 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8745  | total loss: \u001b[1m\u001b[32m1.70080\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8745 | loss: 1.70080 - acc: 0.6221 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8746  | total loss: \u001b[1m\u001b[32m1.81897\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8746 | loss: 1.81897 - acc: 0.5742 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8747  | total loss: \u001b[1m\u001b[32m1.69810\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8747 | loss: 1.69810 - acc: 0.6167 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8748  | total loss: \u001b[1m\u001b[32m1.88063\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8748 | loss: 1.88063 - acc: 0.5551 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8749  | total loss: \u001b[1m\u001b[32m1.75330\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8749 | loss: 1.75330 - acc: 0.5996 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8750  | total loss: \u001b[1m\u001b[32m1.89221\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8750 | loss: 1.89221 - acc: 0.5467 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8751  | total loss: \u001b[1m\u001b[32m1.76367\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8751 | loss: 1.76367 - acc: 0.5921 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8752  | total loss: \u001b[1m\u001b[32m1.93505\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8752 | loss: 1.93505 - acc: 0.5329 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8753  | total loss: \u001b[1m\u001b[32m1.80241\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8753 | loss: 1.80241 - acc: 0.5796 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8754  | total loss: \u001b[1m\u001b[32m1.92677\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8754 | loss: 1.92677 - acc: 0.5359 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8755  | total loss: \u001b[1m\u001b[32m1.79518\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8755 | loss: 1.79518 - acc: 0.5823 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8756  | total loss: \u001b[1m\u001b[32m1.94846\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8756 | loss: 1.94846 - acc: 0.5241 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8757  | total loss: \u001b[1m\u001b[32m1.81494\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8757 | loss: 1.81494 - acc: 0.5717 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8758  | total loss: \u001b[1m\u001b[32m1.97983\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8758 | loss: 1.97983 - acc: 0.5145 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8759  | total loss: \u001b[1m\u001b[32m1.84366\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8759 | loss: 1.84366 - acc: 0.5631 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8760  | total loss: \u001b[1m\u001b[32m1.98171\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8760 | loss: 1.98171 - acc: 0.5139 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8761  | total loss: \u001b[1m\u001b[32m1.84602\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8761 | loss: 1.84602 - acc: 0.5625 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8762  | total loss: \u001b[1m\u001b[32m1.98570\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8762 | loss: 1.98570 - acc: 0.5134 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8763  | total loss: \u001b[1m\u001b[32m1.85036\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8763 | loss: 1.85036 - acc: 0.5621 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8764  | total loss: \u001b[1m\u001b[32m1.98924\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8764 | loss: 1.98924 - acc: 0.5130 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8765  | total loss: \u001b[1m\u001b[32m1.85437\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8765 | loss: 1.85437 - acc: 0.5617 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8766  | total loss: \u001b[1m\u001b[32m1.73306\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8766 | loss: 1.73306 - acc: 0.6055 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8767  | total loss: \u001b[1m\u001b[32m1.62328\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8767 | loss: 1.62328 - acc: 0.6450 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8768  | total loss: \u001b[1m\u001b[32m1.78279\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8768 | loss: 1.78279 - acc: 0.5876 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8769  | total loss: \u001b[1m\u001b[32m1.66638\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8769 | loss: 1.66638 - acc: 0.6289 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8770  | total loss: \u001b[1m\u001b[32m1.77826\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8770 | loss: 1.77826 - acc: 0.5874 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8771  | total loss: \u001b[1m\u001b[32m1.66077\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8771 | loss: 1.66077 - acc: 0.6287 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8772  | total loss: \u001b[1m\u001b[32m1.82103\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8772 | loss: 1.82103 - acc: 0.5729 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8773  | total loss: \u001b[1m\u001b[32m1.69804\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8773 | loss: 1.69804 - acc: 0.6156 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8774  | total loss: \u001b[1m\u001b[32m1.86859\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8774 | loss: 1.86859 - acc: 0.5541 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8775  | total loss: \u001b[1m\u001b[32m1.74015\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8775 | loss: 1.74015 - acc: 0.5987 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8776  | total loss: \u001b[1m\u001b[32m1.89360\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8776 | loss: 1.89360 - acc: 0.5459 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8777  | total loss: \u001b[1m\u001b[32m1.76239\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8777 | loss: 1.76239 - acc: 0.5914 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8778  | total loss: \u001b[1m\u001b[32m1.64390\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8778 | loss: 1.64390 - acc: 0.6322 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8779  | total loss: \u001b[1m\u001b[32m1.53632\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8779 | loss: 1.53632 - acc: 0.6690 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8780  | total loss: \u001b[1m\u001b[32m1.43806\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8780 | loss: 1.43806 - acc: 0.7021 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8781  | total loss: \u001b[1m\u001b[32m1.34776\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8781 | loss: 1.34776 - acc: 0.7319 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8782  | total loss: \u001b[1m\u001b[32m1.53058\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8782 | loss: 1.53058 - acc: 0.6658 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8783  | total loss: \u001b[1m\u001b[32m1.42760\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8783 | loss: 1.42760 - acc: 0.6993 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8784  | total loss: \u001b[1m\u001b[32m1.57827\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8784 | loss: 1.57827 - acc: 0.6508 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8785  | total loss: \u001b[1m\u001b[32m1.46797\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8785 | loss: 1.46797 - acc: 0.6857 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8786  | total loss: \u001b[1m\u001b[32m1.67388\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8786 | loss: 1.67388 - acc: 0.6171 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8787  | total loss: \u001b[1m\u001b[32m1.55234\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8787 | loss: 1.55234 - acc: 0.6554 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8788  | total loss: \u001b[1m\u001b[32m1.75607\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8788 | loss: 1.75607 - acc: 0.5970 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8789  | total loss: \u001b[1m\u001b[32m1.62571\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8789 | loss: 1.62571 - acc: 0.6373 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8790  | total loss: \u001b[1m\u001b[32m1.78627\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8790 | loss: 1.78627 - acc: 0.5879 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8791  | total loss: \u001b[1m\u001b[32m1.65300\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8791 | loss: 1.65300 - acc: 0.6291 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8792  | total loss: \u001b[1m\u001b[32m1.53297\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8792 | loss: 1.53297 - acc: 0.6662 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8793  | total loss: \u001b[1m\u001b[32m1.42439\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8793 | loss: 1.42439 - acc: 0.6996 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8794  | total loss: \u001b[1m\u001b[32m1.32574\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8794 | loss: 1.32574 - acc: 0.7296 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8795  | total loss: \u001b[1m\u001b[32m1.23569\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8795 | loss: 1.23569 - acc: 0.7566 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 8796  | total loss: \u001b[1m\u001b[32m1.48765\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8796 | loss: 1.48765 - acc: 0.6810 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8797  | total loss: \u001b[1m\u001b[32m1.37933\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8797 | loss: 1.37933 - acc: 0.7129 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8798  | total loss: \u001b[1m\u001b[32m1.61157\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8798 | loss: 1.61157 - acc: 0.6487 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8799  | total loss: \u001b[1m\u001b[32m1.48995\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8799 | loss: 1.48995 - acc: 0.6839 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8800  | total loss: \u001b[1m\u001b[32m1.64651\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8800 | loss: 1.64651 - acc: 0.6369 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8801  | total loss: \u001b[1m\u001b[32m1.52127\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8801 | loss: 1.52127 - acc: 0.6732 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8802  | total loss: \u001b[1m\u001b[32m1.70680\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8802 | loss: 1.70680 - acc: 0.6130 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8803  | total loss: \u001b[1m\u001b[32m1.57597\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8803 | loss: 1.57597 - acc: 0.6517 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8804  | total loss: \u001b[1m\u001b[32m1.75058\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8804 | loss: 1.75058 - acc: 0.5937 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8805  | total loss: \u001b[1m\u001b[32m1.61636\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8805 | loss: 1.61636 - acc: 0.6343 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8806  | total loss: \u001b[1m\u001b[32m1.82088\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8806 | loss: 1.82088 - acc: 0.5780 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8807  | total loss: \u001b[1m\u001b[32m1.68113\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8807 | loss: 1.68113 - acc: 0.6202 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8808  | total loss: \u001b[1m\u001b[32m1.84802\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8808 | loss: 1.84802 - acc: 0.5654 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8809  | total loss: \u001b[1m\u001b[32m1.70748\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8809 | loss: 1.70748 - acc: 0.6088 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8810  | total loss: \u001b[1m\u001b[32m1.87733\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8810 | loss: 1.87733 - acc: 0.5551 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8811  | total loss: \u001b[1m\u001b[32m1.73608\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8811 | loss: 1.73608 - acc: 0.5996 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8812  | total loss: \u001b[1m\u001b[32m1.92446\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8812 | loss: 1.92446 - acc: 0.5396 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8813  | total loss: \u001b[1m\u001b[32m1.78097\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8813 | loss: 1.78097 - acc: 0.5857 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8814  | total loss: \u001b[1m\u001b[32m1.91733\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8814 | loss: 1.91733 - acc: 0.5414 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8815  | total loss: \u001b[1m\u001b[32m1.77703\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8815 | loss: 1.77703 - acc: 0.5872 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8816  | total loss: \u001b[1m\u001b[32m1.91932\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8816 | loss: 1.91932 - acc: 0.5357 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8817  | total loss: \u001b[1m\u001b[32m1.78115\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8817 | loss: 1.78115 - acc: 0.5821 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8818  | total loss: \u001b[1m\u001b[32m1.93064\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8818 | loss: 1.93064 - acc: 0.5310 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8819  | total loss: \u001b[1m\u001b[32m1.79352\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8819 | loss: 1.79352 - acc: 0.5779 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8820  | total loss: \u001b[1m\u001b[32m1.94876\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8820 | loss: 1.94876 - acc: 0.5273 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8821  | total loss: \u001b[1m\u001b[32m1.81184\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8821 | loss: 1.81184 - acc: 0.5745 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8822  | total loss: \u001b[1m\u001b[32m1.68921\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8822 | loss: 1.68921 - acc: 0.6171 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8823  | total loss: \u001b[1m\u001b[32m1.57877\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8823 | loss: 1.57877 - acc: 0.6554 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8824  | total loss: \u001b[1m\u001b[32m1.71510\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8824 | loss: 1.71510 - acc: 0.6041 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8825  | total loss: \u001b[1m\u001b[32m1.60129\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8825 | loss: 1.60129 - acc: 0.6437 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8826  | total loss: \u001b[1m\u001b[32m1.78273\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8826 | loss: 1.78273 - acc: 0.5793 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8827  | total loss: \u001b[1m\u001b[32m1.66171\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8827 | loss: 1.66171 - acc: 0.6214 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8828  | total loss: \u001b[1m\u001b[32m1.80470\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8828 | loss: 1.80470 - acc: 0.5736 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8829  | total loss: \u001b[1m\u001b[32m1.68136\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8829 | loss: 1.68136 - acc: 0.6162 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8830  | total loss: \u001b[1m\u001b[32m1.82380\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8830 | loss: 1.82380 - acc: 0.5689 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8831  | total loss: \u001b[1m\u001b[32m1.69854\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8831 | loss: 1.69854 - acc: 0.6120 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8832  | total loss: \u001b[1m\u001b[32m1.87087\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8832 | loss: 1.87087 - acc: 0.5508 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8833  | total loss: \u001b[1m\u001b[32m1.74113\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8833 | loss: 1.74113 - acc: 0.5957 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8834  | total loss: \u001b[1m\u001b[32m1.91021\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8834 | loss: 1.91021 - acc: 0.5361 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8835  | total loss: \u001b[1m\u001b[32m1.77714\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8835 | loss: 1.77714 - acc: 0.5825 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8836  | total loss: \u001b[1m\u001b[32m1.65743\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8836 | loss: 1.65743 - acc: 0.6243 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8837  | total loss: \u001b[1m\u001b[32m1.54914\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8837 | loss: 1.54914 - acc: 0.6618 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8838  | total loss: \u001b[1m\u001b[32m1.73776\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8838 | loss: 1.73776 - acc: 0.5957 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8839  | total loss: \u001b[1m\u001b[32m1.62017\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8839 | loss: 1.62017 - acc: 0.6361 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8840  | total loss: \u001b[1m\u001b[32m1.51361\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8840 | loss: 1.51361 - acc: 0.6725 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8841  | total loss: \u001b[1m\u001b[32m1.41648\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8841 | loss: 1.41648 - acc: 0.7052 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8842  | total loss: \u001b[1m\u001b[32m1.62708\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8842 | loss: 1.62708 - acc: 0.6347 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8843  | total loss: \u001b[1m\u001b[32m1.51626\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8843 | loss: 1.51626 - acc: 0.6712 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8844  | total loss: \u001b[1m\u001b[32m1.73024\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8844 | loss: 1.73024 - acc: 0.6041 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8845  | total loss: \u001b[1m\u001b[32m1.60785\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8845 | loss: 1.60785 - acc: 0.6437 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8846  | total loss: \u001b[1m\u001b[32m1.80668\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8846 | loss: 1.80668 - acc: 0.5793 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8847  | total loss: \u001b[1m\u001b[32m1.67633\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8847 | loss: 1.67633 - acc: 0.6214 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8848  | total loss: \u001b[1m\u001b[32m1.85942\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8848 | loss: 1.85942 - acc: 0.5593 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8849  | total loss: \u001b[1m\u001b[32m1.72418\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8849 | loss: 1.72418 - acc: 0.6033 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8850  | total loss: \u001b[1m\u001b[32m1.86916\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8850 | loss: 1.86916 - acc: 0.5501 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8851  | total loss: \u001b[1m\u001b[32m1.73381\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8851 | loss: 1.73381 - acc: 0.5951 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8852  | total loss: \u001b[1m\u001b[32m1.88170\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8852 | loss: 1.88170 - acc: 0.5499 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 8853  | total loss: \u001b[1m\u001b[32m1.74625\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8853 | loss: 1.74625 - acc: 0.5949 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8854  | total loss: \u001b[1m\u001b[32m1.89466\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8854 | loss: 1.89466 - acc: 0.5426 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8855  | total loss: \u001b[1m\u001b[32m1.75923\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8855 | loss: 1.75923 - acc: 0.5883 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8856  | total loss: \u001b[1m\u001b[32m1.94925\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8856 | loss: 1.94925 - acc: 0.5295 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8857  | total loss: \u001b[1m\u001b[32m1.80996\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8857 | loss: 1.80996 - acc: 0.5765 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8858  | total loss: \u001b[1m\u001b[32m1.95381\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8858 | loss: 1.95381 - acc: 0.5260 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8859  | total loss: \u001b[1m\u001b[32m1.81575\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8859 | loss: 1.81575 - acc: 0.5734 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8860  | total loss: \u001b[1m\u001b[32m1.95249\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8860 | loss: 1.95249 - acc: 0.5232 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8861  | total loss: \u001b[1m\u001b[32m1.81614\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8861 | loss: 1.81614 - acc: 0.5709 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8862  | total loss: \u001b[1m\u001b[32m1.94651\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8862 | loss: 1.94651 - acc: 0.5281 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8863  | total loss: \u001b[1m\u001b[32m1.81215\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8863 | loss: 1.81215 - acc: 0.5753 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8864  | total loss: \u001b[1m\u001b[32m1.95170\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8864 | loss: 1.95170 - acc: 0.5249 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8865  | total loss: \u001b[1m\u001b[32m1.81808\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8865 | loss: 1.81808 - acc: 0.5724 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8866  | total loss: \u001b[1m\u001b[32m1.95006\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8866 | loss: 1.95006 - acc: 0.5223 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8867  | total loss: \u001b[1m\u001b[32m1.81785\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8867 | loss: 1.81785 - acc: 0.5701 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8868  | total loss: \u001b[1m\u001b[32m1.99182\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8868 | loss: 1.99182 - acc: 0.5131 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8869  | total loss: \u001b[1m\u001b[32m1.85675\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8869 | loss: 1.85675 - acc: 0.5618 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8870  | total loss: \u001b[1m\u001b[32m1.99804\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8870 | loss: 1.99804 - acc: 0.5127 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8871  | total loss: \u001b[1m\u001b[32m1.86357\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8871 | loss: 1.86357 - acc: 0.5615 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8872  | total loss: \u001b[1m\u001b[32m1.98656\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8872 | loss: 1.98656 - acc: 0.5125 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8873  | total loss: \u001b[1m\u001b[32m1.85411\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8873 | loss: 1.85411 - acc: 0.5612 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8874  | total loss: \u001b[1m\u001b[32m1.95931\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8874 | loss: 1.95931 - acc: 0.5265 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8875  | total loss: \u001b[1m\u001b[32m1.82988\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8875 | loss: 1.82988 - acc: 0.5739 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8876  | total loss: \u001b[1m\u001b[32m1.96914\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8876 | loss: 1.96914 - acc: 0.5236 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8877  | total loss: \u001b[1m\u001b[32m1.83866\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8877 | loss: 1.83866 - acc: 0.5713 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8878  | total loss: \u001b[1m\u001b[32m1.97362\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8878 | loss: 1.97362 - acc: 0.5213 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8879  | total loss: \u001b[1m\u001b[32m1.84254\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 8879 | loss: 1.84254 - acc: 0.5691 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8880  | total loss: \u001b[1m\u001b[32m1.99228\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8880 | loss: 1.99228 - acc: 0.5122 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8881  | total loss: \u001b[1m\u001b[32m1.85919\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8881 | loss: 1.85919 - acc: 0.5610 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8882  | total loss: \u001b[1m\u001b[32m1.97166\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8882 | loss: 1.97166 - acc: 0.5192 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8883  | total loss: \u001b[1m\u001b[32m1.84047\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8883 | loss: 1.84047 - acc: 0.5673 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8884  | total loss: \u001b[1m\u001b[32m1.72195\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8884 | loss: 1.72195 - acc: 0.6105 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8885  | total loss: \u001b[1m\u001b[32m1.61421\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8885 | loss: 1.61421 - acc: 0.6495 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8886  | total loss: \u001b[1m\u001b[32m1.75402\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8886 | loss: 1.75402 - acc: 0.5917 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8887  | total loss: \u001b[1m\u001b[32m1.64067\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8887 | loss: 1.64067 - acc: 0.6325 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8888  | total loss: \u001b[1m\u001b[32m1.82509\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8888 | loss: 1.82509 - acc: 0.5693 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8889  | total loss: \u001b[1m\u001b[32m1.70288\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8889 | loss: 1.70288 - acc: 0.6123 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8890  | total loss: \u001b[1m\u001b[32m1.83719\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8890 | loss: 1.83719 - acc: 0.5654 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8891  | total loss: \u001b[1m\u001b[32m1.71249\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8891 | loss: 1.71249 - acc: 0.6089 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8892  | total loss: \u001b[1m\u001b[32m1.80545\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 8892 | loss: 1.80545 - acc: 0.5765 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8893  | total loss: \u001b[1m\u001b[32m1.68262\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8893 | loss: 1.68262 - acc: 0.6189 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8894  | total loss: \u001b[1m\u001b[32m1.84073\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8894 | loss: 1.84073 - acc: 0.5641 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8895  | total loss: \u001b[1m\u001b[32m1.71320\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8895 | loss: 1.71320 - acc: 0.6077 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8896  | total loss: \u001b[1m\u001b[32m1.87311\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8896 | loss: 1.87311 - acc: 0.5541 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8897  | total loss: \u001b[1m\u001b[32m1.74158\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8897 | loss: 1.74158 - acc: 0.5987 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8898  | total loss: \u001b[1m\u001b[32m1.89044\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8898 | loss: 1.89044 - acc: 0.5460 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8899  | total loss: \u001b[1m\u001b[32m1.75679\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8899 | loss: 1.75679 - acc: 0.5914 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8900  | total loss: \u001b[1m\u001b[32m1.90344\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8900 | loss: 1.90344 - acc: 0.5394 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8901  | total loss: \u001b[1m\u001b[32m1.76840\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8901 | loss: 1.76840 - acc: 0.5854 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8902  | total loss: \u001b[1m\u001b[32m1.94592\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8902 | loss: 1.94592 - acc: 0.5340 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8903  | total loss: \u001b[1m\u001b[32m1.80681\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8903 | loss: 1.80681 - acc: 0.5806 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8904  | total loss: \u001b[1m\u001b[32m1.68147\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8904 | loss: 1.68147 - acc: 0.6226 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8905  | total loss: \u001b[1m\u001b[32m1.56798\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8905 | loss: 1.56798 - acc: 0.6603 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8906  | total loss: \u001b[1m\u001b[32m1.67477\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8906 | loss: 1.67477 - acc: 0.6229 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8907  | total loss: \u001b[1m\u001b[32m1.56022\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8907 | loss: 1.56022 - acc: 0.6606 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8908  | total loss: \u001b[1m\u001b[32m1.71689\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8908 | loss: 1.71689 - acc: 0.6088 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8909  | total loss: \u001b[1m\u001b[32m1.59671\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8909 | loss: 1.59671 - acc: 0.6479 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 8910  | total loss: \u001b[1m\u001b[32m1.48770\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8910 | loss: 1.48770 - acc: 0.6831 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8911  | total loss: \u001b[1m\u001b[32m1.38836\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8911 | loss: 1.38836 - acc: 0.7148 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8912  | total loss: \u001b[1m\u001b[32m1.61906\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8912 | loss: 1.61906 - acc: 0.6433 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8913  | total loss: \u001b[1m\u001b[32m1.50447\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8913 | loss: 1.50447 - acc: 0.6790 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8914  | total loss: \u001b[1m\u001b[32m1.40041\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8914 | loss: 1.40041 - acc: 0.7111 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8915  | total loss: \u001b[1m\u001b[32m1.30549\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8915 | loss: 1.30549 - acc: 0.7400 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8916  | total loss: \u001b[1m\u001b[32m1.52658\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8916 | loss: 1.52658 - acc: 0.6660 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8917  | total loss: \u001b[1m\u001b[32m1.41693\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8917 | loss: 1.41693 - acc: 0.6994 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8918  | total loss: \u001b[1m\u001b[32m1.64469\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8918 | loss: 1.64469 - acc: 0.6295 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8919  | total loss: \u001b[1m\u001b[32m1.52237\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8919 | loss: 1.52237 - acc: 0.6665 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8920  | total loss: \u001b[1m\u001b[32m1.68412\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8920 | loss: 1.68412 - acc: 0.6141 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8921  | total loss: \u001b[1m\u001b[32m1.55777\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8921 | loss: 1.55777 - acc: 0.6527 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8922  | total loss: \u001b[1m\u001b[32m1.71744\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8922 | loss: 1.71744 - acc: 0.6017 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8923  | total loss: \u001b[1m\u001b[32m1.58819\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8923 | loss: 1.58819 - acc: 0.6416 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8924  | total loss: \u001b[1m\u001b[32m1.47196\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8924 | loss: 1.47196 - acc: 0.6774 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8925  | total loss: \u001b[1m\u001b[32m1.36705\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8925 | loss: 1.36705 - acc: 0.7097 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8926  | total loss: \u001b[1m\u001b[32m1.59593\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8926 | loss: 1.59593 - acc: 0.6387 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8927  | total loss: \u001b[1m\u001b[32m1.47818\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8927 | loss: 1.47818 - acc: 0.6748 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8928  | total loss: \u001b[1m\u001b[32m1.69020\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8928 | loss: 1.69020 - acc: 0.6073 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8929  | total loss: \u001b[1m\u001b[32m1.56356\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8929 | loss: 1.56356 - acc: 0.6466 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8930  | total loss: \u001b[1m\u001b[32m1.72920\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8930 | loss: 1.72920 - acc: 0.5962 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8931  | total loss: \u001b[1m\u001b[32m1.59985\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8931 | loss: 1.59985 - acc: 0.6366 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8932  | total loss: \u001b[1m\u001b[32m1.72175\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8932 | loss: 1.72175 - acc: 0.6015 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8933  | total loss: \u001b[1m\u001b[32m1.59443\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8933 | loss: 1.59443 - acc: 0.6414 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8934  | total loss: \u001b[1m\u001b[32m1.48015\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8934 | loss: 1.48015 - acc: 0.6772 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8935  | total loss: \u001b[1m\u001b[32m1.37714\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8935 | loss: 1.37714 - acc: 0.7095 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8936  | total loss: \u001b[1m\u001b[32m1.58564\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8936 | loss: 1.58564 - acc: 0.6457 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8937  | total loss: \u001b[1m\u001b[32m1.47165\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8937 | loss: 1.47165 - acc: 0.6811 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8938  | total loss: \u001b[1m\u001b[32m1.71632\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8938 | loss: 1.71632 - acc: 0.6130 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8939  | total loss: \u001b[1m\u001b[32m1.58968\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8939 | loss: 1.58968 - acc: 0.6517 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8940  | total loss: \u001b[1m\u001b[32m1.79744\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8940 | loss: 1.79744 - acc: 0.5865 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8941  | total loss: \u001b[1m\u001b[32m1.66392\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8941 | loss: 1.66392 - acc: 0.6279 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8942  | total loss: \u001b[1m\u001b[32m1.82110\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8942 | loss: 1.82110 - acc: 0.5722 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8943  | total loss: \u001b[1m\u001b[32m1.68686\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8943 | loss: 1.68686 - acc: 0.6150 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8944  | total loss: \u001b[1m\u001b[32m1.88105\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8944 | loss: 1.88105 - acc: 0.5535 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8945  | total loss: \u001b[1m\u001b[32m1.74283\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8945 | loss: 1.74283 - acc: 0.5982 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8946  | total loss: \u001b[1m\u001b[32m1.61923\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8946 | loss: 1.61923 - acc: 0.6383 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8947  | total loss: \u001b[1m\u001b[32m1.50821\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8947 | loss: 1.50821 - acc: 0.6745 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8948  | total loss: \u001b[1m\u001b[32m1.72965\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8948 | loss: 1.72965 - acc: 0.6071 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8949  | total loss: \u001b[1m\u001b[32m1.60789\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8949 | loss: 1.60789 - acc: 0.6464 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8950  | total loss: \u001b[1m\u001b[32m1.73451\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8950 | loss: 1.73451 - acc: 0.6031 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8951  | total loss: \u001b[1m\u001b[32m1.61295\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8951 | loss: 1.61295 - acc: 0.6428 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8952  | total loss: \u001b[1m\u001b[32m1.78850\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8952 | loss: 1.78850 - acc: 0.5857 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8953  | total loss: \u001b[1m\u001b[32m1.66233\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8953 | loss: 1.66233 - acc: 0.6271 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8954  | total loss: \u001b[1m\u001b[32m1.84856\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8954 | loss: 1.84856 - acc: 0.5716 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8955  | total loss: \u001b[1m\u001b[32m1.71739\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8955 | loss: 1.71739 - acc: 0.6144 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8956  | total loss: \u001b[1m\u001b[32m1.88018\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8956 | loss: 1.88018 - acc: 0.5530 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8957  | total loss: \u001b[1m\u001b[32m1.74701\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8957 | loss: 1.74701 - acc: 0.5977 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8958  | total loss: \u001b[1m\u001b[32m1.62745\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8958 | loss: 1.62745 - acc: 0.6379 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8959  | total loss: \u001b[1m\u001b[32m1.51949\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8959 | loss: 1.51949 - acc: 0.6741 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8960  | total loss: \u001b[1m\u001b[32m1.69198\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8960 | loss: 1.69198 - acc: 0.6138 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8961  | total loss: \u001b[1m\u001b[32m1.57658\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8961 | loss: 1.57658 - acc: 0.6525 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8962  | total loss: \u001b[1m\u001b[32m1.75398\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8962 | loss: 1.75398 - acc: 0.5944 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8963  | total loss: \u001b[1m\u001b[32m1.63191\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8963 | loss: 1.63191 - acc: 0.6349 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8964  | total loss: \u001b[1m\u001b[32m1.80825\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8964 | loss: 1.80825 - acc: 0.5786 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8965  | total loss: \u001b[1m\u001b[32m1.68072\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8965 | loss: 1.68072 - acc: 0.6207 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8966  | total loss: \u001b[1m\u001b[32m1.83479\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8966 | loss: 1.83479 - acc: 0.5729 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 8967  | total loss: \u001b[1m\u001b[32m1.70492\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8967 | loss: 1.70492 - acc: 0.6156 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8968  | total loss: \u001b[1m\u001b[32m1.89338\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8968 | loss: 1.89338 - acc: 0.5541 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8969  | total loss: \u001b[1m\u001b[32m1.75837\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8969 | loss: 1.75837 - acc: 0.5987 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8970  | total loss: \u001b[1m\u001b[32m1.91743\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8970 | loss: 1.91743 - acc: 0.5459 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8971  | total loss: \u001b[1m\u001b[32m1.78095\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8971 | loss: 1.78095 - acc: 0.5913 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8972  | total loss: \u001b[1m\u001b[32m1.90634\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8972 | loss: 1.90634 - acc: 0.5465 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8973  | total loss: \u001b[1m\u001b[32m1.77179\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8973 | loss: 1.77179 - acc: 0.5918 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8974  | total loss: \u001b[1m\u001b[32m1.91786\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8974 | loss: 1.91786 - acc: 0.5398 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8975  | total loss: \u001b[1m\u001b[32m1.78287\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8975 | loss: 1.78287 - acc: 0.5858 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8976  | total loss: \u001b[1m\u001b[32m1.91819\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8976 | loss: 1.91819 - acc: 0.5415 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8977  | total loss: \u001b[1m\u001b[32m1.78385\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8977 | loss: 1.78385 - acc: 0.5874 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8978  | total loss: \u001b[1m\u001b[32m1.92214\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8978 | loss: 1.92214 - acc: 0.5358 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8979  | total loss: \u001b[1m\u001b[32m1.78811\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8979 | loss: 1.78811 - acc: 0.5822 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8980  | total loss: \u001b[1m\u001b[32m1.93878\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8980 | loss: 1.93878 - acc: 0.5240 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8981  | total loss: \u001b[1m\u001b[32m1.80403\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8981 | loss: 1.80403 - acc: 0.5716 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8982  | total loss: \u001b[1m\u001b[32m1.96343\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8982 | loss: 1.96343 - acc: 0.5144 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8983  | total loss: \u001b[1m\u001b[32m1.82744\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8983 | loss: 1.82744 - acc: 0.5630 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8984  | total loss: \u001b[1m\u001b[32m1.70534\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8984 | loss: 1.70534 - acc: 0.6067 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8985  | total loss: \u001b[1m\u001b[32m1.59510\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8985 | loss: 1.59510 - acc: 0.6460 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8986  | total loss: \u001b[1m\u001b[32m1.76695\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8986 | loss: 1.76695 - acc: 0.5886 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8987  | total loss: \u001b[1m\u001b[32m1.64946\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8987 | loss: 1.64946 - acc: 0.6297 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8988  | total loss: \u001b[1m\u001b[32m1.83222\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8988 | loss: 1.83222 - acc: 0.5667 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8989  | total loss: \u001b[1m\u001b[32m1.70762\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8989 | loss: 1.70762 - acc: 0.6101 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8990  | total loss: \u001b[1m\u001b[32m1.84489\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8990 | loss: 1.84489 - acc: 0.5562 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8991  | total loss: \u001b[1m\u001b[32m1.71885\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8991 | loss: 1.71885 - acc: 0.6006 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8992  | total loss: \u001b[1m\u001b[32m1.88113\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8992 | loss: 1.88113 - acc: 0.5405 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8993  | total loss: \u001b[1m\u001b[32m1.75160\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8993 | loss: 1.75160 - acc: 0.5865 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8994  | total loss: \u001b[1m\u001b[32m1.85524\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 8994 | loss: 1.85524 - acc: 0.5564 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8995  | total loss: \u001b[1m\u001b[32m1.72833\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 8995 | loss: 1.72833 - acc: 0.6008 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8996  | total loss: \u001b[1m\u001b[32m1.84503\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8996 | loss: 1.84503 - acc: 0.5621 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8997  | total loss: \u001b[1m\u001b[32m1.71893\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8997 | loss: 1.71893 - acc: 0.6059 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8998  | total loss: \u001b[1m\u001b[32m1.89314\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8998 | loss: 1.89314 - acc: 0.5453 -- iter: 14/14\n",
      "--\n",
      "Training Step: 8999  | total loss: \u001b[1m\u001b[32m1.76228\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 8999 | loss: 1.76228 - acc: 0.5908 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9000  | total loss: \u001b[1m\u001b[32m1.92240\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9000 | loss: 1.92240 - acc: 0.5388 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9001  | total loss: \u001b[1m\u001b[32m1.78897\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9001 | loss: 1.78897 - acc: 0.5850 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9002  | total loss: \u001b[1m\u001b[32m1.66874\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9002 | loss: 1.66874 - acc: 0.6265 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9003  | total loss: \u001b[1m\u001b[32m1.55979\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9003 | loss: 1.55979 - acc: 0.6638 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9004  | total loss: \u001b[1m\u001b[32m1.69446\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9004 | loss: 1.69446 - acc: 0.6189 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9005  | total loss: \u001b[1m\u001b[32m1.58104\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9005 | loss: 1.58104 - acc: 0.6570 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9006  | total loss: \u001b[1m\u001b[32m1.47782\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9006 | loss: 1.47782 - acc: 0.6913 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9007  | total loss: \u001b[1m\u001b[32m1.38333\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9007 | loss: 1.38333 - acc: 0.7222 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9008  | total loss: \u001b[1m\u001b[32m1.60164\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9008 | loss: 1.60164 - acc: 0.6499 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9009  | total loss: \u001b[1m\u001b[32m1.49202\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9009 | loss: 1.49202 - acc: 0.6849 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9010  | total loss: \u001b[1m\u001b[32m1.64586\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9010 | loss: 1.64586 - acc: 0.6307 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9011  | total loss: \u001b[1m\u001b[32m1.53008\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9011 | loss: 1.53008 - acc: 0.6677 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9012  | total loss: \u001b[1m\u001b[32m1.75444\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9012 | loss: 1.75444 - acc: 0.6009 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9013  | total loss: \u001b[1m\u001b[32m1.62688\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9013 | loss: 1.62688 - acc: 0.6408 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9014  | total loss: \u001b[1m\u001b[32m1.82146\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9014 | loss: 1.82146 - acc: 0.5767 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9015  | total loss: \u001b[1m\u001b[32m1.68720\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9015 | loss: 1.68720 - acc: 0.6191 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9016  | total loss: \u001b[1m\u001b[32m1.56629\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9016 | loss: 1.56629 - acc: 0.6571 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9017  | total loss: \u001b[1m\u001b[32m1.45694\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9017 | loss: 1.45694 - acc: 0.6914 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9018  | total loss: \u001b[1m\u001b[32m1.64258\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9018 | loss: 1.64258 - acc: 0.6294 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9019  | total loss: \u001b[1m\u001b[32m1.52456\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9019 | loss: 1.52456 - acc: 0.6665 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9020  | total loss: \u001b[1m\u001b[32m1.71661\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9020 | loss: 1.71661 - acc: 0.6070 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9021  | total loss: \u001b[1m\u001b[32m1.59099\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9021 | loss: 1.59099 - acc: 0.6463 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9022  | total loss: \u001b[1m\u001b[32m1.77028\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9022 | loss: 1.77028 - acc: 0.5888 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9023  | total loss: \u001b[1m\u001b[32m1.63968\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9023 | loss: 1.63968 - acc: 0.6299 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9024  | total loss: \u001b[1m\u001b[32m1.79283\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9024 | loss: 1.79283 - acc: 0.5812 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9025  | total loss: \u001b[1m\u001b[32m1.66054\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9025 | loss: 1.66054 - acc: 0.6231 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9026  | total loss: \u001b[1m\u001b[32m1.84207\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9026 | loss: 1.84207 - acc: 0.5608 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9027  | total loss: \u001b[1m\u001b[32m1.70570\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9027 | loss: 1.70570 - acc: 0.6047 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9028  | total loss: \u001b[1m\u001b[32m1.58324\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9028 | loss: 1.58324 - acc: 0.6442 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9029  | total loss: \u001b[1m\u001b[32m1.47279\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9029 | loss: 1.47279 - acc: 0.6798 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9030  | total loss: \u001b[1m\u001b[32m1.66057\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9030 | loss: 1.66057 - acc: 0.6190 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9031  | total loss: \u001b[1m\u001b[32m1.54172\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9031 | loss: 1.54172 - acc: 0.6571 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9032  | total loss: \u001b[1m\u001b[32m1.70987\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9032 | loss: 1.70987 - acc: 0.5985 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9033  | total loss: \u001b[1m\u001b[32m1.58597\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9033 | loss: 1.58597 - acc: 0.6387 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9034  | total loss: \u001b[1m\u001b[32m1.79953\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9034 | loss: 1.79953 - acc: 0.5748 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9035  | total loss: \u001b[1m\u001b[32m1.66709\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9035 | loss: 1.66709 - acc: 0.6173 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9036  | total loss: \u001b[1m\u001b[32m1.83402\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9036 | loss: 1.83402 - acc: 0.5627 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9037  | total loss: \u001b[1m\u001b[32m1.69909\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9037 | loss: 1.69909 - acc: 0.6065 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9038  | total loss: \u001b[1m\u001b[32m1.57794\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9038 | loss: 1.57794 - acc: 0.6458 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9039  | total loss: \u001b[1m\u001b[32m1.46868\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9039 | loss: 1.46868 - acc: 0.6812 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9040  | total loss: \u001b[1m\u001b[32m1.67773\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9040 | loss: 1.67773 - acc: 0.6131 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9041  | total loss: \u001b[1m\u001b[32m1.55798\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9041 | loss: 1.55798 - acc: 0.6518 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9042  | total loss: \u001b[1m\u001b[32m1.77312\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9042 | loss: 1.77312 - acc: 0.5866 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9043  | total loss: \u001b[1m\u001b[32m1.64406\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9043 | loss: 1.64406 - acc: 0.6280 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9044  | total loss: \u001b[1m\u001b[32m1.81035\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9044 | loss: 1.81035 - acc: 0.5723 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9045  | total loss: \u001b[1m\u001b[32m1.67821\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9045 | loss: 1.67821 - acc: 0.6151 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9046  | total loss: \u001b[1m\u001b[32m1.82314\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9046 | loss: 1.82314 - acc: 0.5678 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9047  | total loss: \u001b[1m\u001b[32m1.69052\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9047 | loss: 1.69052 - acc: 0.6111 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9048  | total loss: \u001b[1m\u001b[32m1.88015\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9048 | loss: 1.88015 - acc: 0.5500 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9049  | total loss: \u001b[1m\u001b[32m1.74296\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9049 | loss: 1.74296 - acc: 0.5950 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9050  | total loss: \u001b[1m\u001b[32m1.92963\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9050 | loss: 1.92963 - acc: 0.5355 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9051  | total loss: \u001b[1m\u001b[32m1.78894\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9051 | loss: 1.78894 - acc: 0.5819 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9052  | total loss: \u001b[1m\u001b[32m1.90556\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9052 | loss: 1.90556 - acc: 0.5380 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9053  | total loss: \u001b[1m\u001b[32m1.76881\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 9053 | loss: 1.76881 - acc: 0.5842 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9054  | total loss: \u001b[1m\u001b[32m1.91415\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9054 | loss: 1.91415 - acc: 0.5329 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9055  | total loss: \u001b[1m\u001b[32m1.77797\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9055 | loss: 1.77797 - acc: 0.5796 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9056  | total loss: \u001b[1m\u001b[32m1.92965\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9056 | loss: 1.92965 - acc: 0.5288 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9057  | total loss: \u001b[1m\u001b[32m1.79338\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9057 | loss: 1.79338 - acc: 0.5759 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9058  | total loss: \u001b[1m\u001b[32m1.95677\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9058 | loss: 1.95677 - acc: 0.5255 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9059  | total loss: \u001b[1m\u001b[32m1.81929\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9059 | loss: 1.81929 - acc: 0.5729 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9060  | total loss: \u001b[1m\u001b[32m1.96431\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9060 | loss: 1.96431 - acc: 0.5228 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9061  | total loss: \u001b[1m\u001b[32m1.82749\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9061 | loss: 1.82749 - acc: 0.5705 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9062  | total loss: \u001b[1m\u001b[32m1.98115\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9062 | loss: 1.98115 - acc: 0.5206 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9063  | total loss: \u001b[1m\u001b[32m1.84394\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9063 | loss: 1.84394 - acc: 0.5685 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9064  | total loss: \u001b[1m\u001b[32m1.72071\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9064 | loss: 1.72071 - acc: 0.6117 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9065  | total loss: \u001b[1m\u001b[32m1.60942\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9065 | loss: 1.60942 - acc: 0.6505 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9066  | total loss: \u001b[1m\u001b[32m1.76609\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9066 | loss: 1.76609 - acc: 0.5926 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9067  | total loss: \u001b[1m\u001b[32m1.64909\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9067 | loss: 1.64909 - acc: 0.6333 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9068  | total loss: \u001b[1m\u001b[32m1.78631\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9068 | loss: 1.78631 - acc: 0.5772 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9069  | total loss: \u001b[1m\u001b[32m1.66645\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9069 | loss: 1.66645 - acc: 0.6194 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9070  | total loss: \u001b[1m\u001b[32m1.82133\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9070 | loss: 1.82133 - acc: 0.5646 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9071  | total loss: \u001b[1m\u001b[32m1.69746\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9071 | loss: 1.69746 - acc: 0.6082 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9072  | total loss: \u001b[1m\u001b[32m1.87890\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9072 | loss: 1.87890 - acc: 0.5474 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9073  | total loss: \u001b[1m\u001b[32m1.74910\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9073 | loss: 1.74910 - acc: 0.5926 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9074  | total loss: \u001b[1m\u001b[32m1.89584\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9074 | loss: 1.89584 - acc: 0.5405 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9075  | total loss: \u001b[1m\u001b[32m1.76439\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9075 | loss: 1.76439 - acc: 0.5865 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9076  | total loss: \u001b[1m\u001b[32m1.92389\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9076 | loss: 1.92389 - acc: 0.5278 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9077  | total loss: \u001b[1m\u001b[32m1.78989\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9077 | loss: 1.78989 - acc: 0.5750 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9078  | total loss: \u001b[1m\u001b[32m1.94893\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9078 | loss: 1.94893 - acc: 0.5247 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9079  | total loss: \u001b[1m\u001b[32m1.81290\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9079 | loss: 1.81290 - acc: 0.5722 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9080  | total loss: \u001b[1m\u001b[32m1.94061\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9080 | loss: 1.94061 - acc: 0.5293 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9081  | total loss: \u001b[1m\u001b[32m1.80581\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9081 | loss: 1.80581 - acc: 0.5763 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9082  | total loss: \u001b[1m\u001b[32m1.96808\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9082 | loss: 1.96808 - acc: 0.5187 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9083  | total loss: \u001b[1m\u001b[32m1.83084\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9083 | loss: 1.83084 - acc: 0.5668 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9084  | total loss: \u001b[1m\u001b[32m1.70718\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9084 | loss: 1.70718 - acc: 0.6102 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9085  | total loss: \u001b[1m\u001b[32m1.59521\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9085 | loss: 1.59521 - acc: 0.6491 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9086  | total loss: \u001b[1m\u001b[32m1.75091\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9086 | loss: 1.75091 - acc: 0.5914 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9087  | total loss: \u001b[1m\u001b[32m1.63301\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9087 | loss: 1.63301 - acc: 0.6322 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9088  | total loss: \u001b[1m\u001b[32m1.52601\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9088 | loss: 1.52601 - acc: 0.6690 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9089  | total loss: \u001b[1m\u001b[32m1.42839\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9089 | loss: 1.42839 - acc: 0.7021 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9090  | total loss: \u001b[1m\u001b[32m1.33885\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9090 | loss: 1.33885 - acc: 0.7319 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9091  | total loss: \u001b[1m\u001b[32m1.25627\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9091 | loss: 1.25627 - acc: 0.7587 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9092  | total loss: \u001b[1m\u001b[32m1.46254\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9092 | loss: 1.46254 - acc: 0.6900 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9093  | total loss: \u001b[1m\u001b[32m1.36403\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9093 | loss: 1.36403 - acc: 0.7210 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9094  | total loss: \u001b[1m\u001b[32m1.27377\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9094 | loss: 1.27377 - acc: 0.7489 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9095  | total loss: \u001b[1m\u001b[32m1.19069\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9095 | loss: 1.19069 - acc: 0.7740 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9096  | total loss: \u001b[1m\u001b[32m1.42264\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9096 | loss: 1.42264 - acc: 0.7037 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9097  | total loss: \u001b[1m\u001b[32m1.32161\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9097 | loss: 1.32161 - acc: 0.7334 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9098  | total loss: \u001b[1m\u001b[32m1.47822\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9098 | loss: 1.47822 - acc: 0.6815 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9099  | total loss: \u001b[1m\u001b[32m1.36967\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9099 | loss: 1.36967 - acc: 0.7133 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9100  | total loss: \u001b[1m\u001b[32m1.60971\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9100 | loss: 1.60971 - acc: 0.6420 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9101  | total loss: \u001b[1m\u001b[32m1.48709\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9101 | loss: 1.48709 - acc: 0.6778 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9102  | total loss: \u001b[1m\u001b[32m1.71418\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9102 | loss: 1.71418 - acc: 0.6100 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9103  | total loss: \u001b[1m\u001b[32m1.58131\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9103 | loss: 1.58131 - acc: 0.6490 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9104  | total loss: \u001b[1m\u001b[32m1.79243\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9104 | loss: 1.79243 - acc: 0.5912 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9105  | total loss: \u001b[1m\u001b[32m1.65277\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9105 | loss: 1.65277 - acc: 0.6321 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9106  | total loss: \u001b[1m\u001b[32m1.82667\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9106 | loss: 1.82667 - acc: 0.5761 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9107  | total loss: \u001b[1m\u001b[32m1.68520\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9107 | loss: 1.68520 - acc: 0.6184 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9108  | total loss: \u001b[1m\u001b[32m1.85880\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9108 | loss: 1.85880 - acc: 0.5709 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9109  | total loss: \u001b[1m\u001b[32m1.71602\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9109 | loss: 1.71602 - acc: 0.6138 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9110  | total loss: \u001b[1m\u001b[32m1.88478\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9110 | loss: 1.88478 - acc: 0.5596 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9111  | total loss: \u001b[1m\u001b[32m1.74149\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9111 | loss: 1.74149 - acc: 0.6036 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9112  | total loss: \u001b[1m\u001b[32m1.91118\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9112 | loss: 1.91118 - acc: 0.5432 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9113  | total loss: \u001b[1m\u001b[32m1.76768\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 9113 | loss: 1.76768 - acc: 0.5889 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9114  | total loss: \u001b[1m\u001b[32m1.96624\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9114 | loss: 1.96624 - acc: 0.5300 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9115  | total loss: \u001b[1m\u001b[32m1.81988\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9115 | loss: 1.81988 - acc: 0.5770 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9116  | total loss: \u001b[1m\u001b[32m1.68918\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9116 | loss: 1.68918 - acc: 0.6193 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9117  | total loss: \u001b[1m\u001b[32m1.57195\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9117 | loss: 1.57195 - acc: 0.6574 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9118  | total loss: \u001b[1m\u001b[32m1.76792\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9118 | loss: 1.76792 - acc: 0.5917 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9119  | total loss: \u001b[1m\u001b[32m1.64336\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9119 | loss: 1.64336 - acc: 0.6325 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9120  | total loss: \u001b[1m\u001b[32m1.53135\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9120 | loss: 1.53135 - acc: 0.6692 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9121  | total loss: \u001b[1m\u001b[32m1.43012\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9121 | loss: 1.43012 - acc: 0.7023 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9122  | total loss: \u001b[1m\u001b[32m1.66053\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9122 | loss: 1.66053 - acc: 0.6321 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9123  | total loss: \u001b[1m\u001b[32m1.54544\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9123 | loss: 1.54544 - acc: 0.6689 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9124  | total loss: \u001b[1m\u001b[32m1.44132\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9124 | loss: 1.44132 - acc: 0.7020 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9125  | total loss: \u001b[1m\u001b[32m1.34666\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9125 | loss: 1.34666 - acc: 0.7318 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9126  | total loss: \u001b[1m\u001b[32m1.53925\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9126 | loss: 1.53925 - acc: 0.6729 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9127  | total loss: \u001b[1m\u001b[32m1.43302\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9127 | loss: 1.43302 - acc: 0.7056 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9128  | total loss: \u001b[1m\u001b[32m1.66425\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9128 | loss: 1.66425 - acc: 0.6350 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9129  | total loss: \u001b[1m\u001b[32m1.54474\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9129 | loss: 1.54474 - acc: 0.6715 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9130  | total loss: \u001b[1m\u001b[32m1.75026\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9130 | loss: 1.75026 - acc: 0.6115 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9131  | total loss: \u001b[1m\u001b[32m1.62220\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9131 | loss: 1.62220 - acc: 0.6504 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9132  | total loss: \u001b[1m\u001b[32m1.81945\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9132 | loss: 1.81945 - acc: 0.5853 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9133  | total loss: \u001b[1m\u001b[32m1.68511\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9133 | loss: 1.68511 - acc: 0.6268 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9134  | total loss: \u001b[1m\u001b[32m1.88389\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9134 | loss: 1.88389 - acc: 0.5641 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9135  | total loss: \u001b[1m\u001b[32m1.74434\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9135 | loss: 1.74434 - acc: 0.6077 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9136  | total loss: \u001b[1m\u001b[32m1.91361\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9136 | loss: 1.91361 - acc: 0.5541 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9137  | total loss: \u001b[1m\u001b[32m1.77269\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9137 | loss: 1.77269 - acc: 0.5987 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9138  | total loss: \u001b[1m\u001b[32m1.91032\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9138 | loss: 1.91032 - acc: 0.5531 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9139  | total loss: \u001b[1m\u001b[32m1.77137\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9139 | loss: 1.77137 - acc: 0.5978 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9140  | total loss: \u001b[1m\u001b[32m1.90747\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9140 | loss: 1.90747 - acc: 0.5523 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9141  | total loss: \u001b[1m\u001b[32m1.77047\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9141 | loss: 1.77047 - acc: 0.5971 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9142  | total loss: \u001b[1m\u001b[32m1.90404\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9142 | loss: 1.90404 - acc: 0.5445 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9143  | total loss: \u001b[1m\u001b[32m1.76908\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9143 | loss: 1.76908 - acc: 0.5900 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9144  | total loss: \u001b[1m\u001b[32m1.91993\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9144 | loss: 1.91993 - acc: 0.5382 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9145  | total loss: \u001b[1m\u001b[32m1.78506\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9145 | loss: 1.78506 - acc: 0.5844 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9146  | total loss: \u001b[1m\u001b[32m1.94889\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9146 | loss: 1.94889 - acc: 0.5259 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9147  | total loss: \u001b[1m\u001b[32m1.81274\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9147 | loss: 1.81274 - acc: 0.5733 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9148  | total loss: \u001b[1m\u001b[32m1.96756\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9148 | loss: 1.96756 - acc: 0.5231 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9149  | total loss: \u001b[1m\u001b[32m1.83109\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9149 | loss: 1.83109 - acc: 0.5708 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9150  | total loss: \u001b[1m\u001b[32m1.95729\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9150 | loss: 1.95729 - acc: 0.5209 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9151  | total loss: \u001b[1m\u001b[32m1.82326\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9151 | loss: 1.82326 - acc: 0.5688 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9152  | total loss: \u001b[1m\u001b[32m1.96167\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9152 | loss: 1.96167 - acc: 0.5191 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9153  | total loss: \u001b[1m\u001b[32m1.82840\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9153 | loss: 1.82840 - acc: 0.5672 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9154  | total loss: \u001b[1m\u001b[32m1.94744\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9154 | loss: 1.94744 - acc: 0.5247 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9155  | total loss: \u001b[1m\u001b[32m1.81641\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9155 | loss: 1.81641 - acc: 0.5723 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9156  | total loss: \u001b[1m\u001b[32m1.92840\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9156 | loss: 1.92840 - acc: 0.5293 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9157  | total loss: \u001b[1m\u001b[32m1.79974\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9157 | loss: 1.79974 - acc: 0.5764 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9158  | total loss: \u001b[1m\u001b[32m1.95933\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9158 | loss: 1.95933 - acc: 0.5187 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9159  | total loss: \u001b[1m\u001b[32m1.82801\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9159 | loss: 1.82801 - acc: 0.5669 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9160  | total loss: \u001b[1m\u001b[32m1.95704\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9160 | loss: 1.95704 - acc: 0.5173 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9161  | total loss: \u001b[1m\u001b[32m1.82643\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9161 | loss: 1.82643 - acc: 0.5656 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9162  | total loss: \u001b[1m\u001b[32m1.96593\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9162 | loss: 1.96593 - acc: 0.5162 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9163  | total loss: \u001b[1m\u001b[32m1.83479\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9163 | loss: 1.83479 - acc: 0.5646 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9164  | total loss: \u001b[1m\u001b[32m1.97112\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9164 | loss: 1.97112 - acc: 0.5081 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9165  | total loss: \u001b[1m\u001b[32m1.83973\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9165 | loss: 1.83973 - acc: 0.5573 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9166  | total loss: \u001b[1m\u001b[32m1.99388\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9166 | loss: 1.99388 - acc: 0.5016 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9167  | total loss: \u001b[1m\u001b[32m1.86048\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9167 | loss: 1.86048 - acc: 0.5514 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9168  | total loss: \u001b[1m\u001b[32m1.74023\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9168 | loss: 1.74023 - acc: 0.5963 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9169  | total loss: \u001b[1m\u001b[32m1.63122\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9169 | loss: 1.63122 - acc: 0.6366 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9170  | total loss: \u001b[1m\u001b[32m1.78881\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9170 | loss: 1.78881 - acc: 0.5801 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9171  | total loss: \u001b[1m\u001b[32m1.67309\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9171 | loss: 1.67309 - acc: 0.6221 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9172  | total loss: \u001b[1m\u001b[32m1.84243\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9172 | loss: 1.84243 - acc: 0.5599 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9173  | total loss: \u001b[1m\u001b[32m1.71990\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9173 | loss: 1.71990 - acc: 0.6039 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9174  | total loss: \u001b[1m\u001b[32m1.87553\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9174 | loss: 1.87553 - acc: 0.5507 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9175  | total loss: \u001b[1m\u001b[32m1.74857\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9175 | loss: 1.74857 - acc: 0.5956 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9176  | total loss: \u001b[1m\u001b[32m1.89345\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9176 | loss: 1.89345 - acc: 0.5432 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9177  | total loss: \u001b[1m\u001b[32m1.76388\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9177 | loss: 1.76388 - acc: 0.5889 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9178  | total loss: \u001b[1m\u001b[32m1.92740\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9178 | loss: 1.92740 - acc: 0.5371 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9179  | total loss: \u001b[1m\u001b[32m1.79393\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9179 | loss: 1.79393 - acc: 0.5834 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9180  | total loss: \u001b[1m\u001b[32m1.67336\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9180 | loss: 1.67336 - acc: 0.6251 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9181  | total loss: \u001b[1m\u001b[32m1.56393\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9181 | loss: 1.56393 - acc: 0.6626 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9182  | total loss: \u001b[1m\u001b[32m1.72481\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9182 | loss: 1.72481 - acc: 0.6034 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9183  | total loss: \u001b[1m\u001b[32m1.60830\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9183 | loss: 1.60830 - acc: 0.6431 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9184  | total loss: \u001b[1m\u001b[32m1.77496\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9184 | loss: 1.77496 - acc: 0.5859 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9185  | total loss: \u001b[1m\u001b[32m1.65212\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9185 | loss: 1.65212 - acc: 0.6273 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9186  | total loss: \u001b[1m\u001b[32m1.78749\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9186 | loss: 1.78749 - acc: 0.5789 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9187  | total loss: \u001b[1m\u001b[32m1.66249\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9187 | loss: 1.66249 - acc: 0.6210 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9188  | total loss: \u001b[1m\u001b[32m1.79452\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9188 | loss: 1.79452 - acc: 0.5732 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9189  | total loss: \u001b[1m\u001b[32m1.66812\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9189 | loss: 1.66812 - acc: 0.6159 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9190  | total loss: \u001b[1m\u001b[32m1.76513\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9190 | loss: 1.76513 - acc: 0.5829 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9191  | total loss: \u001b[1m\u001b[32m1.64107\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9191 | loss: 1.64107 - acc: 0.6246 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9192  | total loss: \u001b[1m\u001b[32m1.83080\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9192 | loss: 1.83080 - acc: 0.5693 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9193  | total loss: \u001b[1m\u001b[32m1.69992\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9193 | loss: 1.69992 - acc: 0.6123 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9194  | total loss: \u001b[1m\u001b[32m1.58190\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9194 | loss: 1.58190 - acc: 0.6511 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9195  | total loss: \u001b[1m\u001b[32m1.47502\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9195 | loss: 1.47502 - acc: 0.6860 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9196  | total loss: \u001b[1m\u001b[32m1.67688\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9196 | loss: 1.67688 - acc: 0.6174 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9197  | total loss: \u001b[1m\u001b[32m1.55929\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9197 | loss: 1.55929 - acc: 0.6556 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9198  | total loss: \u001b[1m\u001b[32m1.75151\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9198 | loss: 1.75151 - acc: 0.5972 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9199  | total loss: \u001b[1m\u001b[32m1.62596\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9199 | loss: 1.62596 - acc: 0.6375 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9200  | total loss: \u001b[1m\u001b[32m1.81303\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9200 | loss: 1.81303 - acc: 0.5809 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9201  | total loss: \u001b[1m\u001b[32m1.68143\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9201 | loss: 1.68143 - acc: 0.6228 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9202  | total loss: \u001b[1m\u001b[32m1.82290\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9202 | loss: 1.82290 - acc: 0.5748 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9203  | total loss: \u001b[1m\u001b[32m1.69078\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9203 | loss: 1.69078 - acc: 0.6173 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9204  | total loss: \u001b[1m\u001b[32m1.85589\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9204 | loss: 1.85589 - acc: 0.5627 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9205  | total loss: \u001b[1m\u001b[32m1.72124\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9205 | loss: 1.72124 - acc: 0.6065 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9206  | total loss: \u001b[1m\u001b[32m1.88072\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9206 | loss: 1.88072 - acc: 0.5530 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9207  | total loss: \u001b[1m\u001b[32m1.74470\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9207 | loss: 1.74470 - acc: 0.5977 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9208  | total loss: \u001b[1m\u001b[32m1.62264\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9208 | loss: 1.62264 - acc: 0.6379 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9209  | total loss: \u001b[1m\u001b[32m1.51264\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9209 | loss: 1.51264 - acc: 0.6741 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9210  | total loss: \u001b[1m\u001b[32m1.41306\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9210 | loss: 1.41306 - acc: 0.7067 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9211  | total loss: \u001b[1m\u001b[32m1.32246\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9211 | loss: 1.32246 - acc: 0.7360 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9212  | total loss: \u001b[1m\u001b[32m1.55466\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9212 | loss: 1.55466 - acc: 0.6624 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9213  | total loss: \u001b[1m\u001b[32m1.44811\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9213 | loss: 1.44811 - acc: 0.6962 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9214  | total loss: \u001b[1m\u001b[32m1.65885\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9214 | loss: 1.65885 - acc: 0.6266 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9215  | total loss: \u001b[1m\u001b[32m1.54101\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9215 | loss: 1.54101 - acc: 0.6639 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9216  | total loss: \u001b[1m\u001b[32m1.67523\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9216 | loss: 1.67523 - acc: 0.6189 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9217  | total loss: \u001b[1m\u001b[32m1.55534\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9217 | loss: 1.55534 - acc: 0.6571 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9218  | total loss: \u001b[1m\u001b[32m1.73952\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9218 | loss: 1.73952 - acc: 0.5985 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9219  | total loss: \u001b[1m\u001b[32m1.61309\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9219 | loss: 1.61309 - acc: 0.6386 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9220  | total loss: \u001b[1m\u001b[32m1.76622\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9220 | loss: 1.76622 - acc: 0.5891 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9221  | total loss: \u001b[1m\u001b[32m1.63732\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9221 | loss: 1.63732 - acc: 0.6302 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9222  | total loss: \u001b[1m\u001b[32m1.78045\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 9222 | loss: 1.78045 - acc: 0.5814 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9223  | total loss: \u001b[1m\u001b[32m1.65056\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9223 | loss: 1.65056 - acc: 0.6233 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9224  | total loss: \u001b[1m\u001b[32m1.80817\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9224 | loss: 1.80817 - acc: 0.5752 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9225  | total loss: \u001b[1m\u001b[32m1.67614\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9225 | loss: 1.67614 - acc: 0.6177 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9226  | total loss: \u001b[1m\u001b[32m1.84549\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9226 | loss: 1.84549 - acc: 0.5631 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9227  | total loss: \u001b[1m\u001b[32m1.71063\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 9227 | loss: 1.71063 - acc: 0.6068 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9228  | total loss: \u001b[1m\u001b[32m1.89128\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9228 | loss: 1.89128 - acc: 0.5461 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9229  | total loss: \u001b[1m\u001b[32m1.75300\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9229 | loss: 1.75300 - acc: 0.5915 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9230  | total loss: \u001b[1m\u001b[32m1.86245\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9230 | loss: 1.86245 - acc: 0.5538 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9231  | total loss: \u001b[1m\u001b[32m1.72825\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9231 | loss: 1.72825 - acc: 0.5984 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9232  | total loss: \u001b[1m\u001b[32m1.89152\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9232 | loss: 1.89152 - acc: 0.5457 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9233  | total loss: \u001b[1m\u001b[32m1.75567\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9233 | loss: 1.75567 - acc: 0.5911 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9234  | total loss: \u001b[1m\u001b[32m1.91519\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9234 | loss: 1.91519 - acc: 0.5392 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9235  | total loss: \u001b[1m\u001b[32m1.77833\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9235 | loss: 1.77833 - acc: 0.5852 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9236  | total loss: \u001b[1m\u001b[32m1.94244\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9236 | loss: 1.94244 - acc: 0.5339 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9237  | total loss: \u001b[1m\u001b[32m1.80423\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9237 | loss: 1.80423 - acc: 0.5805 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9238  | total loss: \u001b[1m\u001b[32m1.97264\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9238 | loss: 1.97264 - acc: 0.5224 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9239  | total loss: \u001b[1m\u001b[32m1.83284\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9239 | loss: 1.83284 - acc: 0.5702 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9240  | total loss: \u001b[1m\u001b[32m1.70742\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9240 | loss: 1.70742 - acc: 0.6132 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9241  | total loss: \u001b[1m\u001b[32m1.59439\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9241 | loss: 1.59439 - acc: 0.6518 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9242  | total loss: \u001b[1m\u001b[32m1.77903\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9242 | loss: 1.77903 - acc: 0.5938 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9243  | total loss: \u001b[1m\u001b[32m1.65823\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9243 | loss: 1.65823 - acc: 0.6344 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9244  | total loss: \u001b[1m\u001b[32m1.54905\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9244 | loss: 1.54905 - acc: 0.6710 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9245  | total loss: \u001b[1m\u001b[32m1.44986\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9245 | loss: 1.44986 - acc: 0.7039 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9246  | total loss: \u001b[1m\u001b[32m1.62705\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9246 | loss: 1.62705 - acc: 0.6406 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9247  | total loss: \u001b[1m\u001b[32m1.51821\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9247 | loss: 1.51821 - acc: 0.6766 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9248  | total loss: \u001b[1m\u001b[32m1.71298\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9248 | loss: 1.71298 - acc: 0.6089 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9249  | total loss: \u001b[1m\u001b[32m1.59447\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9249 | loss: 1.59447 - acc: 0.6480 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9250  | total loss: \u001b[1m\u001b[32m1.76024\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9250 | loss: 1.76024 - acc: 0.5904 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9251  | total loss: \u001b[1m\u001b[32m1.63654\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9251 | loss: 1.63654 - acc: 0.6313 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9252  | total loss: \u001b[1m\u001b[32m1.83714\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9252 | loss: 1.83714 - acc: 0.5682 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9253  | total loss: \u001b[1m\u001b[32m1.70586\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9253 | loss: 1.70586 - acc: 0.6114 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9254  | total loss: \u001b[1m\u001b[32m1.85900\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9254 | loss: 1.85900 - acc: 0.5574 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9255  | total loss: \u001b[1m\u001b[32m1.72604\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9255 | loss: 1.72604 - acc: 0.6016 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9256  | total loss: \u001b[1m\u001b[32m1.60644\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9256 | loss: 1.60644 - acc: 0.6415 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9257  | total loss: \u001b[1m\u001b[32m1.49839\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9257 | loss: 1.49839 - acc: 0.6773 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9258  | total loss: \u001b[1m\u001b[32m1.68635\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9258 | loss: 1.68635 - acc: 0.6167 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9259  | total loss: \u001b[1m\u001b[32m1.56942\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9259 | loss: 1.56942 - acc: 0.6551 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9260  | total loss: \u001b[1m\u001b[32m1.46368\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9260 | loss: 1.46368 - acc: 0.6896 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9261  | total loss: \u001b[1m\u001b[32m1.36762\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9261 | loss: 1.36762 - acc: 0.7206 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9262  | total loss: \u001b[1m\u001b[32m1.55185\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9262 | loss: 1.55185 - acc: 0.6628 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9263  | total loss: \u001b[1m\u001b[32m1.44516\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9263 | loss: 1.44516 - acc: 0.6965 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9264  | total loss: \u001b[1m\u001b[32m1.60260\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9264 | loss: 1.60260 - acc: 0.6483 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9265  | total loss: \u001b[1m\u001b[32m1.48962\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9265 | loss: 1.48962 - acc: 0.6835 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9266  | total loss: \u001b[1m\u001b[32m1.66868\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9266 | loss: 1.66868 - acc: 0.6223 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9267  | total loss: \u001b[1m\u001b[32m1.54846\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9267 | loss: 1.54846 - acc: 0.6601 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9268  | total loss: \u001b[1m\u001b[32m1.75578\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9268 | loss: 1.75578 - acc: 0.5940 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9269  | total loss: \u001b[1m\u001b[32m1.62686\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9269 | loss: 1.62686 - acc: 0.6346 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9270  | total loss: \u001b[1m\u001b[32m1.80155\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9270 | loss: 1.80155 - acc: 0.5783 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9271  | total loss: \u001b[1m\u001b[32m1.66859\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9271 | loss: 1.66859 - acc: 0.6205 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9272  | total loss: \u001b[1m\u001b[32m1.84617\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9272 | loss: 1.84617 - acc: 0.5656 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9273  | total loss: \u001b[1m\u001b[32m1.70965\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9273 | loss: 1.70965 - acc: 0.6090 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9274  | total loss: \u001b[1m\u001b[32m1.87527\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9274 | loss: 1.87527 - acc: 0.5553 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9275  | total loss: \u001b[1m\u001b[32m1.73706\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9275 | loss: 1.73706 - acc: 0.5997 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9276  | total loss: \u001b[1m\u001b[32m1.92529\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9276 | loss: 1.92529 - acc: 0.5398 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9277  | total loss: \u001b[1m\u001b[32m1.78359\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9277 | loss: 1.78359 - acc: 0.5858 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9278  | total loss: \u001b[1m\u001b[32m1.96035\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9278 | loss: 1.96035 - acc: 0.5272 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9279  | total loss: \u001b[1m\u001b[32m1.81686\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9279 | loss: 1.81686 - acc: 0.5745 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9280  | total loss: \u001b[1m\u001b[32m1.97077\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9280 | loss: 1.97077 - acc: 0.5242 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9281  | total loss: \u001b[1m\u001b[32m1.82794\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9281 | loss: 1.82794 - acc: 0.5718 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9282  | total loss: \u001b[1m\u001b[32m1.97334\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9282 | loss: 1.97334 - acc: 0.5217 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9283  | total loss: \u001b[1m\u001b[32m1.83188\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9283 | loss: 1.83188 - acc: 0.5696 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9284  | total loss: \u001b[1m\u001b[32m1.96432\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9284 | loss: 1.96432 - acc: 0.5197 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9285  | total loss: \u001b[1m\u001b[32m1.82529\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9285 | loss: 1.82529 - acc: 0.5678 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9286  | total loss: \u001b[1m\u001b[32m1.97487\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9286 | loss: 1.97487 - acc: 0.5181 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9287  | total loss: \u001b[1m\u001b[32m1.83622\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9287 | loss: 1.83622 - acc: 0.5663 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9288  | total loss: \u001b[1m\u001b[32m1.91159\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9288 | loss: 1.91159 - acc: 0.5383 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9289  | total loss: \u001b[1m\u001b[32m1.78042\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9289 | loss: 1.78042 - acc: 0.5844 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9290  | total loss: \u001b[1m\u001b[32m1.90174\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9290 | loss: 1.90174 - acc: 0.5403 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9291  | total loss: \u001b[1m\u001b[32m1.77230\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9291 | loss: 1.77230 - acc: 0.5862 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9292  | total loss: \u001b[1m\u001b[32m1.94253\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9292 | loss: 1.94253 - acc: 0.5276 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9293  | total loss: \u001b[1m\u001b[32m1.80968\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9293 | loss: 1.80968 - acc: 0.5749 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9294  | total loss: \u001b[1m\u001b[32m1.91279\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9294 | loss: 1.91279 - acc: 0.5388 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9295  | total loss: \u001b[1m\u001b[32m1.78340\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9295 | loss: 1.78340 - acc: 0.5849 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9296  | total loss: \u001b[1m\u001b[32m1.93399\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9296 | loss: 1.93399 - acc: 0.5264 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9297  | total loss: \u001b[1m\u001b[32m1.80294\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9297 | loss: 1.80294 - acc: 0.5738 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9298  | total loss: \u001b[1m\u001b[32m1.93932\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9298 | loss: 1.93932 - acc: 0.5236 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9299  | total loss: \u001b[1m\u001b[32m1.80827\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9299 | loss: 1.80827 - acc: 0.5712 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9300  | total loss: \u001b[1m\u001b[32m1.97032\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9300 | loss: 1.97032 - acc: 0.5141 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9301  | total loss: \u001b[1m\u001b[32m1.83669\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9301 | loss: 1.83669 - acc: 0.5627 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9302  | total loss: \u001b[1m\u001b[32m1.97679\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9302 | loss: 1.97679 - acc: 0.5135 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9303  | total loss: \u001b[1m\u001b[32m1.84299\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9303 | loss: 1.84299 - acc: 0.5622 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9304  | total loss: \u001b[1m\u001b[32m1.96627\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9304 | loss: 1.96627 - acc: 0.5203 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9305  | total loss: \u001b[1m\u001b[32m1.83378\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9305 | loss: 1.83378 - acc: 0.5682 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9306  | total loss: \u001b[1m\u001b[32m1.96328\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9306 | loss: 1.96328 - acc: 0.5186 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9307  | total loss: \u001b[1m\u001b[32m1.83126\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9307 | loss: 1.83126 - acc: 0.5667 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9308  | total loss: \u001b[1m\u001b[32m1.98224\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9308 | loss: 1.98224 - acc: 0.5100 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9309  | total loss: \u001b[1m\u001b[32m1.84862\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9309 | loss: 1.84862 - acc: 0.5590 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9310  | total loss: \u001b[1m\u001b[32m1.98829\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9310 | loss: 1.98829 - acc: 0.5103 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9311  | total loss: \u001b[1m\u001b[32m1.85447\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9311 | loss: 1.85447 - acc: 0.5592 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9312  | total loss: \u001b[1m\u001b[32m1.97326\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9312 | loss: 1.97326 - acc: 0.5176 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9313  | total loss: \u001b[1m\u001b[32m1.84128\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9313 | loss: 1.84128 - acc: 0.5658 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9314  | total loss: \u001b[1m\u001b[32m1.98443\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9314 | loss: 1.98443 - acc: 0.5164 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9315  | total loss: \u001b[1m\u001b[32m1.85161\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9315 | loss: 1.85161 - acc: 0.5648 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9316  | total loss: \u001b[1m\u001b[32m1.98266\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9316 | loss: 1.98266 - acc: 0.5154 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9317  | total loss: \u001b[1m\u001b[32m1.85018\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9317 | loss: 1.85018 - acc: 0.5639 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9318  | total loss: \u001b[1m\u001b[32m1.97787\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9318 | loss: 1.97787 - acc: 0.5218 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9319  | total loss: \u001b[1m\u001b[32m1.84588\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9319 | loss: 1.84588 - acc: 0.5696 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9320  | total loss: \u001b[1m\u001b[32m1.96665\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9320 | loss: 1.96665 - acc: 0.5198 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9321  | total loss: \u001b[1m\u001b[32m1.83574\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9321 | loss: 1.83574 - acc: 0.5678 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9322  | total loss: \u001b[1m\u001b[32m1.71757\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9322 | loss: 1.71757 - acc: 0.6110 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9323  | total loss: \u001b[1m\u001b[32m1.61031\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9323 | loss: 1.61031 - acc: 0.6499 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9324  | total loss: \u001b[1m\u001b[32m1.74929\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9324 | loss: 1.74929 - acc: 0.5992 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9325  | total loss: \u001b[1m\u001b[32m1.63674\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9325 | loss: 1.63674 - acc: 0.6393 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9326  | total loss: \u001b[1m\u001b[32m1.79431\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9326 | loss: 1.79431 - acc: 0.5825 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9327  | total loss: \u001b[1m\u001b[32m1.67558\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9327 | loss: 1.67558 - acc: 0.6243 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9328  | total loss: \u001b[1m\u001b[32m1.84118\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9328 | loss: 1.84118 - acc: 0.5618 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9329  | total loss: \u001b[1m\u001b[32m1.71660\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9329 | loss: 1.71660 - acc: 0.6056 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9330  | total loss: \u001b[1m\u001b[32m1.88313\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9330 | loss: 1.88313 - acc: 0.5451 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9331  | total loss: \u001b[1m\u001b[32m1.75372\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9331 | loss: 1.75372 - acc: 0.5906 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9332  | total loss: \u001b[1m\u001b[32m1.91256\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 9332 | loss: 1.91256 - acc: 0.5315 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9333  | total loss: \u001b[1m\u001b[32m1.78004\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 9333 | loss: 1.78004 - acc: 0.5784 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9334  | total loss: \u001b[1m\u001b[32m1.66049\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9334 | loss: 1.66049 - acc: 0.6205 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9335  | total loss: \u001b[1m\u001b[32m1.55209\u001b[0m\u001b[0m | time: 0.016s\n",
      "| Adam | epoch: 9335 | loss: 1.55209 - acc: 0.6585 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9336  | total loss: \u001b[1m\u001b[32m1.71818\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9336 | loss: 1.71818 - acc: 0.5998 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9337  | total loss: \u001b[1m\u001b[32m1.60222\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9337 | loss: 1.60222 - acc: 0.6398 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9338  | total loss: \u001b[1m\u001b[32m1.76016\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9338 | loss: 1.76016 - acc: 0.5830 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9339  | total loss: \u001b[1m\u001b[32m1.63880\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9339 | loss: 1.63880 - acc: 0.6247 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9340  | total loss: \u001b[1m\u001b[32m1.80469\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 9340 | loss: 1.80469 - acc: 0.5622 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9341  | total loss: \u001b[1m\u001b[32m1.67832\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9341 | loss: 1.67832 - acc: 0.6060 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9342  | total loss: \u001b[1m\u001b[32m1.82999\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9342 | loss: 1.82999 - acc: 0.5597 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9343  | total loss: \u001b[1m\u001b[32m1.70106\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9343 | loss: 1.70106 - acc: 0.6037 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9344  | total loss: \u001b[1m\u001b[32m1.84657\u001b[0m\u001b[0m | time: 0.016s\n",
      "| Adam | epoch: 9344 | loss: 1.84657 - acc: 0.5505 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9345  | total loss: \u001b[1m\u001b[32m1.71628\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9345 | loss: 1.71628 - acc: 0.5954 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9346  | total loss: \u001b[1m\u001b[32m1.59895\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9346 | loss: 1.59895 - acc: 0.6359 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9347  | total loss: \u001b[1m\u001b[32m1.49278\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9347 | loss: 1.49278 - acc: 0.6723 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9348  | total loss: \u001b[1m\u001b[32m1.67549\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9348 | loss: 1.67549 - acc: 0.6122 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9349  | total loss: \u001b[1m\u001b[32m1.56034\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9349 | loss: 1.56034 - acc: 0.6510 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9350  | total loss: \u001b[1m\u001b[32m1.45594\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9350 | loss: 1.45594 - acc: 0.6859 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9351  | total loss: \u001b[1m\u001b[32m1.36079\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9351 | loss: 1.36079 - acc: 0.7173 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9352  | total loss: \u001b[1m\u001b[32m1.56306\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9352 | loss: 1.56306 - acc: 0.6456 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9353  | total loss: \u001b[1m\u001b[32m1.45510\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 9353 | loss: 1.45510 - acc: 0.6810 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9354  | total loss: \u001b[1m\u001b[32m1.35696\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9354 | loss: 1.35696 - acc: 0.7129 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9355  | total loss: \u001b[1m\u001b[32m1.26732\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9355 | loss: 1.26732 - acc: 0.7416 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9356  | total loss: \u001b[1m\u001b[32m1.18505\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9356 | loss: 1.18505 - acc: 0.7675 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9357  | total loss: \u001b[1m\u001b[32m1.10918\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9357 | loss: 1.10918 - acc: 0.7907 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9358  | total loss: \u001b[1m\u001b[32m1.03890\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9358 | loss: 1.03890 - acc: 0.8116 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9359  | total loss: \u001b[1m\u001b[32m0.97352\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9359 | loss: 0.97352 - acc: 0.8305 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9360  | total loss: \u001b[1m\u001b[32m1.27177\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9360 | loss: 1.27177 - acc: 0.7474 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9361  | total loss: \u001b[1m\u001b[32m1.17976\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9361 | loss: 1.17976 - acc: 0.7727 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9362  | total loss: \u001b[1m\u001b[32m1.09564\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9362 | loss: 1.09564 - acc: 0.7954 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9363  | total loss: \u001b[1m\u001b[32m1.01848\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9363 | loss: 1.01848 - acc: 0.8159 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9364  | total loss: \u001b[1m\u001b[32m1.22843\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9364 | loss: 1.22843 - acc: 0.7557 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9365  | total loss: \u001b[1m\u001b[32m1.13563\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9365 | loss: 1.13563 - acc: 0.7801 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9366  | total loss: \u001b[1m\u001b[32m1.05115\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9366 | loss: 1.05115 - acc: 0.8021 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9367  | total loss: \u001b[1m\u001b[32m0.97401\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9367 | loss: 0.97401 - acc: 0.8219 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9368  | total loss: \u001b[1m\u001b[32m0.90340\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9368 | loss: 0.90340 - acc: 0.8397 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9369  | total loss: \u001b[1m\u001b[32m0.83858\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9369 | loss: 0.83858 - acc: 0.8558 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9370  | total loss: \u001b[1m\u001b[32m1.14647\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9370 | loss: 1.14647 - acc: 0.7773 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9371  | total loss: \u001b[1m\u001b[32m1.05559\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9371 | loss: 1.05559 - acc: 0.7996 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9372  | total loss: \u001b[1m\u001b[32m1.32928\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9372 | loss: 1.32928 - acc: 0.7268 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9373  | total loss: \u001b[1m\u001b[32m1.21976\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9373 | loss: 1.21976 - acc: 0.7541 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9374  | total loss: \u001b[1m\u001b[32m1.51126\u001b[0m\u001b[0m | time: 0.017s\n",
      "| Adam | epoch: 9374 | loss: 1.51126 - acc: 0.6858 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9375  | total loss: \u001b[1m\u001b[32m1.38433\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9375 | loss: 1.38433 - acc: 0.7172 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9376  | total loss: \u001b[1m\u001b[32m1.27061\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9376 | loss: 1.27061 - acc: 0.7455 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9377  | total loss: \u001b[1m\u001b[32m1.16852\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9377 | loss: 1.16852 - acc: 0.7710 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9378  | total loss: \u001b[1m\u001b[32m1.46981\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9378 | loss: 1.46981 - acc: 0.6939 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9379  | total loss: \u001b[1m\u001b[32m1.34862\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9379 | loss: 1.34862 - acc: 0.7245 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9380  | total loss: \u001b[1m\u001b[32m1.62093\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9380 | loss: 1.62093 - acc: 0.6592 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9381  | total loss: \u001b[1m\u001b[32m1.48638\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9381 | loss: 1.48638 - acc: 0.6933 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9382  | total loss: \u001b[1m\u001b[32m1.36620\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 9382 | loss: 1.36620 - acc: 0.7239 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9383  | total loss: \u001b[1m\u001b[32m1.25863\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9383 | loss: 1.25863 - acc: 0.7515 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9384  | total loss: \u001b[1m\u001b[32m1.50037\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9384 | loss: 1.50037 - acc: 0.6835 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9385  | total loss: \u001b[1m\u001b[32m1.38067\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 9385 | loss: 1.38067 - acc: 0.7152 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9386  | total loss: \u001b[1m\u001b[32m1.27359\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9386 | loss: 1.27359 - acc: 0.7437 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9387  | total loss: \u001b[1m\u001b[32m1.17752\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9387 | loss: 1.17752 - acc: 0.7693 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9388  | total loss: \u001b[1m\u001b[32m1.41767\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9388 | loss: 1.41767 - acc: 0.6995 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9389  | total loss: \u001b[1m\u001b[32m1.30791\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9389 | loss: 1.30791 - acc: 0.7296 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9390  | total loss: \u001b[1m\u001b[32m1.20948\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9390 | loss: 1.20948 - acc: 0.7566 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9391  | total loss: \u001b[1m\u001b[32m1.12091\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9391 | loss: 1.12091 - acc: 0.7809 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9392  | total loss: \u001b[1m\u001b[32m1.04093\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9392 | loss: 1.04093 - acc: 0.8028 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9393  | total loss: \u001b[1m\u001b[32m0.96839\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9393 | loss: 0.96839 - acc: 0.8226 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9394  | total loss: \u001b[1m\u001b[32m1.20997\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9394 | loss: 1.20997 - acc: 0.7546 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9395  | total loss: \u001b[1m\u001b[32m1.11973\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9395 | loss: 1.11973 - acc: 0.7791 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9396  | total loss: \u001b[1m\u001b[32m1.03820\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9396 | loss: 1.03820 - acc: 0.8012 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9397  | total loss: \u001b[1m\u001b[32m0.96428\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9397 | loss: 0.96428 - acc: 0.8211 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9398  | total loss: \u001b[1m\u001b[32m1.19088\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9398 | loss: 1.19088 - acc: 0.7604 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9399  | total loss: \u001b[1m\u001b[32m1.10084\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9399 | loss: 1.10084 - acc: 0.7844 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9400  | total loss: \u001b[1m\u001b[32m1.35383\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9400 | loss: 1.35383 - acc: 0.7202 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9401  | total loss: \u001b[1m\u001b[32m1.24750\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9401 | loss: 1.24750 - acc: 0.7482 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9402  | total loss: \u001b[1m\u001b[32m1.51559\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9402 | loss: 1.51559 - acc: 0.6805 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9403  | total loss: \u001b[1m\u001b[32m1.39397\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9403 | loss: 1.39397 - acc: 0.7125 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9404  | total loss: \u001b[1m\u001b[32m1.65199\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9404 | loss: 1.65199 - acc: 0.6484 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9405  | total loss: \u001b[1m\u001b[32m1.51844\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9405 | loss: 1.51844 - acc: 0.6835 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9406  | total loss: \u001b[1m\u001b[32m1.75990\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9406 | loss: 1.75990 - acc: 0.6152 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9407  | total loss: \u001b[1m\u001b[32m1.61800\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9407 | loss: 1.61800 - acc: 0.6537 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9408  | total loss: \u001b[1m\u001b[32m1.81013\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9408 | loss: 1.81013 - acc: 0.5954 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9409  | total loss: \u001b[1m\u001b[32m1.66611\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9409 | loss: 1.66611 - acc: 0.6359 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9410  | total loss: \u001b[1m\u001b[32m1.85959\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9410 | loss: 1.85959 - acc: 0.5794 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9411  | total loss: \u001b[1m\u001b[32m1.71383\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9411 | loss: 1.71383 - acc: 0.6215 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9412  | total loss: \u001b[1m\u001b[32m1.58405\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9412 | loss: 1.58405 - acc: 0.6594 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9413  | total loss: \u001b[1m\u001b[32m1.46810\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9413 | loss: 1.46810 - acc: 0.6934 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9414  | total loss: \u001b[1m\u001b[32m1.66245\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9414 | loss: 1.66245 - acc: 0.6384 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9415  | total loss: \u001b[1m\u001b[32m1.53999\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9415 | loss: 1.53999 - acc: 0.6745 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9416  | total loss: \u001b[1m\u001b[32m1.72365\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9416 | loss: 1.72365 - acc: 0.6142 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9417  | total loss: \u001b[1m\u001b[32m1.59663\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9417 | loss: 1.59663 - acc: 0.6528 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9418  | total loss: \u001b[1m\u001b[32m1.75477\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9418 | loss: 1.75477 - acc: 0.6018 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9419  | total loss: \u001b[1m\u001b[32m1.62647\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9419 | loss: 1.62647 - acc: 0.6416 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9420  | total loss: \u001b[1m\u001b[32m1.51165\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9420 | loss: 1.51165 - acc: 0.6775 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9421  | total loss: \u001b[1m\u001b[32m1.40842\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9421 | loss: 1.40842 - acc: 0.7097 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9422  | total loss: \u001b[1m\u001b[32m1.31511\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9422 | loss: 1.31511 - acc: 0.7387 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9423  | total loss: \u001b[1m\u001b[32m1.23031\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9423 | loss: 1.23031 - acc: 0.7649 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9424  | total loss: \u001b[1m\u001b[32m1.45258\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9424 | loss: 1.45258 - acc: 0.6955 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9425  | total loss: \u001b[1m\u001b[32m1.35245\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9425 | loss: 1.35245 - acc: 0.7260 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9426  | total loss: \u001b[1m\u001b[32m1.57989\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9426 | loss: 1.57989 - acc: 0.6534 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9427  | total loss: \u001b[1m\u001b[32m1.46631\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9427 | loss: 1.46631 - acc: 0.6880 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9428  | total loss: \u001b[1m\u001b[32m1.66514\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9428 | loss: 1.66514 - acc: 0.6264 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9429  | total loss: \u001b[1m\u001b[32m1.54305\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9429 | loss: 1.54305 - acc: 0.6637 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9430  | total loss: \u001b[1m\u001b[32m1.75921\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9430 | loss: 1.75921 - acc: 0.5974 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9431  | total loss: \u001b[1m\u001b[32m1.62842\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9431 | loss: 1.62842 - acc: 0.6376 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9432  | total loss: \u001b[1m\u001b[32m1.79500\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9432 | loss: 1.79500 - acc: 0.5810 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9433  | total loss: \u001b[1m\u001b[32m1.66188\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9433 | loss: 1.66188 - acc: 0.6229 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9434  | total loss: \u001b[1m\u001b[32m1.85103\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9434 | loss: 1.85103 - acc: 0.5678 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9435  | total loss: \u001b[1m\u001b[32m1.71387\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9435 | loss: 1.71387 - acc: 0.6110 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9436  | total loss: \u001b[1m\u001b[32m1.86062\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9436 | loss: 1.86062 - acc: 0.5642 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9437  | total loss: \u001b[1m\u001b[32m1.72414\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9437 | loss: 1.72414 - acc: 0.6078 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9438  | total loss: \u001b[1m\u001b[32m1.92303\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9438 | loss: 1.92303 - acc: 0.5470 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9439  | total loss: \u001b[1m\u001b[32m1.78201\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9439 | loss: 1.78201 - acc: 0.5923 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9440  | total loss: \u001b[1m\u001b[32m1.65567\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9440 | loss: 1.65567 - acc: 0.6331 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9441  | total loss: \u001b[1m\u001b[32m1.54197\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9441 | loss: 1.54197 - acc: 0.6697 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9442  | total loss: \u001b[1m\u001b[32m1.71996\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9442 | loss: 1.71996 - acc: 0.6099 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9443  | total loss: \u001b[1m\u001b[32m1.59954\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9443 | loss: 1.59954 - acc: 0.6489 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9444  | total loss: \u001b[1m\u001b[32m1.76586\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9444 | loss: 1.76586 - acc: 0.5912 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9445  | total loss: \u001b[1m\u001b[32m1.64086\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9445 | loss: 1.64086 - acc: 0.6321 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9446  | total loss: \u001b[1m\u001b[32m1.82957\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9446 | loss: 1.82957 - acc: 0.5689 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9447  | total loss: \u001b[1m\u001b[32m1.69863\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9447 | loss: 1.69863 - acc: 0.6120 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9448  | total loss: \u001b[1m\u001b[32m1.88151\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9448 | loss: 1.88151 - acc: 0.5579 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9449  | total loss: \u001b[1m\u001b[32m1.74614\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9449 | loss: 1.74614 - acc: 0.6021 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9450  | total loss: \u001b[1m\u001b[32m1.89934\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9450 | loss: 1.89934 - acc: 0.5491 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9451  | total loss: \u001b[1m\u001b[32m1.76310\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9451 | loss: 1.76310 - acc: 0.5941 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9452  | total loss: \u001b[1m\u001b[32m1.88585\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9452 | loss: 1.88585 - acc: 0.5490 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9453  | total loss: \u001b[1m\u001b[32m1.75184\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9453 | loss: 1.75184 - acc: 0.5941 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9454  | total loss: \u001b[1m\u001b[32m1.63136\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9454 | loss: 1.63136 - acc: 0.6347 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9455  | total loss: \u001b[1m\u001b[32m1.52253\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9455 | loss: 1.52253 - acc: 0.6712 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9456  | total loss: \u001b[1m\u001b[32m1.71847\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9456 | loss: 1.71847 - acc: 0.6041 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9457  | total loss: \u001b[1m\u001b[32m1.60004\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9457 | loss: 1.60004 - acc: 0.6437 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9458  | total loss: \u001b[1m\u001b[32m1.78937\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9458 | loss: 1.78937 - acc: 0.5793 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9459  | total loss: \u001b[1m\u001b[32m1.66357\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9459 | loss: 1.66357 - acc: 0.6214 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9460  | total loss: \u001b[1m\u001b[32m1.85029\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9460 | loss: 1.85029 - acc: 0.5593 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9461  | total loss: \u001b[1m\u001b[32m1.71862\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9461 | loss: 1.71862 - acc: 0.6033 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9462  | total loss: \u001b[1m\u001b[32m1.89820\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9462 | loss: 1.89820 - acc: 0.5430 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9463  | total loss: \u001b[1m\u001b[32m1.76239\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9463 | loss: 1.76239 - acc: 0.5887 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9464  | total loss: \u001b[1m\u001b[32m1.92405\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9464 | loss: 1.92405 - acc: 0.5298 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9465  | total loss: \u001b[1m\u001b[32m1.78669\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9465 | loss: 1.78669 - acc: 0.5768 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9466  | total loss: \u001b[1m\u001b[32m1.91837\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9466 | loss: 1.91837 - acc: 0.5334 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9467  | total loss: \u001b[1m\u001b[32m1.78271\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9467 | loss: 1.78271 - acc: 0.5801 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9468  | total loss: \u001b[1m\u001b[32m1.95452\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9468 | loss: 1.95452 - acc: 0.5221 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9469  | total loss: \u001b[1m\u001b[32m1.81638\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9469 | loss: 1.81638 - acc: 0.5699 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9470  | total loss: \u001b[1m\u001b[32m1.99228\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9470 | loss: 1.99228 - acc: 0.5129 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9471  | total loss: \u001b[1m\u001b[32m1.85164\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9471 | loss: 1.85164 - acc: 0.5616 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9472  | total loss: \u001b[1m\u001b[32m1.72541\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9472 | loss: 1.72541 - acc: 0.6054 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9473  | total loss: \u001b[1m\u001b[32m1.61157\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9473 | loss: 1.61157 - acc: 0.6449 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9474  | total loss: \u001b[1m\u001b[32m1.77682\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9474 | loss: 1.77682 - acc: 0.5876 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9475  | total loss: \u001b[1m\u001b[32m1.65701\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9475 | loss: 1.65701 - acc: 0.6288 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9476  | total loss: \u001b[1m\u001b[32m1.76732\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9476 | loss: 1.76732 - acc: 0.5873 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9477  | total loss: \u001b[1m\u001b[32m1.64776\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9477 | loss: 1.64776 - acc: 0.6286 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9478  | total loss: \u001b[1m\u001b[32m1.79699\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9478 | loss: 1.79699 - acc: 0.5729 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9479  | total loss: \u001b[1m\u001b[32m1.67389\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9479 | loss: 1.67389 - acc: 0.6156 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9480  | total loss: \u001b[1m\u001b[32m1.75923\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9480 | loss: 1.75923 - acc: 0.5826 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9481  | total loss: \u001b[1m\u001b[32m1.63935\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9481 | loss: 1.63935 - acc: 0.6244 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9482  | total loss: \u001b[1m\u001b[32m1.83066\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9482 | loss: 1.83066 - acc: 0.5619 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9483  | total loss: \u001b[1m\u001b[32m1.70325\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9483 | loss: 1.70325 - acc: 0.6057 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9484  | total loss: \u001b[1m\u001b[32m1.86886\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9484 | loss: 1.86886 - acc: 0.5523 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9485  | total loss: \u001b[1m\u001b[32m1.73763\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9485 | loss: 1.73763 - acc: 0.5971 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9486  | total loss: \u001b[1m\u001b[32m1.90615\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9486 | loss: 1.90615 - acc: 0.5374 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9487  | total loss: \u001b[1m\u001b[32m1.77150\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9487 | loss: 1.77150 - acc: 0.5836 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9488  | total loss: \u001b[1m\u001b[32m1.90764\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9488 | loss: 1.90764 - acc: 0.5324 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9489  | total loss: \u001b[1m\u001b[32m1.77342\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9489 | loss: 1.77342 - acc: 0.5792 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9490  | total loss: \u001b[1m\u001b[32m1.93889\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9490 | loss: 1.93889 - acc: 0.5212 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9491  | total loss: \u001b[1m\u001b[32m1.80232\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9491 | loss: 1.80232 - acc: 0.5691 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9492  | total loss: \u001b[1m\u001b[32m1.94796\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9492 | loss: 1.94796 - acc: 0.5194 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9493  | total loss: \u001b[1m\u001b[32m1.81141\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9493 | loss: 1.81141 - acc: 0.5674 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9494  | total loss: \u001b[1m\u001b[32m1.96153\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9494 | loss: 1.96153 - acc: 0.5178 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9495  | total loss: \u001b[1m\u001b[32m1.82463\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9495 | loss: 1.82463 - acc: 0.5660 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9496  | total loss: \u001b[1m\u001b[32m1.94410\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9496 | loss: 1.94410 - acc: 0.5166 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9497  | total loss: \u001b[1m\u001b[32m1.80996\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9497 | loss: 1.80996 - acc: 0.5649 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9498  | total loss: \u001b[1m\u001b[32m1.95250\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9498 | loss: 1.95250 - acc: 0.5156 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9499  | total loss: \u001b[1m\u001b[32m1.81847\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9499 | loss: 1.81847 - acc: 0.5640 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9500  | total loss: \u001b[1m\u001b[32m1.96403\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9500 | loss: 1.96403 - acc: 0.5076 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9501  | total loss: \u001b[1m\u001b[32m1.82977\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9501 | loss: 1.82977 - acc: 0.5569 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9502  | total loss: \u001b[1m\u001b[32m1.94298\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9502 | loss: 1.94298 - acc: 0.5155 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9503  | total loss: \u001b[1m\u001b[32m1.81156\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9503 | loss: 1.81156 - acc: 0.5639 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9504  | total loss: \u001b[1m\u001b[32m1.69324\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9504 | loss: 1.69324 - acc: 0.6075 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9505  | total loss: \u001b[1m\u001b[32m1.58612\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9505 | loss: 1.58612 - acc: 0.6468 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9506  | total loss: \u001b[1m\u001b[32m1.48857\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9506 | loss: 1.48857 - acc: 0.6821 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9507  | total loss: \u001b[1m\u001b[32m1.39919\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9507 | loss: 1.39919 - acc: 0.7139 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9508  | total loss: \u001b[1m\u001b[32m1.61073\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9508 | loss: 1.61073 - acc: 0.6425 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9509  | total loss: \u001b[1m\u001b[32m1.50621\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9509 | loss: 1.50621 - acc: 0.6782 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9510  | total loss: \u001b[1m\u001b[32m1.67466\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9510 | loss: 1.67466 - acc: 0.6176 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9511  | total loss: \u001b[1m\u001b[32m1.56171\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9511 | loss: 1.56171 - acc: 0.6558 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9512  | total loss: \u001b[1m\u001b[32m1.72592\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9512 | loss: 1.72592 - acc: 0.6045 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9513  | total loss: \u001b[1m\u001b[32m1.60634\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9513 | loss: 1.60634 - acc: 0.6441 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9514  | total loss: \u001b[1m\u001b[32m1.77085\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9514 | loss: 1.77085 - acc: 0.5939 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9515  | total loss: \u001b[1m\u001b[32m1.64571\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9515 | loss: 1.64571 - acc: 0.6345 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9516  | total loss: \u001b[1m\u001b[32m1.79320\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9516 | loss: 1.79320 - acc: 0.5782 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9517  | total loss: \u001b[1m\u001b[32m1.66519\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9517 | loss: 1.66519 - acc: 0.6204 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9518  | total loss: \u001b[1m\u001b[32m1.86874\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9518 | loss: 1.86874 - acc: 0.5584 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9519  | total loss: \u001b[1m\u001b[32m1.73313\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9519 | loss: 1.73313 - acc: 0.6025 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9520  | total loss: \u001b[1m\u001b[32m1.89703\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9520 | loss: 1.89703 - acc: 0.5494 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9521  | total loss: \u001b[1m\u001b[32m1.75900\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9521 | loss: 1.75900 - acc: 0.5945 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9522  | total loss: \u001b[1m\u001b[32m1.91084\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9522 | loss: 1.91084 - acc: 0.5493 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9523  | total loss: \u001b[1m\u001b[32m1.77205\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9523 | loss: 1.77205 - acc: 0.5944 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9524  | total loss: \u001b[1m\u001b[32m1.92757\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9524 | loss: 1.92757 - acc: 0.5421 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9525  | total loss: \u001b[1m\u001b[32m1.78790\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9525 | loss: 1.78790 - acc: 0.5879 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9526  | total loss: \u001b[1m\u001b[32m1.66237\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9526 | loss: 1.66237 - acc: 0.6291 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9527  | total loss: \u001b[1m\u001b[32m1.54907\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9527 | loss: 1.54907 - acc: 0.6662 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9528  | total loss: \u001b[1m\u001b[32m1.44635\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9528 | loss: 1.44635 - acc: 0.6996 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9529  | total loss: \u001b[1m\u001b[32m1.35278\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9529 | loss: 1.35278 - acc: 0.7296 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9530  | total loss: \u001b[1m\u001b[32m1.54548\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9530 | loss: 1.54548 - acc: 0.6638 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9531  | total loss: \u001b[1m\u001b[32m1.43992\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9531 | loss: 1.43992 - acc: 0.6974 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9532  | total loss: \u001b[1m\u001b[32m1.65699\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9532 | loss: 1.65699 - acc: 0.6277 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9533  | total loss: \u001b[1m\u001b[32m1.53914\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9533 | loss: 1.53914 - acc: 0.6649 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9534  | total loss: \u001b[1m\u001b[32m1.75600\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9534 | loss: 1.75600 - acc: 0.5984 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9535  | total loss: \u001b[1m\u001b[32m1.62797\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9535 | loss: 1.62797 - acc: 0.6386 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9536  | total loss: \u001b[1m\u001b[32m1.78383\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9536 | loss: 1.78383 - acc: 0.5890 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9537  | total loss: \u001b[1m\u001b[32m1.65323\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9537 | loss: 1.65323 - acc: 0.6301 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9538  | total loss: \u001b[1m\u001b[32m1.83254\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9538 | loss: 1.83254 - acc: 0.5742 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9539  | total loss: \u001b[1m\u001b[32m1.69759\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9539 | loss: 1.69759 - acc: 0.6168 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9540  | total loss: \u001b[1m\u001b[32m1.87322\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9540 | loss: 1.87322 - acc: 0.5623 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9541  | total loss: \u001b[1m\u001b[32m1.73510\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9541 | loss: 1.73510 - acc: 0.6060 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9542  | total loss: \u001b[1m\u001b[32m1.88725\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9542 | loss: 1.88725 - acc: 0.5526 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9543  | total loss: \u001b[1m\u001b[32m1.74884\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9543 | loss: 1.74884 - acc: 0.5973 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9544  | total loss: \u001b[1m\u001b[32m1.92224\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9544 | loss: 1.92224 - acc: 0.5376 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9545  | total loss: \u001b[1m\u001b[32m1.78163\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9545 | loss: 1.78163 - acc: 0.5838 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9546  | total loss: \u001b[1m\u001b[32m1.93815\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9546 | loss: 1.93815 - acc: 0.5326 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9547  | total loss: \u001b[1m\u001b[32m1.79737\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9547 | loss: 1.79737 - acc: 0.5793 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9548  | total loss: \u001b[1m\u001b[32m1.97828\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9548 | loss: 1.97828 - acc: 0.5214 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9549  | total loss: \u001b[1m\u001b[32m1.83501\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9549 | loss: 1.83501 - acc: 0.5693 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9550  | total loss: \u001b[1m\u001b[32m1.70657\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9550 | loss: 1.70657 - acc: 0.6123 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9551  | total loss: \u001b[1m\u001b[32m1.59094\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9551 | loss: 1.59094 - acc: 0.6511 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9552  | total loss: \u001b[1m\u001b[32m1.48635\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9552 | loss: 1.48635 - acc: 0.6860 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9553  | total loss: \u001b[1m\u001b[32m1.39128\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9553 | loss: 1.39128 - acc: 0.7174 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9554  | total loss: \u001b[1m\u001b[32m1.57997\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9554 | loss: 1.57997 - acc: 0.6528 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9555  | total loss: \u001b[1m\u001b[32m1.47369\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9555 | loss: 1.47369 - acc: 0.6875 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9556  | total loss: \u001b[1m\u001b[32m1.66211\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9556 | loss: 1.66211 - acc: 0.6259 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9557  | total loss: \u001b[1m\u001b[32m1.54648\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9557 | loss: 1.54648 - acc: 0.6633 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9558  | total loss: \u001b[1m\u001b[32m1.70752\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9558 | loss: 1.70752 - acc: 0.6113 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9559  | total loss: \u001b[1m\u001b[32m1.58674\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9559 | loss: 1.58674 - acc: 0.6501 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9560  | total loss: \u001b[1m\u001b[32m1.78005\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9560 | loss: 1.78005 - acc: 0.5851 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9561  | total loss: \u001b[1m\u001b[32m1.65191\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9561 | loss: 1.65191 - acc: 0.6266 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9562  | total loss: \u001b[1m\u001b[32m1.80177\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9562 | loss: 1.80177 - acc: 0.5782 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9563  | total loss: \u001b[1m\u001b[32m1.67175\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9563 | loss: 1.67175 - acc: 0.6204 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9564  | total loss: \u001b[1m\u001b[32m1.83419\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9564 | loss: 1.83419 - acc: 0.5655 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9565  | total loss: \u001b[1m\u001b[32m1.70153\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9565 | loss: 1.70153 - acc: 0.6090 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9566  | total loss: \u001b[1m\u001b[32m1.87901\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9566 | loss: 1.87901 - acc: 0.5481 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9567  | total loss: \u001b[1m\u001b[32m1.74280\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9567 | loss: 1.74280 - acc: 0.5933 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9568  | total loss: \u001b[1m\u001b[32m1.62047\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9568 | loss: 1.62047 - acc: 0.6339 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9569  | total loss: \u001b[1m\u001b[32m1.51014\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9569 | loss: 1.51014 - acc: 0.6705 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9570  | total loss: \u001b[1m\u001b[32m1.67435\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9570 | loss: 1.67435 - acc: 0.6106 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9571  | total loss: \u001b[1m\u001b[32m1.55792\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9571 | loss: 1.55792 - acc: 0.6496 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9572  | total loss: \u001b[1m\u001b[32m1.68708\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9572 | loss: 1.68708 - acc: 0.6060 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9573  | total loss: \u001b[1m\u001b[32m1.56893\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9573 | loss: 1.56893 - acc: 0.6454 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9574  | total loss: \u001b[1m\u001b[32m1.73946\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9574 | loss: 1.73946 - acc: 0.5880 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9575  | total loss: \u001b[1m\u001b[32m1.61597\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9575 | loss: 1.61597 - acc: 0.6292 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9576  | total loss: \u001b[1m\u001b[32m1.80652\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9576 | loss: 1.80652 - acc: 0.5663 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9577  | total loss: \u001b[1m\u001b[32m1.67679\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9577 | loss: 1.67679 - acc: 0.6097 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9578  | total loss: \u001b[1m\u001b[32m1.81448\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9578 | loss: 1.81448 - acc: 0.5630 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9579  | total loss: \u001b[1m\u001b[32m1.68481\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9579 | loss: 1.68481 - acc: 0.6067 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9580  | total loss: \u001b[1m\u001b[32m1.84932\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9580 | loss: 1.84932 - acc: 0.5532 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9581  | total loss: \u001b[1m\u001b[32m1.71715\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9581 | loss: 1.71715 - acc: 0.5979 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9582  | total loss: \u001b[1m\u001b[32m1.90668\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9582 | loss: 1.90668 - acc: 0.5381 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9583  | total loss: \u001b[1m\u001b[32m1.76989\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9583 | loss: 1.76989 - acc: 0.5843 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9584  | total loss: \u001b[1m\u001b[32m1.90452\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9584 | loss: 1.90452 - acc: 0.5401 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9585  | total loss: \u001b[1m\u001b[32m1.76911\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9585 | loss: 1.76911 - acc: 0.5861 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9586  | total loss: \u001b[1m\u001b[32m1.64752\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9586 | loss: 1.64752 - acc: 0.6275 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9587  | total loss: \u001b[1m\u001b[32m1.53778\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9587 | loss: 1.53778 - acc: 0.6647 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9588  | total loss: \u001b[1m\u001b[32m1.73210\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9588 | loss: 1.73210 - acc: 0.5983 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9589  | total loss: \u001b[1m\u001b[32m1.61311\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9589 | loss: 1.61311 - acc: 0.6384 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9590  | total loss: \u001b[1m\u001b[32m1.77966\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9590 | loss: 1.77966 - acc: 0.5817 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9591  | total loss: \u001b[1m\u001b[32m1.65566\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 9591 | loss: 1.65566 - acc: 0.6236 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9592  | total loss: \u001b[1m\u001b[32m1.54379\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9592 | loss: 1.54379 - acc: 0.6612 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9593  | total loss: \u001b[1m\u001b[32m1.44235\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9593 | loss: 1.44235 - acc: 0.6951 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9594  | total loss: \u001b[1m\u001b[32m1.66027\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9594 | loss: 1.66027 - acc: 0.6256 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9595  | total loss: \u001b[1m\u001b[32m1.54575\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9595 | loss: 1.54575 - acc: 0.6630 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9596  | total loss: \u001b[1m\u001b[32m1.44196\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9596 | loss: 1.44196 - acc: 0.6967 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9597  | total loss: \u001b[1m\u001b[32m1.34743\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9597 | loss: 1.34743 - acc: 0.7270 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9598  | total loss: \u001b[1m\u001b[32m1.56277\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9598 | loss: 1.56277 - acc: 0.6615 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9599  | total loss: \u001b[1m\u001b[32m1.45407\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9599 | loss: 1.45407 - acc: 0.6953 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9600  | total loss: \u001b[1m\u001b[32m1.65343\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9600 | loss: 1.65343 - acc: 0.6329 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9601  | total loss: \u001b[1m\u001b[32m1.53446\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9601 | loss: 1.53446 - acc: 0.6697 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9602  | total loss: \u001b[1m\u001b[32m1.73847\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9602 | loss: 1.73847 - acc: 0.6098 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9603  | total loss: \u001b[1m\u001b[32m1.61055\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9603 | loss: 1.61055 - acc: 0.6488 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9604  | total loss: \u001b[1m\u001b[32m1.79051\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9604 | loss: 1.79051 - acc: 0.5911 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9605  | total loss: \u001b[1m\u001b[32m1.65755\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9605 | loss: 1.65755 - acc: 0.6320 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9606  | total loss: \u001b[1m\u001b[32m1.82727\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9606 | loss: 1.82727 - acc: 0.5759 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9607  | total loss: \u001b[1m\u001b[32m1.69131\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9607 | loss: 1.69131 - acc: 0.6183 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9608  | total loss: \u001b[1m\u001b[32m1.88242\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9608 | loss: 1.88242 - acc: 0.5565 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9609  | total loss: \u001b[1m\u001b[32m1.74204\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9609 | loss: 1.74204 - acc: 0.6009 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9610  | total loss: \u001b[1m\u001b[32m1.92063\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9610 | loss: 1.92063 - acc: 0.5408 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9611  | total loss: \u001b[1m\u001b[32m1.77785\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9611 | loss: 1.77785 - acc: 0.5867 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9612  | total loss: \u001b[1m\u001b[32m1.96024\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9612 | loss: 1.96024 - acc: 0.5280 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9613  | total loss: \u001b[1m\u001b[32m1.81519\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9613 | loss: 1.81519 - acc: 0.5752 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9614  | total loss: \u001b[1m\u001b[32m1.97498\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9614 | loss: 1.97498 - acc: 0.5248 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9615  | total loss: \u001b[1m\u001b[32m1.83026\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9615 | loss: 1.83026 - acc: 0.5724 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9616  | total loss: \u001b[1m\u001b[32m1.70062\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9616 | loss: 1.70062 - acc: 0.6151 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9617  | total loss: \u001b[1m\u001b[32m1.58400\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9617 | loss: 1.58400 - acc: 0.6536 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9618  | total loss: \u001b[1m\u001b[32m1.77185\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9618 | loss: 1.77185 - acc: 0.5883 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9619  | total loss: \u001b[1m\u001b[32m1.64797\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9619 | loss: 1.64797 - acc: 0.6294 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9620  | total loss: \u001b[1m\u001b[32m1.83615\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9620 | loss: 1.83615 - acc: 0.5665 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9621  | total loss: \u001b[1m\u001b[32m1.70611\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9621 | loss: 1.70611 - acc: 0.6098 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9622  | total loss: \u001b[1m\u001b[32m1.58900\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9622 | loss: 1.58900 - acc: 0.6489 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9623  | total loss: \u001b[1m\u001b[32m1.48303\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9623 | loss: 1.48303 - acc: 0.6840 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9624  | total loss: \u001b[1m\u001b[32m1.38667\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9624 | loss: 1.38667 - acc: 0.7156 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9625  | total loss: \u001b[1m\u001b[32m1.29856\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9625 | loss: 1.29856 - acc: 0.7440 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9626  | total loss: \u001b[1m\u001b[32m1.21758\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9626 | loss: 1.21758 - acc: 0.7696 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9627  | total loss: \u001b[1m\u001b[32m1.14275\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9627 | loss: 1.14275 - acc: 0.7926 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9628  | total loss: \u001b[1m\u001b[32m1.37829\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9628 | loss: 1.37829 - acc: 0.7205 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9629  | total loss: \u001b[1m\u001b[32m1.28396\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9629 | loss: 1.28396 - acc: 0.7485 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9630  | total loss: \u001b[1m\u001b[32m1.51851\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9630 | loss: 1.51851 - acc: 0.6736 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9631  | total loss: \u001b[1m\u001b[32m1.40809\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9631 | loss: 1.40809 - acc: 0.7063 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9632  | total loss: \u001b[1m\u001b[32m1.62349\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9632 | loss: 1.62349 - acc: 0.6428 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9633  | total loss: \u001b[1m\u001b[32m1.50174\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9633 | loss: 1.50174 - acc: 0.6785 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9634  | total loss: \u001b[1m\u001b[32m1.39182\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9634 | loss: 1.39182 - acc: 0.7107 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9635  | total loss: \u001b[1m\u001b[32m1.29222\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9635 | loss: 1.29222 - acc: 0.7396 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9636  | total loss: \u001b[1m\u001b[32m1.51216\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9636 | loss: 1.51216 - acc: 0.6728 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9637  | total loss: \u001b[1m\u001b[32m1.39938\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9637 | loss: 1.39938 - acc: 0.7055 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9638  | total loss: \u001b[1m\u001b[32m1.66338\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9638 | loss: 1.66338 - acc: 0.6349 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9639  | total loss: \u001b[1m\u001b[32m1.53545\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9639 | loss: 1.53545 - acc: 0.6715 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9640  | total loss: \u001b[1m\u001b[32m1.78258\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9640 | loss: 1.78258 - acc: 0.6043 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9641  | total loss: \u001b[1m\u001b[32m1.64377\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9641 | loss: 1.64377 - acc: 0.6439 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9642  | total loss: \u001b[1m\u001b[32m1.85950\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9642 | loss: 1.85950 - acc: 0.5795 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9643  | total loss: \u001b[1m\u001b[32m1.71476\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9643 | loss: 1.71476 - acc: 0.6215 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9644  | total loss: \u001b[1m\u001b[32m1.88631\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9644 | loss: 1.88631 - acc: 0.5665 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9645  | total loss: \u001b[1m\u001b[32m1.74095\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9645 | loss: 1.74095 - acc: 0.6099 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9646  | total loss: \u001b[1m\u001b[32m1.61091\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9646 | loss: 1.61091 - acc: 0.6489 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9647  | total loss: \u001b[1m\u001b[32m1.49416\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9647 | loss: 1.49416 - acc: 0.6840 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9648  | total loss: \u001b[1m\u001b[32m1.67565\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9648 | loss: 1.67565 - acc: 0.6227 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9649  | total loss: \u001b[1m\u001b[32m1.55275\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9649 | loss: 1.55275 - acc: 0.6605 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9650  | total loss: \u001b[1m\u001b[32m1.77466\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9650 | loss: 1.77466 - acc: 0.5944 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9651  | total loss: \u001b[1m\u001b[32m1.64266\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9651 | loss: 1.64266 - acc: 0.6350 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9652  | total loss: \u001b[1m\u001b[32m1.82019\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9652 | loss: 1.82019 - acc: 0.5786 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9653  | total loss: \u001b[1m\u001b[32m1.68490\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9653 | loss: 1.68490 - acc: 0.6208 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9654  | total loss: \u001b[1m\u001b[32m1.88431\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9654 | loss: 1.88431 - acc: 0.5587 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9655  | total loss: \u001b[1m\u001b[32m1.74430\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9655 | loss: 1.74430 - acc: 0.6028 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9656  | total loss: \u001b[1m\u001b[32m1.87895\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9656 | loss: 1.87895 - acc: 0.5568 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9657  | total loss: \u001b[1m\u001b[32m1.74134\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9657 | loss: 1.74134 - acc: 0.6011 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9658  | total loss: \u001b[1m\u001b[32m1.92350\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9658 | loss: 1.92350 - acc: 0.5410 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9659  | total loss: \u001b[1m\u001b[32m1.78338\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9659 | loss: 1.78338 - acc: 0.5869 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9660  | total loss: \u001b[1m\u001b[32m1.95031\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9660 | loss: 1.95031 - acc: 0.5354 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9661  | total loss: \u001b[1m\u001b[32m1.80954\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9661 | loss: 1.80954 - acc: 0.5818 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9662  | total loss: \u001b[1m\u001b[32m1.68354\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9662 | loss: 1.68354 - acc: 0.6237 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9663  | total loss: \u001b[1m\u001b[32m1.57025\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9663 | loss: 1.57025 - acc: 0.6613 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9664  | total loss: \u001b[1m\u001b[32m1.76986\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9664 | loss: 1.76986 - acc: 0.5952 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9665  | total loss: \u001b[1m\u001b[32m1.64789\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9665 | loss: 1.64789 - acc: 0.6356 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9666  | total loss: \u001b[1m\u001b[32m1.80229\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9666 | loss: 1.80229 - acc: 0.5792 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9667  | total loss: \u001b[1m\u001b[32m1.67730\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9667 | loss: 1.67730 - acc: 0.6213 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9668  | total loss: \u001b[1m\u001b[32m1.84342\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9668 | loss: 1.84342 - acc: 0.5663 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9669  | total loss: \u001b[1m\u001b[32m1.71474\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9669 | loss: 1.71474 - acc: 0.6097 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9670  | total loss: \u001b[1m\u001b[32m1.59895\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9670 | loss: 1.59895 - acc: 0.6487 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9671  | total loss: \u001b[1m\u001b[32m1.49425\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9671 | loss: 1.49425 - acc: 0.6838 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9672  | total loss: \u001b[1m\u001b[32m1.69866\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9672 | loss: 1.69866 - acc: 0.6155 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9673  | total loss: \u001b[1m\u001b[32m1.58293\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9673 | loss: 1.58293 - acc: 0.6539 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9674  | total loss: \u001b[1m\u001b[32m1.76758\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9674 | loss: 1.76758 - acc: 0.5957 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9675  | total loss: \u001b[1m\u001b[32m1.64446\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9675 | loss: 1.64446 - acc: 0.6361 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9676  | total loss: \u001b[1m\u001b[32m1.80042\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9676 | loss: 1.80042 - acc: 0.5796 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9677  | total loss: \u001b[1m\u001b[32m1.67390\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9677 | loss: 1.67390 - acc: 0.6217 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9678  | total loss: \u001b[1m\u001b[32m1.82560\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9678 | loss: 1.82560 - acc: 0.5666 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9679  | total loss: \u001b[1m\u001b[32m1.69673\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9679 | loss: 1.69673 - acc: 0.6100 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9680  | total loss: \u001b[1m\u001b[32m1.82956\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9680 | loss: 1.82956 - acc: 0.5633 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9681  | total loss: \u001b[1m\u001b[32m1.70062\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9681 | loss: 1.70062 - acc: 0.6069 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9682  | total loss: \u001b[1m\u001b[32m1.85181\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9682 | loss: 1.85181 - acc: 0.5534 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9683  | total loss: \u001b[1m\u001b[32m1.72110\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9683 | loss: 1.72110 - acc: 0.5981 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9684  | total loss: \u001b[1m\u001b[32m1.83466\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9684 | loss: 1.83466 - acc: 0.5597 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9685  | total loss: \u001b[1m\u001b[32m1.70615\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9685 | loss: 1.70615 - acc: 0.6037 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9686  | total loss: \u001b[1m\u001b[32m1.89100\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9686 | loss: 1.89100 - acc: 0.5433 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9687  | total loss: \u001b[1m\u001b[32m1.75747\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9687 | loss: 1.75747 - acc: 0.5890 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9688  | total loss: \u001b[1m\u001b[32m1.63740\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9688 | loss: 1.63740 - acc: 0.6301 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9689  | total loss: \u001b[1m\u001b[32m1.52894\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9689 | loss: 1.52894 - acc: 0.6671 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9690  | total loss: \u001b[1m\u001b[32m1.71191\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9690 | loss: 1.71191 - acc: 0.6004 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9691  | total loss: \u001b[1m\u001b[32m1.59509\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9691 | loss: 1.59509 - acc: 0.6403 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9692  | total loss: \u001b[1m\u001b[32m1.48942\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9692 | loss: 1.48942 - acc: 0.6763 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9693  | total loss: \u001b[1m\u001b[32m1.39337\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9693 | loss: 1.39337 - acc: 0.7087 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9694  | total loss: \u001b[1m\u001b[32m1.30561\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9694 | loss: 1.30561 - acc: 0.7378 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9695  | total loss: \u001b[1m\u001b[32m1.22502\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9695 | loss: 1.22502 - acc: 0.7640 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9696  | total loss: \u001b[1m\u001b[32m1.41749\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9696 | loss: 1.41749 - acc: 0.6948 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9697  | total loss: \u001b[1m\u001b[32m1.32282\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9697 | loss: 1.32282 - acc: 0.7253 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9698  | total loss: \u001b[1m\u001b[32m1.55286\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9698 | loss: 1.55286 - acc: 0.6528 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9699  | total loss: \u001b[1m\u001b[32m1.44283\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9699 | loss: 1.44283 - acc: 0.6875 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9700  | total loss: \u001b[1m\u001b[32m1.66526\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9700 | loss: 1.66526 - acc: 0.6187 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9701  | total loss: \u001b[1m\u001b[32m1.54309\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9701 | loss: 1.54309 - acc: 0.6569 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9702  | total loss: \u001b[1m\u001b[32m1.71216\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9702 | loss: 1.71216 - acc: 0.6055 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9703  | total loss: \u001b[1m\u001b[32m1.58510\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9703 | loss: 1.58510 - acc: 0.6449 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9704  | total loss: \u001b[1m\u001b[32m1.78987\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9704 | loss: 1.78987 - acc: 0.5804 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9705  | total loss: \u001b[1m\u001b[32m1.65547\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9705 | loss: 1.65547 - acc: 0.6224 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9706  | total loss: \u001b[1m\u001b[32m1.53466\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9706 | loss: 1.53466 - acc: 0.6601 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9707  | total loss: \u001b[1m\u001b[32m1.42568\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9707 | loss: 1.42568 - acc: 0.6941 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9708  | total loss: \u001b[1m\u001b[32m1.61441\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9708 | loss: 1.61441 - acc: 0.6319 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9709  | total loss: \u001b[1m\u001b[32m1.49691\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9709 | loss: 1.49691 - acc: 0.6687 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9710  | total loss: \u001b[1m\u001b[32m1.66244\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9710 | loss: 1.66244 - acc: 0.6161 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9711  | total loss: \u001b[1m\u001b[32m1.54006\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9711 | loss: 1.54006 - acc: 0.6545 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9712  | total loss: \u001b[1m\u001b[32m1.76414\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9712 | loss: 1.76414 - acc: 0.5890 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9713  | total loss: \u001b[1m\u001b[32m1.63202\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9713 | loss: 1.63202 - acc: 0.6301 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9714  | total loss: \u001b[1m\u001b[32m1.79179\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9714 | loss: 1.79179 - acc: 0.5743 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9715  | total loss: \u001b[1m\u001b[32m1.65775\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9715 | loss: 1.65775 - acc: 0.6168 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9716  | total loss: \u001b[1m\u001b[32m1.78443\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9716 | loss: 1.78443 - acc: 0.5766 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9717  | total loss: \u001b[1m\u001b[32m1.65205\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9717 | loss: 1.65205 - acc: 0.6189 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9718  | total loss: \u001b[1m\u001b[32m1.82696\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9718 | loss: 1.82696 - acc: 0.5642 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9719  | total loss: \u001b[1m\u001b[32m1.69140\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9719 | loss: 1.69140 - acc: 0.6078 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9720  | total loss: \u001b[1m\u001b[32m1.88146\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9720 | loss: 1.88146 - acc: 0.5470 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9721  | total loss: \u001b[1m\u001b[32m1.74183\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 9721 | loss: 1.74183 - acc: 0.5923 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9722  | total loss: \u001b[1m\u001b[32m1.89250\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9722 | loss: 1.89250 - acc: 0.5473 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9723  | total loss: \u001b[1m\u001b[32m1.75332\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9723 | loss: 1.75332 - acc: 0.5926 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9724  | total loss: \u001b[1m\u001b[32m1.87878\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9724 | loss: 1.87878 - acc: 0.5476 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9725  | total loss: \u001b[1m\u001b[32m1.74245\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9725 | loss: 1.74245 - acc: 0.5929 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9726  | total loss: \u001b[1m\u001b[32m1.89577\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9726 | loss: 1.89577 - acc: 0.5407 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9727  | total loss: \u001b[1m\u001b[32m1.75915\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9727 | loss: 1.75915 - acc: 0.5867 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9728  | total loss: \u001b[1m\u001b[32m1.63659\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9728 | loss: 1.63659 - acc: 0.6280 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9729  | total loss: \u001b[1m\u001b[32m1.52618\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9729 | loss: 1.52618 - acc: 0.6652 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9730  | total loss: \u001b[1m\u001b[32m1.62383\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9730 | loss: 1.62383 - acc: 0.6272 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9731  | total loss: \u001b[1m\u001b[32m1.51397\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9731 | loss: 1.51397 - acc: 0.6645 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9732  | total loss: \u001b[1m\u001b[32m1.71193\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9732 | loss: 1.71193 - acc: 0.6052 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9733  | total loss: \u001b[1m\u001b[32m1.59275\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9733 | loss: 1.59275 - acc: 0.6447 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9734  | total loss: \u001b[1m\u001b[32m1.71516\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9734 | loss: 1.71516 - acc: 0.6088 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9735  | total loss: \u001b[1m\u001b[32m1.59539\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9735 | loss: 1.59539 - acc: 0.6479 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9736  | total loss: \u001b[1m\u001b[32m1.76201\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9736 | loss: 1.76201 - acc: 0.5903 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9737  | total loss: \u001b[1m\u001b[32m1.63757\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9737 | loss: 1.63757 - acc: 0.6312 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9738  | total loss: \u001b[1m\u001b[32m1.52545\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9738 | loss: 1.52545 - acc: 0.6681 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9739  | total loss: \u001b[1m\u001b[32m1.42395\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9739 | loss: 1.42395 - acc: 0.7013 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9740  | total loss: \u001b[1m\u001b[32m1.33161\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9740 | loss: 1.33161 - acc: 0.7312 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9741  | total loss: \u001b[1m\u001b[32m1.24716\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9741 | loss: 1.24716 - acc: 0.7581 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9742  | total loss: \u001b[1m\u001b[32m1.46603\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9742 | loss: 1.46603 - acc: 0.6894 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9743  | total loss: \u001b[1m\u001b[32m1.36572\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9743 | loss: 1.36572 - acc: 0.7205 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9744  | total loss: \u001b[1m\u001b[32m1.55245\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9744 | loss: 1.55245 - acc: 0.6627 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9745  | total loss: \u001b[1m\u001b[32m1.44194\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9745 | loss: 1.44194 - acc: 0.6964 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9746  | total loss: \u001b[1m\u001b[32m1.67695\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9746 | loss: 1.67695 - acc: 0.6268 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9747  | total loss: \u001b[1m\u001b[32m1.55324\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9747 | loss: 1.55324 - acc: 0.6641 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9748  | total loss: \u001b[1m\u001b[32m1.73371\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9748 | loss: 1.73371 - acc: 0.6120 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9749  | total loss: \u001b[1m\u001b[32m1.60429\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9749 | loss: 1.60429 - acc: 0.6508 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9750  | total loss: \u001b[1m\u001b[32m1.81436\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9750 | loss: 1.81436 - acc: 0.5857 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9751  | total loss: \u001b[1m\u001b[32m1.67746\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9751 | loss: 1.67746 - acc: 0.6271 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9752  | total loss: \u001b[1m\u001b[32m1.55448\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9752 | loss: 1.55448 - acc: 0.6644 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9753  | total loss: \u001b[1m\u001b[32m1.44358\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9753 | loss: 1.44358 - acc: 0.6980 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9754  | total loss: \u001b[1m\u001b[32m1.34320\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9754 | loss: 1.34320 - acc: 0.7282 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9755  | total loss: \u001b[1m\u001b[32m1.25195\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9755 | loss: 1.25195 - acc: 0.7554 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9756  | total loss: \u001b[1m\u001b[32m1.41207\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9756 | loss: 1.41207 - acc: 0.7084 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9757  | total loss: \u001b[1m\u001b[32m1.31211\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9757 | loss: 1.31211 - acc: 0.7376 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9758  | total loss: \u001b[1m\u001b[32m1.50360\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9758 | loss: 1.50360 - acc: 0.6781 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9759  | total loss: \u001b[1m\u001b[32m1.39340\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9759 | loss: 1.39340 - acc: 0.7103 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9760  | total loss: \u001b[1m\u001b[32m1.60795\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9760 | loss: 1.60795 - acc: 0.6464 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9761  | total loss: \u001b[1m\u001b[32m1.48710\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9761 | loss: 1.48710 - acc: 0.6818 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9762  | total loss: \u001b[1m\u001b[32m1.72597\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9762 | loss: 1.72597 - acc: 0.6136 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9763  | total loss: \u001b[1m\u001b[32m1.59399\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9763 | loss: 1.59399 - acc: 0.6522 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9764  | total loss: \u001b[1m\u001b[32m1.81840\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9764 | loss: 1.81840 - acc: 0.5870 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9765  | total loss: \u001b[1m\u001b[32m1.67862\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9765 | loss: 1.67862 - acc: 0.6283 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9766  | total loss: \u001b[1m\u001b[32m1.88898\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9766 | loss: 1.88898 - acc: 0.5655 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9767  | total loss: \u001b[1m\u001b[32m1.74425\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9767 | loss: 1.74425 - acc: 0.6089 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9768  | total loss: \u001b[1m\u001b[32m1.94510\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9768 | loss: 1.94510 - acc: 0.5480 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9769  | total loss: \u001b[1m\u001b[32m1.79732\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9769 | loss: 1.79732 - acc: 0.5932 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9770  | total loss: \u001b[1m\u001b[32m1.95817\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9770 | loss: 1.95817 - acc: 0.5410 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9771  | total loss: \u001b[1m\u001b[32m1.81179\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9771 | loss: 1.81179 - acc: 0.5869 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9772  | total loss: \u001b[1m\u001b[32m1.97441\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9772 | loss: 1.97441 - acc: 0.5354 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9773  | total loss: \u001b[1m\u001b[32m1.82910\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9773 | loss: 1.82910 - acc: 0.5819 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9774  | total loss: \u001b[1m\u001b[32m1.96446\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9774 | loss: 1.96446 - acc: 0.5380 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9775  | total loss: \u001b[1m\u001b[32m1.82258\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9775 | loss: 1.82258 - acc: 0.5842 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9776  | total loss: \u001b[1m\u001b[32m1.94340\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9776 | loss: 1.94340 - acc: 0.5400 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9777  | total loss: \u001b[1m\u001b[32m1.80565\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9777 | loss: 1.80565 - acc: 0.5860 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9778  | total loss: \u001b[1m\u001b[32m1.93480\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9778 | loss: 1.93480 - acc: 0.5346 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9779  | total loss: \u001b[1m\u001b[32m1.79964\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9779 | loss: 1.79964 - acc: 0.5811 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9780  | total loss: \u001b[1m\u001b[32m1.91013\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9780 | loss: 1.91013 - acc: 0.5373 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9781  | total loss: \u001b[1m\u001b[32m1.77887\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9781 | loss: 1.77887 - acc: 0.5836 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9782  | total loss: \u001b[1m\u001b[32m1.91970\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9782 | loss: 1.91970 - acc: 0.5323 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9783  | total loss: \u001b[1m\u001b[32m1.78866\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9783 | loss: 1.78866 - acc: 0.5791 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9784  | total loss: \u001b[1m\u001b[32m1.95259\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9784 | loss: 1.95259 - acc: 0.5212 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9785  | total loss: \u001b[1m\u001b[32m1.81937\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9785 | loss: 1.81937 - acc: 0.5691 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9786  | total loss: \u001b[1m\u001b[32m1.96052\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9786 | loss: 1.96052 - acc: 0.5193 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9787  | total loss: \u001b[1m\u001b[32m1.82753\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9787 | loss: 1.82753 - acc: 0.5674 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9788  | total loss: \u001b[1m\u001b[32m1.70796\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9788 | loss: 1.70796 - acc: 0.6106 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9789  | total loss: \u001b[1m\u001b[32m1.59987\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9789 | loss: 1.59987 - acc: 0.6496 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9790  | total loss: \u001b[1m\u001b[32m1.78169\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9790 | loss: 1.78169 - acc: 0.5846 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9791  | total loss: \u001b[1m\u001b[32m1.66498\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9791 | loss: 1.66498 - acc: 0.6262 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9792  | total loss: \u001b[1m\u001b[32m1.79936\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9792 | loss: 1.79936 - acc: 0.5707 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9793  | total loss: \u001b[1m\u001b[32m1.68002\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9793 | loss: 1.68002 - acc: 0.6136 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9794  | total loss: \u001b[1m\u001b[32m1.85804\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9794 | loss: 1.85804 - acc: 0.5523 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9795  | total loss: \u001b[1m\u001b[32m1.73227\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9795 | loss: 1.73227 - acc: 0.5970 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9796  | total loss: \u001b[1m\u001b[32m1.86742\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9796 | loss: 1.86742 - acc: 0.5516 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9797  | total loss: \u001b[1m\u001b[32m1.74036\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9797 | loss: 1.74036 - acc: 0.5965 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9798  | total loss: \u001b[1m\u001b[32m1.87797\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9798 | loss: 1.87797 - acc: 0.5439 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9799  | total loss: \u001b[1m\u001b[32m1.74963\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9799 | loss: 1.74963 - acc: 0.5896 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9800  | total loss: \u001b[1m\u001b[32m1.88019\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9800 | loss: 1.88019 - acc: 0.5449 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9801  | total loss: \u001b[1m\u001b[32m1.75164\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9801 | loss: 1.75164 - acc: 0.5904 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9802  | total loss: \u001b[1m\u001b[32m1.90177\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9802 | loss: 1.90177 - acc: 0.5385 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9803  | total loss: \u001b[1m\u001b[32m1.77118\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9803 | loss: 1.77118 - acc: 0.5846 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9804  | total loss: \u001b[1m\u001b[32m1.86810\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9804 | loss: 1.86810 - acc: 0.5476 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9805  | total loss: \u001b[1m\u001b[32m1.74088\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9805 | loss: 1.74088 - acc: 0.5929 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9806  | total loss: \u001b[1m\u001b[32m1.90384\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9806 | loss: 1.90384 - acc: 0.5407 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9807  | total loss: \u001b[1m\u001b[32m1.77302\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9807 | loss: 1.77302 - acc: 0.5866 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9808  | total loss: \u001b[1m\u001b[32m1.93678\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9808 | loss: 1.93678 - acc: 0.5280 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9809  | total loss: \u001b[1m\u001b[32m1.80300\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9809 | loss: 1.80300 - acc: 0.5752 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9810  | total loss: \u001b[1m\u001b[32m1.96619\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9810 | loss: 1.96619 - acc: 0.5177 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9811  | total loss: \u001b[1m\u001b[32m1.83007\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9811 | loss: 1.83007 - acc: 0.5659 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9812  | total loss: \u001b[1m\u001b[32m1.97710\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9812 | loss: 1.97710 - acc: 0.5093 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9813  | total loss: \u001b[1m\u001b[32m1.84061\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9813 | loss: 1.84061 - acc: 0.5584 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9814  | total loss: \u001b[1m\u001b[32m1.98999\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9814 | loss: 1.98999 - acc: 0.5025 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9815  | total loss: \u001b[1m\u001b[32m1.85305\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9815 | loss: 1.85305 - acc: 0.5523 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9816  | total loss: \u001b[1m\u001b[32m1.72990\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9816 | loss: 1.72990 - acc: 0.5971 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9817  | total loss: \u001b[1m\u001b[32m1.61857\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9817 | loss: 1.61857 - acc: 0.6373 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9818  | total loss: \u001b[1m\u001b[32m1.70279\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9818 | loss: 1.70279 - acc: 0.6022 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9819  | total loss: \u001b[1m\u001b[32m1.59256\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9819 | loss: 1.59256 - acc: 0.6420 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9820  | total loss: \u001b[1m\u001b[32m1.77796\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9820 | loss: 1.77796 - acc: 0.5778 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9821  | total loss: \u001b[1m\u001b[32m1.65890\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9821 | loss: 1.65890 - acc: 0.6200 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9822  | total loss: \u001b[1m\u001b[32m1.79646\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9822 | loss: 1.79646 - acc: 0.5723 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9823  | total loss: \u001b[1m\u001b[32m1.67463\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9823 | loss: 1.67463 - acc: 0.6151 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9824  | total loss: \u001b[1m\u001b[32m1.85892\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9824 | loss: 1.85892 - acc: 0.5535 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9825  | total loss: \u001b[1m\u001b[32m1.73032\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9825 | loss: 1.73032 - acc: 0.5982 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9826  | total loss: \u001b[1m\u001b[32m1.90183\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9826 | loss: 1.90183 - acc: 0.5455 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9827  | total loss: \u001b[1m\u001b[32m1.76887\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9827 | loss: 1.76887 - acc: 0.5910 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9828  | total loss: \u001b[1m\u001b[32m1.89018\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9828 | loss: 1.89018 - acc: 0.5462 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9829  | total loss: \u001b[1m\u001b[32m1.75853\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9829 | loss: 1.75853 - acc: 0.5915 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9830  | total loss: \u001b[1m\u001b[32m1.88826\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9830 | loss: 1.88826 - acc: 0.5467 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9831  | total loss: \u001b[1m\u001b[32m1.75702\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9831 | loss: 1.75702 - acc: 0.5920 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9832  | total loss: \u001b[1m\u001b[32m1.92656\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9832 | loss: 1.92656 - acc: 0.5328 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9833  | total loss: \u001b[1m\u001b[32m1.79179\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9833 | loss: 1.79179 - acc: 0.5795 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9834  | total loss: \u001b[1m\u001b[32m1.67040\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9834 | loss: 1.67040 - acc: 0.6216 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9835  | total loss: \u001b[1m\u001b[32m1.56050\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9835 | loss: 1.56050 - acc: 0.6594 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9836  | total loss: \u001b[1m\u001b[32m1.75730\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9836 | loss: 1.75730 - acc: 0.5935 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9837  | total loss: \u001b[1m\u001b[32m1.63734\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9837 | loss: 1.63734 - acc: 0.6341 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9838  | total loss: \u001b[1m\u001b[32m1.80586\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9838 | loss: 1.80586 - acc: 0.5779 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9839  | total loss: \u001b[1m\u001b[32m1.68026\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9839 | loss: 1.68026 - acc: 0.6201 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9840  | total loss: \u001b[1m\u001b[32m1.88808\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9840 | loss: 1.88808 - acc: 0.5581 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9841  | total loss: \u001b[1m\u001b[32m1.75405\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9841 | loss: 1.75405 - acc: 0.6023 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9842  | total loss: \u001b[1m\u001b[32m1.88235\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9842 | loss: 1.88235 - acc: 0.5563 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9843  | total loss: \u001b[1m\u001b[32m1.74902\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9843 | loss: 1.74902 - acc: 0.6007 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9844  | total loss: \u001b[1m\u001b[32m1.90319\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9844 | loss: 1.90319 - acc: 0.5478 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9845  | total loss: \u001b[1m\u001b[32m1.76804\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9845 | loss: 1.76804 - acc: 0.5930 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9846  | total loss: \u001b[1m\u001b[32m1.64633\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9846 | loss: 1.64633 - acc: 0.6337 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9847  | total loss: \u001b[1m\u001b[32m1.53622\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9847 | loss: 1.53622 - acc: 0.6703 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9848  | total loss: \u001b[1m\u001b[32m1.73896\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9848 | loss: 1.73896 - acc: 0.6033 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9849  | total loss: \u001b[1m\u001b[32m1.61842\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9849 | loss: 1.61842 - acc: 0.6430 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9850  | total loss: \u001b[1m\u001b[32m1.76728\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9850 | loss: 1.76728 - acc: 0.5929 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9851  | total loss: \u001b[1m\u001b[32m1.64330\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9851 | loss: 1.64330 - acc: 0.6337 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9852  | total loss: \u001b[1m\u001b[32m1.81335\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9852 | loss: 1.81335 - acc: 0.5774 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9853  | total loss: \u001b[1m\u001b[32m1.68451\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9853 | loss: 1.68451 - acc: 0.6197 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9854  | total loss: \u001b[1m\u001b[32m1.87188\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9854 | loss: 1.87188 - acc: 0.5577 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9855  | total loss: \u001b[1m\u001b[32m1.73742\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9855 | loss: 1.73742 - acc: 0.6019 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9856  | total loss: \u001b[1m\u001b[32m1.93305\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9856 | loss: 1.93305 - acc: 0.5418 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9857  | total loss: \u001b[1m\u001b[32m1.79319\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9857 | loss: 1.79319 - acc: 0.5876 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9858  | total loss: \u001b[1m\u001b[32m1.93764\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9858 | loss: 1.93764 - acc: 0.5360 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9859  | total loss: \u001b[1m\u001b[32m1.79829\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9859 | loss: 1.79829 - acc: 0.5824 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9860  | total loss: \u001b[1m\u001b[32m1.96675\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9860 | loss: 1.96675 - acc: 0.5241 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9861  | total loss: \u001b[1m\u001b[32m1.82570\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9861 | loss: 1.82570 - acc: 0.5717 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9862  | total loss: \u001b[1m\u001b[32m2.00449\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9862 | loss: 2.00449 - acc: 0.5145 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9863  | total loss: \u001b[1m\u001b[32m1.86110\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9863 | loss: 1.86110 - acc: 0.5631 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9864  | total loss: \u001b[1m\u001b[32m1.97404\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9864 | loss: 1.97404 - acc: 0.5211 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9865  | total loss: \u001b[1m\u001b[32m1.83509\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9865 | loss: 1.83509 - acc: 0.5690 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9866  | total loss: \u001b[1m\u001b[32m1.88756\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9866 | loss: 1.88756 - acc: 0.5478 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9867  | total loss: \u001b[1m\u001b[32m1.75817\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9867 | loss: 1.75817 - acc: 0.5930 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9868  | total loss: \u001b[1m\u001b[32m1.64170\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9868 | loss: 1.64170 - acc: 0.6337 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9869  | total loss: \u001b[1m\u001b[32m1.53630\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9869 | loss: 1.53630 - acc: 0.6703 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9870  | total loss: \u001b[1m\u001b[32m1.70816\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9870 | loss: 1.70816 - acc: 0.6104 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9871  | total loss: \u001b[1m\u001b[32m1.59463\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9871 | loss: 1.59463 - acc: 0.6494 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9872  | total loss: \u001b[1m\u001b[32m1.49155\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9872 | loss: 1.49155 - acc: 0.6845 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9873  | total loss: \u001b[1m\u001b[32m1.39745\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9873 | loss: 1.39745 - acc: 0.7160 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9874  | total loss: \u001b[1m\u001b[32m1.59437\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9874 | loss: 1.59437 - acc: 0.6444 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9875  | total loss: \u001b[1m\u001b[32m1.48744\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9875 | loss: 1.48744 - acc: 0.6800 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9876  | total loss: \u001b[1m\u001b[32m1.63619\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9876 | loss: 1.63619 - acc: 0.6263 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9877  | total loss: \u001b[1m\u001b[32m1.52328\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9877 | loss: 1.52328 - acc: 0.6636 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9878  | total loss: \u001b[1m\u001b[32m1.42071\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9878 | loss: 1.42071 - acc: 0.6973 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9879  | total loss: \u001b[1m\u001b[32m1.32710\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9879 | loss: 1.32710 - acc: 0.7275 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9880  | total loss: \u001b[1m\u001b[32m1.53538\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9880 | loss: 1.53538 - acc: 0.6619 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9881  | total loss: \u001b[1m\u001b[32m1.42794\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9881 | loss: 1.42794 - acc: 0.6957 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9882  | total loss: \u001b[1m\u001b[32m1.66637\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9882 | loss: 1.66637 - acc: 0.6262 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9883  | total loss: \u001b[1m\u001b[32m1.54452\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9883 | loss: 1.54452 - acc: 0.6635 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9884  | total loss: \u001b[1m\u001b[32m1.71969\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9884 | loss: 1.71969 - acc: 0.6115 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9885  | total loss: \u001b[1m\u001b[32m1.59199\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9885 | loss: 1.59199 - acc: 0.6503 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9886  | total loss: \u001b[1m\u001b[32m1.76700\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9886 | loss: 1.76700 - acc: 0.5924 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9887  | total loss: \u001b[1m\u001b[32m1.63455\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9887 | loss: 1.63455 - acc: 0.6332 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9888  | total loss: \u001b[1m\u001b[32m1.85242\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9888 | loss: 1.85242 - acc: 0.5699 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9889  | total loss: \u001b[1m\u001b[32m1.71201\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9889 | loss: 1.71201 - acc: 0.6129 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9890  | total loss: \u001b[1m\u001b[32m1.87526\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9890 | loss: 1.87526 - acc: 0.5516 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9891  | total loss: \u001b[1m\u001b[32m1.73365\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9891 | loss: 1.73365 - acc: 0.5964 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9892  | total loss: \u001b[1m\u001b[32m1.60659\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9892 | loss: 1.60659 - acc: 0.6368 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9893  | total loss: \u001b[1m\u001b[32m1.49217\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9893 | loss: 1.49217 - acc: 0.6731 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9894  | total loss: \u001b[1m\u001b[32m1.69946\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9894 | loss: 1.69946 - acc: 0.6129 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9895  | total loss: \u001b[1m\u001b[32m1.57558\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9895 | loss: 1.57558 - acc: 0.6517 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9896  | total loss: \u001b[1m\u001b[32m1.46393\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9896 | loss: 1.46393 - acc: 0.6865 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9897  | total loss: \u001b[1m\u001b[32m1.36290\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9897 | loss: 1.36290 - acc: 0.7178 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9898  | total loss: \u001b[1m\u001b[32m1.60246\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9898 | loss: 1.60246 - acc: 0.6461 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9899  | total loss: \u001b[1m\u001b[32m1.48665\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9899 | loss: 1.48665 - acc: 0.6814 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9900  | total loss: \u001b[1m\u001b[32m1.69247\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9900 | loss: 1.69247 - acc: 0.6204 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9901  | total loss: \u001b[1m\u001b[32m1.56750\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9901 | loss: 1.56750 - acc: 0.6584 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9902  | total loss: \u001b[1m\u001b[32m1.77919\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9902 | loss: 1.77919 - acc: 0.5926 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9903  | total loss: \u001b[1m\u001b[32m1.64604\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9903 | loss: 1.64604 - acc: 0.6333 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9904  | total loss: \u001b[1m\u001b[32m1.83898\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9904 | loss: 1.83898 - acc: 0.5771 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9905  | total loss: \u001b[1m\u001b[32m1.70088\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9905 | loss: 1.70088 - acc: 0.6194 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9906  | total loss: \u001b[1m\u001b[32m1.89394\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9906 | loss: 1.89394 - acc: 0.5646 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9907  | total loss: \u001b[1m\u001b[32m1.75168\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9907 | loss: 1.75168 - acc: 0.6081 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9908  | total loss: \u001b[1m\u001b[32m1.96170\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9908 | loss: 1.96170 - acc: 0.5473 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9909  | total loss: \u001b[1m\u001b[32m1.81436\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9909 | loss: 1.81436 - acc: 0.5926 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9910  | total loss: \u001b[1m\u001b[32m1.68244\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9910 | loss: 1.68244 - acc: 0.6333 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9911  | total loss: \u001b[1m\u001b[32m1.56389\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9911 | loss: 1.56389 - acc: 0.6700 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9912  | total loss: \u001b[1m\u001b[32m1.75672\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9912 | loss: 1.75672 - acc: 0.6030 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9913  | total loss: \u001b[1m\u001b[32m1.63085\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9913 | loss: 1.63085 - acc: 0.6427 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9914  | total loss: \u001b[1m\u001b[32m1.82461\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9914 | loss: 1.82461 - acc: 0.5784 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9915  | total loss: \u001b[1m\u001b[32m1.69243\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9915 | loss: 1.69243 - acc: 0.6206 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9916  | total loss: \u001b[1m\u001b[32m1.85879\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9916 | loss: 1.85879 - acc: 0.5585 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9917  | total loss: \u001b[1m\u001b[32m1.72398\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9917 | loss: 1.72398 - acc: 0.6027 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9918  | total loss: \u001b[1m\u001b[32m1.60285\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9918 | loss: 1.60285 - acc: 0.6424 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9919  | total loss: \u001b[1m\u001b[32m1.49356\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9919 | loss: 1.49356 - acc: 0.6782 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9920  | total loss: \u001b[1m\u001b[32m1.70080\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9920 | loss: 1.70080 - acc: 0.6104 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9921  | total loss: \u001b[1m\u001b[32m1.58112\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9921 | loss: 1.58112 - acc: 0.6493 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9922  | total loss: \u001b[1m\u001b[32m1.74331\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9922 | loss: 1.74331 - acc: 0.5915 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9923  | total loss: \u001b[1m\u001b[32m1.61932\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9923 | loss: 1.61932 - acc: 0.6324 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9924  | total loss: \u001b[1m\u001b[32m1.78863\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9924 | loss: 1.78863 - acc: 0.5763 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9925  | total loss: \u001b[1m\u001b[32m1.66044\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9925 | loss: 1.66044 - acc: 0.6187 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9926  | total loss: \u001b[1m\u001b[32m1.54508\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9926 | loss: 1.54508 - acc: 0.6568 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9927  | total loss: \u001b[1m\u001b[32m1.44082\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9927 | loss: 1.44082 - acc: 0.6911 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9928  | total loss: \u001b[1m\u001b[32m1.59622\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9928 | loss: 1.59622 - acc: 0.6363 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9929  | total loss: \u001b[1m\u001b[32m1.48577\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9929 | loss: 1.48577 - acc: 0.6727 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9930  | total loss: \u001b[1m\u001b[32m1.64892\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9930 | loss: 1.64892 - acc: 0.6197 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9931  | total loss: \u001b[1m\u001b[32m1.53258\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9931 | loss: 1.53258 - acc: 0.6577 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9932  | total loss: \u001b[1m\u001b[32m1.74804\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9932 | loss: 1.74804 - acc: 0.5919 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9933  | total loss: \u001b[1m\u001b[32m1.62180\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9933 | loss: 1.62180 - acc: 0.6327 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9934  | total loss: \u001b[1m\u001b[32m1.80942\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9934 | loss: 1.80942 - acc: 0.5695 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9935  | total loss: \u001b[1m\u001b[32m1.67775\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9935 | loss: 1.67775 - acc: 0.6125 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9936  | total loss: \u001b[1m\u001b[32m1.55949\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9936 | loss: 1.55949 - acc: 0.6513 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9937  | total loss: \u001b[1m\u001b[32m1.45282\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9937 | loss: 1.45282 - acc: 0.6861 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9938  | total loss: \u001b[1m\u001b[32m1.64228\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9938 | loss: 1.64228 - acc: 0.6318 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9939  | total loss: \u001b[1m\u001b[32m1.52672\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9939 | loss: 1.52672 - acc: 0.6686 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9940  | total loss: \u001b[1m\u001b[32m1.42232\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9940 | loss: 1.42232 - acc: 0.7018 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9941  | total loss: \u001b[1m\u001b[32m1.32755\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9941 | loss: 1.32755 - acc: 0.7316 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9942  | total loss: \u001b[1m\u001b[32m1.51974\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9942 | loss: 1.51974 - acc: 0.6727 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9943  | total loss: \u001b[1m\u001b[32m1.41366\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9943 | loss: 1.41366 - acc: 0.7054 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9944  | total loss: \u001b[1m\u001b[32m1.31740\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9944 | loss: 1.31740 - acc: 0.7349 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9945  | total loss: \u001b[1m\u001b[32m1.22965\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9945 | loss: 1.22965 - acc: 0.7614 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9946  | total loss: \u001b[1m\u001b[32m1.46675\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9946 | loss: 1.46675 - acc: 0.6853 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9947  | total loss: \u001b[1m\u001b[32m1.36219\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9947 | loss: 1.36219 - acc: 0.7167 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9948  | total loss: \u001b[1m\u001b[32m1.56193\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9948 | loss: 1.56193 - acc: 0.6522 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9949  | total loss: \u001b[1m\u001b[32m1.44691\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9949 | loss: 1.44691 - acc: 0.6870 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9950  | total loss: \u001b[1m\u001b[32m1.68228\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9950 | loss: 1.68228 - acc: 0.6183 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9951  | total loss: \u001b[1m\u001b[32m1.55510\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9951 | loss: 1.55510 - acc: 0.6565 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9952  | total loss: \u001b[1m\u001b[32m1.76762\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9952 | loss: 1.76762 - acc: 0.5908 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9953  | total loss: \u001b[1m\u001b[32m1.63268\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9953 | loss: 1.63268 - acc: 0.6317 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9954  | total loss: \u001b[1m\u001b[32m1.83965\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9954 | loss: 1.83965 - acc: 0.5686 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9955  | total loss: \u001b[1m\u001b[32m1.69896\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9955 | loss: 1.69896 - acc: 0.6117 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9956  | total loss: \u001b[1m\u001b[32m1.88743\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9956 | loss: 1.88743 - acc: 0.5505 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9957  | total loss: \u001b[1m\u001b[32m1.74391\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9957 | loss: 1.74391 - acc: 0.5955 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9958  | total loss: \u001b[1m\u001b[32m1.91568\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9958 | loss: 1.91568 - acc: 0.5431 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9959  | total loss: \u001b[1m\u001b[32m1.77160\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9959 | loss: 1.77160 - acc: 0.5888 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9960  | total loss: \u001b[1m\u001b[32m1.95914\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9960 | loss: 1.95914 - acc: 0.5299 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9961  | total loss: \u001b[1m\u001b[32m1.81321\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9961 | loss: 1.81321 - acc: 0.5769 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9962  | total loss: \u001b[1m\u001b[32m1.94834\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9962 | loss: 1.94834 - acc: 0.5335 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9963  | total loss: \u001b[1m\u001b[32m1.80591\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9963 | loss: 1.80591 - acc: 0.5801 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9964  | total loss: \u001b[1m\u001b[32m1.93333\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9964 | loss: 1.93333 - acc: 0.5364 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9965  | total loss: \u001b[1m\u001b[32m1.79452\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9965 | loss: 1.79452 - acc: 0.5828 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9966  | total loss: \u001b[1m\u001b[32m1.93859\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9966 | loss: 1.93859 - acc: 0.5388 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9967  | total loss: \u001b[1m\u001b[32m1.80107\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9967 | loss: 1.80107 - acc: 0.5849 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9968  | total loss: \u001b[1m\u001b[32m1.90300\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9968 | loss: 1.90300 - acc: 0.5478 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9969  | total loss: \u001b[1m\u001b[32m1.77052\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9969 | loss: 1.77052 - acc: 0.5931 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9970  | total loss: \u001b[1m\u001b[32m1.88646\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9970 | loss: 1.88646 - acc: 0.5480 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9971  | total loss: \u001b[1m\u001b[32m1.75684\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9971 | loss: 1.75684 - acc: 0.5932 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9972  | total loss: \u001b[1m\u001b[32m1.64043\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9972 | loss: 1.64043 - acc: 0.6339 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9973  | total loss: \u001b[1m\u001b[32m1.53535\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9973 | loss: 1.53535 - acc: 0.6705 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9974  | total loss: \u001b[1m\u001b[32m1.71411\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9974 | loss: 1.71411 - acc: 0.6106 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9975  | total loss: \u001b[1m\u001b[32m1.60072\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9975 | loss: 1.60072 - acc: 0.6496 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9976  | total loss: \u001b[1m\u001b[32m1.79241\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9976 | loss: 1.79241 - acc: 0.5846 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9977  | total loss: \u001b[1m\u001b[32m1.67069\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9977 | loss: 1.67069 - acc: 0.6261 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9978  | total loss: \u001b[1m\u001b[32m1.86259\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9978 | loss: 1.86259 - acc: 0.5635 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9979  | total loss: \u001b[1m\u001b[32m1.73386\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9979 | loss: 1.73386 - acc: 0.6072 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9980  | total loss: \u001b[1m\u001b[32m1.88497\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9980 | loss: 1.88497 - acc: 0.5536 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9981  | total loss: \u001b[1m\u001b[32m1.75440\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9981 | loss: 1.75440 - acc: 0.5982 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9982  | total loss: \u001b[1m\u001b[32m1.63689\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9982 | loss: 1.63689 - acc: 0.6384 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9983  | total loss: \u001b[1m\u001b[32m1.53060\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9983 | loss: 1.53060 - acc: 0.6746 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9984  | total loss: \u001b[1m\u001b[32m1.43397\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9984 | loss: 1.43397 - acc: 0.7071 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9985  | total loss: \u001b[1m\u001b[32m1.34564\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9985 | loss: 1.34564 - acc: 0.7364 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9986  | total loss: \u001b[1m\u001b[32m1.48585\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9986 | loss: 1.48585 - acc: 0.6842 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9987  | total loss: \u001b[1m\u001b[32m1.38961\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9987 | loss: 1.38961 - acc: 0.7158 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9988  | total loss: \u001b[1m\u001b[32m1.30163\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9988 | loss: 1.30163 - acc: 0.7442 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9989  | total loss: \u001b[1m\u001b[32m1.22080\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9989 | loss: 1.22080 - acc: 0.7698 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9990  | total loss: \u001b[1m\u001b[32m1.14617\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9990 | loss: 1.14617 - acc: 0.7928 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9991  | total loss: \u001b[1m\u001b[32m1.07693\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9991 | loss: 1.07693 - acc: 0.8135 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9992  | total loss: \u001b[1m\u001b[32m1.29759\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9992 | loss: 1.29759 - acc: 0.7465 -- iter: 14/14\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9993  | total loss: \u001b[1m\u001b[32m1.20961\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9993 | loss: 1.20961 - acc: 0.7718 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9994  | total loss: \u001b[1m\u001b[32m1.44661\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9994 | loss: 1.44661 - acc: 0.7018 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9995  | total loss: \u001b[1m\u001b[32m1.34139\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 9995 | loss: 1.34139 - acc: 0.7316 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9996  | total loss: \u001b[1m\u001b[32m1.59331\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9996 | loss: 1.59331 - acc: 0.6584 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9997  | total loss: \u001b[1m\u001b[32m1.47229\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9997 | loss: 1.47229 - acc: 0.6926 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9998  | total loss: \u001b[1m\u001b[32m1.70292\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 9998 | loss: 1.70292 - acc: 0.6233 -- iter: 14/14\n",
      "--\n",
      "Training Step: 9999  | total loss: \u001b[1m\u001b[32m1.57085\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 9999 | loss: 1.57085 - acc: 0.6610 -- iter: 14/14\n",
      "--\n",
      "Training Step: 10000  | total loss: \u001b[1m\u001b[32m1.45198\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 10000 | loss: 1.45198 - acc: 0.6949 -- iter: 14/14\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\Steven\\Documents\\HTN v. 4\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# reset underlying graph data\n",
    "tf.reset_default_graph()\n",
    "# Build neural network\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 200)\n",
    "net = tflearn.fully_connected(net, 100)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model.fit(train_x, train_y, n_epoch=10000, batch_size=16, show_metric=True)\n",
    "model.save('model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all of our data structures\n",
    "import pickle\n",
    "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y, 'classes_details':classes_details}, open( \"training_data\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
