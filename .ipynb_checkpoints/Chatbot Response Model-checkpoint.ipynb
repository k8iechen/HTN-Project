{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "# things we need for NLP\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# things we need for Tensorflow\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore all of our data structures\n",
    "import pickle\n",
    "data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
    "words = data['words']\n",
    "classes = data['classes']\n",
    "train_x = data['train_x']\n",
    "train_y = data['train_y']\n",
    "classes_details = data['classes_details']\n",
    "\n",
    "# import obo file data\n",
    "import pronto\n",
    "ont = pronto.Ontology('https://raw.githubusercontent.com/DiseaseOntology/HumanDiseaseOntology/master/src/ontology/doid.obo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.response\n",
    "import sys\n",
    "import os, glob\n",
    "import http.client, urllib\n",
    "import json\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
    "\n",
    "# Replace the accessKey string value with your valid access key.\n",
    "accessKey = '82e4612615634c398bfd26e7d6327833'\n",
    "url = 'westcentralus.api.cognitive.microsoft.com'\n",
    "path = '/text/analytics/v2.0/keyPhrases'\n",
    "\n",
    "def extract_keywords(body):\n",
    "    headers = {'Ocp-Apim-Subscription-Key': accessKey}\n",
    "    conn = http.client.HTTPSConnection(url)\n",
    "    body_req = {'documents':[{'language': 'en', 'id': 1, 'text': body}]}\n",
    "    body_json = json.dumps(body_req)\n",
    "    conn.request (\"POST\", path, body_json, headers)\n",
    "    response = conn.getresponse ()\n",
    "    string = response.read().decode('utf-8')\n",
    "    json_obj = json.loads(string)\n",
    "    print(\"json_obj: \", json_obj)\n",
    "    keyphrases_list = ((json_obj['documents'])[0])['keyPhrases']\n",
    "    print(\"keyphrases_list: \", keyphrases_list)\n",
    "    return keyphrases_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\steven\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tflearn\\objectives.py:66: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# reset underlying graph data\n",
    "tf.reset_default_graph()\n",
    "# Build neural network\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a table structure to hold the different punctuation used\n",
    "\n",
    "## method to remove punctuations from sentences.\n",
    "# def remove_punctuation(text):\n",
    "#     return text.translate(tbl)\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    #strip punctuations, numbers, and words containing numbers\n",
    "    no_nums1 = ' '.join(s for s in sentence.split() if not any(c.isdigit() for c in s))\n",
    "    no_nums2 = re.sub(r'\\d+', '', no_nums1)\n",
    "    keywords = extract_keywords(no_nums2)\n",
    "    keyword_str = ' '.join(s for s in keywords)\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(keyword_str)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    print(\"sentence after stemming: \", sentence_words)\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if s in w.strip().split(): \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json_obj:  {'documents': [{'id': '1', 'keyPhrases': ['symptom chills', 'symptom seizures', 'symptom disorientation', 'symptom vomiting', 'symptom coma', 'symptom high fever']}], 'errors': []}\n",
      "keyphrases_list:  ['symptom chills', 'symptom seizures', 'symptom disorientation', 'symptom vomiting', 'symptom coma', 'symptom high fever']\n",
      "sentence after stemming:  ['symptom', 'chil', 'symptom', 'seiz', 'symptom', 'dis', 'symptom', 'vomit', 'symptom', 'com', 'symptom', 'high', 'fev']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "['Astrakhan spotted fever', 'Herpes simplex virus encephalitis', 'Herpes simplex virus keratitis', 'Jamestown Canyon encephalitis', 'Mumps virus meningitis', 'Mumps virus orchitis', 'acute canaliculitis', 'aleutian mink disease', 'aspiration pneumonitis', 'astrovirus gastroenteritis', 'borna disease', 'coxsackievirus encephalitis', 'early congenital syphilis', 'equine encephalitis', 'focal epithelial hyperplasia', 'hand, foot and mouth disease', 'intracranial hypertension', 'jejunoileitis', 'parenchymatous neurosyphilis', 'penicilliosis', 'postherpetic polyneuropathy', 'tick-borne encephalitis', 'tinea barbae', 'tinea cruris', 'visna']\n"
     ]
    }
   ],
   "source": [
    "p = bow(\"has symptom sudden onset of headache, has symptom high fever, has symptom chills, has symptom vomiting, has symptom disorientation, has symptom seizures, and has symptom coma\", words)\n",
    "print (p)\n",
    "print (classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Steven\\Documents\\HTN PROJECT 2\\model.tflearn\n"
     ]
    }
   ],
   "source": [
    "# load our saved model\n",
    "model.load('./model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_THRESHOLD = 0.01\n",
    "def classify(sentence):\n",
    "    # generate probabilities from the model\n",
    "    results = model.predict([bow(sentence, words)])[0]\n",
    "    #print(results)\n",
    "    # filter out predictions below a threshold\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], r[1]))\n",
    "    # return tuple of intent and probability\n",
    "    return return_list\n",
    "\n",
    "def more_response(key):\n",
    "    if key in [\"yes\", \"of course\", \"sure\", \"certainly\", \"absolutely\", \"yeah\", \"yep\", \"yup\", \"ya\", \"yea\", \"aye\"]:\n",
    "        sentence_new = input(\"Okay, please tell me any possible symptoms. The more details, the better!\")\n",
    "        return response(sentence_new,0)\n",
    "    elif key in [\"negative\", \"nope\", \"no\", \"nah\", \"no way\", \"nay\"]:\n",
    "        return print(\"Okay, thanks for visiting iDoctor. Enjoy the rest of your day!\")\n",
    "    else:\n",
    "        more = input(\"Sorry, I couldn't understand that. Would you like more diagnosis?\")\n",
    "        return more_response(more.strip().lower())\n",
    "\n",
    "def response(sentence, counter):\n",
    "    if counter <= 2:\n",
    "        results = classify(sentence)\n",
    "        print(\"YO RESULTS: \")\n",
    "        print(results)\n",
    "    # if we have a classification then find the matching intent tag\n",
    "        if results and len(sentence) >= 10:\n",
    "            # loop as long as there are matches to process\n",
    "            print(\"Here are some likely conditions you may have:\\n\")\n",
    "            for i in range(0,min(3,len(results))):\n",
    "                print((results[i])[0] + \": \" + classes_details[(results[i])[0]])\n",
    "            print(\"\\nPlease read more into these conditions for possible solutions.\\n\")\n",
    "            more = input(\"Is there anything else you would like me to diagnose?\")\n",
    "            return more_response(more.strip().lower())\n",
    "        else:\n",
    "            counter += 1\n",
    "            sentence_add = input(\"Unfortunately, I do not have enough information. Please tell me more!\")\n",
    "            sentence = sentence + \" \" + sentence_add\n",
    "            print(\"SENTENCE UP UNTIL THIS POINT: \" + sentence)\n",
    "            return response(sentence, counter)\n",
    "    else:\n",
    "        total_responses = \"\"\n",
    "        return print(\"Unfortunately, I cannot help diagnose your medical condition. Please consult a medical professional such as your family doctor. Enjoy the rest of your day!\")\n",
    "\n",
    "def main():\n",
    "    print(\"Hi there, my name is iDoctor. Today I will be helping to diagnose any possible medical conditions you may have. Let's get started!\")\n",
    "    name = input(\"Firstly, what is your name?\")\n",
    "    sentence = input(\"Hi \" + name + \"! Please tell me how you are feeling, and any possible symptoms you may be experiencing. The more details, the better!\")\n",
    "    response(sentence, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json_obj:  {'documents': [{'id': '1', 'keyPhrases': ['trouble', 'constant tilt', 'experienced paresis', 'paraplegia', 'muscle tremors', 'head']}], 'errors': []}\n",
      "keyphrases_list:  ['trouble', 'constant tilt', 'experienced paresis', 'paraplegia', 'muscle tremors', 'head']\n",
      "sentence after stemming:  ['troubl', 'const', 'tilt', 'expery', 'pares', 'parapleg', 'musc', 'trem', 'head']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Herpes simplex virus keratitis', 0.46747723),\n",
       " ('equine encephalitis', 0.25365168),\n",
       " ('visna', 0.18971978),\n",
       " ('penicilliosis', 0.025831074),\n",
       " ('Mumps virus meningitis', 0.012702483)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(\"my head seems to be on a constant tilt, and i have had trouble coordinating due to muscle tremors. i have also experienced paresis and paraplegia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "response() missing 1 required positional argument: 'counter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-df384c5992ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"i have a fever, arthralgia, and a rash on my arm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: response() missing 1 required positional argument: 'counter'"
     ]
    }
   ],
   "source": [
    "response(\"i have a fever, arthralgia, and a rash on my arm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
